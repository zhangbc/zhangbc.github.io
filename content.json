{"meta":{"title":"天堂的鸽子","subtitle":"天道酬勤","description":"每天积累一点，学习一点，只有不坚持的，没有做不到的！","author":"Bocheng Zhang","url":"https://zhangbc.github.io","root":"/"},"pages":[{"title":"","date":"2019-04-09T13:11:32.521Z","updated":"2019-04-09T13:11:32.521Z","comments":true,"path":"404.html","permalink":"https://zhangbc.github.io/404.html","excerpt":"","text":""},{"title":"","date":"2019-04-09T13:11:32.531Z","updated":"2019-04-09T13:11:32.531Z","comments":true,"path":"aboutme.html","permalink":"https://zhangbc.github.io/aboutme.html","excerpt":"","text":"关于作者 学习蜗牛，慢慢向前爬，终有一天会到达目的地； 学习IT，学会学习，独立思考，终有一天，会找到IT的乐趣！"},{"title":"","date":"2019-08-08T16:30:13.948Z","updated":"2019-08-08T16:30:13.948Z","comments":true,"path":"README.html","permalink":"https://zhangbc.github.io/README.html","excerpt":"","text":"博客目录一览表数据库系列 理论篇 【数据库理论】数据模型【数据库理论】关系数据库【数据库理论】关系模式的规范化与查询优化【数据库理论】数据库的设计与实施【数据库理论】数据库的安全和保护【数据库理论】数据库与SQL Server2005简介【数据库理论】管理数据库【数据库理论】管理表【数据库理论】操作查询【数据库理论】T-SQL语言【数据库理论】存储过程【数据库理论】SQL Server2005高级功能 实战 【数据库实战】SQL Server数据库常用脚本 Java编程 入门基础篇 【Java基础】Java入门知识【Java基础】Java基础知识【Java基础】Java面向对象【Java基础】Java进阶编程【Java基础】Java网络编程【Java基础】Java扩展知识【Java基础】Java基础100实例 Python编程 编码规范系列 【Python编码规范】编程入门(引论)【Python编码规范】编程惯用法【Python编码规范】基础语法【Python编码规范】库【Python编码规范】设计模式 爬虫系列 【Python爬虫实例】Python解决521反爬方案 数据结构与算法 算法题 【经典算法】字符串旋转和包含算法 数据结构 生活日常篇 心得体会 【心路历程】贫困，不能阻碍你的梦【心路历程】面对现实，做好自己（一）【心路历程】面对现实，做好自己（二）【心路历程】面对现实，做好自己（三）【心路历程】面对现实，做好自己（四）【心路历程】面对现实，做好自己（五）【心路历程】面对现实，做好自己（六）【心路历程】面对现实，做好自己（七） 工作学习总结"},{"title":"Schedule","date":"2019-04-12T18:31:33.000Z","updated":"2019-08-08T16:30:36.362Z","comments":true,"path":"schedule/index.html","permalink":"https://zhangbc.github.io/schedule/index.html","excerpt":"","text":"博客目录一览表数据库系列 理论篇 【数据库理论】数据模型【数据库理论】关系数据库【数据库理论】关系模式的规范化与查询优化【数据库理论】数据库的设计与实施【数据库理论】数据库的安全和保护【数据库理论】数据库与SQL Server2005简介【数据库理论】管理数据库【数据库理论】管理表【数据库理论】操作查询【数据库理论】T-SQL语言【数据库理论】存储过程【数据库理论】SQL Server2005高级功能 实战 【数据库实战】SQL Server数据库常用脚本 Java编程 入门基础篇 【Java基础】Java入门知识【Java基础】Java基础知识【Java基础】Java面向对象【Java基础】Java进阶编程【Java基础】Java网络编程【Java基础】Java扩展知识【Java基础】Java基础100实例 Python编程 编码规范系列 【Python编码规范】编程入门(引论)【Python编码规范】编程惯用法【Python编码规范】基础语法【Python编码规范】库【Python编码规范】设计模式 爬虫系列 【Python爬虫实例】Python解决521反爬方案 数据结构与算法 算法题 【经典算法】字符串旋转和包含算法 数据结构 生活日常篇 心得体会 【心路历程】贫困，不能阻碍你的梦【心路历程】面对现实，做好自己（一）【心路历程】面对现实，做好自己（二）【心路历程】面对现实，做好自己（三）【心路历程】面对现实，做好自己（四）【心路历程】面对现实，做好自己（五）【心路历程】面对现实，做好自己（六）【心路历程】面对现实，做好自己（七） 工作学习总结"},{"title":"分类","date":"2019-04-07T07:00:02.000Z","updated":"2019-04-10T12:08:59.536Z","comments":true,"path":"categories/index.html","permalink":"https://zhangbc.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-04-07T06:57:35.000Z","updated":"2019-04-14T11:04:51.007Z","comments":true,"path":"tags/index.html","permalink":"https://zhangbc.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"【机器学习基础】贝叶斯神经网络","slug":"【机器学习基础】贝叶斯神经网络","date":"2019-10-18T10:21:17.000Z","updated":"2019-10-18T15:53:22.804Z","comments":true,"path":"2019/10/18/prml_05_04/","link":"","permalink":"https://zhangbc.github.io/2019/10/18/prml_05_04/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，混合密度网络作为逆问题，考虑机械臂的运动学问题。正向问题（forward problem）是在给定连接角的情况下求解机械臂末端的位置，这个问题有唯⼀解。然⽽，在实际应⽤中，我们想把机械臂末端移动到⼀个具体的位置，为了完成移动，必须设定合适的连接角。正向问题通常对应于物理系统的因果关系，通常有唯⼀解。 图5.29～5.30，图5.29给展⽰了⼀个具有两个连接的机械臂，其中，末端的笛卡尔坐标 $(x_1, x_2)$ 由两个连接角 $\\theta_1$ 和 $\\theta_2$ 以及机械臂的（固定）长度 $L_1$ 和 $L_2$ 唯⼀确定。这被称为机械臂的正向运动学 （forward kinematics）。在实际应⽤中，必须寻找给出所需的末端位置的连接角，如图5.30所⽰，这个逆向运动学（inverse kinematics）有两个对应的解，即“肘部向上”和“肘部向下”。 考虑⼀个具有多峰性质的问题，数据的⽣成⽅式为：对服从区间 $(0, 1)$ 的均匀分布的变量 $x$ 进⾏取样，得到⼀组值 $\\{x_n\\}$ ，对应的⽬标值 $t_n$ 通过下⾯的⽅式得到：计算函数 $x_n + 0.3 \\sin(2\\pi{x_n})$ ，然后添加⼀个服从 $(−0.1, 0.1)$ 上的均匀分布的噪声。这样，逆问题就可以这样得到：使⽤相同的数据点，但是交换 $x$ 和 $t$ 的⾓⾊。 图5.31～5.32，图5.31是⼀个简单的“正向问题”的数据集，其中红⾊曲线给出了通过最⼩化平⽅和误差函数调节⼀个两层神经⽹络的结果。对应的逆问题，如图5.32所⽰，通过交换 $x$ 和 $t$ 的顺序的⽅式得到。这⾥，通过最⼩化平⽅和误差函数的⽅式训练的神经⽹络给出了对数据的⾮常差的拟合，因为数据集是多峰的。 寻找⼀个对条件概率密度建模的⼀般的框架：为 $p(\\boldsymbol{t}|\\boldsymbol{x})$ 使⽤⼀个混合模型，模型的混合系数和每个分量的概率分布都是输⼊向量 $\\boldsymbol{x}$ 的⼀个⽐较灵活的函数，这就构成了混合密度⽹络（mixture density network）。对于任意给定的 $\\boldsymbol{x}$ 值，混合模型提供了⼀个通⽤的形式，⽤来对任意条件概率密度函数 $p(\\boldsymbol{t}|\\boldsymbol{x})$ 进⾏建模。 图5.33，混合密度⽹络（mixture density network）可以表⽰⼀般的条件概率密度 $p(\\boldsymbol{t}|\\boldsymbol{x})$ ， ⽅法为：考虑 $\\boldsymbol{t}$ 的⼀个参数化的混合模型，参数由以 $\\boldsymbol{x}$ 为输⼊的神经⽹络的输出确定。 显式地令模型的分量为⾼斯分布，即 p(\\boldsymbol{t}|\\boldsymbol{x})=\\sum_{k=1}^{K}\\pi_{k}(\\boldsymbol{x})\\mathcal{N}(\\boldsymbol{t}|\\boldsymbol{\\mu}(\\boldsymbol{x}),\\sigma_{k}^{2}(\\boldsymbol{x})\\boldsymbol{I})\\tag{5.99}混合系数必须满⾜下⾯的限制 \\sum_{k=1}^{K}\\pi_{k}(\\boldsymbol{x})=1, 0 \\le \\pi_{k}(\\boldsymbol{x}) \\le 1可以通过使⽤⼀组softmax输出来实现 \\pi_{k}(\\boldsymbol{x})=\\frac{\\exp(a_{k}^{\\pi})}{\\sum_{l=1}^{K}\\exp(a_{l}^{\\pi})}\\tag{5.100}⽅差必须满⾜ $\\sigma_{k}^{2}(\\boldsymbol{x})\\ge0$ ，因此可以使⽤对应的⽹络激活的指数形式表⽰，即 \\sigma_{k}(\\boldsymbol{x})=\\exp(a_{k}^{\\sigma})\\tag{5.101}由于均值 $\\mu_{k}(\\boldsymbol{x})$ 有实数分量，因此可以直接⽤⽹络的输出激活表⽰ \\mu_{kj}(\\boldsymbol{x})=a_{kj}^{\\mu}\\tag{5.102}混合密度⽹络的可调节参数由权向量 $\\boldsymbol{w}$ 和偏置组成。这些参数可以通过最⼤似然法确定，或者等价地，使⽤最⼩化误差函数（负对数似然函数）的⽅法确定。对于独⽴的数据，误差函数的形式为 E(\\boldsymbol{w})=-\\sum_{n=1}^{N}\\ln\\left\\{\\sum_{k=1}^{K}\\pi_{k}(\\boldsymbol{x}_n,\\boldsymbol{w})\\mathcal{N}(\\boldsymbol{t}_n|\\boldsymbol{\\mu}_k(\\boldsymbol{x}_n,\\boldsymbol{w}),\\sigma_{k}^{2}(\\boldsymbol{x}_n,\\boldsymbol{w})\\boldsymbol{I})\\right\\}\\tag{5.103}把混合系数 $\\pi_k(\\boldsymbol{x})$ 看成与 $\\boldsymbol{x}$ 相关的先验概率分布，从⽽就引⼊了对应的后验概率，形式为 \\gamma_{nk}=\\gamma_{k}(\\boldsymbol{t}_n|\\boldsymbol{x}_n)=\\frac{\\pi_{k}\\mathcal{N}_{nk}}{\\sum_{l=1}^{K}\\pi_{l}\\mathcal{N}_{nl}}\\tag{5.104}其中 $\\mathcal{N}_{nk}$ 表⽰ $\\mathcal{N}(\\boldsymbol{t}_n | \\boldsymbol{\\mu}_k(\\boldsymbol{x}_n),\\sigma_{k}^{2}(\\boldsymbol{x}_n))$ 。 关于控制混合系数的⽹络输出激活的导数为 \\frac{\\partial{E_n}}{\\partial{a_{k}^{\\pi}}}=\\pi_{k}-\\gamma_{nk}\\tag{5.105}关于控制分量均值的⽹络输出激活的导数为 \\frac{\\partial{E_n}}{\\partial{a_{kl}^{\\mu}}}=\\gamma_{k}\\left\\{\\frac{\\mu_{kl}-t_{nl}}{\\sigma_{k}^{2}}\\right\\}\\tag{5.106}关于控制分量⽅差的⽹络激活函数为 \\frac{\\partial{E_n}}{\\partial{a_{k}^{\\sigma}}}=\\gamma_{nk}\\left\\{L-\\frac{\\|\\boldsymbol{t}_n-\\boldsymbol{\\mu}_{k}\\|^{2}}{\\sigma_{k}^{2}}\\right\\}\\tag{5.107}如图5.34～5.37，(a)对于给出的数据训练的混合密度⽹络的三个核函数，混合系数 $\\pi_k(\\boldsymbol{x})$ 与 $\\boldsymbol{x}$ 的函数关系图像。模型有三个⾼斯分量，使⽤了⼀个多层感知器，在隐含层有五个“ $tanh$ ”单元，同时有9个输出单元 （对应于⾼斯分量的3个均值、3个⽅差以及3个混合系数）。在较⼩的 $x$ 值和较⼤的 $x$ 值处，⽬标数据的条件概率密度是单峰的，对于它的先验概率分布，只有⼀个核具有最⼤的值。⽽在中间的 $x$ 值处，条件概率分布具有3个峰，3个混合系数具有可⽐的值。(b)使⽤与混合系数相同的颜⾊表⽰⽅法来表⽰均值 $\\mu_k (x)$ 。 (c)对于同样的混合密度⽹络，⽬标数据的条件概率密度的图像。 (d)条件概率密度的近似条件峰值的图像，⽤红⾊点表⽰。 ⼀旦混合密度⽹络训练结束，可以预测对于任意给定的输⼊向量的⽬标数据的条件密度函数。只要我们关注的是预测输出向量的值的问题，那么这个条件概率密度就能完整地描述⽤于⽣成数据的概率分布。根据这个概率密度函数，可以计算不同应⽤中我们感兴趣的更加具体的量。⼀个最简单的量就是⽬标数据的条件均值，即 \\mathbb{E}[\\boldsymbol{t}|\\boldsymbol{x}]=\\int\\boldsymbol{t}p(\\boldsymbol{t}|\\boldsymbol{x})\\mathrm{d}\\boldsymbol{t}=\\sum_{k=1}^{K}\\pi_{k}(\\boldsymbol{x})\\boldsymbol{\\mu}_{k}(\\boldsymbol{x})\\tag{5.108}密度函数的⽅差，结果为 \\begin{aligned}\\boldsymbol{s}^{2}(\\boldsymbol{x})&=\\mathbb{E}[\\|\\boldsymbol{t}-\\mathbb{E}[\\boldsymbol{t}|\\boldsymbol{x}]\\|^{2}|\\boldsymbol{x}]\\\\&=\\sum_{k=1}^{K}\\pi_{k}(\\boldsymbol{x})\\left\\{\\boldsymbol{\\sigma}_{k}^{2}+\\left\\|\\boldsymbol{\\mu}_k(\\boldsymbol{x})-\\sum_{l=1}^{K}\\pi_{l}(\\boldsymbol{x})\\boldsymbol{\\mu}_{l}(\\boldsymbol{x})\\right\\|^{2}\\right\\}\\end{aligned}\\tag{5.109}二，贝叶斯神经网络1，后验参数分布考虑从输⼊向量 $\\boldsymbol{x}$ 预测单⼀连续⽬标变量 $t$ 的问题。假设条件概率分布 $p(t|\\boldsymbol{x})$ 是⼀个⾼斯分布，均值与 $\\boldsymbol{x}$ 有关，由神经⽹络模型的输出 $y(\\boldsymbol{x}, \\boldsymbol{x})$ 确定，精度（⽅差的倒数）$\\beta$ 为 p(t|\\boldsymbol{x},\\boldsymbol{w},\\beta)=\\mathcal{N}(t|y(\\boldsymbol{x},\\boldsymbol{w}),\\beta^{-1})\\tag{5.110}将权值 $\\boldsymbol{w}$ 的先验概率分布选为⾼斯分布，形式为 p(\\boldsymbol{w}|\\alpha)=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{0},\\alpha^{-1}\\boldsymbol{I})\\tag{5.111}对于 $N$ 次独⽴同分布的观测 $\\boldsymbol{x}_1,\\dots,\\boldsymbol{x}_N$ ，对应的⽬标值集合 $\\mathcal{D}=\\{t_1,\\dots, t_N\\}$ ，似然函数为 p(\\mathcal{D}|\\boldsymbol{w},\\beta)=\\prod_{n=1}^{N}\\mathcal{N}(t_n|y(\\boldsymbol{x}_n,\\boldsymbol{w}),\\beta^{-1})\\tag{5.112}最终的后验概率为 p(\\boldsymbol{w}|\\mathcal{D},\\alpha,\\beta)\\propto p(\\boldsymbol{w}|\\alpha)p(\\mathcal{D}|\\boldsymbol{w},\\beta)\\tag{5.113}由于 $y(\\boldsymbol{x},\\boldsymbol{w})$ 与 $\\boldsymbol{w}$ 的关系是⾮线性的，因此后验概率不是⾼斯分布。 使⽤拉普拉斯近似，可以找到对于后验概率分布的⼀个⾼斯近似。⾸先找到后验概率分布的⼀个（局部）最⼤值，这必须使⽤迭代的数值最优化算法才能找到，⽐较⽅便的做法是最⼤化后验概率分布的对数，可以写成 \\ln p(\\boldsymbol{w}|\\mathcal{D})=-\\frac{\\alpha}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}-\\frac{\\beta}{2}\\sum_{n=1}^{N}\\{y(\\boldsymbol{x}_n,\\boldsymbol{w})-t_n\\}^{2}+常数\\tag{5.114}负对数后验概率的⼆阶导数为 \\boldsymbol{A}=-\\nabla\\nabla\\ln p(\\boldsymbol{w}|\\mathcal{D},\\alpha,\\beta)=\\alpha\\boldsymbol{I}+\\beta\\boldsymbol{H}\\tag{5.115}其中，$\\boldsymbol{H}$ 是⼀个Hessian矩阵，由平⽅和误差函数关于 $\\boldsymbol{w}$ 的分量组成，后验概率对应的⾼斯近似形式为 q(\\boldsymbol{w}|\\mathcal{D})=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{w}_{MAP},\\boldsymbol{A}^{-1})\\tag{5.116}预测分布可以通过将后验概率分布求积分的⽅式获得 p(t|\\boldsymbol{x},\\mathcal{D})=\\int p(t|\\boldsymbol{x},\\boldsymbol{w})q(\\boldsymbol{w}|\\mathcal{D})\\mathrm{d}\\boldsymbol{w}\\tag{5.117}现在假设与 $y(\\boldsymbol{x}, \\boldsymbol{w})$ 发⽣变化造成的 $\\boldsymbol{w}$ 的变化幅度相⽐，后验概率分布的⽅差较⼩。这使得可以在 $\\boldsymbol{w}_{MAP}$ 附近对⽹络函数进⾏泰勒展开，只保留展开式的现⾏项，可得 y(\\boldsymbol{x},\\boldsymbol{w})\\simeq y(\\boldsymbol{x},\\boldsymbol{w}_{WAP})+\\boldsymbol{g}^{T}(\\boldsymbol{w}-\\boldsymbol{w}_{MAP})\\tag{5.118}其中， \\boldsymbol{g}=\\nabla_{\\boldsymbol{w}}y(\\boldsymbol{x},\\boldsymbol{w})|_{\\boldsymbol{w}=\\boldsymbol{w}_{WAP}}使⽤这个近似，现在得到了⼀个线性⾼斯模型，$p(\\boldsymbol{w})$ 为⾼斯分布，并且，$p(t|\\boldsymbol{w})$ 也是⾼斯分布，均值是 $\\boldsymbol{w}$ 的线性函数，分布的形式为 p(t|\\boldsymbol{x},\\boldsymbol{w},\\beta)\\simeq \\mathcal{N}(t|y(\\boldsymbol{x},\\boldsymbol{w}_{MAP})+\\boldsymbol{g}^{T}(\\boldsymbol{w}-\\boldsymbol{w}_{MAP}),\\beta^{-1})\\tag{5.119}边缘分布 $p(t)$ 的⼀般结果，得到 p(t|\\boldsymbol{x},\\mathcal{D},\\alpha,\\beta)=\\mathcal{N}(t|y(\\boldsymbol{x},\\boldsymbol{w}_{MAP}),\\sigma^{2}(\\boldsymbol{x}))\\tag{5.120}其中，与输⼊相关的⽅差为 \\sigma^{2}(\\boldsymbol{x})=\\beta^{-1}+\\boldsymbol{g}^{T}\\boldsymbol{A}^{-1}\\boldsymbol{g}由此可见，预测分布 $p(t|\\boldsymbol{x},\\mathcal{D})$ 是⼀个⾼斯分布， 它的均值由⽹络函数 $y(\\boldsymbol{x},w_{MAP})$ 给出， 参数设置为 $MAP$ 值。⽅差由两项组成，第⼀项来⾃⽬标变量的固有噪声，第⼆项是⼀个与 $\\boldsymbol{x}$ 相关的项，表⽰由于模型参数 $\\boldsymbol{w}$ 的不确定性造成的内插的不确定性。 2，超参数最优化超参数的边缘似然函数，或者模型证据，可以通过对⽹络权值进⾏积分的⽅法得到，即 p(\\mathcal{D}|\\alpha,\\beta)=\\int p(\\mathcal{D}|\\boldsymbol{w},\\beta)p(\\boldsymbol{w}|\\alpha)\\mathrm{d}\\boldsymbol{w}\\tag{5.121}取对数，可得 \\ln p(\\mathcal{D}|\\alpha,\\beta)\\simeq -E(\\boldsymbol{w}_{MAP})-\\frac{1}{2}\\ln|\\boldsymbol{A}|+\\frac{W}{2}\\ln\\alpha+\\frac{N}{2}\\ln\\beta-\\frac{N}{2}\\ln(2\\pi)\\tag{5.122}其中 $W$ 是 $\\boldsymbol{w}$ 中参数的总数。正则化误差函数的定义为 E(\\boldsymbol{w}_{MAP})=\\frac{\\beta}{2}\\sum_{n=1}^{N}\\{y(\\boldsymbol{x}_n,\\boldsymbol{w}_{MAP})-t_n\\}^{2}+\\frac{\\alpha}{2}\\boldsymbol{w}_{MAP}^{T}\\boldsymbol{w}_{MAP}\\tag{5.123}在模型证据框架中，我们通过最⼤化 $\\ln p(\\mathcal{D}|\\alpha,\\beta)$ 对 $\\alpha$ 和 $\\beta$ 进⾏点估计。⾸先考虑关于 $\\alpha$ 进⾏最⼤化，定义特征值⽅程 \\beta\\boldsymbol{H}\\boldsymbol{\\mu}_i=\\lambda_{i}\\boldsymbol{\\mu}_i\\tag{5.124}其中 $\\boldsymbol{H}$ 是在 $\\boldsymbol{w}=\\boldsymbol{w}_{MAP}$ 处计算的Hessian矩阵，由平⽅和误差函数的⼆阶导数组成。 有 \\alpha=\\frac{\\gamma}{\\boldsymbol{w}_{MAP}^{T}\\boldsymbol{w}_{MAP}}\\tag{5.125}其中 $\\gamma$ 表⽰参数的有效数量，定义为 \\gamma=\\sum_{i=1}^{W}\\frac{\\lambda_i}{\\alpha+\\lambda_i}关于 $\\beta$ 最⼤化模型证据，可以得到下⾯的重估计公式 \\frac{1}{\\beta}=\\frac{1}{N-\\gamma}\\sum_{n=1}^{N}\\{y(\\boldsymbol{x}_n,\\boldsymbol{w}_{MAP})-t_n\\}^{2}\\tag{5.126}3，⽤于分类的贝叶斯神经⽹络考虑的⽹络有⼀个logistic sigmoid输出，对应于⼀个⼆分类问题。 模型的对数似然函数为 \\ln p(\\mathcal{D}|\\boldsymbol{w})=\\sum_{n=1}^{N}\\{t_n\\ln{y_n}+(1-t_n)\\ln(1-y_n)\\}\\tag{5.127}其中 $t_n \\in\\{0,1\\}$ 是⽬标值，且 $y_n \\equiv y(\\boldsymbol{x}_n,\\boldsymbol{w})$ 。 将拉普拉斯框架⽤在这个模型中的第⼀个阶段是初始化超参数 $\\alpha$，然后通过最⼤化对数后验概率分布的⽅法确定参数向量 $\\boldsymbol{w}$ ，这等价于最⼩化正则化误差函数 E(\\boldsymbol{w})=-\\ln{p(\\mathcal{D}|\\boldsymbol{w})}+\\frac{\\alpha}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}\\tag{5.128}找到权向量的解 $\\boldsymbol{w}_{MAP}$ 之后， 下⼀步是计算由负对数似然函数的⼆阶导数组成的Hessian矩阵 $\\boldsymbol{H}$ 。 为了最优化超参数 $\\alpha$ ，再次最⼤化边缘似然函数，边缘似然函数的形式为 \\ln p(\\mathcal{D}|\\alpha)\\simeq -E(\\boldsymbol{w}_{MAP})-\\frac{1}{2}\\ln|\\boldsymbol{A}|+\\frac{W}{2}\\ln\\alpha\\tag{5.129}其中，正则化的误差函数为 E(\\boldsymbol{w}_{MAP})=-\\sum_{n=1}^{N}\\{t_n\\ln{y_n}+(1-t_n)\\ln(1-y_n)\\}+\\frac{\\alpha}{2}\\boldsymbol{w}_{MAP}^{T}\\boldsymbol{w}_{MAP}\\tag{5.130}其中 $y_n \\equiv y(\\boldsymbol{x}_n,\\boldsymbol{w}_{MAP})$ 。 图5.38，模型证据框架应⽤于⼈⼯⽣成的⼆分类数据集的说明。绿⾊曲线表⽰最优的决策边界，⿊⾊曲线表⽰通过最⼤化似然函数调节⼀个具有8个隐含结点的两层神经⽹络的结果，红⾊曲线表⽰包含⼀个正则化项的结果，其中 $\\alpha$ 使⽤模型证据的步骤进⾏了最优化，初始值为 $\\alpha = 0$ 。注意，模型证据步骤极⼤地缓 解了模型的过拟合现象。 最后，需要找到的预测分布，由于⽹络函数的⾮线性的性质，积分是⽆法直接计算的。最简单的近似⽅法是假设后验概率⾮常窄，因此可以进⾏下⾯的近似 p(t|\\boldsymbol{x},\\mathcal{D})\\simeq p(t|\\boldsymbol{x},\\boldsymbol{w}_{MAP})\\tag{5.131}对输出激活函数进⾏线性近似，形式为 a(\\boldsymbol{x},\\boldsymbol{w})\\simeq a_{MAP}(\\boldsymbol{x})+\\boldsymbol{b}^{T}(\\boldsymbol{w}-\\boldsymbol{w}_{MAP})\\tag{5.132}其中，$a_{MAP}(\\boldsymbol{x})=a(\\boldsymbol{x},\\boldsymbol{w}_{MAP})$ 及向量 $\\boldsymbol{b} \\equiv \\nabla a(\\boldsymbol{x},\\boldsymbol{w}_{MAP})$ 都可以通过反向传播⽅法求出。 由神经⽹络的权值的分布引出的输出单元激活的值的分布为 p(a|\\boldsymbol{x},\\mathcal{D})=\\int\\delta(a-a_{MAP}(\\boldsymbol{x})-\\boldsymbol{b}^{T}(\\boldsymbol{x})(\\boldsymbol{w}-\\boldsymbol{w}_{MAP}))q(\\boldsymbol{w}|\\mathcal{D})\\mathrm{d}\\boldsymbol{w}\\tag{5.133}其中 $q(\\boldsymbol{w}|\\mathcal{D})$ 是对后验概率分布的⾼斯近似，这个分布是⼀个⾼斯分布，均值为 $a_{MAP}(\\boldsymbol{x})\\equiv a(\\boldsymbol{x},\\boldsymbol{x}_{MAP})$ ，⽅差为 \\sigma_{a}^{2}(\\boldsymbol{x})=\\boldsymbol{b}^{T}(\\boldsymbol{x})\\boldsymbol{A}^{-1}\\boldsymbol{b}(\\boldsymbol{x})\\tag{5.134}为了得到预测分布，必须对 $a$ 进⾏积分 p(t=1|\\boldsymbol{x},\\mathcal{D})=\\int \\delta(a)p(a|\\boldsymbol{w},\\mathcal{D})\\mathrm{d}a\\tag{5.135}⾼斯分布与logistic sigmoid函数的卷积是⽆法计算的。于是 p(t=1|\\boldsymbol{x},\\mathcal{D})=\\sigma\\left(\\kappa(\\sigma_{a}^{2})a_{MAP}\\right)\\tag{5.136}图 5.39～5.40，对于⼀个具有8个隐含结点带有 $tanh$ 激活函数和⼀个logistic sigmoid输出结点的贝叶斯⽹络应⽤拉 普拉斯近似的说明。权参数使⽤缩放的共轭梯度⽅法得到，超参数 $\\alpha$ 使⽤模型证据框架确定。图5.39是使⽤基于参数的 $\\boldsymbol{w}_{MAP}$ 的点估计的简单近似得到的结果，其中绿⾊曲线表⽰ $y = 0.5$ 的决策边界，其他的轮廓线对应于 $y = 0.1, 0.3, 0.7$ 和 $0.9$ 的输出概率。图5.40是使⽤公式(5.136)得到的对应的结果。注意，求边缘概率分布的效果是扩散了轮廓线，使得预测的置信度变低，从⽽在每个输⼊点 $\\boldsymbol{x}$ 处，后验概 率分布向着 $0.5$ 的⽅向偏移，⽽ $y = 0.5$ 的边界本⾝不受影响。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】神经网络正则化","slug":"【机器学习基础】神经网络正则化","date":"2019-10-18T10:02:19.000Z","updated":"2019-10-18T15:46:45.543Z","comments":true,"path":"2019/10/18/prml_05_03/","link":"","permalink":"https://zhangbc.github.io/2019/10/18/prml_05_03/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，相容的⾼斯先验神经⽹络的输⼊单元和输出单元的数量通常由数据集的维度确定，⽽隐含单元的数量 $M$ 是⼀个⾃由的参数，可以通过调节来给出最好的预测性能。 控制神经⽹络的模型复杂度来避免过拟合，根据对多项式曲线拟合问题的讨论，⼀种⽅法是选择⼀个相对⼤的 $M$ 值，然后通过给误差函数增加⼀个正则化项，来控制模型的复杂度。最简单的正则化项是⼆次的，给出了正则化的误差函数，形式为 \\tilde{E}(\\boldsymbol{w})=E(\\boldsymbol{w})+\\frac{\\lambda}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}\\tag{5.73}这个正则化项也被称为权值衰减（weight decay）。模型复杂度可以通过选择正则化系数 $\\lambda$ 来确定，正则化项可以表⽰为权值 $\\boldsymbol{w}$ 上的零均值⾼斯先验分布的负对数。 公式(5.73)给出的简单权值衰减的⼀个局限性是：它与⽹络映射的确定缩放性质不相容。考虑⼀个多层感知器⽹络，这个⽹络有两层权值和线性输出单元，给出了从输⼊变量集合 $\\{x_i\\}$ 到输出变量集合 $\\{y_k\\}$ 的映射。第⼀个隐含层的隐含单元的激活的形式为 z_j=h\\left(\\sum_{i}w_{ji}x_{i}+w_{j0}\\right)\\tag{5.74}输出单元的激活为 y_k=\\sum_{j}w_{kj}z_j+w_{k0}\\tag{5.75}假设对输⼊变量进⾏⼀个线性变换，形式为 x_i\\to\\tilde{x}_{i}=ax_i+b\\tag{5.76}然后根据这个映射对⽹络进⾏调整，使得⽹络给出的映射不变。调整的⽅法为，对从输⼊单元到隐含层单元的权值和偏置也进⾏⼀个对应的线性变换，形式为 w_{ji}\\to\\tilde{w}_{ji}=\\frac{1}{a}w_{ji} w_{j0}\\to\\tilde{w}_{j0}=w_{j0}-\\frac{b}{a}\\sum_{i}w_{ji}⽹络的输出变量的线性变换 y_{k}\\to\\tilde{y}_{k}=cy_k+d\\tag{5.77}可以通过对第⼆层的权值和偏置进⾏线性变换的⽅式实现。变换的形式为 w_{kj}\\to\\tilde{w}_{kj}=cw_{kj} w_{k0}\\to\\tilde{w}_{k0}=cw_{k0}+d于是要寻找⼀个正则化项，它在上述线性变换和下具有不变性，这需要正则化项应该对于权值的重新缩放不变，对于偏置的平移不变。这样的正则化项为 \\frac{\\lambda_1}{2}\\sum_{w\\in\\mathcal{W_1}}w^2+\\frac{\\lambda_2}{2}\\sum_{w\\in\\mathcal{W_2}}w^2其中 $\\mathcal{W}_1$ 表⽰第⼀层的权值集合，$\\mathcal{W}_2$ 表⽰第⼆层的权值集合， 偏置未出现在求和式中。这个正则化项在权值的变换下不会发⽣变化，只要正则化参数进⾏下⾯的重新放缩即可：$\\lambda_1 \\to a^{\\frac{1}{2}}\\lambda_1$ 和 $\\lambda_2 \\to a^{-\\frac{1}{2}}\\lambda_2$ ， 正则化项对应于下⾯形式的先验概率分布。 p(\\boldsymbol{w}|\\alpha_1,\\alpha_2)\\propto\\exp\\left(-\\frac{\\alpha_1}{2}\\sum_{w\\in\\mathcal{W_1}}w^2-\\frac{\\alpha_2}{2}\\sum_{w\\in\\mathcal{W_2}}w^2\\right)\\tag{5.78}注意， 这种形式的先验是反常的（improper）（不能够被归⼀化），因为偏置参数没有限制。 图5.15～5.18，控制两层神经⽹络的权值和偏置的先验概率分布的超参数的效果说明。其中，神经⽹络有⼀个输⼊，⼀个线性输出，以及12个隐含结点，隐含结点的激活函数为 $tanh$。先验概率分布通过四个超参数 $\\alpha_{1}^{b}$， $\\alpha_{1}^{w}$， $\\alpha_{2}^{b}$ ， $\\alpha_{2}^{w}$ 控制，它们分别表⽰第⼀层的偏置、第⼀层的权值、第⼆层的偏置、第⼆层的权值。 ⼀般地，可以考虑权值被分为任意数量的组 $\\mathcal{W}_k$ 的情况下的先验，即 p(\\boldsymbol{w})\\propto\\exp\\left(-\\frac{1}{2}\\sum_{k}\\alpha_{k}\\|\\boldsymbol{w}\\|_{k}^2\\right)\\tag{5.79}其中， \\|\\boldsymbol{w}\\|_{k}^2=\\sum_{j\\in\\mathcal{W}_k}w_{j}^{2}二，早停止另⼀种控制⽹络的复杂度的正则化⽅法是早停⽌（early stopping）。⾮线性⽹络模型的训练对应于误差函数的迭代减⼩，其中误差函数是关于训练数据集定义的。对于许多⽤于⽹络训练的最优化算法（例如共轭梯度法），误差函数是⼀个关于迭代次数的不增函数。然⽽，在独⽴数据（通常被称为验证集）上测量的误差，通常⾸先减⼩，接下来由于模型开始过拟合⽽逐渐增⼤。于是，训练过程可以在关于验证集误差最⼩的点停⽌，这样可以得到⼀个有着较好泛化性能的⽹络。 图5.19～5.20，训练集误差和验证集误差在典型的训练阶段的⾏为说明。图像给出了误差与迭代次数的函数，数据集为正弦数据集。得到最好的泛化表现的⽬标表明，训练应该在垂直虚线表⽰的点处停⽌，对应于验证集误差的最⼩值。 图5.21，在⼆次误差函数的情况下，关于早停⽌可以给出与权值衰减类似的结果的原因说明。椭圆给出了常数误差函数的轮廓线，$\\boldsymbol{w}_{ML}$ 表⽰误差函数的最⼩值。如果权向量的起始点为原点，按照局部负梯度的⽅向移动，那么它会沿着曲线给出的路径移动。通过对训练过程早停⽌，我们找到了⼀个权值向量 $\\tilde{\\boldsymbol{w}}$。 定性地说，它类似于使⽤简单的权值衰减正则化项，然后最⼩化正则化误差函数的⽅法得到的权值。 三，不变性寻找让可调节的模型能够表述所需的不变性，⼤致可以分为四类： 1）通过复制训练模式，同时根据要求的不变性进⾏变换，对训练集进⾏扩展。例如，在⼿写数字识别的例⼦中，我们可以将每个样本复制多次，每个复制后的样本中，图像被平移到 了不同的位置。 2）为误差函数加上⼀个正则化项，⽤来惩罚当输⼊进⾏变换时，输出发⽣的改变。 3）通过抽取在要求的变换下不发⽣改变的特征，不变性被整合到预处理过程中。任何后续的使⽤这些特征作为输⼊的回归或者分类系统就会具有这些不变性。 4）把不变性的性质整合到神经⽹络的构建过程中，或者对于相关向量机的⽅法，整合到核函数中。 图5.22，对⼿写数字进⾏⼈⼯形变的说明。 原始图像见左图。 在右图中， 上⾯⼀⾏给出了三个经过了形变的数字，对应的位移场在下⾯⼀⾏给出。这些位移场按照下⾯的⽅法⽣成：在每个像素处， 对唯 ⼀ $\\Delta{x}$ , $\\Delta{y}\\in(0,1)$ 进⾏随机取样，然后分别与宽度为0.01,30,60的⾼斯分布做卷积，进⾏平滑。 图5.23，⼆维输⼊空间的例⼦，展⽰了在⼀个特定的输⼊向量 $\\boldsymbol{x}_n$ 上的连续变换的效果。⼀个参数为连续变量 $\\xi$ 的⼀维变换作⽤于 $\\boldsymbol{x}_n$ 上会使它扫过⼀个⼀维流形 $\\mathcal{M}$ 。局部来看，变换的效果可以⽤切向量 $\\boldsymbol{\\tau}_n$ 来近似。 四，切线传播通过切线传播（tangent propagation）的⽅法，可以使⽤正则化来让模型对于输⼊的变换具有不变性（Simard et al., 1992）。对于⼀个特定的输⼊向量 $\\boldsymbol{x}_n$ ，考虑变换产⽣的效果。 假设变换是连续的（例如平移或者旋转，⽽不是镜像翻转），那么变换的模式会扫过 $D$ 维输⼊空间的⼀个流形 $\\mathcal{M}$ 。假设变换由单⼀参数 $\\xi$ 控制（例如，$\\xi$ 可能是旋转的角度）。那么被 $\\boldsymbol{x}_n$ 扫过的⼦空间 $\\mathcal{M}$ 是⼀维的，并且以 $\\xi$ 为参数。令这个变换作⽤于 $\\boldsymbol{x}_n$ 上产⽣的向量为 $\\boldsymbol{s}(\\boldsymbol{x}_n,\\xi)$ ， 且 $\\boldsymbol{s}(\\boldsymbol{x}_n,0)=\\boldsymbol{x}$ 。 这样曲线 $\\mathcal{M}$ 的切线就由⽅向导数 $\\boldsymbol{\\tau} =\\frac{\\partial\\boldsymbol{x}}{\\partial\\xi}$ 给出，且点 $\\boldsymbol{x}_n$ 处的切线向量为 \\boldsymbol{\\tau}_n=\\frac{\\partial{\\boldsymbol{s}}(\\boldsymbol{x}_n,\\xi)}{\\partial{\\xi}}\\Bigg{|}_{\\xi=0}\\tag{5.80}对于输⼊向量进⾏变换之后，⽹络的输出通常会发⽣变化。输出 $k$ 关于 $\\xi$ 的导数为 \\frac{\\partial{y_k}}{\\partial{\\xi}}\\Bigg{|}_{\\xi=0}=\\sum_{i=1}^{D}\\frac{\\partial{y_k}}{\\partial{x_i}}\\frac{\\partial{x_i}}{\\partial{\\xi}}\\Bigg{|}_{\\xi=0}=\\sum_{i=1}^{D}J_{ki}\\tau_{i}其中 $J_{ki}$ 为Jacobian矩阵 $\\boldsymbol{J}$ 的第 $(k, i)$ 个元素。这个结果可以⽤于修改标准的误差函数，使得在数据点的邻域之内具有不变性。修改的⽅法为：给原始的误差函数 $E$ 增加⼀个正则化函数 $\\Omega$ ，得到下⾯形式的误差函数 \\tilde{E}=E+\\lambda\\Omega\\tag{5.81}其中 $\\lambda$ 是正则化系数，且 \\Omega=\\frac{1}{2}\\sum_{n}\\sum_{k}\\left(\\frac{\\partial{y_{nk}}}{\\partial{\\xi}}\\bigg{|}_{\\xi=0}\\right)^{2}=\\frac{1}{2}\\sum_{n}\\sum_{k}\\left(\\sum_{i=1}^{D}J_{nki}\\tau_{ni}\\right)^{2}当⽹络映射函数在每个模式向量的邻域内具有变换不变性时，正则化函数等于零。$\\lambda$ 的值确定了训练数据和学习不变性之间的平衡。在实际执⾏过程中，切线向量 $\\boldsymbol{\\tau}_n$ 可以使⽤有限差近似，即将原始向量 $\\boldsymbol{x}$ 从使⽤了⼩的 $\\xi$ 进⾏变换后的对应的向量中减去，再除以 $\\xi$ 。 图5.24～5.27，(a)原始的⼿写数字 $\\boldsymbol{x}$ ，(b)对应于⽆穷⼩顺时针旋转的切向量 $\\boldsymbol{\\tau}$ ，其中蓝⾊和黄⾊分别对应于正值和负值，(c)将来⾃这个切向量的微⼩贡献作⽤于原始图像的结果，得到了 $\\boldsymbol{x}+\\epsilon\\boldsymbol{\\tau}$ ，其中 $\\epsilon=15$ 度。(d)真实的图像旋转，⽤作对⽐。 如果变换由 $L$ 个参数控制（例如，对于⼆维图像的平移变换与⾯内旋转变换项结合），那么流形 $\\mathcal{M}$ 的维度为 $L$ ，对应的正则化项由形如公式 $\\Omega$ 的项求和得到，每个变换都对应求和式中的⼀项。如果同时考虑若⼲个变换，并且让⽹络映射对于每个变换分别具有不变性，那么对于变换的组合来说就会具有（局部）不变性（Simard et al., 1992）。⼀个相关的技术，被称为切线距离（tangent distance），可以⽤来构造基于距离的⽅法（例如最近邻分类器）的不变性（Simard et al., 1993）。 五，⽤变换后的数据训练考虑由单⼀参数 $\\xi$ 控制的变换，且这个变换由函数 $\\boldsymbol{s}(\\boldsymbol{x},\\xi)$ 描述， 其中 $\\boldsymbol{s}(\\boldsymbol{x}_n,0)=\\boldsymbol{x}$ ，也会考虑平⽅和误差函数。对于未经过变换的输⼊，误差函数可以写成 （在⽆限数据集的极限情况下） E=\\frac{1}{2}\\int\\int\\{y(\\boldsymbol{x})-t\\}^{2}p(t|\\boldsymbol{x})p(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x}\\mathcal{d}t\\tag{5.82}为了保持记号的简洁，考虑有⼀个输出单元的⽹络。如果现在考虑每个数据点的⽆穷多个副本，每个副本都由⼀个变换施加了扰动，这个变换的参数为 $\\xi$ ，且 $\\xi$ 服从概率分布 $p(\\xi)$ ，那么在这个扩展的误差函数上定义的误差函数可以写成 \\tilde{E}=\\frac{1}{2}\\int\\int\\int\\{y(\\boldsymbol{s}(\\boldsymbol{x},\\xi))-t\\}^{2}p(t|\\boldsymbol{x})p(\\boldsymbol{x})p(\\xi)\\mathrm{d}\\boldsymbol{x}\\mathcal{d}t\\mathcal{d}\\xi\\tag{5.83}现在假设分布 $p(\\xi)$ 的均值为零，⽅差很⼩，即只考虑对原始输⼊向量的⼩的变换。可以对变换函数进⾏关于 $\\xi$ 的展开，可得 \\begin{aligned}\\boldsymbol{s}(\\boldsymbol{x},\\xi)&=\\boldsymbol{s}(\\boldsymbol{x},0)+\\xi\\frac{\\partial}{\\partial{\\xi}}\\boldsymbol{s}(\\boldsymbol{x},\\xi)\\bigg{|}_{\\xi=0}+\\frac{\\xi^{2}}{2}\\frac{\\partial^{2}}{\\partial{\\xi^{2}}}\\bigg{|}_{\\xi=0}+O(\\xi^{3})\\\\&=\\boldsymbol{x}+\\xi\\boldsymbol{\\tau}+\\frac{1}{2}\\xi^{2}\\boldsymbol{\\tau}^{\\prime}+O(\\xi^{3})\\end{aligned}\\tag{5.84}其中 $\\boldsymbol{\\tau}^{\\prime}$ 表⽰ $\\boldsymbol{s}(\\boldsymbol{x},\\xi)$ 关于 $\\xi$ 的⼆阶导数在 $\\xi = 0$ 处的值。这使得可以展开模型函数，可得 y(\\boldsymbol{s}(\\boldsymbol{x},\\xi))=y(\\boldsymbol{x})+\\xi\\boldsymbol{\\tau}^{T}\\nabla{y(\\boldsymbol{x})}+\\frac{\\xi^{2}}{2}[(\\boldsymbol{\\tau}^{\\prime})^{T}\\nabla{y(\\boldsymbol{x})}+\\boldsymbol{\\tau}^{T}\\nabla\\nabla{y(\\boldsymbol{x})}\\boldsymbol{\\tau}]+O(\\xi^{3})代⼊平均误差函数，有 \\begin{aligned} \\tilde{E} &=\\frac{1}{2} \\iint\\{y(x)-t\\}^{2} p(t | \\boldsymbol{x}) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} \\mathrm{d} t \\\\ &+\\mathbb{E}[\\xi] \\iint\\{y(\\boldsymbol{x})-t\\} \\boldsymbol{\\tau}^{T} \\nabla y(\\boldsymbol{x}) p(t | \\boldsymbol{x}) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} \\mathrm{d} t \\\\ &+\\mathbb{E}\\left[\\xi^{2}\\right] \\frac{1}{2} \\iint\\left[\\{y(\\boldsymbol{x})-t\\}\\left\\{\\left(\\boldsymbol{\\tau}^{\\prime}\\right)^{T} \\nabla y(\\boldsymbol{x})+\\boldsymbol{\\tau}^{T} \\nabla \\nabla y(\\boldsymbol{x}) \\boldsymbol{\\tau}\\right\\}\\right.\\\\ &\\left.+\\left(\\boldsymbol{\\tau}^{T} \\nabla y(\\boldsymbol{x})\\right)^{2}\\right] p(t | \\boldsymbol{x}) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} \\mathrm{d} t+O\\left(\\xi^{3}\\right) \\end{aligned}\\tag{5.85}由于变换的分布的均值为零， 因此有 $\\mathbb{E}[\\xi] = 0$ ，并且把 $\\mathbb{E}[\\xi^{2}]$ 记作 $\\lambda$ ，省略 $O(\\xi^{3})$ 项，这样平均误差函数就变成了 \\tilde{E}=E+\\lambda\\Omega\\tag{5.86}其中 $E$ 是原始的平⽅和误差，正则化项 $\\Omega$ 的形式为 \\Omega=\\frac{1}{2}\\int\\bigg{[}\\{y(\\boldsymbol{x})-\\mathbb{E}[t|\\boldsymbol{x}]\\}\\left\\{(\\boldsymbol{\\tau}^{\\prime})^{T}\\nabla{y(\\boldsymbol{x})}+\\boldsymbol{\\tau}^{T}\\nabla\\nabla{y(\\boldsymbol{x})\\boldsymbol{\\tau}}\\right\\}+(\\boldsymbol{\\tau}^{T}\\nabla{y(\\boldsymbol{x})})^{2}p(\\boldsymbol{x})\\bigg{]}\\mathrm{d}\\boldsymbol{x}进⼀步简化这个正则化项，发现正则化的误差函数等于⾮正则化的误差函数加上⼀个 $O(\\xi^{2})$ 的项，因此最⼩化总误差函数的⽹络函数的形式为 y(\\boldsymbol{x})=\\mathbb{E}[t|\\boldsymbol{x}]+O(\\xi^{3})\\tag{5.87}从⽽，正则化项中的第⼀项消失，剩下的项为 \\Omega=\\frac{1}{2}\\int(\\boldsymbol{\\tau}^{T}\\nabla{y(\\boldsymbol{x})})^{2}p(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x}这等价于切线传播的正则化项。如果考虑⼀个特殊情况，即输⼊变量的变换只是简单地添加随机噪声，从⽽ $\\boldsymbol{x}\\to\\boldsymbol{x}+\\boldsymbol{\\xi}$ ，那么正则化项的形式为 \\Omega=\\frac{1}{2}\\int\\|\\nabla{y(\\boldsymbol{x})}\\|^{2}p(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x}\\tag{5.88}这被称为 Tikhonov正则化 （Tikohonov and Arsenin, 1977; Bishop, 1995b）。这个正则化项关于⽹络权值的导数可以使⽤扩展的反向传播算法求出（Bishop, 1993）。对于⼩的噪 声，Tikhonov正则化与对输⼊添加随机噪声有关系，在恰当的情况下，这种做法会提升模型的泛化能⼒。 六，卷积神经⽹络另⼀种构造对输⼊变量的变换具有不变性的模型的⽅法是将不变性的性质融⼊到神经⽹络结构的构建中。这是卷积神经⽹络（convolutional neural network）（LeCun et al., 1989; LeCun et al., 1998）的基础，它被⼴泛地应⽤于图像处理领域。 考虑⼿写数字识别这个具体的任务。每个输⼊图像由⼀组像素的灰度值组成，输出为10个数字类别的后验概率分布，数字的种类对于平移、缩放以及（微⼩的）旋转具有不变性。⼀种简单的⽅法是把图像作为⼀个完全链接的神经⽹络的输⼊，假设数据集充分⼤，那么这样的⽹络原则上可以产⽣这个问题的⼀个较好的解，从⽽可以从样本中学习到恰当的不变性。然⽽，这种⽅法忽略了图像的⼀个关键性质，即距离较近的像素的相关性要远⼤于距离较远的像素的相关性。这些想法被整合到了卷积神经⽹络中，通过下⾯三种⽅式： （1）局部接收场；（2）权值共享；（3）下采样。 在卷积层， 各个单元被组织在⼀系列平⾯中，每个平⾯被称为⼀个特征地图（feature map）。⼀个特征地图中的每个单元只从图像的⼀个⼩的⼦区域接收输⼊，且⼀个特征地图中的所有单元被限制为共享相同的权值。 例如，⼀个 特征地图可能由100个单元组成， 这些单元被放在了10×10的⽹格中， 每个单元从图像的⼀ 个5×5的像素块接收输⼊。于是，整个特征地图就有25个可调节的参数，加上⼀个可调节的偏置参数。 来⾃⼀个像素块的输⼊值被权值和偏置进⾏线性组合， 线性组合的结果通过公式给出的 $S$ 形⾮线性函数进⾏变换。如果我们把每个单元想象成特征检测器，那么特征地图中的所有单元都检测了输⼊图像中的相同的模式，但是位置不同。由于权值共享，这些单元的激活的计算等价于使⽤⼀个由权向量组成和“核”对图像像素的灰度值进⾏卷积。如果输⼊图像发⽣平移，那么特征地图的激活也会发⽣等量的平移，否则就不发⽣改变。这提供了神经⽹络输出对于输⼊图像的平移和变形的（近似）不变性的基础。由于通常需要检测多个特征来构造⼀个有效的模型，因此通常在卷积层会有多个特征地图，每个都有⾃⼰的权值和偏置参数。 图5.28，卷积神经⽹络的⼀个例⼦，给出了⼀层卷积单元层跟着⼀个下采样单元层，可能连续使⽤这种层对。 卷积单元的输出构成了⽹络的下采样层的输⼊。对于卷积层的每个特征地图，有⼀个下采样层的单元组成的平⾯，并且下采样层的每个单元从对应的卷积层的特征地图中的⼀个⼩的接收场接收输⼊，这些单元完成了下采样。 例如，每个下采样单元可能从对应的特征地图中的⼀个2×2单元的区域中接收输⼊，然后计算这些输⼊的平均值，乘以⼀个可调节的权值和可调节的偏置参数，然后使⽤ $S$ 形⾮线性激活函数进⾏变换。选择的接收场是连续的、⾮重叠的，从⽽ 下采样层的⾏数和列数都是卷积层的⼀半。使⽤这种⽅式，下采样层的单元的响应对于对应的输⼊空间区域中的图⽚的微⼩平移相对不敏感。 七，软权值共享降低具有⼤量权值参数的⽹络复杂度的⼀种⽅法是将权值分组，然后令分组内的权值相等。 然⽽，它只适⽤于限制的形式可以事先确定的问题中。 考虑软权值共享 （soft weight sharing）（Nowlan and Hinton, 1992）。这种⽅法中，权值相等的硬限制被替换为 ⼀种形式的正则化，其中权值的分组倾向于取近似的值。可以将权值分为若⼲组，⽽不是将所有权值分为⼀个组。分组的⽅法是使⽤⾼斯混合概率分布。混合分布中，每个⾼斯分量的均值、⽅差，以及混合系数，都会作为可调节的参数在学习过程中被确定。于是，有下⾯形式的概率密度 p(\\boldsymbol{w})=\\prod_{i}p(w_i)\\tag{5.89}其中， p(w_i)=\\sum_{j=1}^{M}\\pi_{j}\\mathcal{N}(w_i|\\mu_j,\\sigma_{j}^{2})$\\pi_j$ 为混合系数。取负对数，即可得到正则化函数，形式为 \\Omega(\\boldsymbol{w})=-\\sum_{i}\\ln\\left(\\sum_{j=1}^{M}\\pi_{j}\\mathcal{N}(w_i|\\mu_j,\\sigma_{j}^{2})\\right)\\tag{5.90}从⽽，总的误差函数为 \\tilde{E}(\\boldsymbol{w})=E(\\boldsymbol{w})+\\lambda\\Omega(\\boldsymbol{w})\\tag{5.91}其中，$\\lambda$ 是正则化系数。这个误差函数同时关于权值 $w_i$ 和混合模型参数 $\\{\\pi_j,\\mu_j,\\sigma_j\\}$ 进⾏最⼩化。为了最⼩化总的误差函数，把 $\\{\\pi_j\\}$ 当成先验概率，引⼊对应的后验概率，根据相关公式，后验概率由贝叶斯定理给出，形式为 \\gamma_{j}(w)=\\frac{\\pi_{j}\\mathcal{N}(w|\\mu_j,\\sigma_{j}^{2})}{\\sum_{k}\\pi_{k}\\mathcal{N}(w|\\mu_k,\\sigma_{k}^{2})}\\tag{5.92}总的误差函数关于权值的导数为 \\frac{\\partial{\\tilde{E}}}{\\partial{w_i}}=\\frac{\\partial{E}}{\\partial{w_i}}+\\lambda\\sum_{j}\\gamma_{j}(w_i)\\frac{(w_i-\\mu_j)}{\\sigma_{j}^{2}}\\tag{5.93}于是，正则化项的效果是把每个权值拉向第 $j$ 个⾼斯分布的中⼼，拉⼒正⽐于对于给定权值的⾼斯分布的后验概率。误差函数关于⾼斯分布的中⼼的导数为 \\frac{\\partial{\\tilde{E}}}{\\partial{\\mu_j}}=\\lambda\\sum_{i}\\gamma_{j}(w_i)\\frac{(\\mu_j-w_i)}{\\sigma_{j}^{2}}\\tag{5.94}具有简单的直观含义：把 $\\mu_j$ 拉向了权值的平均值，拉⼒为第 $j$ 个⾼斯分量产⽣的权值参数的后验概率。关于⽅差的导数为 \\frac{\\partial{\\tilde{E}}}{\\partial{\\sigma_j}}=\\lambda\\sum_{i}\\gamma_{j}(w_i)\\left(\\frac{1}{\\sigma_j}-\\frac{(w_i-\\mu_j)^{2}}{\\sigma_{j}^{3}}\\right)\\tag{5.95}将 $\\sigma_j$ 拉向权值在对应的中⼼ $\\mu_j$ 附近的偏差的平⽅的加权平均，加权平均的权系数等于由第 $j$ 个⾼斯分量产⽣的权值参数的后验概率。注意，在实际执⾏过程中，引⼊⼀个新的变量 $\\xi_j$ ，它由下式定义 \\sigma_{j}^{2}=\\exp(\\xi_j)\\tag{5.96}并且，最⼩化的过程是关于 $\\xi_j$ 进⾏的，这确保了参数 $\\sigma_j$ 是正数。 对于关于混合系数 $\\pi_j$ 的导数，需要考虑下⾯的限制条件 \\sum_{j}\\pi_{j}=1, 0 \\le \\pi_j \\le 1将混合系数通过⼀组辅助变量 $\\{\\eta_j\\}$ ⽤softmax函数表⽰，即 \\pi_j=\\frac{\\exp(\\eta_j)}{\\sum_{k=1}^{M}\\exp(\\eta_k)}\\tag{5.97}正则化的误差函数关于 $\\{\\eta_j\\}$ 的导数的形式为 \\frac{\\partial{\\tilde{E}}}{\\partial{\\eta_{j}}}=\\sum_{i}\\{\\pi_j-\\gamma_j(w_j)\\}\\tag{5.98}由此可见，$\\pi_j$ 被拉向第 $j$ 个⾼斯分量的平均后验概率。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】Hessian矩阵","slug":"【机器学习基础】Hessian矩阵","date":"2019-10-16T01:05:46.000Z","updated":"2019-10-18T10:04:24.166Z","comments":true,"path":"2019/10/16/prml_05_02/","link":"","permalink":"https://zhangbc.github.io/2019/10/16/prml_05_02/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，Hessian 矩阵反向传播也可以⽤来计算误差函数的⼆阶导数，形式为 \\frac{\\partial^{2}{E}}{\\partial{w_{ji}}\\partial{w_{kl}}}注意，有时将所有的权值和偏置参数看成⼀个向量（记作 $\\boldsymbol{w}$ ）的元素 $w_i$ 更⽅便，此时⼆阶导数组成了Hessian矩阵 $\\boldsymbol{H}$ 的元素 $H_{ij}$ ，其中 $i, j \\in \\{1,\\dots, W\\}$ ，且 $W$ 是权值和偏置的总数。Hessian矩阵在神经⽹络计算的重要的作⽤，包括： 1）⼀些⽤来训练神经⽹络的⾮线性最优化算法是基于误差曲⾯的⼆阶性质的， 这些性质由Hessian矩阵控制（Bishop and Nabney, 2008）；2）对于训练数据的微⼩改变，Hessian矩阵构成了快速重新训练前馈⽹络的算法的基础 （Bishop, 1991）；3）Hessian矩阵的逆矩阵⽤来鉴别神经⽹络中最不重要的权值，这是⽹络“剪枝”算法的⼀部分 （LeCun et al., 1990）；4）Hessian矩阵是贝叶斯神经⽹络的拉普拉斯近似的核⼼。它的逆矩阵⽤来确定训练过的神经⽹络的预测分布，它的特征值确定了超参数的值，它的⾏列式⽤来计算模型证据。 二，对角近似对于模式 $n$ ，Hessian矩阵的对角线元素可以写成 \\frac{\\partial^{2}{E_n}}{\\partial{w_{ji}^{2}}}=\\frac{\\partial^{2}{E_n}}{\\partial{a_{j}^{2}}}z_{i}^{2}\\tag{5.56}从而，反向传播⽅程的形式为 \\frac{\\partial^{2}{E_n}}{\\partial{a_{j}^{2}}}=h^{\\prime}(a_j)^2\\sum_{k}\\sum_{k^{\\prime}}w_{kj}w_{k^{\\prime}j}\\frac{\\partial^{2}{E_n}}{\\partial{a_k}\\partial{a_{k^{\\prime}}}}+h^{\\prime\\prime}(a_j)\\sum_{k}w_{kj}\\frac{\\partial{E_n}}{\\partial{a_k}}\\tag{5.57}如果忽略⼆阶导数中⾮对角线元素， 那么有（Becker and LeCun, 1989; LeCun et al., 1990） \\frac{\\partial^{2}{E_n}}{\\partial{a_{j}^{2}}}=h^{\\prime}(a_j)^2\\sum_{k}w_{kj}^{2}\\frac{\\partial^{2}{E_n}}{\\partial{a_k^{2}}}+h^{\\prime\\prime}(a_j)\\sum_{k}w_{kj}\\frac{\\partial{E_n}}{\\partial{a_k}}\\tag{5.58}三，外积近似当神经⽹络应⽤于回归问题时，通常使⽤下⾯形式的平⽅和误差函数 E=\\frac{1}{2}\\sum_{n=1}^{N}(y_n-t_n)^2\\tag{5.59}考虑单⼀输出的情形（推⼴到多个输出是很直接的），可以把Hessian矩阵写成下⾯的形式 \\boldsymbol{H}=\\nabla\\nabla{E}=\\sum_{n=1}^{N}\\nabla{y_n}(\\nabla{y_n})^{T}+\\sum_{n=1}^{N}(y_n-t_n)\\nabla\\nabla{y_n}\\tag{5.60}通过忽略公式(5.60)的第⼆项，我们就得到了 Levenberg-Marquardt近似，或者称为外积近似（outer product approximation）（因为此时Hessian矩阵由向量外积的求和构造出来），形式为 \\boldsymbol{H}\\simeq\\sum_{n=1}^{N}\\boldsymbol{b}_n\\boldsymbol{b}_n^{T}\\tag{5.61}其中 $\\boldsymbol{b}_n\\equiv\\nabla{a_n}=\\nabla{y_n}$ ， 因为输出单元的激活函数就是恒等函数。这种近似只在⽹络被恰当地训练时才成⽴， 对于⼀个⼀般的⽹络映 射，公式(5.60)的右侧的⼆阶导数项通常不能忽略。 在误差函数为交叉熵误差函数，输出单元激活函数为logistic sigmoid函数的神经⽹络中，对应的近似为 \\boldsymbol{H}\\simeq\\sum_{n=1}^{N}y_n(1-y_n)\\boldsymbol{b}_n\\boldsymbol{b}_n^{T}\\tag{5.62}对于输出函数为softmax函数的多类神经⽹络，可以得到类似的结果。 四，Hessian 矩阵的逆矩阵使⽤外积近似，可以提出⼀个计算Hessian矩阵的逆矩阵的⾼效⽅法（Hassibi and Stork, 1993）。⾸先，⽤矩阵的记号写出外积近似，即 \\boldsymbol{H}_{N}=\\sum_{n=1}^{N}\\boldsymbol{b}_n\\boldsymbol{b}_n^{T}\\tag{5.63}其中， $\\boldsymbol{b}_n\\equiv\\nabla_{\\boldsymbol{w}}{a_n}$ 是数据点 $n$ 产⽣的输出单元激活对梯度的贡献。 现在推导⼀个建⽴Hessian矩阵的顺序步骤，每次处理⼀个数据点。假设已经使⽤前 $L$ 个数据点得到了Hessian矩阵的逆矩阵。通过将第 $L+1$ 个数据点的贡献单独写出来，有 \\boldsymbol{H}_{L+1}=\\boldsymbol{H}_L+\\boldsymbol{b}_{L+1}\\boldsymbol{b}_{L+1}^{T}\\tag{5.64}考虑下⾯的矩阵恒等式 (\\boldsymbol{M}+\\boldsymbol{v}\\boldsymbol{v}^{T})^{-1}=\\boldsymbol{M}^{-1}-\\frac{(\\boldsymbol{M}^{-1}\\boldsymbol{v})(\\boldsymbol{v^{T}\\boldsymbol{M}^{-1}})}{1+\\boldsymbol{v}^{T}\\boldsymbol{M}^{-1}\\boldsymbol{v}}\\tag{5.65}如果令 $\\boldsymbol{H}_L=\\boldsymbol{M}$ ，且 $\\boldsymbol{b}_{L+1}=\\boldsymbol{v}$ ，有 \\boldsymbol{H}_{L+1}^{-1}=\\boldsymbol{H}_{L}^{-1}-\\frac{\\boldsymbol{H}_{L}^{-1}\\boldsymbol{b}_{L+1}\\boldsymbol{b}_{L+1}^{T}\\boldsymbol{H}_{L}^{-1}}{1+\\boldsymbol{b}_{L+1}^{T}\\boldsymbol{H}_{L}^{-1}\\boldsymbol{b}_{L+1}}\\tag{5.66}使⽤这种⽅式，数据点可以依次使⽤，直到 $L+1=N$ ，整个数据集被处理完毕。于是，这个结果表⽰⼀个计算Hessian矩阵的逆矩阵的算法， 这个算法只需对数据集扫描⼀次。最开始的矩阵 $\\boldsymbol{H}_0$ 被选为 $\\alpha\\boldsymbol{I}$ ，其中 $\\alpha$ 是⼀个较⼩的量，从⽽算法实际找的是 $\\boldsymbol{H}+\\alpha\\boldsymbol{I}$ 的逆矩阵。 五，有限差如果对每对可能的权值施加⼀个扰动，那么有 \\begin{aligned}\\frac{\\partial^{2}E}{\\partial{w_{ji}}\\partial_{w_{lk}}}&=\\frac{1}{4\\epsilon^{2}}\\{E(w_{ji}+\\epsilon,w_{lk}+\\epsilon)-E(w_{ji}+\\epsilon,w_{lk}-\\epsilon)\\\\&-E(w_{ji}-\\epsilon,w_{lk}+\\epsilon)+-E(w_{ji}-\\epsilon,w_{lk}-\\epsilon)+O(\\epsilon^{2})\\}\\end{aligned}\\tag{5.67}通过使⽤对称的中⼼差，确保了残留的误差项是 $O(\\epsilon^{2})$ ⽽不是 $O(\\epsilon)$ 。 由于在Hessian矩阵中有 $W^2$ 个元素，且每个元素的计算需要四次正向传播过程，每个传播过程需要 $O(W)$ 次操作（每个模式），因此看到这种⽅法计算完整的Hessian矩阵需要 $O(W^3)$ 次操作。所以，这个⽅法的计算性质很差，虽然在实际应⽤中它对于检查反向传播算法的执⾏的正确性很有⽤。 ⼀个更加⾼效的数值导数的⽅法是将中⼼差应⽤于⼀阶导数，⽽⼀阶导数可以通过反向传播⽅法计算。即 \\frac{\\partial^{2}E}{\\partial{w_{ji}}\\partial_{w_{lk}}}=\\frac{1}{2\\epsilon}\\left\\{\\frac{\\partial{E}}{\\partial{w_{ji}}}(w_{kl}+\\epsilon)-\\frac{\\partial{E}}{\\partial{w_{ji}}}(w_{kl}-\\epsilon)\\right\\}+O(\\epsilon^{2})\\tag{5.68}由于只需要对 $W$ 个权值施加扰动，且梯度可以通过 $O(W)$ 次计算得到，因此看到这种⽅法可以在 $O(W^2)$ 次操作内得到Hessian矩阵。 六，Hessian 矩阵的精确计算考虑⼀个具有两层权值的⽹络，这种⽹络中待求的⽅程很容易推导，将使⽤下标 $i$ 和 $i^{\\prime}$ 表⽰输⼊，⽤下标 $j$ 和 $j^{\\prime}$ 表⽰隐含单元，⽤下标 $k$ 和 $k^{\\prime}$ 表⽰输出。⾸先定义 \\delta_k=\\frac{\\partial{E_n}}{\\partial{a_k}},M_{kk^{\\prime}}=\\frac{\\partial^{2}{E_n}}{\\partial{a_k}\\partial{a_{k^{\\prime}}}}其中 $E_n$ 是数据点 $n$ 对误差函数的贡献。于是，这个⽹络的Hessian矩阵可以被看成三个独⽴的模块，即 1）两个权值都在第⼆层 \\frac{\\partial^{2}E_n}{\\partial{w_{kj}^{(2)}}\\partial{w_{k^{\\prime}j^{\\prime}}^{(2)}}}=z_jz_{j^{\\prime}}M_{kk^{\\prime}}\\tag{5.69}2）两个权值都在第⼀层 \\begin{aligned}\\frac{\\partial^{2}E_n}{\\partial{w_{ji}^{(1)}}\\partial{w_{j^{\\prime}i^{\\prime}}^{(1)}}}&=x_ix_{i^{\\prime}}h^{\\prime\\prime}(a_{j^{\\prime}})I_{jj^{\\prime}}\\sum_{k}w_{kj^{\\prime}}^{(2)}\\delta_{k}\\\\&+x_ix_{i^{\\prime}}h^{\\prime}(a_{j^{\\prime}})h^{\\prime}(a_j)\\sum_{k}\\sum_{k^{\\prime}}w_{k^{\\prime}j^{\\prime}}^{(2)}w_{kj}^{(2)}M_{kk^{\\prime}}\\end{aligned}\\tag{5.70}3）每⼀层有⼀个权值 \\frac{\\partial^{2}E_n}{\\partial{w_{ji}^{(1)}}\\partial{w_{kj^{\\prime}}^{(2)}}}=x_{i}h^{\\prime}(a_{j})\\left\\{\\delta_{k}I_{jj^{\\prime}}+z_{j^{\\prime}}\\sum_{k^{\\prime}}w_{k^{\\prime}j}^{(2)}M_{kk^{\\prime}}\\right\\}\\tag{5.71}其中，$I_{jj^{\\prime}}$ 是单位矩阵的第 $j$, $j^{\\prime}$ 个元素。 七，Hessian 矩阵的快速乘法尝试寻找⼀种只需 $O(W)$ 次操作的直接计算 $\\boldsymbol{v}^{T}\\boldsymbol{H}$ 的⾼效⽅法。⾸先注意到 \\boldsymbol{v}^{T}\\boldsymbol{H}=\\boldsymbol{v}^{T}\\nabla(\\nabla{E})\\tag{5.72}其中 $\\nabla$ 表⽰权空间的梯度算符。然后，可以写下计算 $\\nabla{E}$ 的标准正向传播和反向传播的⽅程， 继而得到⼀组计算 $\\boldsymbol{v}^{T}\\boldsymbol{H}$ 的正向传播和反向传播的⽅程（Møller, 1993; Pearlmutter, 1994）。这对应于将微分算符 $\\boldsymbol{v}^{T}\\nabla$ 作⽤于原始的正向传播和反向传播的⽅程。Pearlmutter（1994）使⽤记号 $\\mathcal{R}\\{·\\}$ 表⽰算符 $\\boldsymbol{v}^{T}\\nabla$ 。下⾯的分析过程很直接，会使⽤通常的微积分规则，以及下⾯的结果 \\mathcal{R}\\{\\boldsymbol{w}\\}=\\boldsymbol{v}使⽤⼀个简单的例⼦来说明这个⽅法，使⽤两层⽹络，以及线性的输出单元和平⽅和误差函数。考虑数据集⾥的⼀个模式对于误差函数的贡献。这样，所要求解的向量可以通过求出每个模式各⾃的贡献然后求和的⽅式得到。对于两层神经⽹络，正向传播⽅程为 a_j=\\sum_{i}w_{ji}x_{i}\\\\ z_j=h(a_j)\\\\ y_k=\\sum_{j}w_{kj}z_j现在使⽤ $\\mathcal{R}\\{·\\}$ 算符作⽤于这些⽅程上，得到⼀组正向传播⽅程，形式为 \\mathcal{R}\\{a_j\\}=\\sum_{i}v_{ji}x_{i}\\\\ \\mathcal{R}\\{z_j\\}=h^{\\prime}(a_j)\\mathcal{R}\\{a_j\\}\\\\ \\mathcal{R}\\{y_k\\}=\\sum_{j}w_{kj}\\mathcal{R}\\{z_j\\}+\\sum_{j}v_{kj}z_j其中，$v_{ji}$ 是向量 $\\boldsymbol{v}$ 中对应于权值 $w_{ji}$ 的元素。 由于考虑的是平⽅和误差函数，因此有下⾯的标准的反向传播表达式 \\delta_{k}=y_k-t_k\\\\ \\delta_j=h^{\\prime}(a_j)\\sum_{k}w_{kj}\\delta_{k}将 $\\mathcal{R}\\{·\\}$ 算符作⽤于这些⽅程上，得到⼀组反向传播⽅程，形式为 \\mathcal{R}\\{\\delta_k\\}=\\mathcal{R}\\{y_k\\}\\\\ \\begin{aligned}\\mathcal{R}\\{\\delta_j\\}&=h^{\\prime\\prime}(a_j)\\mathcal{R}\\{a_j\\}\\sum_{k}w_{kj}\\delta_{k}\\\\&+h^{\\prime}(a_j)\\sum_{k}v_{kj}\\delta_{k}+h^{\\prime}(a_j)\\sum_{k}w_{kj}\\mathcal{R}\\{\\delta_{k}\\}\\end{aligned}最后，有误差函数的⼀阶导数的⽅程 \\frac{\\partial{E}}{\\partial{w_{kj}}}=\\delta_{k}z_j\\\\ \\frac{\\partial{E}}{\\partial{w_{ji}}}=\\delta_{j}x_i使⽤ $\\mathcal{R}\\{·\\}$ 算符作⽤在这些⽅程上，我们得到了下⾯的关于 $\\boldsymbol{v}^{T}\\boldsymbol{H}$ 的表达式 \\mathcal{R}\\left\\{\\frac{\\partial{E}}{\\partial{w_{kj}}}\\right\\}=\\mathcal{R}\\{\\delta_{k}\\}z_j+\\delta_{k}\\mathcal{R}\\{z_j\\}\\\\ \\mathcal{R}\\left\\{\\frac{\\partial{E}}{\\partial{w_{ji}}}\\right\\}=x_i\\mathcal{R}\\left\\{\\delta_j\\right\\}如图5.11～13，使⽤从正弦数据集中抽取的10个数据点训练的两层神经⽹络的例⼦。 各图分别给出了使⽤ $M = 1, 3, 10$ 个隐含单元调节⽹络的结果，调节的⽅法是使⽤放缩的共轭梯度算法来最⼩化平⽅和误差函数。 如图5.14，对于多项式数据集，测试集的平⽅和误差与⽹络的隐含单元的数量的图像。对于每个⽹络规模，都随机选择了30个初始点，这展⽰了局部最⼩值的效果。对于每个新的初始点，权向量通过从⼀个各向 同性的⾼斯分布中取样，这个⾼斯分布的均值为零，⽅差为10。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】前馈神经网络","slug":"【机器学习基础】前馈神经网络","date":"2019-10-16T00:29:43.000Z","updated":"2019-10-16T01:42:01.327Z","comments":true,"path":"2019/10/16/prml_05_01/","link":"","permalink":"https://zhangbc.github.io/2019/10/16/prml_05_01/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，前馈神经网络1，前馈神经网络基于固定⾮线性基函数 $\\phi_{j}(\\boldsymbol{x})$ 的线性组合，形式为 y(\\boldsymbol{x},\\boldsymbol{w})=f\\left(\\sum_{j=1}^{M}w_{j}\\phi_{j}(\\boldsymbol{x})\\right)\\tag{5.1}其中 $f(·)$ 在分类问题中是⼀个⾮线性激活函数， 在回归问题中为恒等函数。现在的⽬标是推⼴这个模型，使得基函数 $\\phi_{j}(\\boldsymbol{x})$ 依赖于参数，从⽽能够让这些参数以及系数 $\\{w_j\\}$ 能够在训练阶段调节。 神经⽹络使⽤与公式(5.1)形式相同的基函数，即每个基函数本⾝是输⼊的线性组合的⾮线性函数，其中线性组合的系数是可调节参数。 ⾸先，构造输⼊变量 $x_1, \\dots , x_D$ 的 $M$ 个线性组合，形式为 a_{j}=\\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\\tag{5.2}其中 $j = 1,\\dots, M$ ， 且上标(1)表⽰对应的参数是神经⽹络的第⼀“层”。参数 $w_{ji}^{(1)}$ 称为 权（weight）， 参数 $w_{j0}^{(1)}$ 称为偏置（bias），$a_{j}$ 被称为激活（activation）。每个激活都使⽤⼀个可微的⾮线性激活函数（activation function）$h(·)$ 进⾏变换，可得 z_{j}=h(a_{j})\\tag{5.3}这些量对应于公式(5.1)中的基函数的输出， 这些基函数在神经⽹络中被称为隐含单元 （hidden unit）。⾮线性函数 $h(·)$ 通常被选为 S形的函数，例如logistic sigmoid函数或者双曲正切函数。根据公式(5.1)，这些值再次线性组合，得到输出单元激活（output unit activation） a_{k}=\\sum_{j=1}^{M}w_{kj}^{(2)}z_{j}+w_{k0}^{(2)}\\tag{5.4}其中 $k = 1, \\dots, K$ ， 且 $K$ 是输出的总数量。 这个变换对应于神经⽹络的第⼆层， 并且 $w_{k0}^{(2)}$ 是偏置参数。使⽤⼀个恰当的激活函数对输出单元激活进⾏变换，得到神经⽹络的⼀组输出 $y_k$ ，激活函数的选择由数据本⾝以及⽬标变量的假定的分布确定。因此对于标准的回归问题， 激活函数是恒等函数， 从⽽ $y_k = a_k$ 。 类似地， 对于多个⼆元分类问题， 每个输出单元激活使⽤logistic sigmoid函数进⾏变换，即 y_k=\\sigma(a_k)\\tag{5.5}其中， \\sigma(a)=\\frac{1}{1+\\exp(-a)}综上可知，对于sigmoid输出单元激活函数，整体的⽹络函数为 y(\\boldsymbol{x},\\boldsymbol{w})=\\sigma\\left(\\sum_{j=1}^{M}w_{kj}^{(2)}h\\left(\\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\\right)+w_{k0}^{(2)}\\right)\\tag{5.6}如图5.1，对应于公式(5.6)的两层神经⽹络的⽹络图。输⼊变量、隐含变量、输出变量都表⽰为结点，权参数被表⽰为结点之间的链接，其中偏置参数被表⽰为来⾃额外的输⼊变量 $x_0$ 和隐含变量 $z_0$ 的链接。箭头表⽰信息流在⽹络中进⾏前向传播的⽅向。 可以通过定义额外的输⼊变量 $x_0$ 的⽅式将公式(5.2)中的偏置参数整合到权参数集合中，其中额外的输⼊变量 $x_0$ 的值被限制为 $x_0 = 1$，因此公式(5.2)的形式为 a_{j}=\\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}\\tag{5.7}类似地，把第⼆层的偏置整合到第⼆层的权参数中，从⽽整体的⽹络函数为 y(\\boldsymbol{x},\\boldsymbol{w})=\\sigma\\left(\\sum_{j=1}^{M}w_{kj}^{(2)}h\\left(\\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}\\right)\\right)\\tag{5.8}神经⽹络模型由两个处理阶段组成，每个阶段都类似于感知器模型，因此神经⽹络也被称为多层感知器（multilayer perceptron），或者 MLP。与感知器模型相⽐，⼀个重要的区别是神经⽹络在隐含单元中使⽤连续的sigmoid⾮线性函数，⽽感知器使⽤阶梯函数⾮线性函数。这意味着神经⽹络函数关于神经⽹络参数是可微的，这个性质在神经⽹络的训练过程中起着重要的作⽤。 神经⽹络结构的⼀个扩展是引⼊跨层（skip-layer）链接，每个跨层链接都关联着⼀个对应的可调节参数。 前馈（feed-forward）结构：⽹络中不能存在有向圈，从⽽确保了输出是输⼊的确定函数。 举例：⽹络中每个（隐含或者输出）单元都计算了⼀个下⾯的函数 z_k=h\\left(\\sum_{j}w_{kj}z_{i}\\right)其中，求和的对象是所有向单元 $k$ 发送链接的单元（偏置参数也包含在了求和式当中）。 如图5.2，具有⼀般的前馈拓扑结构的神经⽹络，注意，每个隐含电源和输出单元都与⼀个偏置参数关联。 如图5.3～5.6，多层感知器的能⼒说明，它⽤来近似四个不同的函数。 (a) $f(x) = x^2$ ，(b) $f(x) = \\sin(x)$， (c) $f(x) = |x|$，(d) $f(x) = H(x)$，其中 $H(x)$ 是⼀个硬阶梯函数。在每种情况下，$N = 50$ 个数据点（⽤蓝点 表⽰）从区间 $(−1, 1)$ 中均匀分布的 $x$ 中进⾏取样，然后计算出对应的 $f(x)$ 值。这些数据点之后⽤来训练⼀个具有3个隐含单元的两层神经⽹络，隐含单元的激活函数为tanh函数，输出为线性输出单元。⽣成的⽹络函数使⽤红⾊曲线表⽰，三个隐含单元的输出⽤三条虚线表⽰。 2，权空间对称性前馈神经⽹络的⼀个性质是，对于多个不同的权向量 $\\boldsymbol{w}$ 的选择，⽹络可能产⽣同样的从输⼊到输出的映射函数（Chen et al., 1993）。 考虑两层⽹络，⽹络有 $M$ 个隐含结点，激活函数是双曲正切函数，且两层之间完全链接。如果把作⽤于某个特定的隐含单元的所有的权值以及偏置全部变号，那么对于给定的输⼊模式， 隐含单元的激活的符号也会改变。 这是因为双曲正切函数是⼀个奇函数， 即 $\\tan h(−a) = −\\tan h(a)$。这种变换可以通过改变所有从这个隐含单元到输出单元的权值的符号的⽅式进⾏精确补偿。因此，通过改变特定⼀组权值（以及偏置）的符号，⽹络表⽰的输⼊-输出映射函数不会改变，因此我们已经找到了两个不同的权向量产⽣同样的映射函数。对于 $M$ 个 隐含单元，会有 $M$ 个这样的“符号改变”对称性，因此任何给定的权向量都是 $2^M$ 个等价的权向量中的⼀个。 类似地，假设将与某个特定的隐含结点相关联的所有输⼊和输出的权值（和偏置）都变为与不同的隐含结点相关联的对应的权值（和偏置）。与之前⼀样，这显然使得⽹络的输⼊-输出映射不变，但是对应了⼀个不同的权向量。对于 $M$ 个隐含结点，任何给定的权向量都属于这种交换对称性产⽣的 $M!$ 个等价的权向量中的⼀个，它对应于 $M!$ 个不同的隐含单元的顺序。于是，⽹络有⼀个整体的权空间对称性因⼦ $M!2^M$ 。 二，网络训练1，回归问题给定⼀个由输⼊向量 $\\{\\boldsymbol{x_n}\\}(n = 1, \\dots , N)$ 组成的训练集，以及⼀ 个对应的⽬标向量 $\\boldsymbol{t}_n$ 组成的集合，要最⼩化误差函数 E(\\boldsymbol{w})=\\frac{1}{2}\\sum_{n=1}^{N}||\\boldsymbol{y}(\\boldsymbol{x_n},\\boldsymbol{w})-\\boldsymbol{t}_n||^{2}\\tag{5.9}⾸先， 讨论回归问题。考虑⼀元⽬标变量 $t$ 的情形，假定 $t$ 服从⾼斯分布，均值与 $\\boldsymbol{x}$ 相关，由神经⽹络的输出确定，即 p(t|\\boldsymbol{x},\\boldsymbol{w})=\\mathcal{N}(t|y(\\boldsymbol{x},\\boldsymbol{w}),\\beta^{-1})\\tag{5.10}其中 $\\beta$ 是⾼斯噪声的精度（⽅差的倒数）。 给定⼀个由 $N$ 个独⽴同分布的观测组成的数据集 $\\mathbf{X} = \\{\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_N\\}$， 以及对应的⽬标值 $\\mathbf{t} = \\{t_1, \\dots, t_N\\}$ ，构造对应的似然函数 p(\\mathbf{t}|\\boldsymbol{X},\\boldsymbol{w},\\beta)=\\prod_{n=1}^{N}p(t_n|\\boldsymbol{x}_n,\\boldsymbol{w},\\beta)\\tag{5.11}取负对数，可得到误差函数 \\frac{\\beta}{2}\\sum_{n=1}^{N}\\{y(\\boldsymbol{x},\\boldsymbol{w})-t_n\\}^{2}-\\frac{N}{2}\\ln\\beta+\\frac{N}{2}\\ln(2\\pi)这可以⽤来学习参数 $\\boldsymbol{w}$ 和 $\\beta$ 。 ⾸先考虑 $\\boldsymbol{w}$ 的确定。最⼤化似然函数等价于最⼩化平⽅和误差函数： E(\\boldsymbol{w})=\\frac{1}{2}\\sum_{n=1}^{N}\\{\\boldsymbol{y}(\\boldsymbol{x_n},\\boldsymbol{w})-\\boldsymbol{t}_n\\}^{2}\\tag{5.12}其中去掉了相加的和相乘的常数。通过最⼩化 $E(\\boldsymbol{w})$ 的⽅式得到 $\\boldsymbol{w}$ 值被记作 $\\boldsymbol{w}_{ML}$ ， 因为它对应于最⼤化似然函数。 $\\beta$ 的值可以通过最⼩化似然函数的负对数的⽅式求得，为 \\frac{1}{\\beta_{ML}}=\\frac{1}{N}\\sum_{n=1}^{N}\\{\\boldsymbol{y}(\\boldsymbol{x_n},\\boldsymbol{w}_{ML})-\\boldsymbol{t}_n\\}^{2}\\tag{5.13}如果有多个⽬标变量，并且假设给定 $\\boldsymbol{x}$ 和 $\\boldsymbol{w}$ 的条件下，⽬标变量之间相互独⽴，且噪声精度均为 $\\beta$ ，那么⽬标变量的条件分布为 p(\\boldsymbol{t}|\\boldsymbol{x},\\boldsymbol{w})=\\mathcal{N}(\\boldsymbol{t}|y(\\boldsymbol{x},\\boldsymbol{w}),\\beta^{-1}\\boldsymbol{I})\\tag{5.14}噪声的精度为 \\frac{1}{\\beta_{ML}}=\\frac{1}{NK}\\sum_{n=1}^{N}\\|\\boldsymbol{y}(\\boldsymbol{x_n},\\boldsymbol{w}_{ML})-\\boldsymbol{t}_n\\|^{2}\\tag{5.15}其中 $K$ 是⽬标变量的数量。 在回归问题中，我们可以把神经⽹络看成具有⼀个恒等输出激活函数的模型，即 $y_k = a_k$ 。对应的平⽅和误差函数有下⾯的性质： \\frac{\\partial E}{\\partial a_k}=y_k-t_k\\tag{5.16}现在考虑⼆分类的情形。⼆分类问题中，有⼀个单⼀⽬标变量 $t$，且 $t = 1$ 表⽰类别 $\\mathcal{C}_1$ ，$t = 0$ 表⽰类别 $\\mathcal{C}_2$ 。遵循对于标准链接函数的讨论，考虑⼀个具有单⼀输出的⽹络，它的激活函数是logistic sigmoid函数 y=\\sigma(a)\\equiv\\frac{1}{1+\\exp(-a)}\\tag{5.17}从⽽ $0\\le y(\\boldsymbol{x}, \\boldsymbol{w})\\le1$ 。可以把 $y(\\boldsymbol{x},\\boldsymbol{w})$ 表⽰为条件概率 $p(\\mathcal{C}_1|\\boldsymbol{x})$ ， 此时 $p(\\mathcal{C}_2|\\boldsymbol{x})$ 为 $1 − y(\\boldsymbol{x},\\boldsymbol{w})$。如果给定了输⼊，那么⽬标变量的条件概率分布是⼀个伯努利分布，形式为 p(t|\\boldsymbol{x},\\boldsymbol{w})=y(\\boldsymbol{x},\\boldsymbol{w})^{t}\\{1-y(\\boldsymbol{x},\\boldsymbol{w})\\}^{1-t}\\tag{5.18}如果考虑⼀个由独⽴的观测组成的训练集，那么由负对数似然函数给出的误差函数就是⼀个交叉熵（cross-entropy）误差函数，形式为 E(\\boldsymbol{w})=-\\sum_{n=1}^{N}\\{t_n\\ln y_n+(1-t_n)\\ln(1-y_n)\\}\\tag{5.19}其中 $y_n$ 表⽰ $y(\\boldsymbol{x}_n,\\boldsymbol{w})$ 。 如果有 $K$ 个相互独⽴的⼆元分类问题， 那么可以使⽤具有 $K$ 个输出的神经⽹络， 每个输出都有⼀个logistic sigmoid激活函数，与每个输出相关联的是⼀个⼆元类别标签 $t_k\\in\\{0,1\\}$ ，其中 $k=1, \\dots, K$ 。如果假定类别标签是独⽴的，那么给定输⼊向量，⽬标向量的条件概率分布为 p(\\boldsymbol{t}|\\boldsymbol{x},\\boldsymbol{w})=\\prod_{k=1}^{K}y_k(\\boldsymbol{x},\\boldsymbol{w})^{t_k}[1-y_k(\\boldsymbol{x},\\boldsymbol{w})]^{1-t_k}\\tag{5.20}取似然函数的负对数，可以得误差函数 E(\\boldsymbol{w})=-\\sum_{n=1}^{N}\\sum_{k=1}^{K}\\{t_{nk}\\ln y_{nk}+(1-t_{nk})\\ln(1-y_{nk})\\}\\tag{5.21}其中 $y_{nk}$ 表⽰ $y_k(\\boldsymbol{x}_n,\\boldsymbol{w})$ 。 如图5.7，误差函数 $E(\\boldsymbol{w})$ 的⼏何表⽰，其中，误差函数被表⽰为权空间上的⼀个曲⾯。点 $\\boldsymbol{w}_A$ 是⼀个局部最⼩值，点 $\\boldsymbol{w}_B$ 是全局最⼩值。在任意点 $\\boldsymbol{w}_C$ 处，误差函数的局部梯度由向量 $\\nabla{E}$ 给出。 最后，我们考虑标准的多分类问题，其中每个输⼊被分到 $K$ 个互斥的类别中。⼆元⽬标变量 $t_k\\in\\ {0,1}$ 使 ⽤“1-of-K”表达⽅式来表⽰类别，从⽽⽹络的输出可以表⽰为 $y_k(\\boldsymbol{x},\\boldsymbol{w}) = p(t_k=1|\\boldsymbol{x})$ ，因此误差函数为 E(\\boldsymbol{w})=-\\sum_{n=1}^{N}\\sum_{k=1}^{K}t_{nk}\\ln y_{k}(\\boldsymbol{x}_n,\\boldsymbol{w})\\tag{5.22}输出单元激活函数（对应于标准链接函数）是softmax函数 y_k(\\boldsymbol{x},\\boldsymbol{w})=\\frac{\\exp(a_k(\\boldsymbol{x},\\boldsymbol{w}))}{\\sum_{j}\\exp(a_j(\\boldsymbol{x},\\boldsymbol{w}))}\\tag{5.23}其中，$0 \\le y_k \\le 1$ ，且 $\\sum_{k} y_k=1$ 。 总之，根据解决的问题类型，关于输出单元激活函数和对应的误差函数，都存在⼀个⾃然的选择。对于回归问题，使⽤线性输出和平⽅和误差函数，对于（多类独⽴的）⼆元分类问题， 使⽤logistic sigmoid输出以及交叉熵误差函数，对于多类分类问题， 使⽤softmax输出以及对应的多分类交叉熵错误函数。对于涉及到两类分类问题，可以使⽤单⼀的logistic sigmoid输出，也可以使⽤神经⽹络，这个神经⽹络有两个输出，且输出激活函数为softmax函数。 2，参数最优化考虑寻找能够使得选定的误差函数 $E(\\boldsymbol{w})$ 达到最⼩值的权向量 $\\boldsymbol{w}$ 。现在，考虑误差函数的⼏何表⽰是很有⽤的，可以把误差函数看成位于权空间的⼀个曲⾯。⾸先注意到，如果在权空间中⾛⼀⼩步， 从 $\\boldsymbol{w}$ ⾛到 $\\boldsymbol{w}+\\delta\\boldsymbol{w}$ ， 那么误差函数的改变为 $\\delta E \\simeq \\delta \\boldsymbol{w}^{T}\\nabla E(\\boldsymbol{w})$ ，其中向量 $\\delta E(\\boldsymbol{w})$ 在误差函数增加速度最⼤的⽅向上。 由于误差 $E(\\boldsymbol{w})$ 是 $\\boldsymbol{w}$ 的光滑连续函数，因此它的最⼩值出现在权空间中误差函数梯度等于零的位置上， 即 \\nabla E(\\boldsymbol{w})=0如果最⼩值不在这个位置上，我们就可以沿着⽅向 $−\\nabla E(\\boldsymbol{w})$ ⾛⼀⼩步，进⼀步减⼩误差。梯度为零的点被称为驻点，它可以进⼀步地被分为极⼩值点、极⼤值点和鞍点。对于所有的权向量，误差函数的最⼩值被称为全局最⼩值（golobal minimum）。任何其他的使误差函数的值较⼤的极⼩值被称为局部极⼩值（local minima）。 由于显然⽆法找到⽅程 $\\nabla E(\\boldsymbol{w})=0$ 的解析解，因此使⽤迭代的数值⽅法。⼤多数⽅法涉及到为权向量选择某个初始值 $\\boldsymbol{w}_0$ ，然后在权空间中进⾏⼀系列移动，形式为 \\boldsymbol{w}^{(\\tau+1)}=\\boldsymbol{w}^{\\tau}+\\nabla w^{(\\tau)}\\tag{5.24}其中 $\\tau$ 表⽰迭代次数。 3，局部⼆次近似考虑 $E(\\boldsymbol{w})$ 在权空间某点 $\\hat{\\boldsymbol{w}}$ 处的泰勒展开 E(\\boldsymbol{w})\\simeq E(\\hat{\\boldsymbol{w}})+(\\boldsymbol{w}-\\hat{\\boldsymbol{w}})^{T}\\boldsymbol{b}+\\frac{1}{2}(\\boldsymbol{w}-\\hat{\\boldsymbol{w}})^{T}\\boldsymbol{H}(\\boldsymbol{w}-\\hat{\\boldsymbol{w}})\\tag{5.25}其中，已省略⽴⽅项和更⾼阶的项， $\\boldsymbol{b}$ 定义为 $E$ 的梯度在 $\\hat{\\boldsymbol{w}}$ 处的值。 \\boldsymbol{b}\\equiv\\nabla E|_{\\boldsymbol{w}=\\hat{\\boldsymbol{w}}}Hessian矩阵 $H=\\nabla\\nabla E$ 的元素为 (\\boldsymbol{H})_{ij}\\equiv \\frac{\\partial E}{\\partial w_i \\partial w_j}\\bigg{|}_{\\boldsymbol{w}=\\hat{\\boldsymbol{w}}}根据公式，梯度的局部近似为 \\nabla E\\simeq \\boldsymbol{b}+\\boldsymbol{H}(\\boldsymbol{w}-\\hat{\\boldsymbol{w}})\\tag{5.26}考虑⼀个特殊情况：在误差函数最⼩值点 $\\boldsymbol{w}^{*}$ 附近的局部⼆次近似。在这种情况下，没有线性项，因为在 $\\boldsymbol{w}^{*}$ 处 $\\nabla E=0$ ，公式(5.25)变成了 E(\\boldsymbol{w})\\simeq E(\\boldsymbol{w}^{*})+\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol{w}^{*})^{T}\\boldsymbol{H}(\\boldsymbol{w}-\\boldsymbol{w}^{*})\\tag{5.27}这⾥Hessian矩阵在点 $\\boldsymbol{w}^{*}$ 处计算。 为了⽤⼏何的形式表⽰这个结果， 考虑Hessian矩阵的特征值⽅程 \\boldsymbol{H}\\boldsymbol{\\mu}_i=\\lambda_i\\boldsymbol{\\mu}_i\\tag{5.28}其中特征向量 $\\boldsymbol{\\mu}_i$ 构成了完备的单位正交集合，即 \\boldsymbol{\\mu}_{i}^{T}\\boldsymbol{\\mu}_{j}=\\delta_{ij}现在把 $(\\boldsymbol{w}-\\boldsymbol{w}^{*})$ 展开成特征值的线性组合的形式 \\boldsymbol{w}-\\boldsymbol{w}^{*}=\\sum_{i}\\alpha_{i}\\boldsymbol{\\mu}_i\\tag{5.29}这可以被看成坐标系的变换，坐标系的原点变为了 $\\boldsymbol{w}^{*}$ ，坐标轴旋转，与特征向量对齐（通过列为 $\\boldsymbol{\\mu}_i$ 的正交矩阵），误差函数可以写成下⾯的形式 E(\\boldsymbol{w})=E(\\boldsymbol{w}^{*})+\\frac{1}{2}\\sum_{i}\\alpha_{i}\\boldsymbol{\\mu}_{i}\\tag{5.30}矩阵 $\\boldsymbol{H}$ 是正定的（positive definite）当且仅当 $\\boldsymbol{v}^{T}\\boldsymbol{H}\\boldsymbol{v}&gt;0$ 对所有的 $\\boldsymbol{v}\\ne 0$ 都成立。 如图5.8，在最⼩值 $\\boldsymbol{w}^{*}$ 的邻域中，误差函数可以⽤⼆次函数近似。这样，常数误差函数的轮廓线为椭圆，它的轴与Hessian矩阵的特征向量 $\\boldsymbol{\\mu}_i$ 给出，长度与对应的特征值 $\\lambda_i$ 的平⽅根成反⽐。 由于特征向量 $\\{\\boldsymbol{\\mu}_i\\}$ 组成了⼀个完备集，因此任意的向量 $\\boldsymbol{v}$ 都可以写成下⾯的形式 \\boldsymbol{v}=\\sum_{i}c_i\\boldsymbol{\\mu}_i有， \\boldsymbol{v}^{T}\\boldsymbol{H}\\boldsymbol{v}=\\sum_{i}c_{i}^{2}\\lambda_{i}\\tag{5.31}因此 $\\boldsymbol{H}$ 是正定的，当且仅当它的所有的特征值均严格为正。在新的坐标系中，基向量是特征向量 $\\{\\boldsymbol{\\mu}_i\\}$ ，$E$ 为常数的轮廓线是以原点为中⼼的椭圆。对于⼀维权空间，驻点 $\\boldsymbol{w}^{∗}$ 满⾜下⾯条件时取得最⼩值 \\frac{\\partial^{2}E}{\\partial w^{2}}\\Bigg{|}_{\\boldsymbol{w}^{*}}>0对应的 $\\boldsymbol{D}$ 维的结论是，在 $\\boldsymbol{w}^{*}$ 处的Hessian矩阵是正定矩阵。 4，使⽤梯度信息可以使⽤误差反向传播的⽅法⾼效地计算误差函数的梯度，这个梯度信息的使⽤可以⼤幅度加快找到极⼩值点的速度。在公式(5.25)给出的误差函数的⼆次近似中，误差曲⾯由 $\\boldsymbol{b}$ 和 $\\boldsymbol{H}$ 确定， 它包含了总共 $\\frac{W(W+3)}{2}$ 个独⽴的元素（因为矩阵 $\\boldsymbol{H}$ 是对称的），其中 $W$ 是 $\\boldsymbol{w}$ 的维度（即⽹络中可调节参数的总数）。这个⼆次近似的极⼩值点的位置因此依赖于 $O(W^2)$ 个参数，并且不应该奢求能够在收集到 $O(W^2)$ 条独⽴的信息之前就能够找到最⼩值。如果不使⽤梯度信息，不得不进⾏ $O(W^2)$ 次函数求值，每次求值都需要 $O(W)$ 个步骤。 因此，使⽤这种⽅法求最⼩值需要的计算复杂度为 $O(W^3)$ 。现在将这种⽅法与使⽤梯度信息的⽅法进⾏对⽐，由于每次计算 $\\nabla{E}$ 都会带来 $W$ 条信息，因此可能预计找到函数的极⼩值需要计算 $O(W)$ 次梯度。通过使⽤误差反向传播算法，每个这样的计算只需要 $O(W)$ 步， 因此使⽤这种⽅法可以在 $O(W^2)$ 个步骤内找到极⼩值。 5，梯度下降最优化最简单的使⽤梯度信息的⽅法是：每次权值更新都是在负梯度⽅向上的⼀次⼩的移动，即 \\boldsymbol{w}^{(\\tau+1)}=\\boldsymbol{w}^{(\\tau)}-\\eta\\nabla E\\left(\\boldsymbol{w}^{(\\tau)}\\right)\\tag{5.32}其中参数 $\\eta&gt;0$ 被称为学习率（learning rate）。 误差函数是关于训练集定义的，因此为了计算 $\\nabla{E}$ ，每⼀步都需要处理整个数据集。在每⼀步，权值向量都会沿着误差函数下降速度最快的⽅向移动， 因此这种⽅法被称为梯度下降法（gradient descent）或者最陡峭下降法（steepest descent）。 对于批量最优化⽅法，存在更⾼效的⽅法，例如共轭梯度法（conjugate gradient）或者拟⽜顿法（quasi-Newton）。与简单的梯度下降⽅法相⽐，这些⽅法更鲁棒，更快（Gill et al., 1981; Fletcher, 1987; Nocedal and Wright, 1999）。 与梯度下降⽅法不同， 这些算法具有这样的性质： 误差函数在每次迭代时总是减⼩的，除⾮权向量到达了局部的或者全局的最⼩值。 基于⼀组独⽴观测的最⼤似然函数的误差函数由⼀个求和式构成，求和式的每⼀项都对应着⼀个数据点 E(\\boldsymbol{w})=\\sum_{n=1}^{N}E_n(\\boldsymbol{w})\\tag{5.33}在线梯度下降，也被称为顺序梯度下降（sequential gradient descent） 或者随机梯度下降（stochastic gradient descent），使得权向量的更新每次只依赖于⼀个数据点，即 \\boldsymbol{w}^{(\\tau+1)}=\\boldsymbol{w}^{(\\tau)}-\\eta\\nabla E_n\\left(\\boldsymbol{w}^{(\\tau)}\\right)\\tag{5.34}这个更新在数据集上循环重复进⾏，并且既可以顺序地处理数据，也可以随机地有重复地选择数据点。 三，误差反向传播在局部信息传递的思想中，信息在神经⽹络中交替地向前、向后传播， 这种⽅法被称为误差反向传播（error backpropagation），有时简称“反传”（backprop）。 1，误差函数导数的计算针对⼀组独⽴同分布的数据的最⼤似然⽅法定义的误差函数，由若⼲项的求和式组成，每⼀项对应于训练集的⼀个数据点，即 E(\\boldsymbol{w})=\\sum_{n=1}^{N}E_n(\\boldsymbol{w})考虑⼀个简单的线性模型，其中输出 $y_k$ 是输⼊变量 $x_i$ 的线性组合，即 y_k=\\sum_{i}w_{ki}x_{i}\\tag{5.35}对于⼀个特定的输⼊模式 $n$，误差函数的形式为 E_n=\\frac{1}{2}\\sum_{k}(y_{nk}-t_{nk})^2\\tag{5.36}其中 $y_{nk} = y_k(\\boldsymbol{x}_n,\\boldsymbol{w})$ 。这个误差函数关于⼀个权值 $w_{ji}$ 的梯度为 \\frac{\\partial{E_n}}{\\partial{w_{ji}}}=(y_{nj}-t_{nj})x_{ni}\\tag{5.37}它可以表⽰为与链接 $w_{ji}$ 的输出端相关联的“误差信号” $y_{nj}−t_{nj}$ 和与链接的输⼊端相关联的变量 $x_{ni}$ 的乘积。 在⼀个⼀般的前馈⽹络中，每个单元都会计算输⼊的⼀个加权和，形式为 a_j=\\sum_{i}w_{ji}z_{i}\\tag{5.38}其中 $z_i$ 是⼀个单元的激活，或者是输⼊，它向单元 $j$ 发送⼀个链接，$w_{ji}$ 是与这个链接关联的权值。偏置可以被整合到这个求和式中，整合的⽅法是引⼊⼀个额外的单元或输⼊，然后令激活恒为 $+1$。求和式通过⼀个⾮线性激活函数 $h(·)$ 进⾏变换，得到单元 $j$ 的激活 $z_j$ ，形式为 z_{i}=h(a_j)\\tag{5.39}对于训练集⾥的每个模式，假定给神经⽹络提供了对应的输⼊向量，然后通过反复应⽤公式(5.38)和公式(5.39)，计算神经⽹络中所有隐含单元和输出单元的激活。这个过程通常被称为正向传播（forward propagation），因为它可以被看做⽹络中的⼀个向前流动的信息流。 现在考虑计算 $E_n$ 关于权值 $w_{ji}$ 的导数，各个单元的输出会依赖于某个特定的输⼊模式 $n$ 。⾸先，注意到 $E_n$ 只通过 单元 $j$ 的经过求和之后的输⼊ $a_j$ 对权值 $w_{ji}$ 产⽣依赖。因此，可以应⽤偏导数的链式法则， 得到 \\frac{\\partial{E_n}}{\\partial{w_{ji}}}=\\frac{\\partial{E_n}}{\\partial{a_{j}}}\\frac{\\partial{a_j}}{\\partial{w_{ji}}}如图5.9，对于隐含单元 $j$ ，计算 $\\delta_{j}$ 的说明。计算时使⽤了向单元 $j$ 发送信息的那些单元 $k$ 的 $\\delta$ ，使⽤反向误差传播⽅法进⾏计算。蓝⾊箭头表⽰在正向传播阶段信息流的⽅向，红⾊箭头表⽰误差信息的反向传播。 现在引⼊⼀个有⽤的记号 \\delta_{j}\\equiv\\frac{\\partial{E_n}}{\\partial{a_{j}}}\\tag{5.40}其中 $\\delta$ 通常被称为误差（error），使⽤公式(5.38)，有 \\frac{\\partial{a_j}}{\\partial{w_{ji}}}=z_i继而有 \\frac{\\partial{E_n}}{\\partial{w_{ji}}}=\\delta_{j}z_{i}\\tag{5.41}从而可知，要找的导数可以通过简单地将权值输出单元的 $\\delta$ 值与权值输⼊端的 $z$ 值相乘的⽅式得到（对于偏置的情形，$z = 1$ ）。 \\delta_{k}=y_k-t_k\\tag{5.42}为了计算隐含单元的 $\\delta$ 值，使⽤偏导数的链式法则 \\delta_j\\equiv\\frac{\\partial{E_n}}{\\partial{a_{j}}}=\\sum_{k}\\frac{\\partial{E_n}}{\\partial{a_{k}}}\\frac{\\partial{a_k}}{\\partial{a_{j}}}其中求和式的作⽤对象是所有向单元 $j$ 发送链接的单元 $k$ 。注 意，单元 $k$ 可以包含其他的隐含单元和（或）输出单元。$a_j$ 的改变所造成的误差函数的改变的唯⼀来源是变量 $a_k$ 的改变。经计算可得，反向传播（backpropagation）公式 \\delta_{j}=h^{\\prime}(a_j)\\sum_{k}w_{kj}\\delta_{k}\\tag{5.43}这表明，⼀个特定的隐含单元的 $\\delta$ 值可以通过将⽹络中更⾼层单元的 $\\delta$ 进⾏反向传播来实现。 反向传播算法可以总结如下： 1）对于⽹络的⼀个输⼊向量 $\\boldsymbol{x}_n$ ，使⽤公式(5.38)和公式(5.39)进⾏正向传播，找到所有隐含单元和输出单元的激活；2） 使⽤公式(5.42)计算所有输出单元的 $\\delta_k$ ；3）使⽤公式(5.43)反向传播 $\\delta$ ，获得⽹络中所有隐含单元的 $\\delta_j$ ；4）使⽤公式(5.41)计算导数。 对于批处理⽅法， 总误差函数 $E$ 的导数可以通过下⾯的⽅式得到：对于训练集⾥的每个模式，重复上⾯的步骤，然后对所有的模式求和，即 \\frac{\\partial{E}}{\\partial{w_{ji}}}=\\sum_{n}\\frac{\\partial{E_n}}{\\partial{w_{ji}}}\\tag{5.44}2，⼀个简单的例⼦考虑简单的两层神经⽹络，误差函数为平⽅和误差函数，输出单元的激活函数为线性激活函数，即 $y_k=a_k$ ，⽽隐含单元的激活函数为 $S$ 形函数，形式为 h(a)\\equiv\\tanh(a)\\tag{5.45}其中， \\tanh(a)=\\frac{e^a-e^{-a}}{e^a+e^{-a}}此函数的⼀个有⽤的特征是：其导数可以表⽰成⼀个相当简单形式 h^{\\prime}(a)=1-h(a)^{2}考虑⼀个标准的平⽅和误差函数，即对于模式 $n$ ，误差为 E_n=\\frac{1}{2}\\sum_{k=1}^{K}(y_{k}-t_{k})^{2}\\tag{5.46}其中，对于⼀个特定的输⼊模式 $\\boldsymbol{x}_n$ ，$y_k$ 是输出单元 $k$ 的激活，$t_k$ 是对应的⽬标值。对于训练集⾥的每个模式，⾸先使⽤下⾯的公式组进⾏前向传播。 a_j=\\sum_{i=0}^{D}w_{ji}^{(1)}x_{i}\\\\ z_j=\\tanh(a_j)\\\\ y_k=\\sum_{j=0}^{M}w_{kj}^{(2)}z_j再使⽤下⾯的公式计算每个输出单元的 $\\delta$ 值。 \\delta_k=y_k-t_k\\tag{5.47}然后，使⽤下⾯的公式将这些值反向传播，得到隐含单元的 $\\delta$ 值。 \\delta_j=(1-z_{j}^{2})\\sum_{k=1}^{K}w_{kj}\\delta_{k}最后，关于第⼀层权值和第⼆层权值的导数为 \\frac{\\partial{E_n}}{\\partial{w_{ji}^{(1)}}}=\\delta_j x_i\\\\ \\frac{\\partial{E_n}}{\\partial{w_{kj}^{(2)}}}=\\delta_k z_j3，反向传播的效率计算误差函数导数的反向传播⽅法是使⽤有限差。⾸先让每个权值有⼀个扰动，然后使⽤下⾯的表达式来近似导数 \\frac{\\partial{E_n}}{\\partial{w_{ji}}}=\\frac{E_n(w_{ji}+\\epsilon)-E_n(w_{ji})}{\\epsilon}+O(\\epsilon)\\tag{5.48}其中 $\\epsilon\\ll1$ 。在软件仿真中，通过让 $\\epsilon$ 变⼩，对于导数的近似的精度可以提升，直到 $\\epsilon$ 过⼩，造成下溢问题。通过使⽤对称的中⼼差（central difference），有限差⽅法的精度可以极⼤地提⾼。 中⼼差的形式为 \\frac{\\partial{E_n}}{\\partial{w_{ji}}}=\\frac{E_n(w_{ji}+\\epsilon)-E_n(w_{ji}-\\epsilon)}{2\\epsilon}+O(\\epsilon^{2})\\tag{5.49}计算数值导数的⽅法的主要问题是，计算复杂度为 $O(W)$ 这⼀性质不再成⽴。每次正向传播需要 $O(W)$ 步，⽽⽹络中有 $W$ 个权值，每个权值必须被单独地施加扰动， 因此整体的时间复杂度为 $O(W^2)$ 。 4，Jacobian 矩阵考虑Jacobian矩阵的计算，它的元素的值是⽹络的输出关于输⼊的导数 J_{ki}\\equiv\\frac{\\partial{y_{k}}}{\\partial{x_{i}}}\\tag{5.50}其中，计算每个这样的导数时，其他的输⼊都固定。 如图5.10，模块化模式识别系统，其中Jacobian矩阵可以⽤来将误差信号从输出模块在系统中反向传播 到更早的模块。 假设我们想关于图5.10中的参数 $w$ ，最⼩化误差函数 $E$ 。误差函数的导数为 \\frac{\\partial{E}}{\\partial{w}}=\\sum_{k,j}\\frac{\\partial{E}}{\\partial{y_k}}\\frac{\\partial{y_k}}{\\partial{z_{j}}}\\frac{\\partial{z_j}}{\\partial{w}}其中，图5.10中的红⾊模块的Jacobian矩阵出现在中间项。 由于Jacobian矩阵度量了输出对于每个输⼊变量的改变的敏感性，因此它也允许与输⼊关联的任意已知的误差 $\\Delta{x_i}$ 在训练过的⽹络中传播，从⽽估计他们对于输出误差 $\\Delta{y_k}$ 的贡献。⼆者的关系为 \\Delta{y_k}\\simeq\\sum_{i}\\frac{\\partial{y_k}}{\\partial{x_i}}\\Delta{x_i}\\tag{5.51}Jacobian矩阵可以使⽤反向传播的⽅法计算，计算⽅法类似于之前推导误差函数关于权值的导数的⽅法。⾸先，把元素 $J_{ki}$ 写成下⾯的形式 \\begin{aligned}J_{ki}=\\frac{\\partial{y_k}}{\\partial{x_i}}&=\\sum_{j}\\frac{\\partial{y_k}}{\\partial{a_j}}\\frac{\\partial{a_j}}{\\partial{x_i}}\\\\&=\\sum_{j}w_{ji}\\frac{\\partial{y_k}}{\\partial{a_j}}\\end{aligned}\\tag{5.52}其中求和式作⽤于所有单元 $i$ 发送链接的单元 $j$ 上 。现在⼀个递归的反向传播公式来确定导数 $\\frac{\\partial{y_k}}{\\partial{a_j}}$ 。 \\begin{aligned}\\frac{\\partial{y_k}}{\\partial{a_j}}&=\\sum_{l}\\frac{\\partial{y_k}}{\\partial{a_l}}\\frac{\\partial{a_l}}{\\partial{a_j}}\\\\&=h^{\\prime}(a_j)\\sum_{j}w_{lj}\\frac{\\partial{y_k}}{\\partial{a_l}}\\end{aligned}其中求和的对象为所有单元 $j$ 发送链接的单元 $l$（对应于 $w_{lj}$ 的第⼀个下标）。如果对于每个输出单元，都有各⾃的sigmoid函数，那么 \\frac{\\partial{y_k}}{\\partial{a_l}}=\\delta_{kl}\\sigma^{\\prime}(a_l)\\tag{5.53}对于softmax输出，有 \\frac{\\partial{y_k}}{\\partial{a_l}}=\\delta_{kl}y_{k}-y_{k}y_{l}\\tag{5.54}计算Jacobian矩阵的⽅法总结：将输⼊空间中要寻找Jacobian矩阵的点映射成⼀个输⼊向量，将这个输⼊向量作为⽹络的输⼊，使⽤通常的正向传播⽅法，得到⽹络的所有隐含单元和输出单元的激活。然后，对于Jacobian矩阵的每⼀⾏ $k$ （对应于输出单元 $k$ ），使⽤递归关系进⾏反向传播。对于⽹络中所有的隐含结点，反向传播开始于公式(5.53)和公式(5.54)。 最后， 使⽤公式(5.52)进⾏对输⼊单元的反向传播。 Jacobian矩阵的另⼀种计算⽅法是正向传播算法，它可以使⽤与这⾥给出的反向传播算法相类似的⽅式推导出来。这个算法的执⾏可以通过下⾯的数值导数的⽅法检验正确性。 \\frac{\\partial{y_k}}{\\partial{x_i}}=\\frac{y_{k}(x_i+\\epsilon)-y_k(x_i-\\epsilon)}{2\\epsilon}+O(\\epsilon^{2})\\tag{5.55}对于⼀个有着 $D$ 个输⼊的⽹络来说，这种⽅法需要 $2D$ 次正向传播。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】概率判别式模型","slug":"【机器学习基础】概率判别式模型","date":"2019-10-10T15:29:58.000Z","updated":"2019-10-11T00:57:50.071Z","comments":true,"path":"2019/10/10/prml_04_03/","link":"","permalink":"https://zhangbc.github.io/2019/10/10/prml_04_03/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，概率判别式模型考察⼆分类问题，对于⼀⼤类的类条件概率密度 $p(\\boldsymbol{x}|\\mathcal{C}_k)$ 的选择， 类别 $\\mathcal{C}_1$ 后验概率分布可以写成作⽤于 $\\boldsymbol{x}$ 的线性函数上的logistic sigmoid函数的形式。类似地，对于多分类的情形，类别 $\\mathcal{C}_k$ 的后验概率由 $\\boldsymbol{x}$ 的线性函数的softmax变换给出。对于类条件概率密度 $p(\\boldsymbol{x}|\\mathcal{C}_k)$ 的具体的选择， 我们已经使⽤了最⼤似然⽅法估计了概率密度的参数以及类别先验 $p(\\mathcal{C}_k)$ ，然后使⽤贝叶斯定理就可以求出后验类概率。寻找⼀般的线性模型参数的间接⽅法是，分别寻找类条件概率密度和类别先验，然后使⽤贝叶斯定理。 1，固定基函数考虑直接对输⼊向量 $(x)$ 进⾏分类的分类模型，然⽽，如果⾸先使⽤⼀个基函数向量 $\\boldsymbol{\\phi}(\\boldsymbol{x})$ 对输⼊变量进⾏⼀个固定的⾮线性变换，所有的这些算法仍然同样适⽤，最终的决策边界在特征空间 $\\boldsymbol{\\phi}$ 中是线性的，因此对应于原始 $\\boldsymbol{x}$ 空间中的⾮线性决策边界。在特征空间 $\\boldsymbol{\\phi}(\\boldsymbol{x})$ 线性可分的类别未必在原始的观测空间 $\\boldsymbol{x}$ 中线性可分，基函数中的某⼀个通常设置为常数，例如 $\\phi_{0}(\\boldsymbol{x})=1$ ，使得对应的参数 $w_0$ 扮演偏置的作⽤。 2，logistic回归考虑⼆分类问题在⼀般的假设条件下，类别 $\\mathcal{C}_1$ 的后验概率可以写成作⽤在特征向量 $\\boldsymbol{\\phi}$ 的线性函数上的logistic sigmoid函数的形式，即 p(\\mathcal{C}_1|\\boldsymbol{\\phi})=y(\\boldsymbol{\\phi})=\\sigma(\\boldsymbol{w}^T\\boldsymbol{\\phi})\\tag{4.55}且 $p(\\mathcal{C}_2|\\boldsymbol{\\phi})=1-p(\\mathcal{C}_1|\\boldsymbol{\\phi})$ ， $\\sigma(·)$ 是logistic sigmoid函数。使⽤统计学的术语，这个模型被称为 logistic回归 ，特别注意，这是⼀个分类模型⽽不是回归模型。对于⼀个 $M$ 维特征空间 $\\boldsymbol{\\phi}$ ，这个模型有 $M$ 个可调节参数。 现在使⽤最⼤似然⽅法来确定logistic回归模型的参数。使⽤logistic sigmoid函数的导数 \\frac{\\mathrm{d}\\sigma}{\\mathrm{d}a}=\\sigma(1-\\sigma)\\tag{4.56}对于⼀个数据集 $\\boldsymbol{\\phi}_n$ , $t_n$ ，其中 $t_n\\in\\{0,1\\}$ 且 $\\boldsymbol{\\phi}_n=\\boldsymbol{\\phi}(\\boldsymbol{x}_n)$ ，并且 $n=1,\\dots,N$，似然函数可以写成 p(\\mathbf{t}|\\boldsymbol{w})=\\prod_{n=1}^{N}y_{n}^{t_n}\\{1-y_n\\}^{1-t_n}\\tag{4.57}其中 $\\mathbf{t} = (t_1,\\dots,t_N)^T$ 且 $y_n=p(\\mathcal{C}_1|\\boldsymbol{\\phi}_n)$ 。通过取似然函数的负对数的⽅式，定义⼀个误差函数，这种⽅式产⽣了交叉熵（cross-entropy）误差函数，形式为 E(\\boldsymbol{w})=-\\ln p(\\mathbf{t}|\\boldsymbol{w}) = -\\sum_{n=1}^{N}\\{t_n\\ln y_{n}+(1-t_n)\\ln(1-y_n)\\}\\tag{4.58}其中 $y_n=\\sigma(a_n)$ 且 $a_n=\\boldsymbol{w}^{T}\\boldsymbol{\\phi}_n$ 。两侧关于 $\\boldsymbol{w}$ 取误差函数的梯度，有 \\nabla E(\\boldsymbol{w})= -\\sum_{n=1}^{N}(y_n-t_n)\\boldsymbol{\\phi}_n\\tag{4.59}3，迭代重加权最⼩平⽅误差函数可以通过⼀种⾼效的迭代⽅法求出最⼩值，这种迭代⽅法基于Newton-Raphson迭代最优化框架， 使⽤了对数似然函数的局部⼆次近似。为了最⼩化函数 $E(\\boldsymbol{w})$ ，Newton-Raphson对权值的更新形式为（Fletcher, 1987; Bishop and Nabney, 2008） \\boldsymbol{w}^{新}=\\boldsymbol{w}^{旧}-\\boldsymbol{H}^{-1}\\nabla E(\\boldsymbol{w})\\tag{4.60}其中 $\\boldsymbol{H}$ 是⼀个 Hessian矩阵，它的元素由 $E(\\boldsymbol{w})$ 关于 $\\boldsymbol{w}$ 的⼆阶导数组成。 ⾸先，把Newton-Raphson⽅法应⽤到线性回归模型上，误差函数为平⽅和误差函数。这个误差函数的梯度和Hessian矩阵为 \\nabla E(\\boldsymbol{w})=\\sum_{n=1}^{N}(\\boldsymbol{w}^{T}\\boldsymbol{\\phi}_n-t_n)\\boldsymbol{\\phi}_n=\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{w}-\\boldsymbol{\\Phi}^{T}\\mathbf{t}\\tag{4.61} \\boldsymbol{H}=\\nabla\\nabla E(\\boldsymbol{w})=\\sum_{n=1}^{N}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_{n}^{T}=\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\tag{4.62}其中 $\\boldsymbol{\\Phi}$ 是 $N \\times M$ 矩阵，第 $n$ ⾏为 $\\boldsymbol{\\phi}_{n}^{T}$ 。于是，Newton-Raphson更新形式为 \\begin{aligned}\\boldsymbol{w}^{新}&=\\boldsymbol{w}^{旧}-(\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1}\\{\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{w}^{旧}-\\boldsymbol{\\Phi}^{T}\\mathbf{t}\\}\\\\&=(\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^{T}\\mathbf{t}\\end{aligned}\\tag{4.63}这是标准的最⼩平⽅解。 现在，把Newton-Raphson更新应⽤到logistic回归模型的交叉熵误差函数上。这个误差函数的梯度和Hessian矩阵为 \\nabla E(\\boldsymbol{w})=\\sum_{n=1}^{N}(y_n-t_n)\\boldsymbol{\\phi}_n=\\boldsymbol{\\Phi}^{T}(\\mathbf{y}-\\mathbf{t})\\tag{4.64} \\boldsymbol{H}=\\nabla\\nabla E(\\boldsymbol{w})=\\sum_{n=1}^{N}y_n(1-y_n)\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_{n}^{T}=\\boldsymbol{\\Phi}^{T}\\boldsymbol{R}\\boldsymbol{\\Phi}\\tag{4.65}其中，引⼊了⼀个 $N \\times N$ 的对⾓矩阵 $\\boldsymbol{R}$ ，元素为 R_{nn}=y_n(1-y_n)\\tag{4.66}由此可见，Hessian矩阵不再是常量，⽽是通过权矩阵 $\\boldsymbol{R}$ 依赖于 $\\boldsymbol{w}$ 。对于任意向量 $\\boldsymbol{\\mu}$ 都有 $\\boldsymbol{\\mu}^{T}\\boldsymbol{H}\\boldsymbol{\\mu}&gt;0$ ， 因此Hessian矩阵 $\\boldsymbol{H}$ 是正定的，误差函数是 $\\boldsymbol{w}$ 的⼀个凸函数， 从 ⽽有唯⼀的最⼩值。 logistic回归模型的Newton-Raphson更新公式就变成了 \\begin{aligned}\\boldsymbol{w}^{新}&=\\boldsymbol{w}^{旧}-(\\boldsymbol{\\Phi}^{T}\\boldsymbol{R}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^{T}(\\mathbf{y}-\\mathbf{t})\\\\&=(\\boldsymbol{\\Phi}^{T}\\boldsymbol{R}\\boldsymbol{\\Phi})^{-1}\\{\\boldsymbol{\\Phi}^{T}\\boldsymbol{R}\\boldsymbol{\\Phi}\\boldsymbol{w}^{旧}-\\boldsymbol{\\Phi}^{T}(\\mathbf{y}-\\mathbf{t})\\}\\\\&=(\\boldsymbol{\\Phi}^{T}\\boldsymbol{R}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^{T}\\boldsymbol{R}\\mathbf{z}\\end{aligned}\\tag{4.67}其中 $\\mathbf{z}$ 是⼀个 $N$ 维向量，元素为 \\mathbf{z}=\\boldsymbol{\\Phi}\\boldsymbol{w}^{旧}-\\boldsymbol{R}^{-1}(\\mathbf{y}-\\mathbf{t})更新公式(4.67)的形式为⼀组加权最⼩平⽅问题的规范⽅程，由于权矩阵 $\\boldsymbol{R}$ 不是常量，⽽是依赖于参数向量 $\\boldsymbol{w}$ ， 因此必须迭代地应⽤规范⽅程， 每次使⽤新的权向量 $\\boldsymbol{w}$ 计算⼀个修正的权矩阵 $\\boldsymbol{R}$ ，这个算法被称为迭代重加权最⼩平⽅（iterative reweighted least squares）， 或者简称为 IRLS（Rubin, 1983）。 对角矩阵 $\\boldsymbol{R}$ 可以看成⽅差，因为logistic回归模型的 $t$ 的均值和⽅差为 \\mathbb{E}[t]=\\sigma(\\boldsymbol{x})=y\\tag{4.68} \\text{var}[t]=\\mathbb{E}[t^2]-\\mathbb{E}[t]^2=\\sigma(\\boldsymbol{x})-\\sigma(\\boldsymbol{x})^2=y(1-y)\\tag{4.69}事实上， 可以把 IRLS 看成变量空间 $a=\\boldsymbol{w}^{T}\\boldsymbol{\\phi}$ 的线性问题的解。这样，$\\mathbf{z}$ 的第 $n$ 个元素 $z_n$ 就可以简单地看成这个空间中的有效的⽬标值。$z_n$ 可以通过对当前操作点 $\\boldsymbol{w}^{旧}$ 附近的logistic sigmoid函数的局部线性近似的⽅式得到。 \\begin{aligned}a_n(\\boldsymbol{w}) &\\simeq a_n(\\boldsymbol{w}^{旧})+\\frac{\\mathrm{d}a_n}{\\mathrm{d}y_n}\\Bigg{|}_{\\boldsymbol{w}^{旧}}(t_n-y_n)\\\\&=\\boldsymbol{\\phi}_{n}^{T}\\boldsymbol{w}^{旧}-\\frac{y_n-t_n}{y_n(1-y_n)}=z_n\\end{aligned}\\tag{4.70}4，多类logistic回归对于⼀⼤类概率分布来说，后验概率由特征变量的线性函数的softmax变换给出，即 p(\\mathcal{C}_k|\\boldsymbol{\\phi})=y_k(\\boldsymbol{\\phi})=\\frac{\\exp(a_k)}{\\sum_{j}\\exp(a_j)}\\tag{4.71}其中，“激活” $a_k$ 为 a_k=\\boldsymbol{w}_{k}^{T}\\boldsymbol{\\phi}$y_k$ 关于所有激活 $a_j$ 的导数为 \\frac{\\partial y_k}{\\partial a_j}=y_k(\\boldsymbol{I}_{kj}-y_j)\\tag{4.72}其中 $\\boldsymbol{I}_{kj}$ 为单位矩阵的元素。 使⽤“1-of-K”表达⽅式计算似然函数。这种表达⽅式中，属于类别 $\\mathcal{C}_k$ 的特征向量 $\\boldsymbol{\\phi}_k$ 的⽬标向量 $\\mathbf{t}_n$ 是⼀个⼆元向量，这个向量的第 $k$ 个元素等于1，其余元素都等于0。从⽽，似然函数为 p(\\boldsymbol{T}|\\boldsymbol{w}_1,\\dots,\\boldsymbol{w}_K)=\\prod_{n=1}^{N}\\prod_{k=1}^{K}p(\\mathcal{C}_k|\\boldsymbol{\\phi}_n)^{t_{nk}}=\\prod_{n=1}^{N}\\prod_{k=1}^{K}y_{nk}^{t_{nk}}\\tag{4.73}其中 $y_{nk}=y_k(\\boldsymbol{\\phi}_n)$ ，$\\boldsymbol{T}$ 是⽬标变量的⼀个 $N \\times K$ 的矩阵，元素为 $t_nk$ 。取负对数，可得 E(\\boldsymbol{w}_1,\\dots,\\boldsymbol{w}_K)=-\\ln p(\\boldsymbol{T}|\\boldsymbol{w}_1,\\dots,\\boldsymbol{w}_K)=-\\sum_{n=1}^{N}\\sum_{k=1}^{K}t_{nk}\\ln y_{nk}\\tag{4.74}被称为多分类问题的交叉熵（cross-entropy）误差函数。 现在取误差函数关于参数向量 $\\boldsymbol{w}_j$ 的梯度。利⽤softmax函数的导数，有 \\nabla_{\\boldsymbol{w}_j} E(\\boldsymbol{w}_1,\\dots,\\boldsymbol{w}_K)=\\sum_{n=1}^{N}(y_{nj}-t_{nj})\\boldsymbol{\\phi}_n\\tag{4.75}其中, $\\sum_{k}t_{nk}=1$ 。 为了找到⼀个批处理算法，再次使⽤Newton-Raphson更新来获得多类问题的对应的IRLS算法。这需要求出由⼤⼩为 $M \\times M$ 的块组成的Hessian矩阵，其中块 $i, j$ 为 \\nabla_{\\boldsymbol{w}_k}\\nabla_{\\boldsymbol{w}_j} E(\\boldsymbol{w}_1,\\dots,\\boldsymbol{w}_K)=\\sum_{n=1}^{N}y_{nk}(\\boldsymbol{I}_{kj}-y_{nj})\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_{n}^{T}\\tag{4.76}多类logistic回归模型的Hessian矩阵是正定的，因此误差函数有唯⼀的最⼩值。 5，probit回归考察⼆分类的情形，使⽤⼀般的线性模型的框架，即 p(t=1|a)=f(a)\\tag{4.77}其中 $a=\\boldsymbol{w}^{T}\\boldsymbol{\\phi}$ ，且 $f(·)$ 为激活函数。对于每个输 ⼊ $\\boldsymbol{\\phi}_n$ ，我们计算 $a_n=\\boldsymbol{w}^{T}\\boldsymbol{\\phi}_n$ ，然后按照下⾯的⽅式设置⽬标值 \\begin{cases}t_n=1, & 如果 a_n\\ge \\theta \\\\ t_n=0, & 其他情况\\end{cases}如果 $\\theta$ 的值从概率密度 $p(\\theta)$ 中抽取，那么对应的激活函数由累积分布函数给出 f(a)=\\int_{-\\infty}^{a}p(\\theta)\\mathrm{d}\\theta\\tag{4.78}假设概率密度 $p(\\theta)$ 是零均值、单位⽅差的⾼斯概率密度，对应的累积分布函数为 \\boldsymbol{\\Phi}(a)=\\int_{-\\infty}^{a}\\mathcal{N}(\\theta|0,1)\\mathrm{d}\\theta\\tag{4.79}这被称为 逆probit（inverse probit）函数。 如图4.17，概率分布 $p(\\theta)$ 的图形表⽰，这个概率分布⽤蓝⾊曲线标记出。这个分布由两个⾼斯分布混合⽽成，同时给出的还有它的累积密度函数 $f(a)$ ，⽤红⾊曲线表⽰。注意，蓝⾊曲线上的任意⼀点，例如垂直绿⾊直线标记出的点，对应于红⾊曲线在相同⼀点处的斜率。相反，红⾊曲线在这点上的值对应于蓝⾊曲线下⽅的绿⾊阴影的⾯积。 erf函数，或者被称为 error函数 \\text{erf}(a)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{a}\\exp(-\\theta^{2})\\mathrm{d}\\theta\\tag{4.80}与逆probit函数的关系为 \\boldsymbol{\\Phi}(a)=\\frac{1}{2}\\left\\{1+\\text{erf}\\left(\\frac{a}{\\sqrt{2}}\\right)\\right\\}\\tag{4.81}基于probit激活函数的⼀般的线性模型被称为 probit回归 。在实际应⽤中经常出现的⼀个问题是离群点，它可能由输⼊向量 $\\boldsymbol{x}$ 的测量误差产⽣，或者由⽬标值 $t$ 的错误标记产⽣。 由于这些点可以位于错误的⼀侧中距离理想决策边界相当远的位置上，因此他们会严重地⼲扰分类器。注意，在这⼀点上，logistic回归模型与probit回归模型的表现不同， 因为对于 $x \\to \\infty$ ，logistic sigmoid函数像 $\\exp(−x)$ 那样渐进地衰减， ⽽probit激活函数像 $\\exp(−x^2)$ 那样衰减，因此probit模型对于离群点会更加敏感。 引⼊⼀个概率 $\\epsilon$ ，它是⽬标值 $t$ 被翻转到错误值的概率（Opper and Winther, 2000a）。这时，数据点 $\\boldsymbol{x}$ 的⽬标值的分布为 \\begin{aligned}p(t|\\boldsymbol{x})&=(1-\\epsilon)\\sigma(\\boldsymbol{x})+\\epsilon(1-\\sigma(\\boldsymbol{x}))\\\\&=\\epsilon+(1-2\\epsilon)\\sigma(\\boldsymbol{x})\\end{aligned}\\tag{4.82}其中 $\\sigma(\\boldsymbol{x})$ 是输⼊向量 $\\boldsymbol{x}$ 的激活函数。这⾥， $\\epsilon$ 可以事先设定，也可以被当成超参数，然后从数据中推断它的值。 6，标准链接函数把指数族分布的假设应⽤于⽬标变量 $t$ ，⽽不是应⽤于输⼊向量 $\\boldsymbol{x}$ 。考虑⽬标变量的条件分布 p(t|\\eta,s)=\\frac{1}{s}h(\\frac{t}{s})g(\\eta)\\exp\\left\\{\\frac{\\eta t}{s}\\right\\}\\tag{4.83}$t$ 的条件均值（记作 $y$ ）为 y\\equiv \\mathbb{E}[t|\\eta]=-s\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\ln g(\\eta)\\tag{4.84}因此 $y$ 和 $\\eta$ ⼀定相关，记作 $\\eta=\\psi(y)$ 。 按照Nelder and Wedderburn（1972）的⽅法，我们将⼀般线性模型（generalised linear model）定义为这样的模型：$y$ 是输⼊变量（或者特征变量）的线性组合的⾮线性函数，即 y=f(\\boldsymbol{w}^{T}\\boldsymbol{\\phi})\\tag{4.85}其中 $f(·)$ 在机器学习的⽂献中被称为激活函数（activation function），$f^{-1}(·)$ 在统计学中被称为链接函数（link function）。 现在考虑这个模型的对数似然函数。它是 $\\eta$ 的⼀个函数，形式为 \\ln p(\\mathbf{t}|\\eta,s)=\\sum_{n=1}^{N}\\ln p(t_n|\\eta,s)=\\sum_{n=1}^{N}\\left\\{\\ln g(\\eta_n)+\\frac{\\eta_n t_n}{s}\\right\\}+常数\\tag{4.86}其中假定所有的观测有⼀个相同的缩放参数（它对应着例如服从⾼斯分布的噪声的⽅差），因此 $s$ 与 $n$ ⽆关。对数似然函数关于模型参数 $\\boldsymbol{w}$ 的导数为 \\begin{aligned}\\nabla_{\\boldsymbol{w}}&=\\sum_{n=1}^{N}\\left\\{\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\ln g(\\eta_n)+\\frac{t_n}{s}\\right\\}\\frac{\\mathrm{d}\\eta_n}{\\mathrm{d}y_n}\\frac{\\mathrm{d}y_n}{\\mathrm{d}a_n}\\nabla a_n\\\\&=\\sum_{n=1}^{N}\\frac{1}{s}\\left\\{t_n-y_n\\right\\}\\psi^{\\prime}(y_n)f^{\\prime}(a_n)\\boldsymbol{\\phi}_n\\end{aligned}\\tag{4.87}其中 $a_n=\\boldsymbol{w}^{T}\\boldsymbol{\\phi}_n$ 。 令 f^{-1}(y)=\\psi(y)则误差函数的梯度可以化简为 \\nabla E(\\boldsymbol{w})=\\frac{1}{s}\\sum_{n=1}^{N}\\left\\{t_n-y_n\\right\\}\\boldsymbol{\\phi}_n\\tag{4.88}对于⾼斯分布，$s = \\beta^ {-1}$ ，⽽对于logistic模型，$s=1$ 。 二，拉普拉斯近似1，拉普拉斯近似拉普拉斯近似的⽬标是找到定义在⼀组连续变量上的概率密度的⾼斯近似。⾸先考虑单⼀连续变量 $z$ 的情形，假设分布 $p(z)$ 的定义为 p(z)=\\frac{1}{Z}f(z)\\tag{4.89}其中 $Z =\\int f(z)\\mathrm{d}z$ 是归⼀化系数。假定 $Z$ 的值是未知的，在拉普拉斯⽅法中，⽬标是寻找⼀个⾼斯近似 $q(z)$ ， 它的中⼼位于 $p(z)$ 的众数的位置。第⼀步是寻找 $p(z)$ 的众数， 即寻找⼀个点 $z_0$ 使得 $p^{\\prime}(z_0)=0$ ，或者等价地 \\frac{\\mathrm{d}f(z)}{\\mathrm{d}z}\\bigg{|}_{z=z_0}=0⾼斯分布有⼀个性质：它的对数是变量的⼆次函数。于是考虑 $\\ln f(z)$ 以众数 $z_0$ 为中⼼的泰勒展开，即 \\ln f(z)\\simeq \\ln f(z_0)-\\frac{1}{2}A(z-z_0)^2\\tag{4.90}其中， A=-\\frac{\\mathrm{d}^2}{\\mathrm{d}z^2}\\ln f(z)\\bigg{|}_{z=z_0}注意，泰勒展开式中的⼀阶项没有出现，因为 $z_0$ 是概率分布的局部最⼤值。两侧同时取指数， 有 f(z)\\simeq f(z_0)\\exp\\left\\{-\\frac{A}{2}(z-z_0)^2\\right\\}这样，使⽤归⼀化的⾼斯分布的标准形式，就可以得到归⼀化的概率分布 $q(z)$ ，即 q(z)=\\left(\\frac{A}{2\\pi}\\right)^{\\frac{1}{2}}\\exp\\left\\{-\\frac{A}{2}(z-z_0)^2\\right\\}\\tag{4.91}举例：应⽤于概率分布 $p(z) \\propto \\exp(−\\frac{z^2}{2})\\sigma(20z+4)$ 的拉普拉斯近似，其中 $\\sigma(z)$ 是logistic sigmoid函数，定义为 $\\sigma(z) = (1 + \\exp^{−z})^{-1}$ 。如图4.18，归⼀化的概率分布 $p(z)$，⽤黄⾊表⽰。同时给出了以 $p(z)$ 的众数 $z_0$ 为中⼼的拉普拉斯近似，⽤红⾊表⽰。 如图4.19，图4.18中对应的曲线的负对数。 将拉普拉斯⽅法推⼴，去近似定义在 $M$ 维空间 $\\boldsymbol{z}$ 上的概率分布 $p(\\boldsymbol{z}) =\\frac{f(\\boldsymbol{z})}{Z}$ 。在驻点 $\\boldsymbol{z}_0$ 处，梯度 $\\nabla f(\\boldsymbol{z})$ 将会消失。在驻点处展开，有 \\ln f(\\boldsymbol{z})\\simeq \\ln f(\\boldsymbol{z}_0)-\\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{z}_0)^{T}\\boldsymbol{A}(\\boldsymbol{z}-\\boldsymbol{z}_0)\\tag{4.92}其中 $M \\times M$ 的Hessian矩阵 $\\boldsymbol{A}$ 的定义为 \\boldsymbol{A}=-\\nabla\\nabla\\ln f(\\boldsymbol{z})|_{\\boldsymbol{z}=\\boldsymbol{z}_0}其中 $\\nabla$ 为梯度算⼦。两边同时取指数，有 f(\\boldsymbol{z})\\simeq f(\\boldsymbol{z}_0)\\exp\\left\\{-\\frac{1}{2}(z-z_0)^{T}\\boldsymbol{A}(\\boldsymbol{z}-\\boldsymbol{z}_0)\\right\\}分布 $q(\\boldsymbol{z})$ 正⽐于 $f(\\boldsymbol{z})$ ，归⼀化系数可以通过观察归⼀化的多元⾼斯分布的标准形式得到。因此 q(\\boldsymbol{z})=\\frac{|\\boldsymbol{A}|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{M}{2}}}\\exp\\left\\{-\\frac{1}{2}(z-z_0)^{T}\\boldsymbol{A}(\\boldsymbol{z}-\\boldsymbol{z}_0)\\right\\}=\\mathcal{N(\\boldsymbol{z}|\\boldsymbol{z}_0,\\boldsymbol{A}^{-1})}\\tag{4.93}其中 $|\\boldsymbol{A}|$ 是 $\\boldsymbol{A}$ 的⾏列式。这个⾼斯分布有良好定义的前提是，精度矩阵 $\\boldsymbol{A}$ 是正定的， 表明驻点 $\\boldsymbol{z}_0$ ⼀定是⼀个局部最⼤值，⽽不是⼀个最⼩值或者鞍点。 拉普拉斯近似的⼀个主要缺点是，由于它是以⾼斯分布为基础的，因此它只能直接应⽤于实值变量。拉普拉斯框架的最严重的局限性是，它完全依赖于真实概率分布在变量的某个具体值位置上的性质，因此会⽆法描述⼀些重要的全局属性。 2，模型⽐较和 BIC除了近似概率分布 $p(\\boldsymbol{z})$ ，也可以获得对归⼀化常数 $Z$ 的⼀个近似，有 \\begin{aligned}Z&=\\int f(\\boldsymbol{z})\\mathrm{d}\\boldsymbol{z}\\\\&\\simeq f(\\boldsymbol{z}_0)\\int\\exp\\left\\{-\\frac{1}{2}(z-z_0)^{T}\\boldsymbol{A}(\\boldsymbol{z}-\\boldsymbol{z}_0)\\right\\}\\mathrm{d}\\boldsymbol{z}\\\\&=f(\\boldsymbol{z}_0)\\frac{(2\\pi)^{\\frac{M}{2}}}{|\\boldsymbol{A}|^{\\frac{1}{2}}}\\end{aligned}\\tag{4.94}考虑⼀个数据集 $\\mathcal{D}$ 以及⼀组模型 $\\{\\mathcal{M}_i\\}$ ， 模型参数为 $\\{\\boldsymbol{\\theta}_i\\}$ 。 对于每个模型， 定义⼀个似然函数 $p(\\mathcal{D}|\\boldsymbol{\\theta}_i,\\mathcal{M}_i)$ 。如果引⼊⼀个参数的先验概率 $p(\\boldsymbol{\\theta}_i|\\mathcal{M}_i)$ ，那么感兴趣的是计算不同模型的模型证据 $p(\\mathcal{D}|\\mathcal{M}_i)$ 。为了简化记号，省略对于 $\\{\\mathcal{M}_i\\}$ 的条件依赖。 根据贝叶斯定理，模型证据为 p(\\mathcal{D})=\\int p(\\mathcal{D}|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}\\tag{4.95}令 $f(\\boldsymbol{\\theta})=p(\\mathcal{D}|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})$ 以及 $Z=p(\\mathcal{D})$，然后使⽤公式(4.94)，有 \\ln p(\\mathcal{D}) \\simeq \\ln p\\left(\\mathcal{D} | \\boldsymbol{\\theta}_{MAP}\\right)+\\underbrace{\\ln p\\left(\\boldsymbol{\\theta}_{MAP}\\right)+\\frac{M}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln |\\boldsymbol{A}|}_{\\text {Occam因子}}\\tag{4.96}其中 $\\boldsymbol{\\theta}_{MAP}$ 是在后验概率分布众数位置的 $\\boldsymbol{\\theta}$ 值，$\\boldsymbol{A}$ 是负对数后验概率的⼆阶导数组成的Hessian矩阵。 \\boldsymbol{A}=-\\nabla\\nabla\\ln p(\\mathcal{D}|\\boldsymbol{\\theta}_{MAP})p(\\boldsymbol{\\theta}_{MAP})=-\\nabla\\nabla\\ln p(\\boldsymbol{\\theta}_{MAP}|\\mathcal{D})公式(4.96)表⽰使⽤最优参数计算的对数似然值，⽽余下的三项由“ Occam因⼦ ”组成， 它对模型的复杂度进⾏惩罚。 如果假设参数的⾼斯先验分布⽐较宽，且Hessian矩阵是满秩的， 那么可以使⽤下式来⾮常粗略地近似公式(4.96) \\ln p(\\mathcal{D}) \\simeq \\ln p(\\mathcal{D} | \\boldsymbol{\\theta}_{MAP})-\\frac{1}{2}M\\ln N\\tag{4.97}其中 $N$ 是数据点的总数，$M$ 是 $\\boldsymbol{\\theta}$ 中参数的数量， 并且省略了⼀些额外的常数。 这被称为贝叶斯信息准则（Bayesian Information Criterion）（BIC）， 或者称为 Schwarz准则（Schwarz, 1978）。 历史上各种各样的“信息准则”被提出来，这些“信息准则”尝试修正最⼤似然的偏差。修正的⽅法是增加⼀个惩罚项来补偿过于复杂的模型造成的过拟合。 例如，⾚池信息准则（Akaike information criterion），或者简称为 AIC（Akaike, 1974），选择下⾯使这个量最⼤的模型： \\ln p(\\mathcal{D}|\\boldsymbol{w}_{ML})-M其中，$p(\\mathcal{D}|\\boldsymbol{w}_{ML})$ 是最合适的对数似然函数，$M$ 是模型中可调节参数的数量。 与 AIC 相比，BIC 对模型复杂度的惩罚更严重。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】概率生成式模型","slug":"【机器学习基础】概率生成式模型","date":"2019-10-10T15:03:30.000Z","updated":"2019-10-11T01:08:23.340Z","comments":true,"path":"2019/10/10/prml_04_02/","link":"","permalink":"https://zhangbc.github.io/2019/10/10/prml_04_02/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，概率生成式模型⾸先考虑⼆分类的情形。类别 $\\mathcal{C}_1$ 的后验概率可以写成 \\begin{aligned}p(\\mathcal{C}_1|\\boldsymbol{x})&=\\frac{p(\\boldsymbol{x}|\\mathcal{C}_1)p(\\mathcal{C}_1)}{p(\\boldsymbol{x}|\\mathcal{C}_1)p(\\mathcal{C}_1)+p(\\boldsymbol{x}|\\mathcal{C}_2)p(\\mathcal{C}_2)}\\\\&=\\frac{1}{1+\\exp(-a)}=\\sigma(a)\\end{aligned}\\tag{4.36}其中， a=\\ln\\frac{p(\\boldsymbol{x}|\\mathcal{C}_1)p(\\mathcal{C}_1)}{p(\\boldsymbol{x}|\\mathcal{C}_2)p(\\mathcal{C}_2)}$\\sigma(a)$ 称之为 logistic sigmoid函数 。 如图4.12，logistic sigmoid函数 $\\sigma(a)$ 的图像， ⽤红⾊表⽰，同时给出的是放缩后的逆probit函数 $\\Phi(\\lambda a)$ 的图像， 其中 $\\lambda^2=\\frac{\\pi}{8}$ ， ⽤蓝⾊曲线表⽰。 logistic sigmoid函数 在许多分类算法中都有着重要的作⽤，满⾜下⾯的对称性 \\sigma(-a)=1-\\sigma(a)\\tag{4.37}logistic sigmoid的反函数为 a=\\ln\\left(\\frac{\\sigma}{1-\\sigma}\\right)\\tag{4.38}被称为 logit函数。它表⽰两类的概率⽐值的对数 $\\ln[\\frac{p(\\mathcal{C}_1|\\boldsymbol{x})}{p(\\mathcal{C}_2|\\boldsymbol{x})}]$ ，也被称为 log odds函数 。 对于 $K &gt; 2$ 个类别的情形，有 \\begin{aligned}p(\\mathcal{C}_k|\\boldsymbol{x})&=\\frac{p(\\boldsymbol{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)}{\\sum_{j}p(\\boldsymbol{x}|\\mathcal{C}_j)p(\\mathcal{C}_j)}\\\\&=\\frac{\\exp(a_k)}{\\sum_{j}\\exp(a_j)}\\end{aligned}\\tag{4.39}被称为归⼀化指数（normalized exponential），也叫 softmax函数 ，可以被当做logistic sigmoid函数对于多类情况的推⼴。其中， $a_k$ 被定义为 a_k=\\ln p(\\boldsymbol{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)如果对于所有的 $j\\ne k$ 都有 $a_k \\gg a_j$ ，那么 $p(\\mathcal{C}_k|\\boldsymbol{x})\\simeq 1$ 且 $p(\\mathcal{C}_j|\\boldsymbol{x}) \\simeq 0$。 1，连续输⼊假设类条件概率密度是⾼斯分布，然后求解后验概率的形式。假定所有的类别的协⽅差矩阵相同，这样类别 $\\mathcal{C}_k$ 的类条件概率为 p(\\boldsymbol{x}|\\mathcal{C}_k)=\\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_k)^{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_k)\\right\\}\\tag{4.40}⾸先考虑两类的情形。根据公式(4.36)，有 p(\\mathcal{C}_1|\\boldsymbol{x})=\\sigma(\\boldsymbol{w}^{T}\\boldsymbol{x}+w_0)\\tag{4.41}其中， \\boldsymbol{w}=\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)\\\\ w_0=-\\frac{1}{2}\\boldsymbol{\\mu}_{1}^{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1}+\\frac{1}{2}\\boldsymbol{\\mu}_{2}^{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{2}+\\ln\\frac{p(\\mathcal{C}_1)}{p(\\mathcal{C}_2)}如图4.13，左图给出了两个类别的类条件概率密度，分别⽤红⾊和蓝⾊表⽰。 右图给出了对应的后验概率分布 $p(\\mathcal{C}_1|\\boldsymbol{x})$ ， 它由 $\\boldsymbol{x}$ 的线性函数的 logistic sigmoid 函数给出。 右图的曲⾯的颜⾊中， 红⾊所占的⽐例由 $p(\\mathcal{C}_1|\\boldsymbol{x})$ 给出，蓝⾊所占的⽐例由 $p(\\mathcal{C}_2|\\boldsymbol{x})=1-p(\\mathcal{C}_1|\\boldsymbol{x})$ 给出。 对于 $K$ 个类别的⼀般情形，根据公式(4.39)，有 a_k(\\boldsymbol{x})=\\boldsymbol{w}_{k}^{T}\\boldsymbol{x}+w_{k0}\\tag{4.42}其中， \\boldsymbol{w}_k=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{k}\\\\ w_{k0}=-\\frac{1}{2}\\boldsymbol{\\mu}_{k}^{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{k}+\\ln p(\\mathcal{C}_k)如图4.14，左图给出了三个类别的类条件概率密度，每个都是⾼斯分布，分别⽤红⾊、绿⾊、蓝⾊表⽰，其中红⾊和绿⾊的类别有相同的协⽅差矩阵。右图给出了对应的后验概率分布，其中 $RGB$ 的颜⾊向量表⽰三个类别各⾃的后验概率。决策边界也被画出。注意，具有相同协⽅差矩阵的红⾊类别和绿⾊类别的决策边界是线性的，⽽其他类别之间的类别的决策边界是⼆次的。 2，最⼤似然解⾸先考虑两类的情形，每个类别都有⼀个⾼斯类条件概率密度，且协⽅差矩阵相同。假设有⼀个数据集 $\\{\\boldsymbol{x}_n, t_n\\}$ ，其中 $n = 1,\\dots,N$ 。这⾥ $t_n = 1$ 表⽰类别 $\\mathcal{C}_1$ ，$t_n=0$ 表⽰类别 $\\mathcal{C}_2$ 。 把先验概率记作 $p(\\mathcal{C}_1)=\\pi$ ， 从⽽ $p(\\mathcal{C}_2)=1-\\pi$ 。 对于⼀个来⾃类别 $\\mathcal{C}_1$ 的数据点 $\\boldsymbol{x}_n$ ， 有 $t_n = 1$ ，因此 p(\\boldsymbol{x}_n,\\mathcal{C}_1)=p(\\mathcal{C}_1)p(\\boldsymbol{x}_n|\\mathcal{C}_1)=\\pi\\mathcal{N}(\\boldsymbol{x}_n|\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})类似地，对于类别 $\\mathcal{C}_2$ ，有 $t_n = 0$ ，因此 p(\\boldsymbol{x}_n,\\mathcal{C}_2)=p(\\mathcal{C}_2)p(\\boldsymbol{x}_n|\\mathcal{C}_2)=(1-\\pi)\\mathcal{N}(\\boldsymbol{x}_n|\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})于是似然函数为 p(\\mathbf{t},\\mathbf{X}|\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})=\\prod_{n=1}^{N}[\\pi\\mathcal{N}(\\boldsymbol{x}_n|\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})]^{t_n}[(1-\\pi)\\mathcal{N}(\\boldsymbol{x}_n|\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})]^{1-t_n}\\tag{4.43}其中，$\\mathbf{t}=(t_1,\\dots,t_n)^{T}$ 。 ⾸先考虑关于 $\\pi$ 的最⼤化，对数似然函数中与 $\\pi$ 相关的项为 \\sum_{n=1}^{N}\\{t_n\\ln\\pi+(1-\\pi)\\ln(1-\\pi)\\}令其关于 $\\pi$ 的导数等于零，整理，可得 \\pi=\\frac{1}{N}\\sum_{n=1}^{N}t_n=\\frac{N_1}{N}=\\frac{N_1}{N_1+N_2}\\tag{4.44}其中 $N_1$ 表⽰类别 $\\mathcal{C}_1$ 的数据点的总数，⽽ $N_2$ 表⽰类别 $\\mathcal{C}_2$ 的数据点总数。 现在考虑关于 $\\mu_1$ 的最⼤化，把对数似然函数中与 $\\mu_1$ 相关的量挑出来，即 \\sum_{n=1}^{N}t_n\\ln\\mathcal{N}(\\boldsymbol{x}_n|\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})=-\\frac{1}{2}\\sum_{n=1}^{N}t_n(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_1)^{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_1)+常数\\tag{4.45}令它关于 $\\mu_1$ 的导数等于零，整理可得 \\boldsymbol{\\mu}_1=\\frac{1}{N_1}\\sum_{n=1}^{N}t_n\\boldsymbol{x}_n\\tag{4.46}通过类似的推导，对应的 $\\mu_2$ 的结果为 \\boldsymbol{\\mu}_2=\\frac{1}{N_2}\\sum_{n=1}^{N}(1-t_n)\\boldsymbol{x}_n\\tag{4.47}最后，考虑协⽅差矩阵 $\\boldsymbol{\\Sigma}$ 的最⼤似然解。选出与 $\\boldsymbol{\\Sigma}$ 相关的项，有 \\begin{array}{lcl}-\\frac{1}{2}\\sum_{n=1}^{N}t_n\\ln|\\boldsymbol{\\Sigma}|-\\frac{1}{2}\\sum_{n=1}^{N}t_n(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_1)^{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_1) \\\\ -\\frac{1}{2}\\sum_{n=1}^{N}(1-t_n)\\ln|\\boldsymbol{\\Sigma}|-\\frac{1}{2}\\sum_{n=1}^{N}(1-t_n)(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_2)^{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_2) \\\\ =-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}|-\\frac{N}{2}\\text{Tr}\\{\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}\\}\\end{array}\\tag{4.48}其中， \\boldsymbol{S}=\\frac{N_1}{N}\\boldsymbol{S}_1+\\frac{N_2}{N}\\boldsymbol{S}_2 \\\\ \\boldsymbol{S}_1=\\frac{1}{N_1}\\sum_{n\\in\\mathcal{C}_1}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_1)(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_1)^{T}\\\\ \\boldsymbol{S}_2=\\frac{1}{N_2}\\sum_{n\\in\\mathcal{C}_2}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_2)(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_2)^{T}使⽤⾼斯分布的最⼤似然解的标准结果，我们看到 $\\boldsymbol{\\Sigma} = \\boldsymbol{S}$ ，它表⽰对⼀个与两类都有关系的协⽅差矩阵求加权平均。 3，离散特征⾸先考察⼆元特征值 $x_i\\in\\{0, 1\\}$ ，如果有 $D$ 个输⼊，那么⼀般的概率分布会对应于⼀个⼤⼩为 $2^D$ 的表格，包含 $2^D−1$ 个独⽴变量（由于要满⾜加和限制）。由于这会 随着特征的数量指数增长，因此我们想寻找⼀个更加严格的表⽰⽅法。这⾥，我们做出朴素贝叶斯（naive Bayes）的假设，这个假设中，特征值被看成相互独⽴的，以类别 $\\mathcal{C}_k$ 为条件。因此得到类条件分布，形式为 p(\\boldsymbol{x}|\\mathcal{C}_k)=\\prod_{i=1}^{D}\\mu_{ki}^{x_i}(1-\\mu_{ki})^{1-x_i}\\tag{4.49}其中对于每个类别，都有 $D$ 个独⽴的参数，有 a_k(\\boldsymbol{x})=\\sum_{i=1}^{D}\\{x_i\\ln\\mu_{ki}+(1-x_i)\\ln(1-\\mu_{ki})\\}+\\ln p(\\mathcal{C}_k)\\tag{4.50}这是输⼊变量 $x_i$ 的线性函数。 4，指数族分布使⽤指数族分布的形式，可以看到 $\\boldsymbol{x}$ 的分布可以写成 p(\\boldsymbol{x}|\\boldsymbol{\\lambda}_k)=h(\\boldsymbol{x})g(\\boldsymbol{\\lambda}_k)\\exp\\{\\boldsymbol{\\lambda}_{k}^{T}\\boldsymbol{\\mu}(\\boldsymbol{x})\\}\\tag{4.51}现在把注意⼒集中在 $\\boldsymbol{\\mu}(\\boldsymbol{x})=\\boldsymbol{x}$ 这种分布上，引⼊⼀个缩放参数 $s$，这样就得到了指数族类条件概率分布的⼀个⼦集 p(\\boldsymbol{x}|\\boldsymbol{\\lambda}_k,s)=\\frac{1}{s}h(\\frac{1}{s}\\boldsymbol{x})g(\\boldsymbol{\\lambda}_k)\\exp\\left\\{\\frac{1}{s}\\boldsymbol{\\lambda}_{k}^{T}\\boldsymbol{x}\\right\\}\\tag{4.52}注意让每个类别有⾃⼰的参数向量 $\\boldsymbol{\\lambda}_k$ ，并且假定各个类别有同样的缩放参数 $s$ 。 对于⼆分类问题，把这个类条件概率密度的表达式代⼊相关公式，计算后可得后验概率是⼀个作⽤在线性函数 $a(\\boldsymbol{x})$ 上的logistic sigmoid函数。$a(\\boldsymbol{x})$ 的形式为 a(\\boldsymbol{x})=\\frac{1}{s}(\\boldsymbol{\\lambda}_1-\\boldsymbol{\\lambda}_2)^{T}\\boldsymbol{x}+\\ln g(\\boldsymbol{\\lambda}_1)-\\ln g(\\boldsymbol{\\lambda}_2)+\\ln p(\\mathcal{C}_1)-\\ln p(\\mathcal{C}_2)\\tag{4.53}类似地，对于 $K$ 类问题，有 a_k(\\boldsymbol{x})=\\frac{1}{s}(\\boldsymbol{\\lambda}_k)^{T}\\boldsymbol{x}+\\ln g(\\boldsymbol{\\lambda}_k)+\\ln p(\\mathcal{C}_k)\\tag{4.54}这又是⼀个 $\\boldsymbol{x}$ 的线性函数。 如图4.15，线性分类模型的⾮线性基函数的作⽤的说明。下图给出了原始的输⼊空间 $(x_1,x_2)$ 以及标记为红⾊和蓝⾊的数据点。这个空间中定义了两个“⾼斯”基函数 $\\phi_1(\\boldsymbol{x})$ 和 $\\phi_2(\\boldsymbol{x})$ ，中⼼⽤绿⾊⼗字表⽰，轮廓线⽤绿⾊圆形表⽰。 如图4.16，对应图4.15中的特征空间 $(\\phi_1,\\phi_2)$ 以及线性决策边界。 二，贝叶斯logistics回归对于logistic回归，精确的贝叶斯推断是⽆法处理的。特别地，计算后验概率分布需要对先验概率分布于似然函数的乘积进⾏归⼀化，⽽似然函数本⾝由⼀系列logistic sigmoid函数的乘积组成，每个数据点都有⼀个logistic sigmoid函数，对于预测分布的计算类似地也是⽆法处理的。 1， 拉普拉斯近似由于寻找后验概率分布的⼀个⾼斯表⽰，因此在开始的时候选择⾼斯先验是很⾃然的。故把⾼斯先验写成⼀般的形式 p(\\boldsymbol{w})=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{m}_0,\\boldsymbol{S}_0)\\tag{4.98}其中 $\\boldsymbol{m}_0$ 和 $\\boldsymbol{S}_0$ 是固定的超参数。 $\\boldsymbol{w}$ 的后验概率分布为 p(\\boldsymbol{w}|\\mathbf{t})\\propto p(\\boldsymbol{w})p(\\mathbf{t}|\\boldsymbol{w})其中 $\\mathbf{t} = (t_1,\\dots, t_N)^{T}$ 。两侧取对数，然后代⼊先验分布，对于似然函数，有 \\begin{aligned}\\ln p(\\boldsymbol{w}|\\mathbf{t}) &= -\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol{m}_0)^{T}\\boldsymbol{S}_{0}^{-1}(\\boldsymbol{w}-\\boldsymbol{m}_0)\\\\&+\\sum_{n=1}^{N}\\{t_n\\ln y_{n}+(1-t_n)\\ln(1-y_n)\\}+常数\\end{aligned}\\tag{4.99}其中 $y_n=\\sigma(\\boldsymbol{w}^{T}\\boldsymbol{\\phi}_n)$ 。为了获得后验概率的⾼斯近似，⾸先最⼤化后验概率分布，得到MAP（最⼤后验）解 $\\boldsymbol{w}_{WAP}$ ，定义了⾼斯分布的均值，这样协⽅差就是负对数似然函数的⼆阶导数矩阵的逆矩阵，形式为 \\boldsymbol{S}_{N}^{-1}=-\\nabla\\nabla \\ln p(\\boldsymbol{w}|\\mathbf{t})=\\boldsymbol{S}_{0}^{-1}+\\sum_{n=1}^{N}y_n(1-y_n)\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_{n}^{T}\\tag{4.100}后验概率分布的⾼斯近似的形式为 q(\\boldsymbol{w})=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{w}_{MAP},\\boldsymbol{S}_{N})\\tag{4.101}2， 预测分布给定⼀个新的特征向量 $\\boldsymbol{\\phi}(\\boldsymbol{x})$ ，类别 $\\mathcal{C}_1$ 的预测分布可以通过对后验概率 $p(\\boldsymbol{w} | \\mathbf{t})$ 积分，后验概率本⾝由⾼斯分布 $q(\\boldsymbol{w})$ 近似，即 p(\\mathcal{C}_1|\\boldsymbol{\\phi},\\mathbf{t})=\\int p(\\mathcal{C}_1|\\boldsymbol{\\phi},\\boldsymbol{w})p(\\boldsymbol{w}|\\mathbf{t})\\mathrm{d}\\boldsymbol{w}\\simeq \\int \\sigma(\\boldsymbol{w}^{T}\\boldsymbol{\\phi})q(\\boldsymbol{w})\\mathrm{d}\\boldsymbol{w}\\tag{4.102}且类别 $\\mathcal{C}_2$ 的对应的概率为 $p(\\mathcal{C}_2|\\boldsymbol{\\phi},\\mathbf{t})=1-p(\\mathcal{C}_1|\\boldsymbol{\\phi},\\mathbf{t})$ 。为了计算预测分布，⾸先注意到函数 $\\sigma(\\boldsymbol{w}^{T}\\boldsymbol{\\phi})$ 对于 $\\boldsymbol{w}$ 的依赖只通过它在 $\\boldsymbol{\\phi}$ 上的投影⽽实现。记 $a=\\boldsymbol{w}^{T}\\boldsymbol{\\phi}$，有 \\sigma(\\boldsymbol{w}^{T}\\boldsymbol{\\phi})=\\int \\delta(a-\\boldsymbol{w}^{T}\\boldsymbol{\\phi})\\sigma(a)\\mathrm{d}a其中 $\\delta(·)$ 是 狄拉克Delta函数 。由此有 \\int \\sigma(\\boldsymbol{w}^{T}\\boldsymbol{\\phi})q(\\boldsymbol{w})\\mathrm{d}\\boldsymbol{w}=\\int\\sigma(a)p(a)\\mathrm{d}a\\tag{4.103}其中， p(a)=\\int \\delta(a-\\boldsymbol{w}^{T}\\boldsymbol{\\phi})q(\\boldsymbol{w})\\mathrm{d}\\boldsymbol{w}计算 $p(a)$ ：注意到Delta函数给 $\\boldsymbol{w}$ 施加了⼀个线性限制，因此在所有与 $\\boldsymbol{\\phi}$ 正交的⽅向上积分，就得到了联合概率分布 $q(\\boldsymbol{w})$ 的边缘分布。通过计算各阶矩然后交换 $a$ 和 $\\boldsymbol{w}$ 的积分顺序的⽅式计算均值和协⽅差，即 \\mu_a=\\mathbb{E}[a]=\\int p(a)a\\mathrm{d}a=\\int q(\\boldsymbol{w})\\boldsymbol{w}^{T}\\boldsymbol{\\phi}\\mathrm{d}\\boldsymbol{w}=\\boldsymbol{w}_{WAP}^{T}\\boldsymbol{\\phi}\\tag{4.104} \\begin{aligned}\\sigma_{a}^{2}&=\\text{var}[a]=\\int p(a)\\{a^2-\\mathbb{E}[a]^{2}\\}\\mathrm{d}a\\\\&=\\int q(\\boldsymbol{w})\\{(\\boldsymbol{w}^{T}\\boldsymbol{\\phi})^{2}-(\\boldsymbol{m}_{N}^{T}\\boldsymbol{\\phi})^{2}\\}\\mathrm{d}\\boldsymbol{w}=\\boldsymbol{\\phi}^{T}\\boldsymbol{S}_{N}\\boldsymbol{\\phi}\\end{aligned}\\tag{4.105}注意，$a$ 的分布的函数形式与线性回归模型的预测分布相同， 其中噪声⽅差被设置为零。因此对于预测分布的近似变成了 p(\\mathcal{C}_1|\\mathbf{t})=\\int \\sigma(a)p(a)\\mathrm{d}a=\\int \\sigma(a)\\mathcal{N}(a|\\mu_{a},\\sigma_{a}^{2})\\mathrm{d}a\\tag{4.106}使⽤ 逆probit函数 的⼀个优势是：它与⾼斯的卷积可以⽤另⼀个逆probit函数解析地表⽰出来。 特别地，可以证明 \\int \\Phi(\\lambda a)\\mathcal{N}(a|\\mu,\\sigma^{2})\\mathrm{d}a=\\Phi\\left(\\frac{\\mu}{(\\lambda^{-2}+\\sigma^{2})^{\\frac{1}{2}}}\\right)\\tag{4.107}现在将逆probit函数的近似 $\\sigma(a)\\simeq\\Phi(\\lambda a)$ 应⽤于这个⽅程的两侧， 得到下⾯的对于logistic sigmoid函数与⾼斯的卷积近似 \\int \\sigma(a)\\mathcal{N}(a|\\mu,\\sigma^{2})\\mathrm{d}a\\simeq\\sigma\\left(\\kappa(\\sigma^{2})\\mu\\right)\\tag{4.108}其中， \\kappa(\\sigma^{2})=\\left(1+\\frac{\\pi\\sigma^{2}}{8}\\right)^{-\\frac{1}{2}}得到了近似的预测分布，形式为 p(\\mathcal{C}_1|\\boldsymbol{\\phi},\\mathbf{t})=\\sigma\\left(\\kappa(\\sigma_{a}^{2})\\mu_{a}\\right)\\tag{4.109}","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】判别函数","slug":"【机器学习基础】判别函数","date":"2019-10-09T14:36:47.000Z","updated":"2019-10-09T15:08:23.208Z","comments":true,"path":"2019/10/09/prml_04_01/","link":"","permalink":"https://zhangbc.github.io/2019/10/09/prml_04_01/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，分类线性模型概述分类的⽬标是将输⼊变量 $\\boldsymbol{x}$ 分到 $K$ 个离散的类别 $\\mathcal{C}_k$ 中的某⼀类。 最常见的情况是， 类别互相不相交， 因此每个输⼊被分到唯⼀的⼀个类别中。因此输⼊空间被划分为不同的决策区域（decision region），它的边界被称为决策边界（decision boundary）或者决策⾯（decision surface）。 分类线性模型是指决策⾯是输⼊向量 $\\boldsymbol{x}$ 的线性函数，因此被定义为 $D$ 维输⼊空间中的 $(D − 1)$ 维超平⾯。如果数据集可以被线性决策⾯精确地分类，那么我们说这个数据集是线性可分的（linearly separable）。 在线性回归模型中，使⽤⾮线性函数 $f(·)$ 对 $\\boldsymbol{w}$ 的线性函数进⾏变换，即 y(\\boldsymbol{x})=f(\\boldsymbol{w}^{T}\\boldsymbol{x}+w_0)\\tag{4.1}在机器学习的⽂献中，$f(·)$ 被称为激活函数（activation function），⽽它的反函数在统计学的⽂献中被称为链接函数（link function）。决策⾯对应于 $y(\\boldsymbol{x}) = 常数$，即 $\\boldsymbol{w}^{T}\\boldsymbol{x} + w_0 = 常数$，因此决策⾯是 $\\boldsymbol{x}$ 的线性函数，即使函数 $f(·)$ 是⾮线性函数也是如此。因此，由公式(4.1)描述的⼀类模型被称为推⼴的线性模型（generalized linear model）（McCullagh and Nelder, 1989）。 如图4.1，⼆维线性判别函数的⼏何表⽰。决策⾯（红⾊）垂直于 $\\boldsymbol{w}$ ，它距离原点的偏移量由偏置参数 $w_0$ 控制。 二，判别函数判别函数是⼀个以向量 $\\boldsymbol{x}$ 为输⼊，把它分配到 $K$ 个类别中的某⼀个类别（记作 $\\mathcal{C}_k$ ）的函数。 1，⼆分类线性判别函数的最简单的形式是输⼊向量的线性函数，即 y(\\boldsymbol{x})=\\boldsymbol{w}^{T}\\boldsymbol{x}+w_0\\tag{4.2}其中 $\\boldsymbol{w}$ 被称为权向量（weight vector），$w_0$ 被称为偏置（bias）。偏置的相反数有时被称为阈值（threshold）。 考虑两个点 $\\boldsymbol{x}_A$ 和 $\\boldsymbol{x}_B$ ，两个点都位于决策⾯上。 由于 $y(\\boldsymbol{x}_A)=y(\\boldsymbol{x}_B)=0$，我们有 $\\boldsymbol{w}^{T}(\\boldsymbol{x}_A-\\boldsymbol{x}_B) = 0$，因此向量 $\\boldsymbol{w}$ 与决策⾯内的任何向量都正交，从⽽ $\\boldsymbol{w}$ 确定了决策⾯的⽅向。类似地，如果 $\\boldsymbol{x}$ 是决策⾯内的⼀个点，那么 $y(\\boldsymbol{x}) = 0$ ，因此从原点到决策⾯的垂直距离为 \\frac{\\boldsymbol{w}^{T}\\boldsymbol{x}}{\\|\\boldsymbol{w}\\|}=-\\frac{w_0}{\\|\\boldsymbol{x}\\|}\\tag{4.3}其中，偏置参数 $\\boldsymbol{w}_0$ 确定了决策⾯的位置。 记任意⼀点 $\\boldsymbol{x}$ 到决策⾯的垂直距离 $r$ ，在决策⾯上的投影 $\\boldsymbol{x}_{\\perp}$ ，则有 \\boldsymbol{x}=\\boldsymbol{x}_{\\perp}+r \\frac{\\boldsymbol{w}}{\\|\\boldsymbol{w}\\|}\\tag{4.4}利用已知公式和 $y(\\boldsymbol{x}_{\\perp})=0$ 可得 r=\\frac{y(\\boldsymbol{x})}{\\|w\\|}\\tag{4.5}为方便简洁，引⼊“虚”输⼊ $x_0=1$ ，并且定义 $\\tilde{\\boldsymbol{w}} = (w_0,\\boldsymbol{w})$ 以及 $\\tilde{\\boldsymbol{x}} = (x_0,\\boldsymbol{x})$ ，从⽽ y(\\boldsymbol{x})=\\tilde{\\boldsymbol{w}}^{T}\\tilde{\\boldsymbol{x}}\\tag{4.6}在这种情况下， 决策⾯是⼀个 $D$ 维超平⾯， 并且这个超平⾯会穿过 $D+1$ 维扩展输⼊空间的原点。 2，多分类考虑把线性判别函数推⼴到 $K&gt;2$ 个类别。 方法一，使⽤ $K − 1$ 个分类器，每个分类器⽤来解决⼀个⼆分类问题，把属于类别 $\\mathcal{C}_k$ 和不属于那个类别的点分开。这被称为“1对其他”（one-versus-the-rest）分类器。此方法的缺点在于产⽣了输⼊空间中⽆法分类的区域。 方法二，引⼊ $\\frac{K(K−1)}{2}$ 个⼆元判别函数， 对每⼀对类别都设置⼀个判别函数。 这被称为“1对1”（one-versus-one）分类器。每个点的类别根据这些判别函数中的⼤多数输出类别确定，但是，这也会造成输⼊空间中的⽆法分类的区域。 如图4.2，尝试从⼀组两类的判别准则中构建出⼀个 $K$ 类的判别准则会导致具有奇异性的区域， ⽤绿⾊表⽰。 方法三，通过引⼊⼀个 $K$ 类判别函数，可以避免上述问题。这个 $K$ 类判别函数由 $K$ 个线性函数组成，形式为 y_{k}(\\boldsymbol{x})=\\boldsymbol{w}_{k}^{T}\\boldsymbol{x}+w_{k0}\\tag{4.7}对于点 $\\boldsymbol{x}$ ，如果对于所有的 $j \\ne k$ 都 有 $y_{k}(\\boldsymbol{x})\\gt y_{j}(\\boldsymbol{x})$ ，那么就把它分到 $\\mathcal{C}_k$ 。 于是类别 $\\mathcal{C}_k$ 和 $\\mathcal{C}_j$ 之间的决策⾯为 $y_{k}(\\boldsymbol{x})=y_{j}(\\boldsymbol{x})$，并且对应于⼀个 $(D − 1)$ 维超平⾯，形式为 (\\boldsymbol{w}_{k}-\\boldsymbol{w}_{j})^{T}\\boldsymbol{x}+(w_{k0}-w_{j0})=0\\tag{4.8}考虑两个点 $\\boldsymbol{x}_A$ 和 $\\boldsymbol{x}_B$ ，两个点都位于决策区域 $\\mathcal{R}_k$ 中， 任何位于连接 $\\boldsymbol{x}_A$ 和 $\\boldsymbol{x}_B$ 的线段上的点都可以表⽰成下⾯的形式 \\hat{\\boldsymbol{x}}=\\lambda \\boldsymbol{x}_{A}+(1-\\lambda)\\boldsymbol{x}_{B}\\tag{4.9}其中，$0\\le\\lambda\\le1$ 。根据判别函数的线性性质，有 y_{k}(\\hat{\\boldsymbol{x}})=\\lambda y_{k}(\\boldsymbol{x}_{A})+(1-\\lambda)y_{k}(\\boldsymbol{x}_{B})\\tag{4.10}由于 $\\boldsymbol{x}_A$ 和 $\\boldsymbol{x}_B$ 位于 $\\mathcal{R}_k$ 内部，因此对于所有 $j \\ne k$ ， 都有 $y_{k}(\\boldsymbol{x}_{A})\\gt y_{j}(\\boldsymbol{x}_{A})$ 以及 $y_{k}(\\boldsymbol{x}_{B})\\gt y_{j}(\\boldsymbol{x}_{B})$ ，因此 $y_{k}(\\hat{\\boldsymbol{x}})\\gt y_{j}(\\hat{\\boldsymbol{x}})$ ，从⽽ $\\hat{\\boldsymbol{x}}$ 也位于 $\\mathcal{R}_k$ 内部，即 $\\mathcal{R}_k$ 是单连通的并且是凸的。 如图4.3，多类判别函数的决策区域的说明， 决策边界⽤红⾊表⽰。 3，⽤于分类的最⼩平⽅⽅法每个类别 $\\mathcal{C}_k$ 由⾃⼰的线性模型描述，即公式(4.7)，其中 $k = 1, \\dots , K$ 。使⽤向量记号表⽰，即 \\boldsymbol{y}(\\boldsymbol{x})=\\tilde{\\boldsymbol{W}}^{T}\\tilde{\\boldsymbol{x}}\\tag{4.11}其中 $\\tilde{\\boldsymbol{W}}$ 是⼀个矩阵，第 $k$ 列由 $D + 1$ 维向量 $\\tilde{\\boldsymbol{w}}_k=(w_{k0},w_{k}^{T})^{T}$ 组成，$\\tilde{\\boldsymbol{x}}$ 是对应的增⼴输⼊向量 $(1, \\boldsymbol{x}^{T})^{T}$， 它带有⼀个虚输⼊ $x_0 = 1$ 。 现在通过最⼩化平⽅和误差函数来确定参数矩阵 $\\tilde{\\boldsymbol{W}}$ ，考虑⼀个训练数据集 $\\{\\boldsymbol{x}_n, \\boldsymbol{t}_n\\}$，其中 $n = 1,\\dots , N $，然后定义⼀个矩阵 $\\boldsymbol{T}$ ，它的第 $n$ ⾏是向量 $\\boldsymbol{t}_{n}^{T}$ ，定义⼀个矩阵 $\\tilde{\\boldsymbol{X}}$ ，它的第 $n$ ⾏是 $\\tilde{\\boldsymbol{x}}_{n}^{T}$ 。这样，平⽅和误差函数可以写成 E_{D}(\\tilde{\\boldsymbol{W}})=\\frac{1}{2}\\text{Tr}\\{(\\tilde{\\boldsymbol{X}}\\tilde{\\boldsymbol{W}}-\\boldsymbol{T})^{T}(\\tilde{\\boldsymbol{X}}\\tilde{\\boldsymbol{W}}-\\boldsymbol{T})\\}\\tag{4.12}令关于 $\\tilde{\\boldsymbol{W}}$ 的导数等于零，整理，可以得到 $\\tilde{\\boldsymbol{W}}$ 的解，形式为 \\tilde{\\boldsymbol{W}}=(\\tilde{\\boldsymbol{X}}^{T}\\tilde{\\boldsymbol{W}})^{-1}\\tilde{\\boldsymbol{X}}^{T}\\boldsymbol{T}=\\tilde{\\boldsymbol{X}}^{\\dagger}\\boldsymbol{T}\\tag{4.13}其中 $\\tilde{\\boldsymbol{X}}^{\\dagger}$ 是矩阵 $\\tilde{\\boldsymbol{X}}$ 的伪逆矩阵。即得判别函数，形式为 y(\\boldsymbol{x})=\\tilde{\\boldsymbol{W}}^{T}\\tilde{\\boldsymbol{x}}=\\boldsymbol{T}^{T}(\\tilde{\\boldsymbol{X}}^{\\dagger})^{T}\\tilde{\\boldsymbol{x}}\\tag{4.14}如图4.4，左图给出了来⾃两个类别的数据，⽤红⾊叉形和蓝⾊圆圈表⽰。同时给出的还有通过最⼩平⽅⽅法找到的决策边界（洋红⾊曲线）以及logistic回归模型给出的决策边界（绿⾊曲线）；右图给出了当额外的数据点被添加到左图的底部之后得到的结果，这表明最⼩平⽅⽅法对于异常点很敏感，这与logistic回归不同。 多⽬标变量的最⼩平⽅解的⼀个重要的性质是：如果训练集⾥的每个⽬标向量都满⾜某个线性限制 \\boldsymbol{a}^{T}\\boldsymbol{t}_{n}+b=0\\tag{4.15}其中 $\\boldsymbol{a}$ 和 $b$ 为常量，那么对于任何 $\\boldsymbol{x}$ 值，模型的预测也满⾜同样的限制，即 \\boldsymbol{a}^{T}\\boldsymbol{y}(\\boldsymbol{x})+b=0\\tag{4.16}因此如果使⽤ $K$ 分类的“1-of-K ”表达⽅式，那么这个模型做出的预测会具有下⾯的性质：对于任意的 $\\boldsymbol{x}$ 的值， $\\boldsymbol{y}(\\boldsymbol{x})$ 的元素的和等于1。 举例，由三个类别组成的⼈⼯数据集，训练数据点分别⽤红⾊（×）、绿⾊（+）、蓝⾊（◦）标出。 直线表⽰决策边界， 背景颜⾊表⽰决策区域代表的类别。如图4.5，使⽤最⼩平⽅判别函数，分配到绿⾊类别的输⼊空间的区域过⼩，⼤部分来⾃这个类别的点都被错误分类。 如图4.6，使⽤logistic回归的结果，给出了训练数据的正确分类情况。 4，Fisher线性判别函数假设有⼀个 $D$ 维输⼊向量 $\\boldsymbol{x}$ ，然后使⽤下式投影到⼀维 y=\\boldsymbol{w}^{T}\\boldsymbol{x}\\tag{4.17}如果在 $y$ 上设置⼀个阈值，然后把 $y\\ge -w_0$ 的样本分为 $\\mathcal{C}_1$ 类，把其余的样本分为 $\\mathcal{C}_2$ 类，那么就得到了一个标准的线性分类器。 考虑⼀个⼆分类问题，这个问题中有 $\\mathcal{C}_1$ 类的 $N_1$ 个点以及 $\\mathcal{C}_2$ 类的 $N_2$ 个点。因此两类的均值向量为 \\boldsymbol{m}_{1}=\\frac{1}{N_1}\\sum_{n\\in\\mathcal{C_1}}\\boldsymbol{x}_{n}\\\\ \\boldsymbol{m}_{2}=\\frac{1}{N_2}\\sum_{n\\in\\mathcal{C_2}}\\boldsymbol{x}_{n}如果投影到 $\\boldsymbol{w}$ 上，那么最简单的度量类别之间分开程度的⽅式就是类别均值投影之后的距离。这说明可以选择 $\\boldsymbol{w}$ 使得下式取得最⼤值 m_2-m_1=\\boldsymbol{w}^{T}(\\boldsymbol{m}_2-\\boldsymbol{m}_1)\\tag{4.18}其中， m_k=\\boldsymbol{w}^{T}\\boldsymbol{m}_{k}是来⾃类别 $\\mathcal{C}_k$ 的投影数据的均值。 如图4.7，左图给出了来⾃两个类别（表⽰为红⾊和蓝⾊）的样本，以及在连接两个类别的均值的直线上的投影的直⽅图。注意，在投影空间中，存在⼀个⽐较严重的类别重叠。右图给出的基于Fisher线性判别准则的对应投影，表明了类别切分的效果得到了极⼤的提升。 Fisher提出的思想是最⼤化⼀个函数，这个函数能够让类均值的投影分开得较⼤，同时让每个类别内部的⽅差较⼩，从⽽最⼩化了类别的重叠。 投影公式(4.17)将 $\\boldsymbol{x}$ 的⼀组有标记的数据点变换为⼀位空间 $y$ 的⼀组有标记数据点。来⾃类别 $\\mathcal{C}_k$ 的数据经过变换后的类内⽅差为 s_{k}^{2}=\\sum_{n\\in \\mathcal{C}_k}(y_n-m_k)^{2}\\tag{4.19}其中，$y_n=\\boldsymbol{w}^{T}\\boldsymbol{x}_{n}$ 。把整个数据集的总的类内⽅差定义为 $s_1^2+s_2^2$ ，Fisher准则 根据类间⽅差和类内⽅差的⽐值定义，即 J(\\boldsymbol{w})=\\frac{(m_2-m_1)^{2}}{s_1^2+s_2^2}\\tag{4.20}不难推导， $J(\\boldsymbol{w})$ 对 $\\boldsymbol{w}$ 的依赖 J(\\boldsymbol{w})=\\frac{\\boldsymbol{w}^{T}\\boldsymbol{S}_B\\boldsymbol{w}}{\\boldsymbol{w}^{T}\\boldsymbol{S}_W\\boldsymbol{w}}\\tag{4.21}其中 $\\boldsymbol{S}_B$ 是类间（between-class）协⽅差矩阵，形式为 \\boldsymbol{S}_B=(\\boldsymbol{m}_2-\\boldsymbol{m}_1)(\\boldsymbol{m}_2-\\boldsymbol{m}_1)^{T}$\\boldsymbol{S}_W$ 被称为类内（within-class）协⽅差矩阵，形式为 \\boldsymbol{S}_W=\\sum_{n\\in \\mathcal{C}_1}(\\boldsymbol{x}_n-\\boldsymbol{m}_1)(\\boldsymbol{x}_n-\\boldsymbol{m}_1)^{T}+\\sum_{n\\in \\mathcal{C}_2}(\\boldsymbol{x}_n-\\boldsymbol{m}_2)(\\boldsymbol{x}_n-\\boldsymbol{m}_2)^{T}对公式(4.21)关于 $\\boldsymbol{w}$ 求导，发现 $J(\\boldsymbol{w})$ 取得最⼤值的条件为 (\\boldsymbol{w}^{T}\\boldsymbol{S}_B\\boldsymbol{w})\\boldsymbol{S}_W\\boldsymbol{w}=(\\boldsymbol{w}^{T}\\boldsymbol{S}_W\\boldsymbol{w})\\boldsymbol{S}_B\\boldsymbol{w}\\tag{4.22}可以发现， $\\boldsymbol{S}_B\\boldsymbol{w}$ 总是在 $(\\boldsymbol{m}_2−\\boldsymbol{m}_1)$ 的⽅向上。 更重要的是， 若不关⼼ $\\boldsymbol{w}$ 的⼤⼩， 只关⼼它的⽅向， 因此可以忽略标量因⼦ $(\\boldsymbol{w}^{T}\\boldsymbol{S}_B\\boldsymbol{w})$ 和 $(\\boldsymbol{w}^{T}\\boldsymbol{S}_W\\boldsymbol{w})$ 。 将公式(4.22)的两侧乘以 $\\boldsymbol{S}_{W}^{-1}$ ，即得 Fisher线性判别函数（Fisher linear discriminant） \\boldsymbol{w}\\propto \\boldsymbol{S}_{W}^{-1}(\\boldsymbol{m}_2-\\boldsymbol{m}_1)\\tag{4.23}如果类内协⽅差矩阵是各向同性的，从⽽ $\\boldsymbol{S}_W$ 正⽐于单位矩阵，那么我们看到 $\\boldsymbol{w}$ 正⽐于类均值的差。 构建 Fisher线性判别函数 ，其⽅法为：选择⼀个阈值 $y_0$ ，使得当 $y(\\boldsymbol{x})\\ge y_0$ 时，把数据点分到 $\\mathcal{C}_1$ ，否则把数据点分到 $\\mathcal{C}_2$ 。 5，与最⼩平⽅的关系最⼩平⽅⽅法确定线性判别函数的⽬标是使模型的预测尽可能地与⽬标值接近。相反， Fisher判别准则 的⽬标是使输出空间的类别有最⼤的区分度。 对于⼆分类问题，Fisher准则可以看成最⼩平⽅的⼀个特例。作如下假设：让属于 $\\mathcal{C}_1$ 的⽬标值等于 $\\frac{N}{N_1}$ ，其中 $N_1$ 是类别 $\\mathcal{C}_1$ 的模式的数量，$N$ 是总的模式数量。这个⽬标值近似于类别 $\\mathcal{C}_1$ 的先验概率的导数。 对于类别 $\\mathcal{C}_2$ ， 令⽬标值等于 $−\\frac{N}{N_2}$ ， 其中 $N_2$ 是类别 $\\mathcal{C}_2$ 的模式的数量。平⽅和误差函数可以写成 E=\\frac{1}{2}\\sum_{n=1}^{N}(\\boldsymbol{w}^{T}\\boldsymbol{x}_{n}+w_0-t_n)^{2}\\tag{4.24}令 $E$ 关于 $w_0$ 和 $\\boldsymbol{w}$ 的导数等于零，使⽤对于⽬标值 $t_n$ 的表⽰⽅法，可以得到偏置的表达式 w_0=-\\boldsymbol{w}^{T}\\boldsymbol{m}\\tag{4.25}其中， \\sum_{n=1}^{N}t_n=N_1\\frac{N}{N_1}-N_2\\frac{N}{N_2}=0\\\\ \\boldsymbol{m}=\\frac{1}{N}\\sum_{n=1}^{N}\\boldsymbol{x}_n=\\frac{1}{N}(N_1\\boldsymbol{m}_1+N_2\\boldsymbol{m}_2)使⽤对于 $t_n$ 的新的表⽰⽅法可得 \\left(\\boldsymbol{S}_W+\\frac{N_1N_2}{N}\\boldsymbol{S}_B\\right)\\boldsymbol{w}=N(\\boldsymbol{m_1}-\\boldsymbol{m}_2)\\tag{4.26}由此可见，可以推导出公式(4.23)，即权向量恰好与根据Fisher判别准则得到的结果相同。 6，多分类的Fisher判别函数现在考虑Fisher判别函数对于 $K&gt;2$ 个类别的推⼴。 假设输⼊空间的维度 $D$ ⼤于类别数量 $K$ ，引⼊ $D^{\\prime} &gt; 1$ 个线性“特征” $y_k = \\boldsymbol{w}_k^{T}\\boldsymbol{x}$ ，其中 $k=1,\\dots,D^{\\prime}$ 。 为了⽅便， 这些特征值可以聚集起来组成向量 $\\boldsymbol{y}$ ，类似地，权向量 $\\{\\boldsymbol{w}_k\\}$ 可以被看成矩阵 $\\boldsymbol{W}$ 的列。因此 \\boldsymbol{y}=\\boldsymbol{W}^{T}\\boldsymbol{x}\\tag{4.27}类内协⽅差矩阵推⼴到 $K$ 类，有 \\boldsymbol{S}_{W}=\\sum_{k=1}^{K}\\boldsymbol{S}_{k}\\tag{4.28}其中， \\boldsymbol{S}_{k}=\\sum_{n\\in \\mathcal{C}_k}(\\boldsymbol{x}_n-\\boldsymbol{m}_k)(\\boldsymbol{x}_n-\\boldsymbol{m}_k)^{T}\\\\ \\boldsymbol{m}_{k}=\\frac{1}{N_k}\\sum_{n\\in\\mathcal{C_k}}\\boldsymbol{x}_{n}其中 $N_k$ 是类别 $\\mathcal{C}_k$ 中模式的数量。 为了找到类间协⽅差矩阵的推⼴，使⽤Duda and Hart（1973）的⽅法，⾸先考虑整体的协⽅差矩阵 \\boldsymbol{S}_{T}=\\sum_{n=1}^{N}(\\boldsymbol{x}_n-\\boldsymbol{m})(\\boldsymbol{x}_n-\\boldsymbol{m})^{T}\\tag{4.29}其中 $\\boldsymbol{m}$ 是全体数据的均值 \\boldsymbol{m}=\\frac{1}{N}\\sum_{n=1}^{N}\\boldsymbol{x}_{n}=\\frac{1}{N}\\sum_{k=1}^{K}N_k\\boldsymbol{m}_{k}其中 $N = \\sum_{k} N_k$ 是数据点的总数。 整体的协⽅差矩阵可以分解为公式(4.28)给出的类内协⽅差矩阵，加上另⼀个矩阵 $\\boldsymbol{S}_B$ ，它可以看做类间协⽅差矩阵。 \\boldsymbol{S}_{T}=\\boldsymbol{S}_{W}+\\boldsymbol{S}_{B}\\tag{4.30}其中， \\boldsymbol{S}_B=\\sum_{k=1}^{K}N_k(\\boldsymbol{m}_k-\\boldsymbol{m})(\\boldsymbol{m}_k-\\boldsymbol{m})^{T}协⽅差矩阵被定义在原始的 $\\boldsymbol{x}$ 空间中。现在在投影的 $D^{\\prime}$ 维 $\\boldsymbol{y}$ 空间中定义类似的矩阵 \\boldsymbol{S}_{W}=\\sum_{k=1}^{K}\\sum_{n\\in \\mathcal{C}_k}(\\boldsymbol{y}_n-\\boldsymbol{\\mu}_k)(\\boldsymbol{y}_n-\\boldsymbol{\\mu}_k)^{T}\\\\ \\boldsymbol{S}_B=\\sum_{k=1}^{K}N_k(\\boldsymbol{\\mu}_k-\\boldsymbol{\\mu})(\\boldsymbol{\\mu}_k-\\boldsymbol{\\mu})^{T}其中， \\boldsymbol{\\mu}_k=\\frac{1}{N_k}\\sum_{n\\in \\mathcal{C}_k}\\boldsymbol{y}_n\\\\ \\boldsymbol{\\mu}=\\frac{1}{N}\\sum_{k=1}^{K}N_k\\boldsymbol{\\mu}_k我们想构造⼀个标量，当类间协⽅差较⼤且类内协⽅差较⼩时，这个标量会较⼤。有许多可能的准则选择⽅式（Fukunaga, 1990）。其中⼀种选择是 J(\\boldsymbol{W})=\\text{Tr}\\{\\boldsymbol{s}_{W}^{-1}\\boldsymbol{s}_{B}\\}\\tag{4.31}这个判别准则可以显式地写成投影矩阵 $\\boldsymbol{W}$ 的函数，形式为 J(\\boldsymbol{W})=\\text{Tr}\\{(\\boldsymbol{W}^{T}\\boldsymbol{S}_{W}\\boldsymbol{W})^{-1}(\\boldsymbol{W}^{T}\\boldsymbol{S}_{B}\\boldsymbol{W})\\}\\tag{4.32}7，感知器算法线性判别模型的另⼀个例⼦是Rosenblatt（1962）提出的感知器算法。对应于⼀个⼆分类的模型，输⼊向量 $\\boldsymbol{x}$ ⾸先使⽤⼀个固定的⾮线性变换得到⼀个特征向量 $\\boldsymbol{\\phi}(\\boldsymbol{x})$ ， 这个特征向量然后被⽤于构造⼀个⼀般的线性模型，形式为 y(\\boldsymbol{x})=f(\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}))\\tag{4.33}其中⾮线性激活函数 $f(·)$ 是⼀个阶梯函数，形式为 f(a)=\\begin{cases}+1,&a\\ge 0\\\\ -1,&a","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】贝叶斯线性模型","slug":"【机器学习基础】贝叶斯线性模型","date":"2019-10-07T08:42:29.000Z","updated":"2019-10-10T01:12:49.406Z","comments":true,"path":"2019/10/07/prml_03_02/","link":"","permalink":"https://zhangbc.github.io/2019/10/07/prml_03_02/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，贝叶斯线性回归1，参数分布关于线性拟合的贝叶斯⽅法的讨论，⾸先引⼊模型参数 $\\boldsymbol{w}$ 的先验概率分布。现在这个阶段，把噪声精度参数 $\\beta$ 当做已知常数。⾸先，由公式(3.8)定义的似然函数 $p(t|\\boldsymbol{w})$ 是 $\\boldsymbol{w}$ 的⼆次函数的指数形式，于是对应的共轭先验是⾼斯分布，形式为： p(\\boldsymbol{w})=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{m}_{0},\\boldsymbol{S}_{0})\\tag{3.30}均值为 $\\boldsymbol{m}_{0}$ ，协⽅差为 $\\boldsymbol{S}_{0}$ 。 由于共轭⾼斯先验分布的选择，后验分布也将是⾼斯分布。 我们可以对指数项进⾏配平⽅， 然后使⽤归⼀化的⾼斯分布的标准结果找到归⼀化系数，这样就计算出了后验分布的形式： p(\\boldsymbol{w}|\\boldsymbol{t})=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{m}_{N},\\boldsymbol{S}_{N})\\tag{3.31}其中， \\boldsymbol{m}_{N}=\\boldsymbol{S}_{N}(\\boldsymbol{S}_{0}^{-1}\\boldsymbol{m}_{0}+\\beta \\boldsymbol{\\Phi}^{T}\\boldsymbol{t}) \\\\ \\boldsymbol{S}_{N}^{-1}=\\boldsymbol{S}_{0}^{-1}+\\beta \\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}为了简化起见，考虑⾼斯先验的⼀个特定的形式，即考虑零均值各向同性⾼斯分布，这个分布由⼀个精度参数 $\\alpha$ 控制，即： p(\\boldsymbol{w}|\\alpha)=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{0},\\alpha^{-1}\\boldsymbol{I})\\tag{3.32}对应的 $\\boldsymbol{w}$ 后验概率分布由公式(3.31)给出，其中， \\boldsymbol{m}_{N}=\\beta \\boldsymbol{S}_{N}\\boldsymbol{\\Phi}^{T}\\boldsymbol{t}\\\\ \\boldsymbol{S}_{N}^{-1}=\\alpha \\boldsymbol{I}+\\beta \\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}后验概率分布的对数由对数似然函数与先验的对数求和的⽅式得到。它是 $\\boldsymbol{w}$ 的函数，形式为： \\ln p(\\boldsymbol{w}|\\boldsymbol{t})=-\\frac{\\beta}{2}\\sum_{n=1}^{N}\\{t_n-\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}-\\frac{\\alpha}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}+常数\\tag{3.33}2，预测分布在实际应⽤中，我们通常感兴趣的不是 $\\boldsymbol{w}$ 本⾝的值，⽽是对于新的 $\\boldsymbol{x}$ 值预测出 $t$ 的值。这需要我们计算出预测分布（predictive distribution），定义为： p(t|\\mathbf{t},\\alpha,\\beta)=\\int p(t|\\boldsymbol{w},\\beta)p(\\boldsymbol{w}|\\mathbf{t},\\alpha,\\beta)\\mathrm{d}\\boldsymbol{w}\\tag{3.34}其中 $\\mathbf{t}$ 是训练数据⽬标变量的值组成的向量。经综合分析，预测分布的形式可以进一步具体化为： p(t|\\boldsymbol{x},\\mathbf{t},\\alpha,\\beta)=\\mathcal{N}(t|\\boldsymbol{m}_{N}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}),\\sigma_{N}^{2}(\\boldsymbol{x}))\\tag{3.35}其中， \\sigma_{N}^{2}(\\boldsymbol{x})=\\frac{1}{\\beta}+\\boldsymbol{\\phi}(\\boldsymbol{x})^{T}\\boldsymbol{S}_{N}\\boldsymbol{\\phi}(\\boldsymbol{x})其中，式中第⼀项表⽰数据中的噪声，第⼆项反映了与参数 $\\boldsymbol{w}$ 关联的不确定性。当额外的数据点被观测到的时候，后验概率分布会变窄。从⽽可以证明出 $\\sigma_{N+1}^{2}(\\boldsymbol{x})\\le \\sigma_{N}^{2}(\\boldsymbol{x})$（Qazaz et al., 1997）。 在极限 $N \\to \\infty$ 的情况下， 式中第⼆项趋于零， 从⽽预测分布的⽅差只与参数 $\\beta$ 控制的具有可加性的噪声有关。 在下图3.15～3.18中，我们调整⼀个由⾼斯基函数线性组合的模型，使其适应于不同规模的数据集，然后观察对应的后验概率分布。其中，绿⾊曲线对应着产⽣数据点的函数 $\\sin(2\\pi x)$（带有附加的⾼斯噪声），⼤⼩为 $N = 1, N = 2, N = 4$ 和 $N = 25$ 的数据集在四幅图中⽤蓝⾊圆圈表⽰。对于每幅图，红⾊曲线是对应的⾼斯预测分布的均值，红⾊阴影区域是均值两侧的⼀个标准差范围的区域。注意，预测的不确定性依赖于 $x$，并且在数据点的邻域内最⼩。 为了更加深刻地认识对于不同的 $x$ 值的预测之间的协⽅差，我们可以从 $\\boldsymbol{w}$ 的后验概率分布中抽取样本，然后画出对应的函数 $y(x, \\boldsymbol{w})$ ，如图3.19～3.22所⽰。 3，等价核考虑以下预测均值形式： y(\\boldsymbol{x},\\boldsymbol{m}_{N})=\\boldsymbol{m}_{N}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x})=\\beta \\boldsymbol{\\phi}(\\boldsymbol{x})^{T}\\boldsymbol{S}_{N}\\boldsymbol{\\Phi}^{T}\\mathbf{t}=\\sum_{n=1}^{N}\\beta \\boldsymbol{\\phi}(\\boldsymbol{x})^{T}\\boldsymbol{S}_{N}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})t_{n}\\tag{3.36}其中， \\boldsymbol{S}_{N}^{-1}=\\boldsymbol{S}_{0}^{-1}+\\beta \\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}因此在点 $\\boldsymbol{x}$ 处的预测均值由训练集⽬标变量 $t_n$ 的线性组合给出，即： y(\\boldsymbol{x},\\boldsymbol{m}_{N})=\\sum_{n=1}^{N}k(\\boldsymbol{x},\\boldsymbol{x}_{n})t_{n}\\tag{3.37}其中， k(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})=\\beta \\boldsymbol{\\phi}(\\boldsymbol{x})^{T}\\boldsymbol{S}_{N}\\boldsymbol{\\phi}(\\boldsymbol{x}^{\\prime})\\tag{3.38}被称为平滑矩阵（smoother matrix）或者等价核（equivalent kernel）。像这样的回归函数，通过对训练集⾥⽬标值进⾏线性组合做预测，被称为线性平滑（linear smoother）。 如图3.23，⾼斯基函数的等价核 $k(x, x^{\\prime})$ ， 图中给出了 $x$ 关于 $x^{\\prime}$ 的图像， 以及通过这个矩阵的三个切⽚， 对应于三个不同的 $x$ 值，⽤来⽣成这个核的数据集由 $x$ 的200个值组成，$x$ 均匀地分布在区 间 $(−1, 1)$ 中。 如图3.24，多项式基函数在 $x=0$ 的等价核 $k(x, x^{\\prime})$ 。 如图3.25，Sigmoid基函数在 $x=0$ 的等价核 $k(x, x^{\\prime})$ 。 考虑 $y(\\boldsymbol{x})$ 和 $y(\\boldsymbol{x}^{\\prime})$ 的协⽅差 \\begin{aligned}\\text{cov}[y(\\boldsymbol{x}),\\boldsymbol{x}^{\\prime}]&=\\text{cov}[\\boldsymbol{\\phi}(\\boldsymbol{x})^{T}\\boldsymbol{w},\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{\\prime})]\\\\&=\\boldsymbol{\\phi}(\\boldsymbol{x})^{T}\\boldsymbol{S}_{N}\\boldsymbol{\\phi}(\\boldsymbol{x}^{\\prime})\\\\&=\\beta^{-1}k(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})\\end{aligned}\\tag{3.39}⽤核函数表⽰线性回归给出了解决回归问题的另⼀种⽅法。通过不引⼊⼀组基函数（它隐式地定义了⼀个等价的核），⽽是直接定义⼀个局部的核函数，然后在给定观测数据集的条件下， 使⽤这个核函数对新的输⼊变量 $\\boldsymbol{x}$ 做预测。 这就引出了⽤于回归问题（以及分类问题）的⼀个很实⽤的框架，被称为⾼斯过程（Gaussian process）。 ⼀个等价核定义了模型的权值，通过这个权值，训练数据集⾥的⽬标值被组合，然后对新的 $\\boldsymbol{x}$ 值做预测，可以证明这些权值的和等于1，即 \\sum_{n=1}^{N}k(\\boldsymbol{x},\\boldsymbol{x}_{n})=1\\tag{3.40}对于所有的 $\\boldsymbol{x}$ 值都成⽴。 公式(3.38)给出的等价核满⾜⼀般的核函数共有的⼀个重要性质：可以表⽰为⾮线性函数的向量 $\\boldsymbol{\\psi}(\\boldsymbol{x})$ 的内积的形式，即 k(\\boldsymbol{x},\\boldsymbol{z})=\\boldsymbol{\\psi}(\\boldsymbol{x})^{T}\\boldsymbol{\\psi}(\\boldsymbol{z})\\tag{3.41}其中，$\\boldsymbol{\\psi}(\\boldsymbol{x})=\\beta^{\\frac{1}{2}}\\boldsymbol{S}_{N}^{\\frac{1}{2}}\\boldsymbol{\\phi}(\\boldsymbol{x})$ 。 二，贝叶斯模型比较假设我们想⽐较 $L$ 个模型 ${\\mathcal{M}_i}$ ，其中 $i=1, \\dots,L$ ，这⾥，⼀个模型指的是观测数据 $\\mathcal{D}$ 上的概率分布。在多项式曲线拟合的问题中，概率分布被定义在⽬标值 $\\mathbf{t}$ 上， ⽽输⼊值 $\\mathbf{X}$ 被假定为已知的。其他类型的模型定义了 $\\mathbf{X}$ 和 $\\mathbf{t}$ 上的联合分布。我们会假设数据是由这些模型中的⼀个⽣成的， 但是我们不知道究竟是哪⼀个，其不确定性通过先验概率分布 $p(\\mathcal{M}_i)$ 表⽰。给定⼀个训练数据集 $\\mathcal{D}$ ，估计后验分布 p(\\mathcal{M}_{i}|\\mathcal{D})\\propto p(\\mathcal{M}_{i})p(\\mathcal{D}|\\mathcal{M}_{i})其中，模型证据（model evidence） $p(\\mathcal{D}|\\mathcal{M}_{i})$ ，也叫边缘似然（marginal likelihood），它表达了数据展现出的不同模型的优先级，也可以被看做在模型空间中的似然函数，在这个空间中参数已经被求和或者积分。两个模型的模型证据的⽐值 $\\frac{p(\\mathcal{D}|\\mathcal{M}_{i})}{p(\\mathcal{D}|\\mathcal{M}_{j})}$ 被称为贝叶斯因⼦（Bayes factor）（Kass and Raftery, 1995）。 ⼀旦知道了模型上的后验概率分布，那么根据概率的加和规则与乘积规则，预测分布为 p(t|\\boldsymbol{x},\\mathcal{D})=\\sum_{i=1}^{L}p(t|\\boldsymbol{x},\\mathcal{M}_{i},\\mathcal{D})p(\\mathcal{M}_{i}|\\mathcal{D})\\tag{3.42}对于模型求平均的⼀个简单的近似是使⽤最可能的⼀个模型⾃⼰做预测，这被称为模型选择 （model selection）。 对于⼀个由参数 $\\boldsymbol{w}$ 控制的模型，根据概率的加和规则和乘积规则，模型证据为 p(\\mathcal{D}|\\mathcal{M}_{i})=\\int p(\\mathcal{D}|\\boldsymbol{w},\\mathcal{M}_{i})p(\\boldsymbol{w}|\\mathcal{M}_{i})\\mathrm{d}\\boldsymbol {w}\\tag{3.43}⾸先考虑模型有⼀个参数 $w$ 的情形。这个参数的后验概率正⽐于 $p(\\mathcal{D}|w)p(w)$ ，其中为了简化记号，我们省略 了它对于模型 ${\\mathcal{M}_i}$ 的依赖。 假设后验分布在最⼤似然值 $w_{MAP}$ 附近是⼀个尖峰，宽度为 $\\Delta w_{后验}$ ，那么可以⽤被积函数的值乘以尖峰的宽度来近似这个积分。进⼀步假设先验分布是平的，宽度为 $\\Delta w_{先验}$ ，即 $p(w)=\\frac{1}{\\Delta w_{先验}}$ ，那么有 \\begin{aligned}p(\\mathcal{D})&=\\int p(\\mathcal{D}|w)p(w)\\mathrm{d}w \\\\ &\\simeq p(\\mathcal{D}|w_{MAP})\\frac{\\Delta w_{后验}}{\\Delta w_{先验}}\\end{aligned}\\tag{3.44}取对数，可得 \\ln p(\\mathcal{D})\\simeq \\ln p(\\mathcal{D}|w_{MAP}) + \\ln\\left(\\frac{\\Delta w_{后验}}{\\Delta w_{先验}}\\right)\\tag{3.45}其中，式中第⼀项表⽰拟合由最可能参数给出的数据，对于平的先验分布来说，这对应于对数似然；第⼆项⽤于根据模型的复杂度来惩罚模型。 如图3.26，近似模型证据，如果我们假设参数上的后验概率分布在众数 $w_{MAP}$ 附近有⼀个尖峰。 对于⼀个有 $M$ 个参数的模型，可以对每个参数进⾏类似的近似。假设所有的参数 $\\frac{\\Delta w_{后验}}{\\Delta w_{先验}}$ 都相同，则有 \\ln p(\\mathcal{D})\\simeq \\ln p(\\mathcal{D}|w_{MAP}) + M\\ln\\left(\\frac{\\Delta w_{后验}}{\\Delta w_{先验}}\\right)\\tag{3.46}如图3.27，对于三个具有不同复杂度的模型，数据集的概率分布的图形表⽰，其中 $\\mathcal{M}_{1}$ 是最简单的，$\\mathcal{M}_{3}$ 是 最复杂的。 贝叶斯模型⽐较框架中隐含的⼀个假设是，⽣成数据的真实的概率分布包含在考虑的模型集合当中。如果这个假设确实成⽴，那么可以证明，平均来看，贝叶斯模型⽐较会倾向于选择出正确的模型。 考虑两个模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ ，其中真实的概率分布对应于模型 $\\mathcal{M}_{1}$ 。对于给定的有限数据集，确实有可能出现错误的模型反⽽使贝叶斯因⼦较⼤的事情。 但是，如果把贝叶斯因⼦在数据集分布上进⾏平均，那么可以得到期望贝叶斯因⼦，即关于数据的真实分布求的平均值： \\int p(\\mathcal{D}|\\mathcal{M}_{1})\\ln \\frac{p(\\mathcal{D}|\\mathcal{M}_{1})}{p(\\mathcal{D}|\\mathcal{M}_{2})}\\mathrm{d}\\mathcal{D}三，证据近似⾸先对参数 $\\boldsymbol{w}$ 求积分， 得到边缘似然函数（marginal likelihood function），然后通过最⼤化边缘似然函数，确定超参数的值。 这个框架在统计学的⽂献中被称为经验贝叶斯（empirical Bayes）（Bernardo and Smith, 1994; Gelman et al., 2004），或者被称为第⼆类最⼤似然（type 2 maximum likelihood）（Berger, 1985），或者被称为推⼴的最⼤似然（generalized maximum likelihood）。在机器学习的⽂献中， 这种⽅法也被称为证据近似（evidence approximation）（Gull, 1989; MacKay, 1992a）。 引⼊ $\\alpha$ 和 $\\beta$ 上的超先验分布，那么预测分布可以通过对 $\\boldsymbol{w}$ , $\\alpha$ 和 $\\beta$ 求积分的⽅法得到， 即 p(t|\\mathbf{t})=\\iiint p(t|\\boldsymbol{w},\\beta)p(\\boldsymbol{w}|\\mathbf{t},\\alpha,\\beta)p(\\alpha,\\beta|\\mathbf{t})\\mathrm{d}\\boldsymbol{w} \\mathrm{d}\\alpha \\mathrm{d}\\beta\\tag{3.47}如果后验分布 $p(\\alpha,\\beta|\\mathbf{t})$ 在 $\\hat{\\alpha}$ 和 $\\hat{\\beta}$ 附近有尖峰，那么预测分布可以通过对 $\\boldsymbol{w}$ 积分的⽅式简单地得到，其中 $\\alpha$ 和 $\\beta$ 被固定为 $\\hat{\\alpha}$ 和 $\\hat{\\beta}$ ，即 \\begin{aligned}p(t|\\mathbf{t}) &\\simeq p(t|\\mathbf{t},\\hat{\\alpha},\\hat{\\beta})\\\\ &=\\int p(t|\\boldsymbol{w},\\hat{\\beta})p(\\boldsymbol{w}|\\mathbf{t},\\hat{\\alpha},\\hat{\\beta})\\mathrm{d}\\boldsymbol{w}\\end{aligned}\\tag{3.48}1，计算证据函数边缘似然函数 $p(\\mathbf{t}|\\alpha,\\beta)$ 是通过对权值参数 $\\boldsymbol{w}$ 进⾏积分得到的，即 p(\\mathbf{t}|\\alpha,\\beta)=\\int p(\\mathbf{t}|\\boldsymbol{w},\\beta)p(\\boldsymbol{w}|\\alpha)\\mathrm{d}\\boldsymbol{w}\\tag{3.49}根据以前的相关公式，也可以写成 p(\\mathbf{t}|\\alpha,\\beta)=\\left(\\frac{\\beta}{2\\pi}\\right)^{\\frac{N}{2}}\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\frac{M}{2}}\\int \\exp\\{-E(\\boldsymbol{w})\\}\\mathrm{d}\\boldsymbol{w}\\tag{3.50}其中 $M$ 是 $\\boldsymbol{w}$ 的维数，并且， \\begin{aligned}E(\\boldsymbol{w})&=\\beta E_{D}(\\boldsymbol{w})+\\alpha E_{W}(\\boldsymbol{w})\\\\&=\\frac{\\beta}{2}||\\mathbf{t}-\\boldsymbol{\\Phi}\\boldsymbol{w}||^{2}+\\frac{\\alpha}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}\\end{aligned}\\tag{3.51}现在对 $\\boldsymbol{w}$ 配平⽅，可得 E(\\boldsymbol{w})=E(\\boldsymbol{m}_{N})+\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol{m}_{N})^{T}\\boldsymbol{A}(\\boldsymbol{w}-\\boldsymbol{m}_{N})\\tag{3.52}令 \\boldsymbol{A}=\\alpha\\boldsymbol{I}+\\beta\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\\\ \\boldsymbol{m}_{N}=\\beta \\boldsymbol{A}^{-1}\\boldsymbol{\\Phi}\\mathbf{t}\\\\ E(\\boldsymbol{m}_{N})=\\frac{\\beta}{2}||\\mathbf{t}-\\boldsymbol{\\Phi}\\boldsymbol{m}_{N}||^{2}+\\frac{\\alpha}{2}\\boldsymbol{m}_{N}^{T}\\boldsymbol{m}_{N}注意 $\\boldsymbol{A}$ 对应于误差函数的⼆阶导数 $\\boldsymbol{A} = \\nabla\\nabla E(\\boldsymbol{w})$ 被称为 Hessian矩阵。 经计算，可得边缘似然函数的对数，即证据函数的表达式： \\ln p(\\mathbf{t}|\\alpha,\\beta)=\\frac{M}{2}\\ln\\alpha+\\frac{N}{2}\\ln\\beta-E(\\boldsymbol{m}_{N})-\\frac{1}{2}\\ln |\\boldsymbol{A}|-\\frac{N}{2}\\ln(2\\pi)\\tag{3.53}如图3.28，模型证据与多项式阶数之间的关系。 2，最⼤化证据函数⾸先考虑 $p(\\mathbf{t}|\\alpha,\\beta)$ 关于 $\\alpha$ 的最⼤化，定义下⾯的特征向量⽅程 (\\beta\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})\\boldsymbol{\\mu}_{i}=\\lambda_{i}\\boldsymbol{\\mu}_{i}\\tag{3.54}根据公式，可知 $\\boldsymbol{A}$ 的特征值为 $\\alpha + \\lambda_{i}$ 。 现在考虑公式(3.53)中涉及到 $\\ln|\\boldsymbol{A}|$ 的项关于 $\\alpha$ 的导数 \\frac{\\mathrm{d}}{\\mathrm{d} \\alpha} \\ln |\\boldsymbol{A}|=\\frac{\\mathrm{d}}{\\mathrm{d} \\alpha} \\ln \\prod_{i}\\left(\\lambda_{i}+\\alpha\\right)=\\frac{\\mathrm{d}}{\\mathrm{d} \\alpha} \\sum_{i} \\ln \\left(\\lambda_{i}+\\alpha\\right)=\\sum_{i} \\frac{1}{\\lambda_{i}+\\alpha}\\tag{3.55}因此函数公式(3.53)关于 $\\alpha$ 的驻点满⾜ 0=\\frac{M}{2\\alpha}-\\frac{1}{2}\\boldsymbol{m}_{N}^{T}\\boldsymbol{m}_{N}-\\frac{1}{2}\\sum_{i}\\frac{1}{\\lambda_{i}+\\alpha}整理，有 \\alpha\\boldsymbol{m}_{N}^{T}\\boldsymbol{m}_{N}=M-\\alpha\\sum_{i}\\frac{1}{\\lambda_{i}+\\alpha}=\\gamma由于 $i$ 的求和式中⼀共有 $M$ 项，因此 $\\gamma$ 可以写成 \\gamma=\\sum_{i}\\frac{\\lambda_{i}}{\\alpha+\\lambda_{i}}\\tag{3.56}因而，最⼤化边缘似然函数的 $\\alpha$ 满⾜ \\alpha=\\frac{\\gamma}{\\boldsymbol{m}_{N}^{T}\\boldsymbol{m}_{N}}\\tag{3.57}注意： $\\alpha$ 的值是纯粹通过观察训练集确定的。 类似地，关于 $\\beta$ 最⼤化对数边缘似然函数，注意到公式(3.54)定义的特征值 $\\lambda_{i}$ 正⽐于 $\\beta$ ，因此 $\\frac{\\mathrm{d}}{\\mathrm{d}\\beta}=\\frac{\\lambda}{\\beta_{i}}$ 。于是 \\frac{\\mathrm{d}}{\\mathrm{d} \\beta} \\ln |\\boldsymbol{A}|=\\frac{\\mathrm{d}}{\\mathrm{d} \\beta} \\sum_{i} \\ln \\left(\\lambda_{i}+\\alpha\\right)=\\frac{1}{\\beta}\\sum_{i} \\frac{\\lambda_{i}}{\\lambda_{i}+\\alpha}=\\frac{\\gamma}{\\beta}\\tag{3.58}边缘似然函数的驻点因此满⾜ 0=\\frac{N}{2\\beta}-\\frac{1}{2}\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{m}_{N}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}-\\frac{\\gamma}{2\\beta}整理，可以得到最⼤化边缘似然函数的 $\\beta$ 满⾜ \\frac{1}{\\beta}=\\frac{1}{N-\\gamma}\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{m}_{N}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}\\tag{3.59}3，参数的有效数量如图3.29，似然函数的轮廓线（红⾊）和先验概率分布（绿⾊），其中参数空间中的坐标轴被旋转，与Hessian矩阵的特征向量 $\\boldsymbol{\\mu}_i$ 对齐。 考察单⼀变量 $x$ 的⾼斯分布的⽅差的最⼤似然估计为 \\sigma_{ML}^{2}=\\frac{1}{N}\\sum_{n=1}^{N}(x_{n}-\\mu_{ML})^{2}\\tag{3.60}这个估计是有偏的，因为均值的最⼤似然解 $\\mu_{ML}$ 拟合了数据中的⼀些噪声。从效果上来看，这占⽤了模型的⼀个⾃由度。对应的⽆偏的估计形式为 \\sigma_{MAP}^{2}=\\frac{1}{N-1}\\sum_{n=1}^{N}(x_{n}-\\mu_{ML})^{2}\\tag{3.61}分母中的因⼦ $N−1$ 反映了模型中的⼀个⾃由度被⽤于拟合均值的事实，它抵消了最⼤似然解的偏差。 如图3.30，$\\gamma$ 与 $\\ln\\alpha$ 的关系（红⾊曲线）以及 $2\\alpha E_{W}(\\boldsymbol{m}_{N})$ 与 $\\ln\\alpha$ 的关系（蓝⾊曲线）， 数据集为正弦数据集。这两条曲线的交点定义了 $\\alpha$ 的最优解，由模型证据的步骤给出。 如图3.31，对应的对数证据 $\\ln p(\\mathbf{t}|\\alpha,\\beta)$ 关于 $\\ln\\alpha$ 的图像（红⾊曲线），说明了峰值与图3.30中曲线的交点恰好重合。同样给出的时测试集误差（蓝⾊曲线），说明模型证据最⼤值的位置接近于具有最好泛化能⼒的点。 如图3.32，独⽴的参数关于有效参数数量 $\\gamma$ 的函数图像。⾼斯基函数模型中的10个参数 $w_i$ 与参数有效数量 $\\gamma$ 的关系，其中超参数的变化范围为 $0\\le\\alpha\\le\\infty$，使得 $\\gamma$ 的变化范围为 $0\\le\\gamma\\le M$ 。 如果考虑极限情况 $N\\gg M$ ， 数据点的数量⼤于参数的数量，那么根据公式，所有的参数都可以根据数据良好确定。因为 $\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}$ 涉及到数据点的隐式求和，因此特征值 $\\lambda_{i}$ 随着数据集规模的增加⽽增⼤。在这种情况下，$\\gamma = M$ ，并且 $\\alpha$ 和 $\\beta$ 的重新估计⽅程变为 \\alpha=\\frac{M}{2E_{W}(\\boldsymbol{m}_{N})}\\\\ \\beta=\\frac{N}{2E_{D}(\\boldsymbol{m}_{N})}四，固定基函数的局限性基函数的数量随着输⼊空间的维度 $D$ 迅速增长，通常是指数⽅式的增长。 真实数据集有两个性质：第⼀， 数据向量 $\\{\\boldsymbol{x}_n\\}$ 通常位于⼀个⾮线性流形内部。由于输⼊变量之间的相关性，这个流形本⾝的维度⼩于输⼊空间的维度。如果我们使⽤局部基函数，那么可以让基函数只分布在输⼊空间中包含数据的区域。这种⽅法被⽤在径向基函数⽹络中，也被⽤在⽀持向量机和相关向量机当中。神经⽹络模型使⽤可调节的基函数，这些基函数有着sigmoid⾮线性的性质。神经⽹络可以通过调节参数，使得在输⼊空间的区域中基函数会按照数据流形发⽣变化。第⼆，⽬标变量可能只依赖于数据流形中的少量可能的⽅向。利⽤这个性质，神经⽹络可以通过选择输⼊空间中基函数产⽣响应的⽅向。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】线性基函数模型","slug":"【机器学习基础】线性基函数模型","date":"2019-10-07T08:16:48.000Z","updated":"2019-10-07T15:22:18.873Z","comments":true,"path":"2019/10/07/prml_03_01/","link":"","permalink":"https://zhangbc.github.io/2019/10/07/prml_03_01/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，线性基函数模型1，线性基函数回归问题的⽬标是在给定 $D$ 维输⼊（input） 变量 $\\boldsymbol{x}$ 的情况下， 预测⼀个或者多个连续⽬标（target）变量 $t$ 的值。 通过将⼀组输⼊变量的⾮线性函数进⾏线性组合， 我们可以获得⼀类更加有⽤的函数， 被称为基函数（basis function）。 回归问题的最简单模型是输⼊变量的线性组合： y(\\boldsymbol{x},\\boldsymbol{w}) = w_0+w_1x_1+\\dots+w_Dx_D\\tag{3.1}其中，$\\boldsymbol{x}=(x_1,x_2,\\dots,x_D)^T$ ，通常称为线性回归（linear regression），这个模型的关键性质在于它是参数 $w_0 ,\\dots ,w_D$ 的⼀个线性函数。 但是， 它也是输⼊变量 $x_i$ 的⼀个线性函数， 这给模型带来了极⼤的局限性。因此扩展模型的类别：将输⼊变量的固定的⾮线性函数进⾏线性组合： y(\\boldsymbol{x},\\boldsymbol{w}) = w_0+\\sum_{j=1}^{M-1}w_{j}\\phi_{j}(\\boldsymbol{x})\\tag{3.2}其中， $\\phi_{j}(\\boldsymbol{x})$ 被称为基函数（basis function），参 数 $w_0$ 使得数据中可以存在任意固定的偏 置，这个值通常被称为偏置参数（bias parameter）。此模型称为线性模型。 通常，定义⼀个额外的虚“基函数” $\\phi_{0}(\\boldsymbol{x}) = 1$ 是很⽅便的，这时， y(\\boldsymbol{x},\\boldsymbol{w}) = \\sum_{j=0}^{M-1}w_{j}\\phi_{j}(\\boldsymbol{x}) = \\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x})\\tag{3.3}其中，$\\boldsymbol{w}=(w_0,x_1,\\dots,w_{M-1})^T$ ，$\\boldsymbol{\\phi}=(\\phi_0,\\phi_2,\\dots,\\phi_{M-1})^T$ 。 在许多模式识别的实际应⽤中， 我们会对 原始的数据变量进⾏某种固定形式的预处理或者特征抽取。如果原始变量由向量 $\\boldsymbol{x}$ 组成，那么特征可以⽤基函数 $\\{\\phi_{j}(\\boldsymbol{x})\\}$ 来表⽰。 多项式基函数的⼀个局限性在于它们是输⼊变量的全局函数，因此对于输⼊空间⼀个区域的改变将会影响所有其他的区域。这个问题的解决方案：把输⼊空间切分成若⼲个区域，然后对于每个区域⽤不同的多项式函数拟合，这样的函数叫做样条函数（spline function）（Hastie et al., 2001）。 ⾼斯基函数： \\phi_{j}(x)=\\exp\\left\\{-\\frac{(x-\\mu_{j})^2}{2s^{2}}\\right\\}\\tag{3.4}其中，$\\mu_{j}$ 控制了基函数在输⼊空间中的位置，参数 $s$ 控制了基函数的空间⼤⼩。 sigmoid基函数： \\phi_{j}(x)=\\sigma\\left(\\frac{x-\\mu_{j}}{s}\\right)\\tag{3.5}其中 $\\sigma(a)$ 是 logistic sigmoid函数，定义为： \\sigma_{a}=\\frac{1}{1+\\exp(-a)}\\tag{3.6}除此之外，基函数还可以选择傅⾥叶基函数，tanh函数等等。其中，tanh函数 和 logistic sigmoid函数 的关系如下：$\\tanh(a)=2\\sigma(2a)-1$。 如图3.1～3.3，分别为是多项式基函数，⾼斯基函数，sigmoid基函数。 2，最⼤似然与最⼩平⽅假设⽬标变量 $t$ 由确定的函数 $y(\\boldsymbol{x},\\boldsymbol{w})$ 给出，这个函数被附加了⾼斯噪声，即 t=y(\\boldsymbol{x},\\boldsymbol{w})+\\epsilon其中，$\\epsilon$ 是⼀个零均值的⾼斯随机变量，精度（⽅差的倒数）为 $\\beta$，则有： p(t|\\boldsymbol{x},\\boldsymbol{w},\\beta)=\\mathcal{N}(t|y(\\boldsymbol{x},\\boldsymbol{w}),\\beta^{-1})\\tag{3.7}均值为： \\mathbb{E}[t|\\boldsymbol{x}]=\\int tp(t|\\boldsymbol{x})\\mathrm{d}t=y(\\boldsymbol{x},\\boldsymbol{w})考虑⼀个输⼊数据集 $\\mathbf{X}=\\{\\boldsymbol{x}_1,\\dots, \\boldsymbol{x}_N\\}$， 对应的⽬标值为 $t_1,\\dots , t_N$ 。 我们把⽬标向量 $\\{t_n\\}$ 组成⼀个列向量， 记作 $\\mathbf{t}$。 假设这些数据点是独⽴地从分布公式(3.7)中抽取的，那么可以得到下⾯的似然函数的表达式， 它是可调节参数 $\\boldsymbol{w}$ 和 $\\beta$ 的函数，形式为： p(\\mathbf{t}|\\mathbf{X},\\boldsymbol{w},\\beta)=\\prod_{n=1}^{N}\\mathcal{N}(t_{n}|\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n}),\\beta^{-1})\\tag{3.8}取似然函数的对数，使⽤⼀元⾼斯分布的标准形式，可得： \\begin{aligned}\\ln p(\\mathbf{t}|\\boldsymbol{w},\\beta)&=\\sum_{n=1}^{N}\\ln \\mathcal{N}(t_{n}|\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n}),\\beta^{-1}) \\\\ &= \\frac{N}{2}\\ln\\beta-\\frac{N}{2}\\ln(2\\pi)-\\beta E_{D}(\\boldsymbol{w}) \\end{aligned}\\tag{3.9}其中，平⽅和误差函数的定义为： E_{D}(\\boldsymbol{w})=\\frac{1}{2}\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}\\tag{3.10}对数似然函数的梯度为： \\nabla\\ln p(\\mathbf{t}|\\boldsymbol{w},\\beta)=\\beta\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})^{T}\\tag{3.11}令梯度等于零，求解 $\\boldsymbol{w}$ 可得： \\boldsymbol{w}_{ML}=(\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^{T}\\mathbf{t}\\tag{3.12}这被称为最⼩平⽅问题的规范⽅程（normal equation）。这⾥ $\\boldsymbol{\\Phi}$ 是⼀个 $N \\times M$ 的矩阵，被称为设计矩阵（design matrix），它的元素为 $\\Phi_{nj}=\\phi_{j}(\\boldsymbol{x}_{n})$ ，即 \\mathbf{\\Phi}=\\left(\\begin{array}{cccc}{\\phi_{0}\\left(\\boldsymbol{x}_{1}\\right)} & {\\phi_{1}\\left(\\boldsymbol{x}_{1}\\right)} & {\\cdots} & {\\phi_{M-1}\\left(\\boldsymbol{x}_{1}\\right)} \\\\ {\\phi_{0}\\left(\\boldsymbol{x}_{2}\\right)} & {\\phi_{1}\\left(\\boldsymbol{x}_{2}\\right)} & {\\cdots} & {\\phi_{M-1}\\left(\\boldsymbol{x}_{2}\\right)} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\phi_{0}\\left(\\boldsymbol{x}_{N}\\right)} & {\\phi_{1}\\left(\\boldsymbol{x}_{N}\\right)} & {\\cdots} & {\\phi_{M-1}\\left(\\boldsymbol{x}_{N}\\right)}\\end{array}\\right)其中，量 \\mathbf{\\Phi}^{\\dagger} \\equiv\\left(\\mathbf{\\Phi}^{T} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{T}被称为矩阵 $\\mathbf{\\Phi}$ 的 Moore-Penrose伪逆矩阵（pseudo-inverse matrix）（Rao and Mitra, 1971; Golub and Van Loan, 1996）。 图3.4，最⼩平⽅解的⼏何表⽰，在⼀个 $N$ 维空间中，坐标轴是 $t_1,\\dots , t_N$ 的值。最⼩平⽅回归函数可以通过下⾯的⽅式得到：寻找数据向量 $\\mathbf{t}$ 在由基函数 $\\phi_{j}(\\boldsymbol{x})$ 张成的⼦空间上的正交投影，其中每个基函数都可以看成⼀个长度为 $N$ 的向量 $\\varphi_j$ ，它的元素为 $\\phi_{j}(\\boldsymbol{x}_{n})$ 。注意， $\\varphi_j$ 对应于 $\\mathbf{\\Phi}$ 的第 $j$ 列， ⽽ $\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})$ 对应于 $\\mathbf{\\Phi}$ 的第 $i$ ⾏。 如果显式地写出偏置参数，那么误差函数公式(3.10)变为： E_{D}(\\boldsymbol{w})=\\frac{1}{2}\\sum_{n=1}^{N}\\{t_{n}-w_{0}-\\sum_{j=1}^{M-1}w_{j}\\phi_{j}(\\boldsymbol{x}_{n})\\}^{2}\\tag{3.13}令关于 $w_0$ 的导数等于零，解出 $w_0$ ，可得 w_0=\\bar{t}-\\sum_{j=1}^{M-1}w_{j}\\bar\\phi_{j}其中， \\bar{t}=\\frac{1}{N}\\sum_{n=1}^{N}t_{n} \\\\ \\bar{\\phi}_{j}=\\frac{1}{N}\\sum_{n=1}^{N}\\phi_{j}(\\boldsymbol{x}_n)因此，偏置 $w_0$ 补偿了⽬标值的平均值（在训练集上的）与基函数的值的平均值的加权求和之间的差。 关于噪声精度参数 $\\beta$ 最⼤化似然函数公式(3.9)，结果为： \\frac{1}{\\beta_{ML}}=\\frac{1}{N}\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{w}_{ML}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}\\tag{3.14}因此，噪声精度的倒数由⽬标值在回归函数周围的残留⽅差（residual variance）给出。 3，顺序学习顺序算法中，每次只考虑⼀个数据点，模型的参数在每观测到⼀个数据点之后进⾏更新。顺序学习也适⽤于实时的应⽤，在实时应⽤中，数据观测以⼀个连续的流的⽅式持续到达，我们必须在观测到所有数据之前就做出预测。 我们可以获得⼀个顺序学习的算法通过考虑随机梯度下降（stochastic gradient descent）也 被称为顺序梯度下降（sequential gradient descent）的⽅法。 如果误差函数由数据点的和组成 $E = \\sum_{n} E_n$ ，那么在观测到模式 $n$ 之后，随机梯度下降算法使⽤下式更新参数向量 $\\boldsymbol{w}$ ： \\boldsymbol{w}^{(\\tau+1)}=\\boldsymbol{w}^{(\\tau)}-\\eta\\nabla E_{n}\\tag{3.15}其中 $\\tau$ 表⽰迭代次数，$\\eta$ 是学习率参数。 $\\boldsymbol{w}$ 被初始化为某个起始向 量 $\\boldsymbol{w}^{(0)}$ 。对于平⽅和误差函数公式(3.10)的情形，我们有： \\boldsymbol{w}^{(\\tau+1)}=\\boldsymbol{w}^{(\\tau)}+\\eta(t_{n}-\\boldsymbol{w}^{(\\tau)T}\\boldsymbol{\\phi}_{n})\\boldsymbol{\\phi}_{n}\\tag{3.16}其中 $\\boldsymbol{\\phi}_{n}=\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})$。 这被称为最⼩均⽅（least-mean-squares）或者 LMS算法。$\\eta$ 的值需要仔细选择，确保算法收敛（Bishop and Nabney, 2008）。 4，正则化最⼩平⽅为误差函数添加正则化项的思想来控制过拟合，因此需要最⼩化的总的误差函数的形式为 E_{D}(\\boldsymbol{w})+\\lambda E_{W}(\\boldsymbol{w})其中 $\\lambda$ 是正则化系数，正则化项的⼀个最简单的形式为权向量的各个元素的平⽅和 E_{W}(\\boldsymbol{w})=\\frac{1}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}考虑平⽅和误差函数 E_{D}(\\boldsymbol{w})=\\frac{1}{2}\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}\\tag{3.17}那么总误差函数就变成了 \\frac{1}{2}\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}+\\frac{\\lambda}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}这种对于正则化项的选择⽅法在机器学习的⽂献中被称为权值衰减（weight decay）。 令总误差函数关于 $\\boldsymbol{w}$ 的梯度等于零，解出 $\\boldsymbol{w}$ ，有： \\boldsymbol{w}=(\\lambda \\boldsymbol{I}+\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^{T}\\boldsymbol{t}\\tag{3.18}有时使⽤⼀个更加⼀般的正则化项，这时正则化的误差函数的形式为： \\frac{1}{2}\\sum_{n=1}^{N}\\{t_{n}-\\boldsymbol{w}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})\\}^{2}+\\frac{\\lambda}{2}\\sum_{j=1}^{M}|w_{j}|^{q}\\tag{3.19}如图3.5～3.8，对于不同的参数 $q$，公式(3.19)中的正则化项的轮廓线。 在统计学的⽂献中，$q=1$ 的情形被称为套索（lasso）（Tibshirani, 1996）。它的性质为：如果 $\\lambda$ 充分⼤，那么某些系数 $w_j$ 会变为零，从⽽产⽣了⼀个稀疏（sparse）模型，这个模型中对应的基函数不起作⽤。 如图3.9，$q=2$ 的⼆次正则化项的限制区域。 如图3.10，$q=1$ 的套索正则化项的限制区域。 5，多个输出对于预测 $K&gt;1$ 个⽬标变量，我们把这些⽬标变量聚集起来，记作⽬标向量 $\\boldsymbol{t}$ ，其解决方案是：对于 $\\boldsymbol{t}$ 的每个分量，引⼊⼀个不同的基函数集合，从⽽变成了多个独⽴的回归问题。但是，⼀个更有趣的并且更常⽤的⽅法是对⽬标向量的所有分量使⽤⼀组相同的基函数来建模，即： \\boldsymbol{y}(\\boldsymbol{x}, \\boldsymbol{w})=\\boldsymbol{W}^{T}\\phi(\\boldsymbol{x})\\tag{3.20}其中 $\\boldsymbol{y}$ 是⼀个 $K$ 维列向量，$\\boldsymbol{W}$ 是⼀个 $M\\times K$ 的参数矩阵，$\\phi(\\boldsymbol{x})$ 是⼀个 $M$ 为列向量， 每个元素 为 $\\phi_{j}(\\boldsymbol{x})$ ，$\\phi_{0}(\\boldsymbol{x}) = 1$ 。 假设令⽬标向量的条件概率分布是⼀个各向同性的⾼斯分布，形式为： p(\\boldsymbol{t}|\\boldsymbol{x},\\boldsymbol{W},\\beta)=\\mathcal{N}(\\boldsymbol{t}|\\boldsymbol{W}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}),\\beta^{-1}\\boldsymbol{I})\\tag{3.21}如果有⼀组观测 $\\boldsymbol{t}_1,\\dots,\\boldsymbol{t}_N$ ，可以把这些观测组合为⼀个 $N \\times K$ 的矩阵 $\\boldsymbol{T}$ ，使得矩阵的第 $n$ ⾏为 $\\boldsymbol{t}_{n}^{T}$ 。类似地，把输⼊向量 $\\boldsymbol{x}_1,\\dots,\\boldsymbol{x}_N$ 组合为矩阵 $\\boldsymbol{X}$ 。这样，对数似然函数： \\begin{aligned}\\ln p(\\mathbf{T}|\\boldsymbol{X},\\boldsymbol{W},\\beta)&=\\sum_{n=1}^{N}\\ln \\mathcal{N}(\\boldsymbol{t}_{n}|\\boldsymbol{W}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n}),\\beta^{-1}\\boldsymbol{I}) \\\\ &= \\frac{NK}{2}\\ln\\left(\\frac{\\beta}{2\\pi}\\right)-\\frac{\\beta}{2}\\sum_{n=1}^{N}||\\boldsymbol{t}_{n}-\\boldsymbol{W}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}_{n})||^{2} \\end{aligned}\\tag{3.22}关于 $\\boldsymbol{W}$ 最⼤化这个函数，可得： \\boldsymbol{W}_{ML}=(\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^{T}\\boldsymbol{T}\\tag{3.23}对于每个⽬标变量 $t_k$ 考察这个结果，那么有 \\boldsymbol{w}_{k}=(\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^{T}\\boldsymbol{t}_{k}=\\mathbf{\\Phi}^{\\dagger}\\boldsymbol{t}_{k}\\tag{3.24}其中，$\\boldsymbol{t}_{k}$ 是⼀个 $N$ 维列向量， 元素为 $t_{nk}$ 其中 $n=1,\\dots,N$ 。 因此不同⽬标变量的回归问题在这⾥被分解开，并且我们只需要计算⼀个伪逆矩阵 $\\mathbf{\\Phi}^{\\dagger}$ ，这个矩阵是被所有向量 $\\boldsymbol{w}_k$ 所共享的。 二， 偏置-方差分解 假如已知条件概率分布 $p(t|\\boldsymbol{x})$，每⼀种损失函数都能够给出对应的最优预测结果。使⽤最多的⼀个选择是平⽅损失函数，此时最优的预测由条件期望（记作 $h(\\boldsymbol{x})$ ）给出，即 h(\\boldsymbol{x}) = \\mathbb{E}[t|\\boldsymbol{x}]=\\int tp(t|\\boldsymbol{x})\\mathrm{d}t\\tag{3.25}考察平⽅损失函数的期望： \\mathbb{E}[\\boldsymbol{L}]=\\int\\{y(\\boldsymbol{x})-h(\\boldsymbol{x})\\}^{2}p(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x}+\\int\\int\\{h(\\boldsymbol{x})-t\\}^{2}p(\\boldsymbol{x},t)\\mathrm{d}\\boldsymbol{x}\\mathrm{d}t\\tag{3.26}其中，与 $y(\\boldsymbol{x})$ ⽆关的第⼆项，是由数据本⾝的噪声造成的，表⽰期望损失能够达到的最⼩值。第⼀项与对函数 $y(\\boldsymbol{x})$ 的选择有关，我们要找⼀个 $y(\\boldsymbol{x})$ 的解，使得这⼀项最⼩。由于它是⾮负的，因此我们希望能够让这⼀项的最⼩值等于零。 考察公式(3.26)的第⼀项被积函数，对于⼀个特定的数据集 $\\mathcal{D}$，它的形式为 \\{y(\\boldsymbol{x};\\mathcal{D})-h(\\boldsymbol{x})\\}^{2}由于这个量与特定的数据集 $\\mathcal{D}$ 相关，因此对所有的数据集取平均。如果我们在括号内减去然后加上 $\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]$ ，然后展开，有 \\begin{aligned}\\{y(\\boldsymbol{x};\\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]+\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]-h(\\boldsymbol{x})\\}^{2} \\\\=\\{y(\\boldsymbol{x};\\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]\\}^{2}+\\{\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]-h(\\boldsymbol{x })\\}^{2}\\\\+2\\{y(\\boldsymbol{x};\\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]\\}\\{\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]-h(\\boldsymbol{x})\\}\\end{aligned}现在关于 $\\mathcal{D}$ 求期望，然后注意到最后⼀项等于零，可得 \\begin{array}{l}{\\mathbb{E}_{\\mathcal{D}}\\left[\\{y(\\boldsymbol{x} ; \\mathcal{D})-h(\\boldsymbol{x})\\}^{2}\\right]} \\\\ {\\quad=\\underbrace{\\left\\{\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x}; \\mathcal{D})]-h(\\boldsymbol{x})\\right\\}^{2}}_{(偏置)^{2}}+\\underbrace{\\mathbb{E}_{\\mathcal{D}}\\left[\\left\\{y(\\boldsymbol{x} ; \\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x} ; \\mathcal{D})]\\right\\}^{2}\\right]}_{方差}}\\end{array}\\tag{3.27}其中，$y(\\boldsymbol{x};\\mathcal{D})$ 与回归函数 $h(\\boldsymbol{x})$ 的差的平⽅的期望可以表⽰为两项的和。第⼀项，被称为平⽅偏置（bias），表⽰所有数据集的平均预测与预期的回归函数之间的差异。第⼆项，被称为⽅差（variance），度量了对于单独的数据集，模型所给出的解在平均值附近波动的情况，因此也就度量了函数 $y(\\boldsymbol{x};\\mathcal{D})$ 对于特定的数据集的选择的敏感程度。 综上，对于期望平⽅损失的分解： 期望损失=偏置^{2}+方差+噪声\\tag{3.28}其中， 偏置^{2}=\\int \\{\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x};\\mathcal{D})]-h(\\boldsymbol{x})\\}^{2}p(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x} \\\\ 方差=\\int \\mathbb{E}_{\\mathcal{D}}[\\{y(\\boldsymbol{x} ; \\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\boldsymbol{x} ; \\mathcal{D})]\\}^{2}p(\\boldsymbol{x} )\\mathrm{d}\\boldsymbol{x} \\\\ 噪声=\\int\\int\\{h(\\boldsymbol{x})-t\\}^{2}p(\\boldsymbol{x},t)\\mathrm{d}\\boldsymbol{x}\\mathrm{d}t对于⾮常灵活的模型来说，偏置较⼩， ⽅差较⼤；对于相对固定的模型来说，偏置较⼤，⽅差较⼩。 图3.11~3.13，模型复杂度对于偏置和⽅差的依赖的说明。左侧⼀列给出了对于不同的 $\\ln \\lambda$ 值，根据数据集拟合模型的结果。 为了清晰起见， 只给出了100个拟合模型中的20个。 右侧⼀列给出了对应的100个拟合的均值 （红⾊）以及⽤于⽣成数据集的正弦函数（绿⾊）。 举例：讨论正弦数据集，我们产⽣了100个数据集合， 每个集合都包含 $N = 25$ 个数据点，都是独⽴地从正弦曲线 $h(x)=\\sin(2\\pi x)$ 抽取的。数据集的编号为 $l = 1, \\dots , L$ ， 其中 $L=100$，并且对于每个数据 集 $\\mathcal{D}^{(l)}$ ，我们通过最⼩化正则化的误差函数拟合了⼀个带有24个⾼斯基函数的模型，然 后给出了预测函数 $y^{(l)}(x)$ ，如图3.11~13所⽰。如图3.11，对应着较⼤的正则化系数 $\\lambda$，这样的模型的⽅差很⼩（因为左侧图中的红⾊曲线看起来很相似），但是偏置很⼤（因为右侧图中的两条曲线看起来相当不同）。相反，如图3.13，正则化系数 $\\lambda$ 很⼩，这样模型的⽅差较⼤（因为左侧图中 的红⾊曲线变化性相当⼤）， 但是偏置很⼩（因为平均拟合的结果与原始正弦曲线⼗分吻合）。注意，把 $M = 25$ 这种复杂模型的多个解进⾏平均，会产⽣对于回归函数⾮常好的拟合， 这表明求平均是⼀个很好的步骤。事实上，将多个解加权平均是贝叶斯⽅法的核⼼，虽然这种求平均针对的是参数的后验分布，⽽不是针对多个数据集。 对于这个例⼦，我们也可以定量地考察偏置-⽅差折中。平均预测由下式求出： \\bar{y}(x)=\\frac{1}{L}\\sum_{l=1}^{L}y^{(l)}(x)\\tag{3.29}有， 偏置^{2}=\\frac{1}{N}\\sum_{n=1}^{N}\\{\\bar{y}(x_{n})-h(x)\\}^{2} 方差=\\frac{1}{N}\\sum_{n=1}^{N}\\frac{1}{L}\\sum_{l=1}^{L}\\{y^{(l)}(x_{n})-\\bar{y}(x_{n})\\}^{2}图3.14，平⽅偏置和⽅差的图像，以及它们的加和。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】概率分布之指数族分布","slug":"【机器学习基础】概率分布之指数族分布","date":"2019-09-29T11:34:14.000Z","updated":"2019-10-07T15:41:45.228Z","comments":true,"path":"2019/09/29/prml_02_03/","link":"","permalink":"https://zhangbc.github.io/2019/09/29/prml_02_03/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，指数族分布1，指数族分布基本概念参数为 $\\boldsymbol{\\eta}$ 的变量 $\\boldsymbol{x}$ 的指数族分布定义为具有下⾯形式的概率分布的集合： p(\\boldsymbol{x|\\eta}) = h(\\boldsymbol{x})g(\\boldsymbol{\\eta})\\exp \\{\\boldsymbol{\\eta}^{T}\\boldsymbol{\\mu}(\\boldsymbol{x})\\}\\tag{2.106}其中 $\\boldsymbol{x}$ 可能是标量或者向量， 可能是离散的或者是连续的。 这⾥ $\\boldsymbol{\\eta}$ 被称为概率分布的 ⾃然参数 （natural parameters），$\\boldsymbol{\\mu}(\\boldsymbol{x})$ 是 $\\boldsymbol{x}$ 的某个函数。函数 $g(\\boldsymbol{\\eta})$ 可以被看成系数，它确保了概率分布是归⼀化的，因此满⾜： g(\\boldsymbol{\\eta})\\int h(\\boldsymbol{x})\\exp \\{\\boldsymbol{\\eta}^{T}\\boldsymbol{\\mu}(\\boldsymbol{x})\\}\\mathrm{d}\\boldsymbol{x}=1\\tag{2.107}如果 $\\boldsymbol{x}$ 是离散变量，那么上式中的积分就要替换为求和。 考虑伯努利分布： p(x|\\mu) = \\text {Bern}(x|\\mu) = \\mu^{x}(1-\\mu)^{1-x}\\tag{2.108}变形，有： \\begin{aligned} p(x|\\mu) &= \\exp \\{x\\ln \\mu +(1-x) \\ln (1-\\mu)\\} \\\\ &= (1-\\mu)\\exp \\left\\{\\ln \\left(\\frac{\\mu}{1-\\mu}\\right)x\\right\\}\\end{aligned}\\tag{2.109}对比公式(2.106)，可得： \\eta = \\ln \\left(\\frac{\\mu}{1-\\mu}\\right)从而，有： \\begin{aligned}\\mu &= \\sigma(\\eta) \\\\ &= \\frac{1}{1+\\exp(-\\eta)}\\end{aligned}\\tag{2.110}被称为 logistic sigmoid函数。因此，伯努利分布的指数族分布标准形式： p(x|\\mu) = \\sigma(-\\eta)\\exp(\\eta x)\\tag{2.111}其中， \\mu(x) = x \\\\ h(x) = 1 \\\\ g(\\eta)=\\sigma(-\\eta)考虑单⼀观测 $\\boldsymbol{x}$ 的多项式分布，形式为： p(\\boldsymbol{x|\\mu}) = \\prod_{k=1}^{K}\\mu_{k}^{x_{k}} = \\exp\\left\\{\\sum_{k=1}^K x_{k}\\ln \\mu_{k}\\right\\}\\tag{2.112}其中 $\\boldsymbol{x} = (\\boldsymbol{x}_1,\\dots ,\\boldsymbol{x}_M)^T$ 。把它写成公式(2.106)的标准形式，即： p(\\boldsymbol{x|\\mu}) = \\exp(\\boldsymbol{\\eta}^{T}\\boldsymbol{x})\\tag{2.113}其中，$\\eta_{k} = \\ln \\mu_{k}$ ，$\\boldsymbol{\\eta}=(\\eta_1,\\dots,\\eta_{M})^T$，并且 \\boldsymbol{\\mu}(\\boldsymbol{x}) = \\boldsymbol{x} \\\\ h(\\boldsymbol{x}) = 1 \\\\ g(\\boldsymbol{\\eta}) = 1 \\\\ \\sum_{k=1}^{K} \\mu_{k}=1考虑只⽤ $M−1$ 个参数来表⽰这个分布，把 $\\mu_M$ ⽤剩余的 $\\{\\mu_k\\}$ 表⽰，其中 $k = 1, \\dots , M−1$，这样就只剩下了 $M−1$ 个参数，公式(2.112)变为： \\begin{aligned}p(\\boldsymbol{x|\\mu}) &= \\exp\\left\\{\\sum_{k=1}^K x_{k}\\ln \\mu_{k}\\right\\} \\\\ &= \\exp \\left\\{\\sum_{k=1}^{M-1}x_{k}\\ln\\left(\\frac{\\mu_{k}}{1-\\sum_{j=1}^{M-1}\\mu_{j}}\\right) + \\ln \\left(1-\\sum_{k=1}^{M-1}\\mu_{k}\\right)\\right\\} \\end{aligned}\\tag{2.114}令 \\eta_{k} = \\ln\\left(\\frac{\\mu_{k}}{1-\\sum_{j=1}^{M-1}\\mu_{j}}\\right)即得： \\mu_{k} = \\frac{\\exp (\\eta_{k})}{1+\\sum_{j=1}^{M-1}\\exp(\\eta_{j})}\\tag{2.115}这被称为 softmax函数，或者归⼀化指数（normalized exponential）。因此，单⼀观测 $\\boldsymbol{x}$ 的多项式分布的指数族分布标准形式： p(\\boldsymbol{x}|\\boldsymbol{\\eta}) = \\left(1+\\sum_{k=1}^{M-1}\\exp(\\eta_{k})\\right)^{-1}\\exp(\\boldsymbol{\\mu}^T\\boldsymbol{x})\\tag{2.116}其中 $\\boldsymbol{\\eta}=(\\eta_1,\\dots,\\eta_{M-1},0)^T$，并且 \\boldsymbol{\\mu}(\\boldsymbol{x}) = \\boldsymbol{x} \\\\ h(\\boldsymbol{x}) = 1 \\\\ g(\\boldsymbol{\\eta}) =\\left(1+\\sum_{k=1}^{M-1}\\exp(\\eta_{k})\\right)^{-1}对于⼀元⾼斯分布，有： \\begin{aligned} p\\left(x | \\mu, \\sigma^{2}\\right) &=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{\\frac{1}{2}}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}(x-\\mu)^{2}\\right\\} \\\\ &=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{\\frac{1}{2}}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}x^{2} + \\frac{\\mu}{\\sigma^{2}}x -\\frac{1}{2 \\sigma^{2}}\\mu^{2}\\right\\} \\end{aligned}\\tag{2.117}其中， \\boldsymbol{\\eta}=\\dbinom{\\frac{\\mu}{\\sigma^{2}}}{\\frac{-1}{2\\sigma^2}} \\\\ \\boldsymbol{\\mu}({x})=\\dbinom{x}{x^2} \\\\ h(x) = (2\\pi)^{-\\frac{1}{2}} \\\\ g(\\boldsymbol{\\eta}) = (-2\\eta_2)^{\\frac{1}{2}}\\exp \\left(\\frac{\\eta_{1}^{2}}{4\\eta_{2}}\\right)2，最⼤似然与充分统计量设二元函数 $z=f(x,y)$ 在平面区域 $D$上具有一阶连续偏导数，则对于每一个点 $P_0(x_0,y_0)\\in D$ 都可定出一个向量 \\left\\{\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial y_0} \\right\\} = f_x(x_0,y_0)\\boldsymbol{i} + f_y(x_0,y_0)\\boldsymbol{j}，该向量称为函数 $z=f(x,y)$ 在点$P_0(x_0,y_0)$的梯度，记作 $\\text{gradf}(x_0,y_0)$ 或 $\\nabla f(x_0, y_0)$即有： \\text { gradf }(x_0, y_0)=\\nabla f(x_0, y_0)=\\left\\{\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial y_0}\\right\\}=f_{x}(x_0, y_0) \\boldsymbol{i}+f_{y}(x_0, y_0) \\boldsymbol{j}\\tag{2.118}其中 $\\nabla =\\frac{\\partial}{\\partial x} \\boldsymbol{i} + \\frac{\\partial}{\\partial y}\\boldsymbol{j}$ 称为（二维的）向量微分算子或 Nabla算子， $\\nabla {f}=\\frac{\\partial {f}}{\\partial x} \\boldsymbol{i} + \\frac{\\partial{f}}{\\partial y}\\boldsymbol{j}$ 。 对公式(2.107)的两侧关于 $\\boldsymbol{\\mu}$ 取梯度，有： \\begin{aligned} \\nabla g(\\boldsymbol{\\eta})\\int h(\\boldsymbol{x})\\exp \\{\\boldsymbol{\\eta}^{T}\\boldsymbol{\\mu}(\\boldsymbol{x})\\}\\mathrm{d}\\boldsymbol{x} + g(\\boldsymbol{\\eta})\\int h(\\boldsymbol{x})\\exp \\{\\boldsymbol{\\eta}^{T}\\boldsymbol{\\mu}(\\boldsymbol{x})\\}\\boldsymbol{\\mu}(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x} =0 \\end{aligned}\\tag{2.119}从而可以推导出： -\\nabla \\ln g(\\boldsymbol{\\eta}) = \\mathbb{E}[\\boldsymbol{\\mu}(\\boldsymbol{x})]\\tag{2.120}现在考虑⼀组独⽴同分布的数据 $\\boldsymbol{X} = \\{\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_N\\}$。对于这个数据集，似然函数为： p(\\boldsymbol{X|\\eta}) = \\left(\\prod_{n=1}^{N}h(\\boldsymbol{x}_{n})\\right) g(\\boldsymbol{\\eta})^{N} \\exp\\left\\{\\sum_{n=1}^{N} \\boldsymbol{\\eta}^{T}\\boldsymbol{\\mu}(\\boldsymbol{x}_n)\\right\\}\\tag{2.121}令 $\\ln p(\\boldsymbol{X|\\eta})$ 关于 $\\boldsymbol{\\eta}$ 的导数等于零，我们可以得到最⼤似然估计 $\\boldsymbol{\\mu}_{ML}$ 满⾜的条件： -\\nabla \\ln g(\\boldsymbol{\\eta}_{ML})=\\frac{1}{N}\\sum_{n=1}^{N}\\boldsymbol{\\mu}(\\boldsymbol{x}_n)\\tag{2.122}原则上可以通过解这个⽅程来得到 $\\boldsymbol{\\mu}_{ML}$ 。我们看到最⼤似然估计的解只通过 $\\boldsymbol{\\mu}(\\boldsymbol{x}_n)$ 对数据产⽣依赖，因此这个量被称为指数族分布的充分统计量（sufficient statistic）。 3，共轭先验对于指数族分布的任何成员，都存在⼀个共轭先验，可以写成下⾯的公式： p(\\boldsymbol{\\eta} | \\boldsymbol{\\chi}, \\nu)=f(\\boldsymbol{\\chi}, \\nu) g(\\boldsymbol{\\eta})^{\\nu} \\exp \\left\\{\\nu \\boldsymbol{\\eta}^{T} \\boldsymbol{\\chi}\\right\\}\\tag{2.123}4，无信息先验在许多情形下， 我们可能对分布应该具有的形式⼏乎完全不知道。 这时， 我们可以寻找⼀种形式的先验分布， 被称为⽆信息先验（noninformative prior）。 这种先验分布的⽬的是尽量对后验分布产⽣尽可能⼩的影响（Jeffreys, 1946; Box and Tiao, 1973; Bernardo and Smith, 1994）。这有时被称为“让数据⾃⼰说话”。 考虑⽆信息先验的两个简单的例⼦（Berger, 1985）。 例1，如果概率密度的形式为： p(x|\\mu)=f(x-\\mu)\\tag{2.124}那么参数 $\\mu$ 被称为位置参数（location parameter）。这⼀类概率分布具有平移不变性（translation invariance），因为如果把 $x$ 平移⼀个常数，得到 $\\hat{x}=x+c$，那么： p(\\hat{x}|\\hat{\\mu})=f(\\hat{x}-\\hat{\\mu})\\tag{2.125}其中，$\\hat{\\mu}=\\mu+c$。新变量的概率密度的形式与原变量相同，因此概率密度与原点的选择⽆关。想要选择⼀个能够反映这种平移不变性的先验分布，因此我们选择的先验概率分布要对区间 $A \\le \\mu \\le B$ 以及平移后的区间 $A−c \\le \\mu \\le B−c$ 赋予相同的概率质量。这说明： \\int_{A}^{B} p(\\mu)\\mathrm{d}\\mu = \\int_{A-c}^{B-c} p(\\mu)\\mathrm{d}\\mu = \\int_{A}^{B} p(\\mu - c)\\mathrm{d}\\mu\\tag{2.126}并且由于这必须对于任意的 $A$ 和 $B$ 的选择都成⽴，因此有： p(\\mu-c)=p(\\mu)\\tag{2.127}这表明 $p(\\mu)$ 是常数。 例2，考虑概率分布的形式为： p(x|\\sigma)=\\frac{1}{\\sigma}f\\left(\\frac{x}{\\sigma}\\right)\\tag{2.128}其中，$\\sigma \\gt 0$。参数 $\\sigma$ 被称为 缩放参数（scale parameter），概率密度具有缩放不变性（scale invariance）因为如果把 $x$ 缩放⼀个常数，得到 $\\hat{x} = cx$，那么： p(\\hat{x}|\\hat{\\sigma})=\\frac{1}{\\hat{\\sigma}}f\\left(\\frac{\\hat{x}}{\\hat{\\sigma}}\\right)\\tag{2.129}其中，$\\hat{\\sigma}=c\\sigma$。这个变换对应于单位的改变。想要选择⼀个能够反映这种缩放不变性的先验分布，因此我们选择的先验概率分布要对区间 $A \\le \\sigma \\le B$ 以及平移后的区间 $\\frac{A}{c} \\le \\sigma \\le \\frac{B}{c}$ 赋予相同的概率质量。这说明： \\int_{A}^{B} p(\\sigma)\\mathrm{d}\\sigma = \\int_{\\frac{A}{c}}^{\\frac{B}{c}} p(\\sigma)\\mathrm{d}\\sigma = \\int_{A}^{B} p\\left(\\frac{1}{c}\\sigma\\right)\\frac{1}{c}\\mathrm{d}\\sigma\\tag{2.130}并且由于这必须对于任意的 $A$ 和 $B$ 的选择都成⽴，因此有： p(\\sigma) = p\\left(\\frac{1}{c}\\sigma\\right)\\frac{1}{c}\\tag{2.131}二，非参数化方法具有具体函数形式的概率分布，并且由少量的参数控制，这些参数的值可以由数据集确定。这被称为概率密度建模的参数化（parametric）⽅法。 1，核密度估计假设观测服从 $D$ 维空间的某个未知的概率密度分布 $p(\\boldsymbol{x})$。把这个 $D$ 维空间选择成欧⼏⾥得空间， 并且我们想估计 $p(\\boldsymbol{x})$ 的值。 根据之前对于局部性的讨论， 考虑包含 $\\boldsymbol{x}$ 的某个⼩区域 $\\mathcal{R}$。这个区域的概率质量为： P = \\int_{\\mathcal{R}} p(\\boldsymbol{x}) \\mathrm{d}\\boldsymbol{x}\\tag{2.132}假设收集了服从 $p(\\boldsymbol{x})$ 分布的 $N$ 次观测，由于每个数据点都有⼀个落在区域 $\\mathcal{R}$ 中的概率 $P$ ，因此位于区域 $\\mathcal{R}$ 内部的数据点的总数 $K$ 将服从⼆项分布： \\text {Bin}(K|N, P) = \\frac{N!}{K!(N-K)!} P^{K}(1-P)^{N-K}\\tag{2.133}对于⼤的 $N$ 值， 这 个分布将会在均值附近产⽣尖峰，并且 $K\\simeq NP$。假定区域 $\\mathcal{R}$ ⾜够⼩，使得在这个区域内的概率密度 $p(\\boldsymbol{x})$ ⼤致为常数，设 $V$ 是区域 $\\mathcal{R}$ 的体积，那么 $P \\simeq p(\\boldsymbol{x})V$ 。 由以上分析，可以得到概率密度的估计： p(\\boldsymbol{x}) = \\frac{K}{NV}\\tag{2.134}我们有两种⽅式利⽤公式(2.232)的结果。 我们可以固定 $K$ 然后从数据中确定 $V$ 的值， 这就 是 $K$ 近邻⽅法。我们还可以固定 $V$ 然后从数据中确定 $K$ ，这就是核⽅法。在极限 $N \\to \\infty$ 的情况下，如果 $V$ 随着 $N$ ⽽合适地收缩，并且 $K$ 随着 $N$ 增⼤，那么可以证明 $K$ 近邻概率密度估计和核⽅法概率密度估计都会收敛到真实的概率密度（Duda and Hart, 1973）。 取区域 $\\mathcal{R}$ 以 $\\boldsymbol{x}$ 为中⼼的⼩超⽴⽅体，确定概率密度。为了统计落在这个区域内的数据点的数量 $K$ ，定义下⾯的函数： k(\\boldsymbol{\\mu})=\\left\\{\\begin{array}{l}{1，|\\mu_i| \\le \\frac{1}{2}, i=1,\\dots,D} \\\\ {0，其他情况}\\end{array}\\right.\\tag{2.135}并且满足： 1）$k(\\boldsymbol{\\mu}) \\ge 0$2）$\\int k(\\boldsymbol{\\mu}) \\mathrm{d} \\boldsymbol{\\mu} = 1$ 这表⽰⼀个以原点为中⼼的单位⽴⽅体。 函数 $k(\\boldsymbol{\\mu})$ 是 核函数（kernel function）的⼀个例⼦， 在这个问题中也被称为 Parzen窗（Parzen window）。 不难发现，如果数据点 $\\boldsymbol{x}_n$ 位于以 $\\boldsymbol{x}$ 为中⼼的边长为 $h$ 的⽴⽅体中，则位于这个⽴⽅体内的数据点的总数为： K=\\sum_{k=1}^{K}k\\left(\\frac{\\boldsymbol{x}-\\boldsymbol{x_n}}{h}\\right)\\tag{2.136}由公式(2.134)可得点 $\\boldsymbol{x}$ 处的概率密度估计，称为核密度估计，或者 Parzen估计： p(\\boldsymbol{x})=\\frac{1}{N}\\sum_{n=1}^{N}\\frac{1}{h^{D}} k \\left(\\frac{\\boldsymbol{x}-\\boldsymbol{x_n}}{h}\\right)\\tag{2.137}其中，记 $D$ 维边长为 $h$ 的⽴⽅体的体积公式 $V = h^D$ 。 使⽤⾼斯核函数，可以得到下⾯的核概率密度模型： p(\\boldsymbol{x})=\\frac{1}{N}\\sum_{n=1}^{N}\\frac{1}{\\left(2 \\pi h^{2}\\right)^{\\frac{D}{2}}} \\exp \\left\\{-\\frac{||\\boldsymbol{x}-\\boldsymbol{x}_n||^{2}}{2 h^{2}}\\right\\}\\tag{2.138}其中 $h$ 表⽰⾼斯分布的标准差。因此概率密度模型可以通过以下⽅式获得：令每个数据点都服从⾼斯分布，然后把数据集⾥的每个数据点的贡献相加，之后除以 $N$ ，使得概率密度正确地被归⼀化。 如图2.33，核密度模型。 2，近邻⽅法核⽅法进⾏概率密度估计的⼀个难点是控制核宽度的参数 $h$ 对于所有的核都是固定的。 在⾼数据密度的区域，⼤的 $h$ 值可能会造成过度平滑，并且破坏了本应从数据中提取出的结构； 但是，减⼩ $h$ 的值可能导致数据空间中低密度区域估计的噪声。因此，$h$ 的最优选择可能依赖于数据空间的位置。这个问题可以通过概率密度的近邻⽅法解决。 假设球体的半径可以⾃由增长，直到它精确地包含 $K$ 个数据点。 这样，概率密度 $p(\\boldsymbol{x})$ 的估计就由公式(2.134)给出， 其中 $V$ 等于最终球体的体积。这种⽅法被称为 K近邻⽅法。 如图2.34，$K$ 近邻⽅法。 现在讨论明概率密度估计的 $K$ 近邻⽅法如何推⼴到分类问题。 假设有⼀个数据集，其中 $N_k$ 个数据点属于类别 $\\mathcal{C}_k$ ，数据点的总数为 $N$ ，因此 $\\sum_{k} N_k = N$ 。如果想对⼀个新的数据点 $\\boldsymbol{x}$ 进⾏分类，那么可以画⼀个以 $\\boldsymbol{x}$ 为中⼼的球体，这个球体精确地包含 $K$ 个数据点（⽆论属于哪个类别）。假设球体的体积为 $V$ ，并且包含来⾃类别 $\\mathcal{C}_k$ 的 $K_k$ 个数据点，这样与每个类别关联的⼀个概率密度的估计： p(\\boldsymbol{x}|\\mathcal{C}_k) = \\frac{K_k}{N_kV}\\tag{2.139}⽆条件概率密度为： p(\\boldsymbol{x}) = \\frac{K}{NV}\\tag{2.140}类先验为： p(\\mathcal{C}_k) = \\frac{N_k}{N}\\tag{2.141}可以得到类别的后验概率公式： p(\\mathcal{C}_k | \\boldsymbol{x}) = \\frac{p(\\boldsymbol{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)}{p(\\boldsymbol{x})} = \\frac{K_k}{K}\\tag{2.142}最近邻 ($K = 1$) 分类器的⼀个重要的性质是：在极限 $N \\to \\infty$ 的情况下，错误率不会超过最优分类器（即使⽤真实概率分布的分类器）可以达到的最⼩错误率的⼆倍（Cover and Hart, 1967）。 如图2.35～2.36，$K$ 近邻分类器（$K=1$ 和 $K=3$）。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】概率分布之高斯分布","slug":"【机器学习基础】概率分布之高斯分布","date":"2019-09-29T08:11:56.000Z","updated":"2019-10-07T15:04:47.210Z","comments":true,"path":"2019/09/29/prml_02_02/","link":"","permalink":"https://zhangbc.github.io/2019/09/29/prml_02_02/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，多元高斯分布考虑⾼斯分布的⼏何形式，⾼斯对于 $\\boldsymbol{x}$ 的依赖是通过下⾯形式的⼆次型： \\Delta^{2} = (\\boldsymbol{x} - \\boldsymbol{\\mu})^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu})\\tag{2.30}其中，$\\Delta$ 被叫做 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{x}$ 之间的马⽒距离（Mahalanobis distance）。 当 $\\boldsymbol{\\Sigma}$ 是单位矩阵时，就变成了欧式距离。对于 $\\boldsymbol{x}$ 空间中这个⼆次型是常数的曲⾯，⾼斯分布也是常数。 现在考虑协⽅差矩阵的特征向量⽅程： \\boldsymbol{\\Sigma} \\boldsymbol{\\mu}_i = \\lambda_{i} \\boldsymbol{\\mu}_{i}\\tag{2.31}其中 $i = 1,\\dots , D$。由于 $\\boldsymbol{\\Sigma}$ 是实对称矩阵，因此它的特征值也是实数，并且特征向量可以被选成单位正交的，即： \\boldsymbol{\\mu}_{i}^{T} \\boldsymbol{\\mu}_{j} = I_{ij}\\tag{2.32}其中 $I_{ij}$ 是单位矩阵的第 $i, j$ 个元素，满⾜： I_{i j}=\\left\\{\\begin{array}{l}{1，如果 i=j} \\\\ {0，其他情况}\\end{array}\\right. \\tag{2.33}协⽅差矩阵 $\\boldsymbol{\\Sigma}$ 可以表⽰成特征向量的展开的形式： \\boldsymbol{\\Sigma} = \\sum_{i=1}^{D} \\lambda_i \\boldsymbol{\\mu}_{i}\\boldsymbol{\\mu}_{i}^{T}\\tag{2.34}协⽅差矩阵的逆矩阵 $\\boldsymbol{\\Sigma}^{-1}$ 可以表⽰成特征向量的展开的形式： \\boldsymbol{\\Sigma}^{-1} = \\sum_{i=1}^{D} \\frac{1}{\\lambda_i} \\boldsymbol{\\mu}_{i}\\boldsymbol{\\mu}_{i}^{T}\\tag{2.35}⼆次型公式(2.30)即可表示为： \\Delta^{2} = \\sum_{i=1}^{D} \\frac{y_{i}^{2}}{\\lambda_{i}}\\tag{2.36}其中，$y_{i}^{2} = \\boldsymbol{u_i^T} (\\boldsymbol{x} - \\boldsymbol{\\mu})$ 。 把 $\\{y_i\\}$ 表⽰成单位正交向量 $\\boldsymbol{\\mu_i}$ 关于原始的 $x_i$ 坐标经过平移和旋转后形成的新的坐标系。定义向量 $\\boldsymbol{y} = (y_1,\\dots, y_D)^T$ ，即有： \\boldsymbol {y} = \\boldsymbol{U} (\\boldsymbol{x} - \\boldsymbol{\\mu})\\tag{2.37}其中 $\\boldsymbol{U}$ 是⼀个矩阵，它的⾏是向量 $\\boldsymbol{u}_{i}^{T}$ 。从公式(2.32)可以看出 $\\boldsymbol{U}$ 是⼀个正交矩阵， 即它满⾜性质 $\\boldsymbol{U}\\boldsymbol{U}^T = \\boldsymbol{I}$ ，因此也满⾜ $\\boldsymbol{U}^T \\boldsymbol{U} = \\boldsymbol{I}$ ，其中 $\\boldsymbol{I}$ 是单位矩阵。 ⼀个特征值严格⼤于零的矩阵被称为正定（positive definite）矩阵。偶尔遇到⼀个或者多个特征值为零的⾼斯分布，那种情况下分布是奇异的，被限制在 了⼀个低维的⼦空间中。如果所有的特征值都是⾮负的，那么这个矩阵被称为半正定（positive semidefine）矩阵。 如图2.12，红⾊曲线表⽰⼆维空间 $\\boldsymbol{x} = (x_1 , x_2)$ 的⾼斯分布的常数概率密度的椭圆⾯， 它表⽰的概率密度为 $\\exp(−\\frac{1}{2})$，值是在 $\\boldsymbol{x} = \\boldsymbol{\\mu}$ 处计算的。椭圆的轴由协⽅差矩阵的特征向量 $\\mu_i$ 定义，对应的特征值为 $\\lambda_i$ 。 现在考虑在由 $y_i$ 定义的新坐标系下⾼斯分布的形式。 从 $\\boldsymbol{x}$ 坐标系到 $\\boldsymbol{y}$ 坐标系， 我们有⼀个 Jacobian矩阵 $\\boldsymbol{J}$ ，它的元素为： \\boldsymbol{J}_{ij} = \\frac{\\partial {x_i}}{\\partial {j_j}} = U_{ij}\\tag{2.38}其中 $U_{ji}$ 是矩阵 $\\boldsymbol{U}^T$ 的元素。使⽤矩阵 $\\boldsymbol{U}$ 的单位正交性质，我们看到 Jacobian矩阵 ⾏列式的平⽅为： | \\boldsymbol{J}^{2} | = |\\boldsymbol{U}^{T}|^{2} = |\\boldsymbol{U}^{T}||\\boldsymbol{U}| = |\\boldsymbol{U}^{T}\\boldsymbol{U}| = |\\boldsymbol{I}| = 1\\tag{2.39}从而可知，$|\\boldsymbol{J}|=1$ ，并且，⾏列式 $|\\boldsymbol{\\Sigma}|$ 的协⽅差矩阵可以写成特征值的乘积，因此： |\\boldsymbol{\\Sigma}|^{\\frac{1}{2}} = \\prod_{j=1}^{D} \\lambda_{j}^{\\frac{1}{2}}\\tag{2.40}因此在 $\\boldsymbol{y}$ 坐标系中，⾼斯分布的形式为： p(\\boldsymbol{y}) = p(\\boldsymbol{x})|\\boldsymbol{J}| = \\prod_{j=1}^{D} \\frac{1}{(2 \\pi \\lambda_{j})^{\\frac{1}{2}}} \\exp \\left \\{- \\frac{y_{i}^2}{2\\lambda_j} \\right \\}\\tag{2.41}这是 $D$ 个独⽴⼀元⾼斯分布的乘积。 在 $\\boldsymbol{y}$ 坐标系中，概率分布的积分为： \\int p(\\boldsymbol{y}) \\mathrm{d} \\boldsymbol{y} = \\prod_{j=1}^{D} \\int_{-\\infty}^{\\infty} \\frac{1}{(2 \\pi \\lambda_{j})^{\\frac{1}{2}}} \\exp \\left \\{- \\frac{y_{i}^2}{2\\lambda_j} \\right \\} \\mathrm{d} y_j = 1\\tag{2.42}⾼斯分布下 $\\boldsymbol{x}$ 的期望为： \\begin{aligned} \\mathbb{E}[\\boldsymbol{x}] &= \\frac{1}{(2 \\pi)^{\\frac{D}{2}}} \\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\int \\exp \\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right\\} \\boldsymbol{x} \\mathrm{d} \\boldsymbol{x} \\\\ &= \\frac{1}{(2 \\pi)^{\\frac{D}{2}}} \\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\int \\exp \\left\\{-\\frac{1}{2}\\boldsymbol{z}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{z}\\right\\} (\\boldsymbol{z+\\mu}) \\mathrm{d} \\boldsymbol{z} \\end{aligned}\\tag{2.43}其中，$\\boldsymbol{z = x - \\mu}$ 。注意到指数位置是 $\\boldsymbol{z}$ 的偶函数，并且由于积分区间为 $(−\\infty, \\infty)$，因此在因⼦ $(\\boldsymbol{z + \\mu})$ 中的 $\\boldsymbol{z}$ 中的项会由于对称性变为零。因此 $\\mathbb{E}[\\boldsymbol{x}] = \\boldsymbol{\\mu}$ 。称 $\\boldsymbol{\\mu}$ 为⾼斯分布的均值。 现在考虑⾼斯分布的⼆阶矩。对于多元⾼斯分布，有 $D^2$ 个由 $\\mathbb{E}[x_i x_j]$ 给出的⼆阶矩，可以聚集在⼀起组成矩阵 $\\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^T ]$。 \\begin{aligned} \\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{T}] &= \\frac{1}{(2 \\pi)^{\\frac{D}{2}}} \\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\int \\exp \\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right\\} \\boldsymbol{x} \\boldsymbol{x}^{T}\\mathrm{d} \\boldsymbol{x} \\\\ &= \\frac{1}{(2 \\pi)^{\\frac{D}{2}}} \\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\int \\exp \\left\\{-\\frac{1}{2}\\boldsymbol{z}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{z}\\right\\} (\\boldsymbol{z+\\mu})(\\boldsymbol{z+\\mu})^{T} \\mathrm{d} \\boldsymbol{z} \\end{aligned}\\tag{2.44}其中，$\\boldsymbol{z = x - \\mu}$ ，$\\boldsymbol{z} = \\sum_{j=1}^{D} y_i \\boldsymbol{u_j}$ ，$y_i = \\boldsymbol{u_j}^{T}\\boldsymbol{z}$ 。 由此可以推导出： \\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{T}] = \\boldsymbol{\\mu}\\boldsymbol{u}^{T} + \\boldsymbol{\\Sigma}\\tag{2.45}随机变量 $\\boldsymbol{x}$ 的协⽅差（covariance），定义为： \\text{var}[\\boldsymbol{x}] = \\mathbb{E}[(\\boldsymbol{x} - \\mathbb{E}[\\boldsymbol{x}])(\\boldsymbol{x} - \\mathbb{E}[\\boldsymbol{x}])^{T}]\\tag{2.46}对于⾼斯分布这⼀特例，我们可以使⽤ $\\mathbb{E}[\\boldsymbol{x}] = \\boldsymbol{\\mu}$ 以及公式(2.45)的结果，得到： \\text{var}[\\boldsymbol{x}] = \\boldsymbol{\\Sigma}\\tag{2.47}由于参数 $\\boldsymbol{\\Sigma}$ 公式了⾼斯分布下 $\\boldsymbol{x}$ 的协⽅差，因此它被称为协⽅差矩阵。 二，条件⾼斯分布多元⾼斯分布的⼀个重要性质：如果两组变量是联合⾼斯分布，那么以⼀组变量为条件， 另⼀组变量同样是⾼斯分布。 假设 $\\boldsymbol{x}$ 是⼀个服从⾼斯分布 $\\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})$ 的 $D$ 维向量。我们把 $\\boldsymbol{x}$ 划分成两个不相交的⼦集 $\\boldsymbol{x}_a$ 和 $\\boldsymbol{x}_b$ 。 不失⼀般性， 令 $\\boldsymbol{x}_a$ 为 $\\boldsymbol{x}$ 的前 $M$ 个分量， 令 $\\boldsymbol{x}_b$ 为剩余的 $D − M$ 个分量，因此 \\boldsymbol{x} = \\dbinom{\\boldsymbol{x}_a}{\\boldsymbol{x}_b}同理，对应的对均值向量 $\\boldsymbol{\\mu}$ 的划分，即 \\boldsymbol{\\mu} = \\dbinom{\\boldsymbol{\\mu}_a}{\\boldsymbol{\\mu}_b}协⽅差矩阵 $\\boldsymbol{\\Sigma}$ 为： \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{aa} & \\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{ba} & \\boldsymbol{\\Sigma}_{bb} \\end{pmatrix}\\tag{2.48}注意，协⽅差矩阵的对称性 $\\boldsymbol{\\Sigma} ^T= \\boldsymbol{\\Sigma}$ 表明 $\\boldsymbol{\\Sigma}_{aa}$ 和 $\\boldsymbol{\\Sigma}_{bb}$ 也是对称的，⽽ $\\boldsymbol{\\Sigma}_{ba} = \\boldsymbol{\\Sigma}_{ab}^{T}$ 。 在许多情况下，使⽤协⽅差矩阵的逆矩阵⽐较⽅便，也叫精度矩阵（precision matrix），即： \\boldsymbol{\\Lambda} \\equiv \\boldsymbol{\\Sigma}^{-1}\\tag{2.49}精度矩阵的划分形式 \\boldsymbol{\\Lambda} = \\begin{pmatrix} \\boldsymbol{\\Lambda}_{aa} & \\boldsymbol{\\Lambda}_{ab} \\\\ \\boldsymbol{\\Lambda}_{ba} & \\boldsymbol{\\Lambda}_{bb} \\end{pmatrix}关于分块矩阵的逆矩阵的恒等式： \\begin{pmatrix} \\boldsymbol{A} & \\boldsymbol{B} \\\\ \\boldsymbol{C} & \\boldsymbol{D} \\end{pmatrix}^{-1} = \\begin{pmatrix} \\boldsymbol{M} & \\boldsymbol{-MBD^{-1}} \\\\ \\boldsymbol{-D^{-1}CM} & \\boldsymbol{D^{-1}+CMBD^{-1}} \\end{pmatrix}\\tag{2.50}其中， $\\boldsymbol{M = (A-BD^{-1}C)^{-1}}$ ，$\\boldsymbol{M}^{-1}$ 被称为公式(2.50)左侧矩阵关于⼦矩阵 $\\boldsymbol{D}$ 的舒尔补（Schur complement）。 由以上公式和相关结论可以推导出条件概率分布 $p(\\boldsymbol{x}_a | \\boldsymbol{x}_b)$ 的均值和协⽅差的表达式： \\boldsymbol{\\mu}_{a|b} = \\boldsymbol{\\mu}_a + \\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}(\\boldsymbol{x}_b-\\boldsymbol{\\mu}_b)\\tag{2.51} \\boldsymbol{\\Sigma}_{a|b} = \\boldsymbol{\\Sigma}_{aa} - \\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\tag{2.52}三，边缘⾼斯分布对于边缘高斯分布： p(\\boldsymbol{x}_a) = \\int p(\\boldsymbol{x}_a, \\boldsymbol{x}_b) \\mathrm{d} \\boldsymbol{x}_b\\tag{2.53}同条件高斯分布一样，可以推导出边缘概率分布 $p(\\boldsymbol{x}_a)$ 的均值和协⽅差的表达式： \\boldsymbol{\\Sigma}_{a} = (\\boldsymbol{\\Lambda}_{aa} - \\boldsymbol{\\Lambda}{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba})^{-1}\\tag{2.54} \\mathbb{E}[\\boldsymbol{x}_a] = \\boldsymbol{\\mu}_a\\tag{2.55} \\text{cov}[\\boldsymbol{x}_a] = \\boldsymbol{\\Sigma}_{aa}\\tag{2.56}如图2.13，两个变量上的⾼斯概率分布 $p(x_a , x_b)$ 的轮廓线。 如图2.14，边缘概率分布 $p(x_a)$（蓝⾊曲线）和 $x_b = 0.7$ 的条件概率分布 $p(x_a|x_b)$（红⾊曲线）。 四，⾼斯变量的贝叶斯定理令边缘概率分布和条件概率分布的形式： p(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x} |\\boldsymbol{\\mu}, \\boldsymbol{\\Lambda}^{-1})\\tag{2.57} p(\\boldsymbol{y} | \\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{y} |\\boldsymbol{Ax+b}, \\boldsymbol{L}^{-1})\\tag{2.58}其中，$\\boldsymbol{\\mu}$ ， $\\boldsymbol{A}$ 和 $\\boldsymbol{b}$ 是控制均值的参数，$\\boldsymbol{\\Lambda}$ 和 $\\boldsymbol{L}$ 是精度矩阵。如果 $\\boldsymbol{x}$ 的维度为 $M$ ，$\\boldsymbol{y}$ 的维度为 $D$，那么矩阵 $A$ 的⼤⼩为 $D \\times M$ 。 ⾸先，我们寻找 $\\boldsymbol{x}$ 和 $\\boldsymbol{y}$ 的联合分布的表达式。令 \\boldsymbol{z} = \\dbinom{\\boldsymbol{x}}{\\boldsymbol{y}}然后考虑联合概率分布的对数： \\begin{aligned}\\ln p(\\boldsymbol{z}) &= \\ln p(\\boldsymbol{x}) + \\ln p(\\boldsymbol{y} | \\boldsymbol{x}) \\\\ &= -\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{T} \\Lambda (\\boldsymbol{x} - \\boldsymbol{\\mu}) \\\\ &-\\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{Ax} - \\boldsymbol{b})^{T} \\boldsymbol{L} (\\boldsymbol{y}-\\boldsymbol{Ax}-\\boldsymbol{b}) + 常数 \\end{aligned} \\tag{2.59}可以推导出，$\\boldsymbol{z}$ 上的⾼斯分布的精度矩阵（协⽅差的逆矩阵）为： \\boldsymbol{R} = \\begin{pmatrix} \\boldsymbol{\\Lambda + A^{T}LA} & \\boldsymbol{-A^{T}L} \\\\ \\boldsymbol{-LA} & \\boldsymbol{L} \\end{pmatrix}从而，$\\boldsymbol{z}$ 上的⾼斯分布的均值和协⽅差的表达式： \\text{cov}[\\boldsymbol{z}] = \\boldsymbol{R}^{-1} = \\begin{pmatrix} \\boldsymbol{\\Lambda^{-1} } & \\boldsymbol{\\Lambda^{-1}A^{T}} \\\\ \\boldsymbol{A\\Lambda^{-1}} & \\boldsymbol{L^{-1}+A\\Lambda^{-1}A^{T}} \\end{pmatrix}\\tag{2.60} \\mathbb{E}[\\boldsymbol{z}] = \\boldsymbol{R}^{-1} \\dbinom{\\boldsymbol{\\Lambda \\mu - A^{T}Lb}}{\\boldsymbol{Lb}}\\tag{2.61} \\mathbb{E}[\\boldsymbol{z}] = \\dbinom{\\boldsymbol{\\mu}}{\\boldsymbol{A\\mu+b}}\\tag{2.62}边缘分布 $p(\\boldsymbol{y})$ 的均值和协⽅差为： \\mathbb{E}[\\boldsymbol{y}] = \\boldsymbol{A\\mu+b}\\tag{2.63} \\text{cov}[\\boldsymbol{y}] = \\boldsymbol{L^{-1}+A\\Lambda^{-1}A^{T}}\\tag{2.64}条件分布 $p(\\boldsymbol{x}|\\boldsymbol{y})$ 的均值和协⽅差为： \\mathbb{E}[\\boldsymbol{x} | \\boldsymbol{y}] = (\\boldsymbol{\\Lambda + A^{T}LA})^{-1}\\{ \\boldsymbol{A^{T}L(y-b) + \\Lambda \\mu} \\}\\tag{2.65} \\text{cov}[\\boldsymbol{x|y}] = (\\boldsymbol{\\Lambda + A^{T}LA})^{-1}\\tag{2.66}五，⾼斯分布的最⼤似然估计给定⼀个数据集 $\\boldsymbol{X} = (\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_N)^T$ ， 其中观测 $\\{\\boldsymbol{x}_n\\}$ 假定是独⽴地从多元⾼斯分布中抽取的。我们可以使⽤最⼤似然法估计分布的参数。对数似然函数为： \\ln p(\\boldsymbol{X|\\mu, \\Sigma}) = -\\frac{ND}{2} \\ln (2\\pi) - \\frac{N}{2}\\ln \\boldsymbol{|\\Sigma|} - \\frac{1}{2}\\sum_{n=1}^{N}\\boldsymbol{(x_n -\\mu)^{T}\\Sigma^{-1}(x_n-\\mu)}\\tag{2.67}令对数似然函数关于 $\\mu$ 的导数为零，可以求得均值的最大似然估计： \\boldsymbol{\\mu}_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}\\boldsymbol{x}_n\\tag{2.68}方差的最大似然估计： \\boldsymbol{\\Sigma}_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_{ML})(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_{ML})^{T}\\tag{2.69}从而， \\mathbb{E}[\\boldsymbol{\\mu}_{ML}] = \\boldsymbol{\\mu}\\tag{2.70} \\mathbb{E}[\\boldsymbol{\\Sigma}_{ML}] = \\frac{N-1}{N}\\boldsymbol{\\Sigma}\\tag{2.71} \\tilde {\\boldsymbol{\\Sigma}}_{ML} = \\frac{1}{N}\\sum_{n=1}^{N-1}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_{ML})(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_{ML})^{T}\\tag{2.72}六，顺序估计考虑公式(2.68)给出的均值的最⼤似然估计结果 $\\boldsymbol{\\mu}_{ML}$ 。 当它依赖于第 $N$ 次观察时， 将记作 $\\boldsymbol{\\mu}_{ML}^{(N)}$ 。如果想分析最后⼀个数据点 $\\boldsymbol{x}_N$ 的贡献，即有： \\begin{aligned} \\boldsymbol{\\mu}_{ML}^{(N)} &= \\frac{1}{N}\\sum_{n=1}^{N}\\boldsymbol{x}_n \\\\ &= \\frac{1}{N}\\boldsymbol{x}_{N} + \\frac{1}{N}\\sum_{n=1}^{N-1}\\boldsymbol{x}_n \\\\ &= \\frac{1}{N}\\boldsymbol{x}_{N} + \\frac{N-1}{N} \\boldsymbol{\\mu}_{ML}^{(N-1)} \\\\ &= \\boldsymbol{\\mu}_{ML}^{(N-1)} + \\frac{1}{N}(\\boldsymbol{x}_{n} -\\boldsymbol{\\mu}_{ML}^{(N-1)}) \\end{aligned}\\tag{2.73}考虑⼀对随机变量 $\\theta$ 和 $z$ ， 它们由⼀个联合概率分布 $p(z, \\theta)$ 所控制。已知 $\\theta$ 的条件下， $z$ 的条件期望定义了⼀个确定的函数 $f(\\theta)$ ，叫回归函数，形式如下： f(\\theta) \\equiv \\mathbb{E}[z|\\theta] = \\int zp(z|\\theta)\\mathrm{d}z\\tag{2.74}如图2.15，回归函数 $f(\\theta)$ 。 ⽬标是寻找根 $\\theta^{∗}$ 使得 $f(\\theta^{∗}) = 0$。 如果有观测 $z$ 和 $\\theta$ 的⼀个⼤数据集， 那么可以直接对回归函数建模， 得到根的⼀个估计。 但是假设每次观测到⼀个 $z$ 的值， 我们想找到⼀个对应的顺序估计⽅法来找到 $\\theta^{∗}$ 。 下⾯的解决这种问题的通⽤步骤由 Robbins and Monro（1951）给出。假定 $z$ 的条件⽅差是有穷的，即： \\mathbb{E}[(z-f)^2|\\theta] \\lt \\infty并且不失⼀般性， 我们也假设当 $\\theta \\gt \\theta^{∗}$ 时 $f(\\theta) \\gt 0$， 当 $\\theta \\lt \\theta^{∗}$ 时 $f(\\theta) \\lt 0$，Robbins-Monro 的⽅法定义了⼀个根 $\\theta^{∗}$ 的顺序估计的序列，由公式(2.75)给出。 \\theta^{(N)} = \\theta^{(N-1)} + \\alpha_{N-1}z(\\theta^{(N-1)})\\tag{2.75}其中 $z(\\theta^{(N)})$ 是当 $\\theta$ 的取值为 $\\theta (N)$ 时 $z$ 的观测值。系数 $\\{\\alpha_N\\}$ 表⽰⼀个满⾜下列条件的正数序列： \\lim_{N \\to \\infty}\\alpha_{N}=0 \\sum_{N=1}^{\\infty} \\alpha_{N} = \\infty \\sum_{N=1}^{\\infty} \\alpha_{N}^{2} \\lt \\infty根据定义，最⼤似然解 $\\theta_{ML}$ 是负对数似然函数的⼀个驻点，因此满⾜： \\left . \\frac{\\partial}{\\partial \\theta} \\left\\{\\frac{1}{N}\\sum_{n=1}^{N}- \\ln p(x_N|\\theta) \\right\\} \\right|_{\\theta_{ML}} = 0\\tag{2.76}交换导数与求和，取极限 $N \\to \\infty$ ，可以寻找最⼤似然解对应于寻找回归函数的根。 于是可以应⽤ Robbins-Monro⽅法，此时它的形式为： \\theta^{(N)} = \\theta^{(N-1)} + \\alpha_{N-1} \\frac{\\partial}{\\partial\\theta^{(N-1)}} \\left [-\\ln p(x_N |\\theta^{(N-1)}) \\right ]\\tag{2.77}七，⾼斯分布的贝叶斯推断考虑⼀个⼀元⾼斯随机变量 $\\mathbf{x}$，我们假设⽅差 $\\sigma^2$ 是已知的，其任务是从⼀组 $N$ 次观测 $\\mathbf{x}=(x_1,\\dots, x_N)^T$ 中推断均值 $\\mu$。 似然函数，即给定 $\\mu$ 的情况下，观测数据集出现的概率。它可以看成 $\\mu$ 的函数，由公式(2.78)给出。 p(\\mathbf{x}|\\mu) = \\prod_{n=1}^{N}p(x_n|\\mu) = \\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{\\frac{N}{2}}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}\\sum_{n=1}^{N}(x_n-\\mu)^{2}\\right\\}\\tag{2.78}注意：似然函数 $p(\\mathbf{x}|\\mu)$ 不是 $\\mu$ 的概率密度，没有被归⼀化。 如图2.16，在⾼斯分布的情形中，回归函数的形式。 令先验概率分布为： p(\\mu) = \\mathcal{N}\\left(\\mu | \\mu_0, \\sigma_{0}^{2}\\right)\\tag{2.79}从⽽后验概率为： p(\\mu | \\mathbf{x}) = \\mathcal{N}\\left(\\mu | \\mu_N, \\sigma_{N}^{2}\\right)\\tag{2.80}其中， \\mu_N = \\frac{\\sigma^2}{N\\sigma_{0}^2 + \\sigma^2}\\mu_0 + \\frac{N\\sigma_{0}^2}{N\\sigma_{0}^2 + \\sigma^2}\\mu_{ML} \\frac{1}{\\sigma_{N}^{2}} = \\frac{1}{\\sigma_{0}^{2}} + \\frac{N}{\\sigma^{2}} \\mu_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}x_n图2.17，⾼斯分布均值的贝叶斯推断。 现在假设均值是已知的，我们要推断⽅差。令 $\\lambda \\equiv \\frac{1}{\\sigma^{2}}$ ，$\\lambda$ 的似然函数的形式为： p(\\mathbf{x}|\\lambda) = \\prod_{n=1}^{N}\\mathcal{N}(x_n|\\mu, \\lambda^{-1}) \\propto \\lambda^{\\frac{N}{2}} \\exp \\left\\{-\\frac{\\lambda}{2}\\sum_{n=1}^{N}(x_n-\\mu)^{2}\\right\\}\\tag{2.81}对应的共轭先验因此应该正⽐于 $\\lambda$ 的幂指数，也正⽐于 $\\lambda$ 的线性函数的指数。这对应于 Gamma分布，定义为： \\text{Gam}(\\lambda|a,b) = \\frac{1}{\\Gamma(a)}b^{a}\\lambda^{a-1}\\exp (-b\\lambda)\\tag{2.82}均值和协⽅差分别为： \\mathbb{E}[\\lambda] = \\frac{a}{b}\\tag{2.83} \\text{var}[\\lambda] = \\frac{a}{b^2}\\tag{2.84}如图2.18～2.20，不同的 $a$ 和 $b$ 的情况下 Gamma分布的图像。 考虑⼀个先验分布 $\\text{Gam}(\\lambda|a_0,b_0)$。如果乘以公式(2.81)给出的似然函数，那么即可得到后验分布： p(\\lambda | \\mathbf{x}) \\propto \\lambda^{a_0-1} \\lambda^{\\frac{N}{2}} \\exp \\left\\{-b_0 \\lambda -\\frac{\\lambda}{2}\\sum_{n=1}^{N}(x_n-\\mu)^{2}\\right\\}\\tag{2.85}我们可以把它看成形式为 $\\text{Gam}(\\lambda|a_N,b_N)$ 的 Gamma分布，其中 a_N = a_0 + \\frac{N}{2} b_N = b_0 \\frac{1}{2}\\sum_{n=1}^{N}(x_n-\\mu)^2 = b_0 + \\frac{N}{2}\\sigma_{ML}^{2}现在假设均值和精度都是未知的。为了找到共轭先验，考虑似然函数对于 $\\mu$ 和 $\\lambda$ 的依赖关系： \\begin{aligned} p(\\mathbf{x}|\\mu,\\lambda) &= \\prod_{n=1}^{N} \\left(\\frac{\\lambda}{2\\pi} \\right)^{\\frac{1}{2}} \\exp \\left\\{-\\frac{\\lambda}{2}(x_n-\\mu)^{2}\\right\\} \\\\ &\\propto \\left[\\lambda^{\\frac{1}{2}} \\exp\\left(-\\frac{\\lambda \\mu^{2}}{2}\\right) \\right]^{N} \\exp \\left\\{\\lambda \\mu \\sum_{n=1}^{N}x_n - \\frac{\\lambda}{2}\\sum_{n=1}^{N}x_{n}^{2}\\right\\} \\end{aligned}\\tag{2.86}假设先验分布的形式为： \\begin{aligned} p(\\mu,\\lambda) &= \\exp \\left\\{-\\frac{\\beta \\lambda}{2}\\left(\\mu-\\frac{c}{\\beta}\\right)^2 \\right\\} \\lambda^{\\frac{\\beta}{2}} \\exp \\left\\{-\\left(d-\\frac{c^2}{2\\beta}\\right)\\lambda \\right\\} \\\\ &\\propto \\left[\\lambda^{\\frac{1}{2}} \\exp\\left(-\\frac{\\lambda \\mu^{2}}{2}\\right) \\right]^{\\beta} \\exp \\left\\{c\\lambda \\mu - d\\lambda\\right\\} \\end{aligned}\\tag{2.87}其 中 $c, d$ 和 $\\beta$ 都是常数。 归⼀化的先验概率的形式为： p(\\mu,\\lambda) = \\mathcal{N}(\\mu|\\mu_0, (\\beta \\lambda)^{-1})\\text{Gam}(\\lambda|a,b)\\tag{2.88}这被称为正态-Gamma分布或者⾼斯-Gamma分布。如图2.21： 对于 $D$ 维向量 $\\boldsymbol{x}$ 的多元⾼斯分布 $\\mathcal{N}(\\boldsymbol{x|\\mu, \\Lambda}^{−1})$，假设精度已知，则均值 $\\boldsymbol{\\mu}$ 的共轭先验分布仍然是⾼斯分布。对于已知均值未知精度矩阵 $\\boldsymbol{\\Lambda}$ 的情形，共轭先验是Wishart分布，定义为： \\mathcal{W}(\\mathbf{\\Lambda} | \\boldsymbol{W}, \\nu)=B|\\boldsymbol{\\Lambda}|^{\\frac{\\nu-D-1}{2}} \\exp \\left(-\\frac{1}{2} \\operatorname{Tr}\\left(\\boldsymbol{W}^{-1} \\boldsymbol{\\Lambda}\\right)\\right)\\tag{2.89}其中 $\\nu$ 被称为分布的⾃由度数量(degrees of freedom)，$\\boldsymbol{W}$ 是⼀个 $D \\times D$ 的标量矩阵，$\\operatorname{Tr}(·)$ 表⽰矩阵的迹。归⼀化系数 $B$ 为： B(\\boldsymbol{W}, \\nu)=|\\boldsymbol{W}|^{-\\frac{\\nu}{2}}\\left(2^{\\frac{\\nu D}{2}} \\pi^{\\frac{D(D-1)}{4}} \\prod_{i=1}^{D} \\Gamma\\left(\\frac{\\nu+1-i}{2}\\right)\\right)^{-1}\\tag{2.90}如果均值和精度都是未知的，那么类似于⼀元变量的推理⽅法，共轭先验为： p(\\boldsymbol{\\mu,\\Lambda|\\mu}_0,\\beta,\\boldsymbol{W}, \\nu) = \\mathcal{N}(\\boldsymbol{\\mu|\\mu}_0, (\\beta \\boldsymbol{\\Lambda})^{-1})\\mathcal{W}(\\mathbf{\\Lambda} | \\boldsymbol{W}, \\nu)\\tag{2.91}这被称为正态-Wishart分布或者⾼斯-Wishart分布。 八，学生 $\\mathbf{t}$ 分布如果有⼀个⼀元⾼斯分布 $\\mathcal{N}\\left(x | \\mu, \\tau^{-1}\\right)$ 和⼀个 Gamma先验分布 $\\text{Gam}(\\tau|a, b)$，把精度积分出来，便可以得到 $x$ 的边缘分布，形式为： \\begin{aligned} p(x | \\mu, a, b) &=\\int_{0}^{\\infty} \\mathcal{N}\\left(x | \\mu, \\tau^{-1}\\right) \\operatorname{Gam}(\\tau | a, b) \\mathrm{d} \\tau \\\\ &=\\int_{0}^{\\infty} \\frac{b^{a} e^{(-b r)} \\tau^{a-1}}{\\Gamma(a)}\\left(\\frac{\\tau}{2 \\pi}\\right)^{\\frac{1}{2}} \\exp \\left\\{-\\frac{\\tau}{2}(x-\\mu)^{2}\\right\\} \\mathrm{d} \\tau \\\\ &=\\frac{b^{a}}{\\Gamma(a)}\\left(\\frac{1}{2 \\pi}\\right)^{\\frac{1}{2}}\\left[b+\\frac{(x-\\mu)^{2}}{2}\\right]^{-a-\\frac{1}{2}} \\Gamma\\left(a+\\frac{1}{2}\\right) \\end{aligned}\\tag{2.92}形如 $p(x|\\mu a,b)$ 如下： \\text{St}(x|\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu}{2}+\\frac{1}{2})}{\\Gamma(\\frac{\\nu}{2})}\\left(\\frac{\\lambda}{\\pi \\nu}\\right)^{\\frac{1}{2}}\\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu}{2}-\\frac{1}{2}}\\tag{2.93}称为学生 t 分布（Student&#39;s t-distribution）。 参数 $\\lambda$ 有时被称为 $\\mathbf{t}$ 分布的精度（precision）， 即使它通常不等于⽅差的倒数。参数 $\\nu$ 被称为⾃由度（degrees of freedom）。如图2.22： 学生 $\\mathbf{t}$ 分布的⼀个重要性质：鲁棒性（robustness），即对于数据集⾥的⼏个离群点outlier的出现，分布不会像⾼斯分布那样敏感。 图 2.23，从⼀个⾼斯分布中抽取的30个数据点的直⽅图，以及得到的最⼤似然拟合。红⾊曲线表⽰使⽤ $\\mathbf{t}$ 分布进⾏的拟合，绿⾊曲线（⼤部分隐藏在了红⾊曲 线后⾯）表⽰使⽤⾼斯分布进⾏的拟合。由于 $\\mathbf{t}$ 分布将⾼斯分布作为⼀种特例，因此它给出了与⾼斯分布⼏乎相同的解。 图 2.24，与图2.23同样的数据集，但是多了三个异常数据点。这幅图展⽰了⾼斯分布（绿⾊曲线）是如 何被异常点强烈地⼲扰的，⽽ $\\mathbf{t}$ 分布（红⾊曲线）相对不受影响。 推⼴到多元⾼斯分布 $\\mathcal{N}(\\boldsymbol{x|\\mu, \\Lambda})$ 来得到对应的多元学生 $\\mathbf{t}$ 分布，形式为： \\operatorname{St}(\\boldsymbol{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda}, \\nu)=\\int_{0}^{\\infty} \\mathcal{N}\\left(\\boldsymbol{x} | \\boldsymbol{\\mu},(\\eta \\boldsymbol{\\Lambda})^{-1}\\right) \\operatorname{Gam}\\left(\\eta | \\frac{\\nu}{2}, \\frac{\\nu}{2}\\right) \\mathrm{d} \\nu \\tag{2.94}求积分，可得： \\text{St}(\\boldsymbol{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda},,\\nu) = \\frac{\\Gamma(\\frac{\\nu}{2}+\\frac{D}{2})}{\\Gamma(\\frac{\\nu}{2})}\\left(\\frac{|\\boldsymbol{\\Lambda}|}{(\\pi \\nu)^D}\\right)^{\\frac{1}{2}}\\left[1+\\frac{\\Delta^{2}}{\\nu}\\right]^{-\\frac{\\nu}{2}-\\frac{D}{2}}\\tag{2.95}其中 $D$ 是 $\\boldsymbol{x}$ 的维度，$\\Delta^2$ 是平⽅马⽒距离，定义为： \\Delta^2 = (\\boldsymbol{x-\\mu})^T \\boldsymbol{\\Lambda} (\\boldsymbol{x-\\mu})\\tag{2.96}多元变量形式的学生 $\\mathbf{t}$ 分布，满⾜下⾯的性质： 1）$\\mathbb{E}[\\boldsymbol{x}] = \\boldsymbol{\\mu}$ 如果 $\\nu \\gt 1$ 2）$\\text{cov}[\\boldsymbol{x}] = \\frac{\\nu}{\\nu-2}\\boldsymbol{\\Lambda}^{-1}$ 如果 $\\nu \\gt 2$ 3）$\\text{mode}[\\boldsymbol{x}] = \\boldsymbol{\\mu}$ 九，周期变量考察⼀个⼆维单位向量 $\\boldsymbol{x}_1,\\dots,\\boldsymbol{x}_N$ ， 其中 $||\\boldsymbol{x}_n|| = 1$ 且 $n = 1,\\dots , N$ ， 如图2.25所⽰。 可以对向量 $\\{\\boldsymbol{x}_n\\}$ 求平均，可得 \\bar{\\boldsymbol{x}} = \\frac{1}{N}\\sum_{n=1}^{N}\\boldsymbol{x}_n注意，$\\bar{\\boldsymbol{x}}$ 通常位于单位圆的内部。 $\\bar{\\boldsymbol{x}}$ 对应的角度 $\\bar{\\theta}$ 为： \\bar{\\theta} = \\tan^{-1} \\left\\{\\frac{\\sum_{n}\\sin \\theta_n}{\\sum_{n}\\cos \\theta_n} \\right\\}\\tag{2.97}考虑的周期概率分布 $p(\\theta)$ 的周期为 $2\\pi$ 。$\\theta$ 上的任何概率密度 $p(\\theta)$ ⼀定⾮负， 积分等于1，并且⼀定是周期性的。因此， $p(\\theta)$ ⼀定满⾜下⾯三个条件： 1） $p(\\theta) \\ge 0$ 2） $\\int_{0}^{2\\pi} p(\\theta) \\mathrm{d}\\theta = 1$ 3） $p(\\theta + 2\\pi) = p(\\theta)$ 考虑两个变量 $\\boldsymbol{x} = (x_1 , x_2)$ 的⾼斯分布，均值为 $\\boldsymbol{\\mu} = (\\mu_1, \\mu_2)$，协⽅差矩阵为 $\\boldsymbol{\\Sigma} = \\sigma^2 \\boldsymbol{I}$ ，其中 $\\boldsymbol{I}$ 是⼀个 $2\\times2$ 的单位矩阵。因此有： p(x_1,x_2) = \\frac{1}{2\\pi \\sigma^{2}} \\exp \\left\\{-\\frac{(x_1-\\mu_1)^2+(x_2-\\mu_2)^{2}}{2\\sigma^{2}}\\right\\}\\tag{2.98}von Mises分布(环形正态分布（circular normal））：在单位圆 $r=1$上的概率分布 $p(\\theta)$ 的最终表达式： p(\\theta|\\theta_0,m) = \\frac{1}{2\\pi I_0(m)} \\exp \\left\\{m\\cos(\\theta-\\theta_0)\\right\\}\\tag{2.99}其中，参数 $\\theta_0$ 对应于分布的均值，$m$ 被称为 concentration参数，类似于⾼斯分布的⽅差的倒数（精度）。归⼀化系数包含项 $I_0 (m)$，是零阶修正的第⼀类Bessel函数（Abramowitz and Stegun, 1965）， 定义为： I_0(m) = \\frac{1}{2\\pi} \\int_{0}^{2\\pi}\\exp\\{m\\cos \\theta\\}\\mathrm{d}\\theta\\tag{2.100}如图2.26～2.27，von Mises分布的图像。 如图2.28， Bessel函数 $I_0 (m)$ 的图像。 现在考虑 von Mises分布 的参数 $\\theta_0$ 和参数 $m$ 的最⼤似然估计。对数似然函数为： \\ln p(\\mathcal{D} | \\theta_0,m)=-N\\ln (2\\pi)-\\ln I_0(m)+m\\sum_{n=1}^{N}\\cos(\\theta_n-\\theta_0)\\tag{2.101}令其关于 $\\theta_0$ 的导数等于零，从⽽可以得到： \\theta_{0}^{ML} = \\tan^{-1} \\left\\{\\frac{\\sum_{n}\\sin \\theta_n}{\\sum_{n}\\cos \\theta_n} \\right\\}\\tag{2.102}关于 $m$ 最⼤化公式(2.101)，使⽤ $I_0^{\\prime}(m)=I_1(m)$（Abramowitz and Stegun, 1965），从⽽可以得到： A(m_{NL})=\\frac{1}{N}\\sum_{n=1}^{N}\\cos(\\theta_{n}-\\theta_{0}^{ML})\\tag{2.103}令 A(m)=\\frac{I_1(m)}{I_0(m)}可以得到： A(m_{ML})=\\left(\\frac{1}{N}\\sum_{n=1}^{N}\\cos \\theta_{n}\\right)\\cos \\theta_{0}^{ML} + \\left(\\frac{1}{N}\\sum_{n=1}^{N}\\sin \\theta_{n}\\right)\\sin \\theta_{0}^{ML}\\tag{2.104}如图2.29， 函数 $A (m)$ 的图像。 十，混合高斯模型通过将更基本的概率分布（例如⾼斯分布）进⾏线性组合的这样的叠加⽅法，可以被形式化为概率模型，被称为混合模型（mixture distributions）（McLachlan and Basford, 1988; McLachlan and Peel, 2000）。 考虑 $K$ 个⾼斯概率密度的叠加，形式为： p(\\boldsymbol{x}) = \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}(\\boldsymbol{x} |\\boldsymbol{\\mu_{k}}, \\boldsymbol{\\Sigma}_{k})\\tag{2.105}这被称为混合⾼斯（mixture of Gaussians）。 每⼀个⾼斯概率密度 $\\mathcal{N}(\\boldsymbol{x} |\\boldsymbol{\\mu_{k}}, \\boldsymbol{\\Sigma}_{k})$ 被称为混合分布的⼀个成分（component），并且有⾃⼰的均值 $\\boldsymbol{\\mu_{k}}$ 和协⽅差 $\\boldsymbol{\\Sigma}_{k}$。参数 $\\pi_{k}$ 被称为混合系数（mixing coefficients），并且满足以下条件： 1）$\\sum_{k=1}^{K} \\pi_{k}=1$2）$0\\le \\pi_{k} \\le 1$ 如图2.30，每个混合分量的常数概率密度轮廓线，其中三个分量分别被标记为红⾊、蓝⾊和绿⾊， 且混合系数的值在每个分量的下⽅给出。 如图2.31， 混合分布的边缘概率密度 $p(\\boldsymbol{x})$ 的轮廓线。 如图2.32， 概率分布 $p(\\boldsymbol{x})$ 的⼀个曲⾯图。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】概率分布之变量","slug":"【机器学习基础】概率分布之变量","date":"2019-09-29T06:26:42.000Z","updated":"2019-10-07T14:37:49.296Z","comments":true,"path":"2019/09/29/prml_02_01/","link":"","permalink":"https://zhangbc.github.io/2019/09/29/prml_02_01/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一，二元变量1，二项分布考虑⼀个⼆元随机变量 $x \\in \\{0, 1\\}$。 例如，$x$ 可能描述了扔硬币的结果，$x = 1$ 表⽰“正⾯”，$x = 0$ 表⽰反⾯。我们可以假设有⼀个损坏的硬币，这枚硬币正⾯朝上的概率未必等于反⾯朝上的概率。$x = 1$ 的概率被记作参数 $\\mu$，因此有： p(x=1|\\mu) = \\mu\\tag{2.1}其中 $0\\le \\mu\\le 1$ 。$x$ 的概率分布因此可以写成： \\text {Bern}(x|\\mu) = \\mu^{x}(1-\\mu)^{1-x}\\tag{2.2}这被叫做伯努利分布（Bernoulli distribution）。容易证明，这个分布是归⼀化的，并且均值和⽅差分别为： \\mathbb{E}[x] = \\mu\\tag{2.3} \\text{var}[x] = \\mu(1-\\mu)\\tag{2.4}如图 2.1，⼆项分布关于 $m$ 的函数的直⽅图，其中 $N = 10$ 且 $\\mu = 0.25$。 假设我们有⼀个 $x$ 的观测值的数据集 $\\mathcal{D} = \\{x_1 ,\\dots, x_N\\}$。假设每次观测都是独⽴地从 $p(x | \\mu)$ 中抽取的，因此可以构造关于 $\\mu$ 的似然函数： p(\\mathcal{D}|\\mu) = \\prod_{n=1}^{N}p(x_{n}|\\mu) = \\prod_{n=1}^{N}\\mu^{x_{n}}(1-\\mu)^{1-x_{n}}\\tag{2.5}其对数似然函数： \\ln p(\\mathcal{D}|\\mu) = \\sum_{n=1}^{N}\\ln p(x_{n}|\\mu) = \\sum_{n=1}^{N}\\{ x^n \\ln \\mu + (1-x^n) \\ln (1-\\mu)\\}\\tag{2.6}在公式(2.6)中，令 $\\ln p(\\mathcal{D}|\\mu)$ 关于 $\\mu$ 的导数等于零，就得到了最⼤似然的估计值，也被称为样本均值（sample mean）： \\mu_{ML} = \\frac{1}{N} \\sum_{n=1}^{N} x_{n}\\tag{2.7}求解给定数据集规模 $N$ 的条件下，$x = 1$ 的观测出现的数量 $m$ 的概率分布。 这被称为⼆项分布 （binomial distribution）： \\text {Bin}(m|N, \\mu) = \\dbinom{N}{m} \\mu^{m}(1-\\mu)^{N-m}\\tag{2.8}其中， \\dbinom{N}{m} = \\frac{N!}{(N-m)!m!}\\tag{2.9}二项分布 的均值和⽅差分别为： \\mathbb{E}[m] = \\sum_{m=0}^{N} \\text{Bin}(m|N, \\mu) = N\\mu\\tag{2.10} \\text{var}[m] = \\sum_{m=0}^{N} (m-\\mathbb{E}[m])^{2} \\text{Bin}(m|N, \\mu) = N\\mu(1-\\mu)\\tag{2.11}2，Beta分布首先，Gamma函数的定义为： \\Gamma(x) \\equiv \\int_{0}^{\\infty} u^{x-1} e^{-u} \\mathrm{d} u\\tag{2.12}Gamma函数具有如下性质： 1）$\\Gamma(x+1) = x \\Gamma(x)$2）$\\Gamma(1)=1$3）当 $n$ 为整数时，$\\Gamma(n+1) = n!$ 如果我们选择⼀个正⽐于 $\\mu$ 和 $(1 − \\mu)$ 的幂指数的先验概率分布， 那么后验概率分布（正⽐于先验和似然函数的乘积）就会有着与先验分布相同的函数形式。这 个性质被叫做共轭性（conjugacy）。先验分布选择Beta分布定义为： \\text {Beta}(\\mu | a,b) = \\frac{\\Gamma{(a+b)}}{\\Gamma{(a)}\\Gamma{(b)}} \\mu^{(a-1)}(1-\\mu)^{(b-1)}\\tag{2.13}其中参数 $a$ 和 $b$ 经常被称为超参数（hyperparameter），均值和⽅差分别为： \\mathbb{E}[\\mu] = \\frac{a}{a+b}\\tag{2.14} \\text{var}[\\mu] = \\frac{ab}{(a+b)^{2}(a+b+1)}\\tag{2.15}把Beta先验与⼆项似然函数相乘，然后归⼀化。只保留依赖于 $\\mu$ 的因⼦，从而得到后验概率分布的形式为： p(\\mu | m, l, a,b) = \\frac{\\Gamma{(m+a+l+b)}}{\\Gamma{(m+a)}\\Gamma{(l+b)}} \\mu^{(m+a-1)}(1-\\mu)^{(l+b-1)}\\tag{2.16}其中 $l = N − m$。 如图2.2～2.5： 对于不同的超参数 $a$ 和 $b$，公式(2.13)给出的Beta分布 $\\text{Beta}(\\mu | a, b)$ 关于 $\\mu$ 的函数图像。 贝叶斯学习过程存在⼀个共有的属性：随着我们观测到越来越多的数据，后验概率表⽰的不确定性将会持续下降。 为了说明这⼀点，我们可以⽤频率学家的观点考虑贝叶斯学习问题。考虑⼀个⼀般的贝叶斯推断问题，参数为 $\\boldsymbol {\\theta}$ ，并且我们观测到了⼀个数据集 $\\mathcal{D}$，由联合概率分布 $p(\\boldsymbol {\\theta}, \\mathcal{D})$ 描述，有： \\mathbb{E}_{\\boldsymbol {\\theta}}[\\boldsymbol {\\theta}] = \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\boldsymbol {\\theta}}[\\boldsymbol {\\theta}|\\mathcal{D}]]\\tag{2.17}其中， \\mathbb{E}_{\\boldsymbol {\\theta}}[\\boldsymbol {\\theta}] = \\int p(\\boldsymbol {\\theta}) \\boldsymbol {\\theta} \\mathrm{d} \\boldsymbol {\\theta}\\tag{2.18} \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\boldsymbol {\\theta}}[\\boldsymbol {\\theta}|\\mathcal{D}]] = \\int \\left \\{ \\int \\boldsymbol {\\theta}p(\\boldsymbol {\\theta}|\\mathcal{D}) \\mathrm{d} \\boldsymbol {\\theta} \\right \\} p(\\mathcal{D})\\mathrm{d} \\mathcal{D}\\tag{2.19}方差， \\text{var}_{\\boldsymbol {\\theta}}[\\boldsymbol {\\theta}] = \\mathbb{E}_{\\mathcal{D}}[\\text{var}[\\boldsymbol {\\theta}|\\mathcal{D}]] + \\text{var}_{\\mathcal{D}} [\\mathbb{E}_{\\boldsymbol {\\theta}}[\\boldsymbol {\\theta}|\\mathcal{D}]]\\tag{2.20}二，多项式变量1，多项式分布“1-of-K ”表⽰法 ： 变量被表⽰成⼀个 $K$ 维向量 $\\boldsymbol{x}$，向量中的⼀个元素 $x_k$ 等于1，剩余的元素等于0。注意，这样的向量 $\\boldsymbol{x}$ 满足 $\\sum_{k=1}^{K} x_k = 1$ ，如果我们⽤参数 $\\mu_k$ 表⽰ $x_k = 1$ 的概率，那么 $\\boldsymbol{x}$ 的分布： p(\\boldsymbol{x}|\\boldsymbol{\\mu}) = \\prod_{k=1}^{K} \\mu_{k}^{x_k}\\tag{2.21}其中 $\\boldsymbol{\\mu} = (\\mu_1 ,\\dots, \\mu_K)^T$ ， 参数 $\\mu_k$ 要满⾜ $\\mu_k \\ge 0$ 和 $\\sum_{k} \\mu_k = 1$ 。 容易看出，这个分布是归⼀化的： \\sum_{\\boldsymbol {x}}p(\\boldsymbol{x} | \\boldsymbol{\\mu}) = \\sum_{k=1}^{K} \\mu_k = 1\\tag{2.22}并且， \\mathbb{E}[\\boldsymbol{x}|\\boldsymbol{\\mu}] = \\sum_{\\boldsymbol {x}}p(\\boldsymbol{x} | \\boldsymbol{\\mu}) \\boldsymbol{x} = (\\mu_1 ,\\dots, \\mu_K)^T = \\boldsymbol {\\mu}\\tag{2.23}现在考虑⼀个有 $N$ 个独⽴观测值 $\\boldsymbol {x}_1 ,\\dots, \\boldsymbol {x}_N$ 的数据集 $\\mathcal{D}$。对应的似然函数的形式为： p(\\mathcal{D}|\\boldsymbol{\\mu}) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\mu_{k}^{x_{nk}} = \\prod_{k=1}^{K} \\mu_{k}^{(\\sum_{n}x_{nk})} = \\prod_{k=1}^{K} \\mu_{k}^{m_k}\\tag{2.24}看到似然函数对于 $N$ 个数据点的依赖只是通过 $K$ 个下⾯形式的量： m_k = \\sum_{n}x_{nk}\\tag{2.25}它表⽰观测到 $x_k = 1$ 的次数。这被称为这个分布的充分统计量（sufficient statistics）。 通过拉格朗⽇乘数法容易求得最大似然函数： \\mu_k^{ML} = \\frac{m_k}{N}\\tag{2.26}考虑 $m_1 ,\\dots , m_K$ 在参数 $\\boldsymbol{\\mu}$ 和观测总数 $N$ 条件下的联合分布。根据公式(2.24)，这个分布的形式为： \\text{Mult}(m_1 ,\\dots , m_K | \\boldsymbol{\\mu}, N) = \\dbinom{N}{m_1 \\dots m_K}\\prod_{k=1}^{K} \\mu_{k}^{m_k}\\tag{2.27}这被称为多项式分布（multinomial distribution）。 归⼀化系数是把 $N$ 个物体分成⼤⼩为 $m_1 ,\\dots , m_K$ 的 $K$ 组的⽅案总数，定义为： \\dbinom{N}{m_1 \\dots m_K} = \\frac{N!}{m_1!m_2! \\dots m_K!}\\tag{2.28}其中，$m_k$ 满足以下限制 $\\sum_{k=1}^{K} m_k = N$ 。 2，狄利克雷分布狄利克雷分布（Dirichlet distribution）或多元Beta分布（multivariate Beta distribution）是一类在实数域以正单纯形（standard simplex）为支撑集（support）的高维连续概率分布，是 Beta分布在高维情形的推广 。狄利克雷分布是指数族分布之一，也是刘维尔分布（Liouville distribution）的特殊形式，将狄利克雷分布的解析形式进行推广可以得到广义狄利克雷分布（generalized Dirichlet distribution）和组合狄利克雷分布（Grouped Dirichlet distribution）。 狄利克雷分布概率的归⼀化形式为： \\text{Dir}(\\boldsymbol{\\mu}|\\boldsymbol{\\alpha}) = \\frac{\\Gamma{(\\alpha_{0})}}{\\Gamma{(\\alpha_{1})} \\dots \\Gamma{(\\alpha_{K})}} \\prod_{k=1}^{K}\\mu_{k}^{\\alpha_{k-1}}\\tag{2.29}其中，$\\alpha_{0}=\\sum_{k=1}^{K} \\alpha_{k}$ 。 如图 2.6～2.8： 在不同的参数 $\\alpha_{k}$ 的情况下，单纯形上的狄利克雷分布的图像。 如图2.9～2.11： 对于不同的 $N$ 值，$N$ 个均匀分布的均值的直⽅图。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】总论","slug":"【机器学习基础】总论","date":"2019-09-19T00:49:10.000Z","updated":"2019-10-07T14:25:30.150Z","comments":true,"path":"2019/09/19/prml_01_pandect/","link":"","permalink":"https://zhangbc.github.io/2019/09/19/prml_01_pandect/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一， 概率论1，离散型随机变量概率假设随机变量 $X$ 可以取任意 的 $x_i$ ，其中 $i = 1, \\dots. , M$ ，并且随机变量 $Y$ 可以取任意的 $y_j$ ，其中 $j = 1,\\dots , L$。考虑 $N$ 次试验，其中我们对 $X$ 和 $Y$ 都进⾏取样， 把 $X = x_i$ 且 $Y = y_j$ 的试验的数量记作 $n_{ij}$ ，并且，把 $X$ 取值 $x_i$ （与 $Y$ 的取值⽆关）的试验的数量记作 $c_i$ ，类似地，把 $Y$ 取值 $y_j$ 的试验的数量记作 $r_j$ 。 $X$ 取值 $x_i$ 且 $Y$ 取值 $y_j$ 的概率被记作 $p(X = x_i , Y = y_j )$， 被称为 $X = x_i$ 和 $Y = y_j$ 的联合概率 （joint probability）。它的计算⽅法为落在单元格 $i, j$ 的点的数量与点的总数的⽐值，即： p\\left(X=x_{i}, Y=y_{i}\\right)=\\frac{n_{i j}}{N}\\tag{1.5}如图1.15所示，联合概率的计算方法。 类似地，$X$ 取值 $x_i$ （与 $Y$ 取值无关）的概率被记作 $p(X = x_i )$ ，也称为边缘概率（marginal probability），计算⽅法为落在列$i$上的点的数量与点的总数的⽐值，即： p\\left(X=x_{i}\\right)=\\frac{c_{j}}{N}\\tag{1.6}由于图1.15中列 $i$ 上的实例总数就是这列的所有单元格中实例的数量之和，即$c_{i}=\\sum_{j} n_{i j}$，因此根据公式(1.5)和公式(1.6)，我们可以得到概率的加和规则（sun rule），即： p\\left(X=x_{j}\\right)=\\sum_{j=1}^{L} p\\left(X=x_{i}, Y=y_{j}\\right)\\tag{1.7}如果我们只考虑那些 $X = x_i$ 的实例， 那么这些实例中 $Y = y_j$ 的实例所占的⽐例被写成 $p(Y = y_j | X = x_i)$，被称为给定 $X = x_i$ 的 $Y = y_j$ 的条件概率（conditional probability），其计算⽅式为：计算落在单元格 $i, j$ 的点的数量列 $i$ 的点的数量的⽐值，即： p\\left(Y=y_{j} | X=x_{i}\\right)=\\frac{n_{i j}}{c_{i}}\\tag{1.8}从公式(1.5)、公式(1.6)、公式(1.8)可以推导出概率的乘积规则（product rule），即： p\\left(X=x_{i}, Y=y_{j}\\right)=\\frac{n_{i j}}{N}=\\frac{n_{i j}}{c_{i}} \\cdot \\frac{c_{i}}{N}=p\\left(Y=y_{j} | X=x_{i}\\right) p\\left(X=x_{i}\\right)\\tag{1.9}根据乘积规则，以及对称性 $p(X, Y ) = p(Y, X)$，我们⽴即得到了下⾯的两个条件概率之间的关系，称为贝叶斯定理（Bayes&#39; theorem）即： p(Y | X)=\\frac{p(X | Y) p(Y)}{p(X)}\\tag{1.10}贝叶斯定理（Bayes&#39; theorem），在模式识别和机器学习领域扮演者中⼼⾓⾊。使⽤加和规则，贝叶斯定理中的分母可以⽤出现在分⼦中的项表⽰，这样就可以把分母看作归一常数，即： p(X)=\\sum_{Y} p(X|Y) p(Y)\\tag{1.11}如果两个变量的联合分布可以分解成两个边缘分布的乘积，即 $p(X, Y) = p(X)p(Y)$， 那么我们说 $X$ 和 $Y$ 相互独⽴（independent）。 2，概率密度如果⼀个实值变量x的概率 落在区间 $(x, x + \\delta x)$ 的概率由 $p(x)\\delta x$ 给出（$\\delta x \\to 0$）， 那么 $p(x)$ 叫做 $x$ 的概率密度（probability density）。$x$ 位于区间 $(a, b)$ 的概率： p(x \\in(a, b))=\\int_{a}^{b} p(x) \\mathrm{d}x\\tag{1.12}如图1.16，概率密度函数。 由于概率是⾮负的，并且 $x$ 的值⼀定位于实数轴上得某个位置，因此概率密度⼀定满⾜下⾯两个条件： 1）$p(x) \\geq 0$ 2) $\\int_{-\\infty}^{\\infty} p(x) \\mathrm{d} x=1$ 在变量以⾮线性的形式变化的情况下，概率密度函数通过Jacobian因⼦变换为与简单的函数不同的形式。 例如，假设我们考虑⼀个变量的变化 $x = g(y)$， 那么函数 $f(x)$ 就变成 了 $\\tilde{f}(y)=f(g(y))$。现在让我们考虑⼀个概率密度函数 $p_x (x)$，它对应于⼀个关于新变量 $y$ 的密度函数 $p_y (y)$，对于很⼩的 $\\delta x$ 的值，落在区间 $(x, x + \\delta x)$ 内的观测会被变换到区间 $(y, y + \\delta y)$ 中。其中 $p_{x}(x) \\delta x \\simeq p_{y}(y) \\delta y$ ，因此有： p_{y}(y)=p_{x}(x)\\left|\\frac{\\mathrm{d} x}{\\mathrm{d} y}\\right|=p_{x}(g(y))\\left|g^{\\prime}(y)\\right|\\tag{1.13}位于区间 $(−\\infty, z)$ 的 $x$ 的概率由累积分布函数（cumulative distribution function）给出。 定义为： P(z)=\\int_{-\\infty}^{z} p(x) \\mathrm{d} x\\tag{1.14}如果我们有⼏个连续变量 $x_1 ,\\dots , x_D$ ， 整体记作向量 $\\boldsymbol{x}$， 那么我们可以定义联合概率密度 $p(\\boldsymbol{x}) = p(x_1 ,\\dots , x_D )$，使得 $\\boldsymbol{x}$ 落在包含点 $\\boldsymbol{x}$ 的⽆穷⼩体积 $\\delta \\boldsymbol{x}$ 的概率由 $p(\\boldsymbol{x})\\delta \\boldsymbol{x}$ 给出。多变量概率密度必须满⾜以下条件： 1）$p(\\boldsymbol{x}) \\geq 0$ 2) $\\int p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}=1$ 其中，积分必须在整个 $\\boldsymbol{x}$ 空间上进⾏。 3，期望和方差在概率分布 $p(x)$ 下，函数 $f(x)$ 的平均值被称为 $f(x)$ 的期望（expectation），记作 $\\mathbb{E}[f]$。对于⼀个离散变量，它的定义为： \\mathbb{E}[f]=\\sum_{x} p(x) f(x)\\tag{1.15}在连续变量的情形下，期望以对应的概率密度的积分的形式表⽰为： \\mathbb{E}[f]=\\int p(x) f(x) \\mathrm{d}{x}\\tag{1.16}如果我们给定有限数量的 $N$ 个点，这些点满⾜某个概率分布或者概率密度函数， 那么期望可以通过求和的⽅式估计，因此有： \\mathbb{E}[f] \\simeq \\frac{1}{N} \\sum_{n=1}^{N} f\\left(x_{n}\\right)\\tag{1.17}$f(x)$ 的⽅差（variance）度量了 $f(x)$ 在均值 $\\mathbb{E} [f(x)]$ 附近变化性的⼤⼩。被定义为： \\operatorname{var}[f]=\\mathbb{E}\\left[(f(x)-\\mathbb{E}[f(x)])^{2}\\right]\\tag{1.18}将公式(1.18)中的平方项展开，即有公式(1.19)： \\operatorname{var}[f]=\\mathbb{E}\\left[f(x)^{2}\\right]-\\mathbb{E}[f(x)]^{2}\\tag{1.19}特别地，我们可以考虑变量 $x$ ⾃⾝的⽅差，即有： \\operatorname{var}[x]=\\mathbb{E}\\left[x^{2}\\right]-\\mathbb{E}[x]^{2}\\tag{1.20}对于两个随机变量 $x$ 和 $y$ ，协⽅差（covariance），表⽰在多⼤程度上 $x$ 和 $y$ 会共同变化。被定义为： \\operatorname{cov}[x, y]=\\mathbb{E}_{x, y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}]=\\mathbb{E}_{x, y}[x y]-\\mathbb{E}[x] \\mathbb{E}[y]\\tag{1.21}显然，由公式(1.21)推知，如果 $x$ 和 $y$ 相互独⽴，那么它们的协⽅差为0。 在两个随机向量 $\\boldsymbol{x}$ 和 $\\boldsymbol{y}$ 的情形下，协⽅差是⼀个矩阵，即有： \\operatorname{cov}[\\boldsymbol{x}, \\boldsymbol{y}]=\\mathbb{E}_{\\boldsymbol{x}, \\boldsymbol{y}}\\left[\\{\\boldsymbol{x}-\\mathbb{E}[\\boldsymbol{x}]\\}\\left\\{\\boldsymbol{y}^{T}-\\mathbb{E}\\left[\\boldsymbol{y}^{T}\\right]\\right\\}\\right]=\\mathbb{E}_{\\boldsymbol{x}, \\boldsymbol{y}}\\left[\\boldsymbol{x} \\boldsymbol{y}^{T}\\right]-\\mathbb{E}[\\boldsymbol{x}] \\mathbb{E}\\left[\\boldsymbol{y}^{T}\\right]\\tag{1.22}4，贝叶斯概率在观察到数据之前，我们有⼀些关于参数 $\\boldsymbol{w}$ 的假设，这以先验概率 $p(\\boldsymbol{w})$ 的形式给出。观测数据 $\\mathcal{D} = {t_1,\\dots, t_N}$ 的效果可以通过条件概率 $p(\\mathcal{D} | \\boldsymbol{w})$ 表达，即贝叶斯定理的形式为： p(\\boldsymbol{w} | \\mathcal{D})=\\frac{p(\\mathcal{D} | \\boldsymbol{w}) p(\\boldsymbol{w})}{p(\\mathcal{D})}\\tag{1.23}其中， 可以⽤后验概率分布和似然函数来表达贝叶斯定理的分母，即得： p(\\mathcal{D})=\\int p(\\mathcal{D} | \\boldsymbol{w}) p(\\boldsymbol{w}) \\mathrm{d} \\boldsymbol{w}\\tag{1.24}让我们能够通过后验概率 $p(\\boldsymbol{w} | \\mathcal{D})$，在观测到 $\\mathcal{D}$ 之后估计 $\\boldsymbol{w}$ 的不确定性。公式(1.23)中 $p(\\mathcal{D} | \\boldsymbol{w})$ 由观测数据集 $\\mathcal{D}$ 来估计，可以被看成参数向量 $\\boldsymbol{w}$ 的函数，被称为似然函数（likelihood function）。 5，高斯分布正态分布（Normal distribution），也称常态分布，又名高斯分布（Gaussian distribution），最早由A.棣莫弗在求二项分布的渐近公式中得到。正态分布概念是由德国的数学家和天文学家Moivre于1733年首次提出的。 正态曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线。 对于⼀元实值变量 $x$，⾼斯分布被定义为： \\mathcal{N}\\left(x | \\mu, \\sigma^{2}\\right)=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{\\frac{1}{2}}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}(x-\\mu)^{2}\\right\\}\\tag{1.25}其中，$\\mu$ 被叫做均值（mean）， $\\sigma^{2}$ 被叫做⽅差（variance）或者方差参数。⽅差的平⽅根， 由 $\\sigma$ 给定， 被叫做标准差（standard deviation）。 ⽅差的倒数， 记作 $\\beta=\\frac{1}{\\sigma^{2}}$ ， 被叫做精度 （precision）。 如图1.17，高斯分布曲线。 不难发现，高斯分布具有以下性质： 1）$\\mathcal{N}\\left(x | \\mu, \\sigma^{2}\\right)&gt;0$ 2）$\\int_{-\\infty}^{\\infty} \\mathcal{N}\\left(x | \\mu, \\sigma^{2}\\right) \\mathrm{d} x=1$ 3）$\\mathbb{E}[x]=\\int_{-\\infty}^{\\infty} \\mathcal{N}\\left(x | \\mu, \\sigma^{2}\\right) x \\mathrm{d} x=\\mu$ 4）$\\mathbb{E}\\left[x^{2}\\right]=\\int_{-\\infty}^{\\infty} \\mathcal{N}\\left(x | \\mu, \\sigma^{2}\\right) x^{2} \\mathrm{d} x=\\mu^{2}+\\sigma^{2}$ 5）$\\operatorname{var}[x]=\\mathbb{E}\\left[x^{2}\\right]-\\mathbb{E}[x]^{2}=\\sigma^{2}$ 分布的最⼤值被叫做众数。对于⾼斯分布，众数与均值恰好相等。 对 $D$ 维向量 $\\boldsymbol{x}$ 的⾼斯分布，定义为： \\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})=\\frac{1}{(2 \\pi)^{\\frac{D}{2}}} \\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\exp \\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right\\}\\tag{1.26}其中 $D$ 维向量 $\\boldsymbol{\\mu}$ 被称为均值，$D \\times D$ 的矩阵 $\\boldsymbol{\\Sigma}$ 被称为协⽅差，$|\\boldsymbol{\\Sigma}|$ 表⽰ $\\boldsymbol{\\Sigma}$ 的⾏列式。 独⽴地从相同的数据点中抽取的数据点被称为独⽴同分布（independent and identically distributed），通常缩写成i.i.d.。 假设给定一个观测的数据集由标量变量 $x$ 的 $N$ 次观测组成，写作 $\\mathbf{x} = (x_1,\\dots, x_N)^T $ ，记向量变量 $\\boldsymbol{x} = (x_1,\\dots, x_N)^T$ ，假定各次观测是独⽴地从⾼斯分布中抽取的， 分布的均值 $\\mu$ 和⽅差 $\\sigma^{2}$ 未知， 我们想根据数据集来确定这些参数。由于数据集 $\\mathbf{x}$ 是独⽴同分布的，因此给定 $\\mu$ 和 $\\sigma^{2}$ ，我们可以给出数据集的概率即为高斯分布的似然函数： p\\left(\\mathbf{x} | \\mu, \\sigma^{2}\\right)=\\prod_{n=1}^{N} \\mathcal{N}\\left(x_{n} | \\mu, \\sigma^{2}\\right)\\tag{1.27}对于似然函数公式(1.27)变形可得到： \\ln p\\left(\\mathbf{x} | \\mu, \\sigma^{2}\\right)=-\\frac{1}{2 \\sigma^{2}} \\sum_{n=1}^{N}\\left(x_{n}-\\mu\\right)^{2}-\\frac{N}{2} \\ln \\sigma^{2}-\\frac{N}{2} \\ln (2 \\pi)\\tag{1.28}关于 $\\mu$ ，最⼤化函数公式(1.28)，我们可以得到均值最⼤似然解： \\mu_{ML} = \\frac{1}{N} \\sum_{n=1}^{N}x_{n}\\tag{1.29}关于 $\\sigma^{2}$ ，最⼤化函数公式(1.28)，我们可以得到方差最⼤似然解： \\sigma^{2}_{ML} = \\frac{1}{N} \\sum_{n=1}^{N} (x_n - \\mu_{ML})^{2}\\tag{1.30}6，考察曲线拟合问题曲线拟合问题的⽬标是能够根据 $N$ 个输⼊ $\\mathbf{x}\\equiv(x_1,\\dots, x_N)^T$ 组成的数据集和它们对应的⽬标值 $\\mathbf{t}\\equiv (t_1,\\dots, t_N)^T$ ，在给出输⼊变量 $x$ 的新值的情况下，对⽬标变量 $t$ 进⾏预测。 现假定给定 $x$ 的值， 对应的 $t$ 值服从⾼斯分布，分布的均值为 $y(x, \\boldsymbol{w})$ ，由公式(1.1)给出。则其概率为: p(t|x, \\boldsymbol{w}, \\beta ) = \\mathcal{N}\\left(t | y(x, \\boldsymbol{w}), \\beta^{-1}\\right)\\tag{1.31}如图1.18，⾼斯条件概率分布。 似然函数为： p\\left(\\mathbf{t} | \\mathbf{x}, \\boldsymbol{w}, \\beta \\right)=\\prod_{n=1}^{N} \\mathcal{N}\\left(t | y(x, \\boldsymbol{w}), \\beta^{-1}\\right)\\tag{1.32}似然函数变形为： \\ln p\\left(\\mathbf{t} | \\mathbf{x}, \\boldsymbol{w}, \\beta \\right)=-\\frac{\\beta}{2} \\sum_{n=1}^{N}\\left\\{y(x_{n},\\boldsymbol{w})-t_n\\right\\}^{2}+\\frac{N}{2} \\ln \\beta-\\frac{N}{2} \\ln (2 \\pi)\\tag{1.33}考虑确定多项式系数的最⼤似然解（记作 $\\boldsymbol{w}_{ML}$ ），这些由公式(1.33)来确定。可以使⽤最⼤似然⽅法来确定⾼斯条件分布的精度参数 $\\beta$ ： \\frac{1}{\\beta_{ML}} = \\frac{1}{N} \\sum_{n=1}^{N}\\left\\{y(x_{n},\\boldsymbol{w}_{ML})-t_n\\right\\}^{2}\\tag{1.34}简单起见，引⼊在多项式系数 $\\boldsymbol{w}$ 上的先验分布，我们考虑下⾯形式的⾼斯分布： p(\\boldsymbol{w} | \\alpha)=\\mathcal{N}\\left(\\boldsymbol{w} | \\mathbf{0}, \\alpha^{-1} \\boldsymbol{I}\\right)=\\left(\\frac{\\alpha}{2 \\pi}\\right)^{\\frac{M+1}{2}} \\exp \\left\\{-\\frac{\\alpha}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}\\right\\}\\tag{1.35}给定数据集，我们现在通过寻找最可能的 $\\boldsymbol{w}$ 值（即最⼤化后验概率）来确定 $\\boldsymbol{w}$ 。这种技术被称 为最⼤后验（maximum posterior），简称MAP。根据公式(1.35)和公式(1.33)可得最⼤化后验概率即最⼩化： \\frac{\\beta}{2} \\sum_{n=1}^{N}\\left\\{y\\left(x_{n}, \\boldsymbol{w}\\right)-t_{n}\\right\\}^{2}+\\frac{\\alpha}{2}\\boldsymbol{w}^{T}\\boldsymbol{w}\\tag{1.36}7，贝叶斯曲线拟合贝叶斯⽅法就是⾃始⾄终地使⽤概率的加和规则和乘积规则。因此预测概率可以写成： p\\left(t | x, \\mathbf{x}, \\mathbf{t} \\right)=\\int p(t | x, \\boldsymbol{w}) p(\\boldsymbol{w} | \\mathbf{x}, \\mathbf{t}) \\mathrm{d} \\boldsymbol{w}\\tag{1.37}预测分布由⾼斯的形式： p\\left(t | x, \\mathbf{x}, \\mathbf{t} \\right) = \\mathcal{N}\\left(t | m(x), s^{2}(x)\\right)\\tag{1.38}其中，均值， m(x) = \\beta \\phi(x)^{T} \\boldsymbol{S} \\sum_{n=1}^{N}\\phi(x_n)t_n\\tag{1.39}⽅差， s^{2}(x) = \\beta^{-1} + \\phi(x)^{T}\\boldsymbol{S}\\phi(x)\\tag{1.40}矩阵， \\boldsymbol{S}^{-1} = \\alpha \\boldsymbol{I} + \\beta \\sum_{n=1}^{N}\\phi({x_n})\\phi({x_n})^{T}\\tag{1.41}其中，$\\boldsymbol{I}$ 是单位矩阵，向量 $\\phi(x)$ 被定义为 $\\phi_i (x) = x^{i} (i = 0, \\dots, M)$。 如图1.19，⽤贝叶斯⽅法处理多项式曲线拟合问题得到的预测分布的结果。使⽤的多项式为 $M$ = 9，超参数被固定为 $\\alpha = 5 \\times 10^{-3}$ 和 $\\beta = 11.1$（对应于已知的噪声⽅差）。 其中， 红⾊曲线表⽰预测概率分布的均值，红⾊区域对应于均值周围 $±1$ 标准差的范围。 如图1.20，参数为 $S$ 的交叉验证⽅法。 二，模型选择为了建⽴好的模型，我们想使⽤尽可能多的可得到的数据进⾏训练。然⽽，如果验证机很⼩，它对预测表现的估计就会有⼀定的噪声。解决这种困境的⼀种⽅法是使⽤交叉验证（cross validation）。这种⽅法能够让可得到数据的 $\\frac{S−1}{S}$ ⽤于训练，同时使⽤所有的数据来评估表现。当数据相当稀疏的时候，考虑 $S = N$ 的情况很合适，其中 $N$ 是数据点的总数。这种技术叫做留⼀法（leave-one-out）。 如图1.21，交叉验证。 交叉验证的⼀个主要的缺点是需要进⾏的训练的次数随着 $S$ ⽽增加，这对于训练本⾝很耗时的问题来说是个⼤问题。对于像交叉验证这种使⽤分开的数据来评估模型表现的⽅法来说，还 有⼀个问题：对于⼀个单⼀的模型，我们可能有多个复杂度参数（例如可能有若⼲个正则化参数）。 三，维度灾难如果我们有 $D$ 个输⼊变量，那么 ⼀个三阶多项式就可以写成如下的形式： y(\\boldsymbol{x}, \\boldsymbol{w}) = w_{0} + \\sum_{i=1}^{D}w_{i}x_{i} + \\sum_{i=1}^{D}\\sum_{j=1}^{D}w_{ij}x_{i}x_{j} + \\sum_{i=1}^{D}\\sum_{j=1}^{D}\\sum_{k=1}^{D}w_{ijk}x_{i}x_{j}x_{k}\\tag{1.42}随着 $D$ 的增加，独⽴的系数的数量（并⾮所有的系数都独⽴，因为变量 $x$ 之间的互换对称性）的 增长速度正⽐于$D^3$ 。 注意到，$D$ 维空间的半径为 $r$ 的球体的体积⼀定是 $r^{D}$ 的倍数，因此有： V_{D}(r) = K_{D}r^{D}\\tag{1.43}其中常数 $K_{D}$ 值依赖于D。因此要求解的体积比，即： \\frac{V_{D}(1)-V_{D}(1-\\epsilon)}{V_{D}(1)} = 1 - (1-\\epsilon)^{D}\\tag{1.44}如图1.22，对于不同的 $D$，位于 $r = 1 − \\epsilon$ 和 $r = 1$ 之间的部分与球的体积⽐。 如图1.23，不同的维度 $D$ 中的⾼斯分布的概率密度关于半径 $r$ 的关系。 维度灾难(Curse of Dimensionality)：通常是指在涉及到向量的计算的问题中，随着维度的增加，计算量呈指数倍增长的一种现象。 四，决策论1，最⼩化错误分类率假定我们的⽬标很简单，即尽可能少地作出错误分类。我们需要⼀个规则来把每个 $x$ 的值分到⼀个合适的类别。这种规则将会把输⼊空间切分成不同的区域 $\\mathcal{R}_{k}$ ，这种区域被称为决策区域 （decision region）。每个类别都有⼀个决策区域，区域 $\\mathcal{R}_{k}$ 中的所有点都被分到 $\\mathcal{C}_{k}$ 类。决策区域间的边界被叫做决策边界（decision boundary）或者决策⾯（decision surface）。注意， 每⼀个 决策区域未必是连续的，可以由若⼲个分离的区域组成。 如果我们把属于 $\\mathcal{C}_{1}$ 类的输⼊向量分到了 $\\mathcal{C}_{2}$ 类（或者相反）， 那么我们就犯了⼀个错误。这种事情发⽣的概率为： \\begin{aligned} p(\\text { mistake }) &=p\\left(\\boldsymbol{x} \\in \\mathcal{R}_{1}, \\mathcal{C}_{2}\\right)+p\\left(\\boldsymbol{x} \\in \\mathcal{R}_{2}, \\mathcal{C}_{1}\\right) \\\\ &=\\int_{\\mathcal{R}_{1}} p\\left(\\boldsymbol{x}, \\mathcal{C}_{2}\\right) \\mathrm{d} \\boldsymbol{x}+\\int_{\\mathcal{R}_{2}} p\\left(\\boldsymbol{x}, \\mathcal{C}_{1}\\right) \\mathrm{d} \\boldsymbol{x} \\end{aligned}\\tag{1.45}如图1.24，两个类别的联合概率分布 $p(x, \\mathcal{C}_k)$ 与 $x$ 的关系。 对于更⼀般的 $K$ 类的情形，最⼤化正确率会稍微简单⼀些，即最⼤化下式： p(\\text {correct}) = \\sum_{k=1}^{K}p\\left(\\boldsymbol{x} \\in \\mathcal{R}_{k}, \\mathcal{C}_{k}\\right) = \\sum_{k=1}^{K} \\int_{\\mathcal{R}_{k}} p\\left(\\boldsymbol{x}, \\mathcal{C}_{k}\\right) \\mathrm{d} \\boldsymbol{x}\\tag{1.46}2，最⼩化期望损失损失函数也被称为代价函数（cost function），是对于所有可能的决策或者动作可能产⽣的损失的⼀种整体的度量。 假设对于新的 $x$ 值，真实的类别为 $\\mathcal{C}_{k}$ ，我们把 $x$ 分类为 $\\mathcal{C}_{j}$ （其中 $j$ 可能与 $k$ 相等，也可能不相等）。这样做的结果是，我们会造成某种程度的损失，记作 $L_{kj}$ ，它可以看成损失矩阵（loss matrix）的第 $k, j$ 个元素。 对于⼀个给定的输⼊向量 $\\boldsymbol{x}$，我们对于真实类别的不确定性通过联合概率分布 $p(\\boldsymbol{x}, \\mathcal{C}_{k})$ 表⽰。因此，我们转⽽去最⼩化平均损失。平均损失根据这个联合概率分布计算，定义为： \\mathbb{E}[L] = \\sum_{k}\\sum_{j} \\int_{\\mathcal{R}_{k}} L_{kj}p(\\boldsymbol{x}, \\mathcal{C}_{k})\\mathrm{d} \\boldsymbol{x}\\tag{1.47}3，拒绝选项在发⽣分类错误的输⼊空间中，后验概率 $p(\\mathcal{C}_{k} | \\boldsymbol{x})$ 通常远⼩于1，或者等价地，不同类别的联合分布 $p(\\boldsymbol{x}, \\mathcal{C}_{k})$ 有着可⽐的值。这些区域中，类别的归属相对不确定。在某些应⽤中， 对于这种困难的情况， 避免做出决策是更合适的选择。 这样会使得模型的分类错误率降低。 这被称为拒绝选项（reject option）。 如图1.25，拒绝选项。 4，推断和决策同时解决两个问题，即简单地学习⼀个函数，将输⼊ $\\boldsymbol{x}$ 直接映射为决策。这样的函数被称为判别函数（discriminant function）。 给出三种不同的⽅法来解决决策问题，具体如下： a）⾸先对于每个类别 $\\mathcal{C}_{k}$ ， 独⽴地确定类条件密度 $p(\\boldsymbol{x} | \\mathcal{C}_{k})$ ，这是⼀个推断问题。 然后， 推断先验类概率 $p(\\mathcal{C}_{k})$ 。之后，使⽤贝叶斯定理求出后验类概率 $p(\\mathcal{C}_{k} | \\boldsymbol{x})$ 。等价地，我们可以直接对联合概率分布 $p(\\boldsymbol{x} , \\mathcal{C}_{k})$ 建模，然后归⼀化，得到后验概率。得到后验概率之后， 我们可以使⽤决策论来确定每个新的输⼊ $\\boldsymbol{x}$ 的类别。显式地或者隐式地对输⼊以及输出进⾏建模的⽅法被称为⽣成式模型（generative model），因为通过取样，可以⽤来⼈⼯⽣成出输⼊空间的数据点。 b）⾸先解决确定后验类密度 $p(\\mathcal{C}_{k} | \\boldsymbol{x})$ 这⼀推断问题，接下来使⽤决策论来对新的输⼊ $\\boldsymbol{x}$ 进⾏分类。这种直接对后验概率建模的⽅法被称为判别式模型（discriminative models）。 c）找到⼀个函数 $f(\\boldsymbol{x})$， 被称为判别函数。 这个函数把每个输⼊ $\\boldsymbol{x}$ 直接映射为类别标签。 5，回归问题的损失函数讨论曲线拟合问题，决策阶段包括对于每个输⼊ $\\boldsymbol{x}$，选择⼀个对于 $t$ 值的具体估计 $y(\\boldsymbol{x})$。假设这样做之后，我们造成了⼀个损失 $L(t, y(\\boldsymbol{x}))$。平均损失（或者说期望损失）就是： \\mathbb{E}[L] = \\int\\int L(t, y(\\boldsymbol{x}, x))p(\\boldsymbol{x}, t)\\mathrm{d} \\boldsymbol{x}\\mathrm{d}{t}\\tag{1.48}回归问题中，损失函数的⼀个通常的选择是平⽅损失，定义为 $L(t, y(\\boldsymbol{x})) = \\{y(\\boldsymbol{x}) − t\\}^{2}$ 。这种情况下，期望损失函数可以写成： \\mathbb{E}[L] = \\int\\int \\{y(\\boldsymbol{x}) − t\\}^{2}p(\\boldsymbol{x}, t)\\mathrm{d} \\boldsymbol{x}\\mathrm{d}{t}\\tag{1.49}假设⼀个完全任意的函数 $y(\\boldsymbol{x})$，我们能够形式化地使⽤变分法： \\frac{\\delta \\mathbb{E}[L]}{\\delta y(\\boldsymbol{x})} = 2 \\int \\{y(\\boldsymbol{x}) − t\\}p(\\boldsymbol{x}, t)\\mathrm{d}{t} = 0\\tag{1.50}求解 $y(\\boldsymbol{x})$，使⽤概率的加和规则和乘积规则，得到： y(\\boldsymbol{x})=\\frac{\\int \\operatorname{tp}(\\boldsymbol{x}, t) \\mathrm{d} t}{p(\\boldsymbol{x})}=\\int t p(t | \\boldsymbol{x}) \\mathrm{d} t=\\mathbb{E}_{t}[t | \\boldsymbol{x}]\\tag{1.51}这是在 $\\boldsymbol{x}$ 的条件下 $t$ 的条件均值， 被称为回归函数（regression function）。 如图1.26，回归函数。 闵可夫斯基损失函数（Minkowski loss），它的期望为： \\mathbb{E}[L_{q}] = \\int\\int |y(\\boldsymbol{x}) − t|^{q}p(\\boldsymbol{x}, t)\\mathrm{d} \\boldsymbol{x}\\mathrm{d}{t}\\tag{1.52}如图1.27～1.30，对于不同的 $q$ 值，$L_{q} = |y − t|^{q}$ 的图像。 由公式(1.52)分析不难发现，当 $q = 2$ 时， 这个函数就变成了平⽅损失函数的期望。当 $q = 2$ 时，$\\mathbb{E}[L_{q}]$ 的最⼩值是条件均值。当 $q = 1$ 时，$\\mathbb{E}[L_{q}]$ 的最⼩值是条件中位数。当$q \\to 0$ 时，$\\mathbb{E}[L_{q}]$ 的最⼩值是条件众数。 五，信息论如果有两个不相关的事件 $x$ 和 $y$ ， 那么我们观察到两个事件同时发⽣时获得的信息应该等于观察到事件各⾃发⽣时获得的信息之和， 即 $h(x, y) = h(x) + h(y)$。 两个不相关事件是统计独⽴的， 因此 $p(x, y) = p(x)p(y)$。根据这两个关系，很容易看出 $h(x)$ ⼀定与 $p(x)$ 的对数有关。因此，我们有： h(x) = - \\text{log}_{2}p(x)\\tag{1.53}概率分布 $p(x)$ 的期望， H[x] = - \\sum_{x}p(x) \\text {log}_{2}p(x)\\tag{1.54}这个重要的量被叫做随机变量 $\\boldsymbol{x}$ 的熵（entropy）。 1，关于理解熵的例子考虑⼀个集合，包含 $N$ 个完全相同的物体，这些 物体要被分到若⼲个箱⼦中，使得第 $i$ 个箱⼦中有 $n_i$ 个物体。考虑把物体分配到箱⼦中的不同⽅案的数量。有 $N$ 种⽅式选择第⼀个物体，有 $(N − 1)$ 种⽅式选择第⼆个物体，以此类推。因此总 共有 $N!$ 种⽅式把 $N$ 个物体分配到箱⼦中，其中 $N!$ 表⽰乘积 $N\\times(N − 1)\\times \\dots \\times 2 \\times 1$。然⽽，我们不想区分每个箱⼦内部物体的重新排列。在第 $i$ 个箱⼦中，有 $n_i !$ 种⽅式对物体重新排序，因此把 $N$ 个物体分配到箱⼦中的总⽅案数量为： W=\\frac{N !}{\\prod_{i} n_{i} !}\\tag{1.55}这被称为乘数（multiplicity）。熵被定义为通过适当的参数放缩后的对数乘数，即： H = \\frac{1}{N} \\ln W = \\frac{1}{N} \\ln N! - \\frac{1}{N}\\ln n_{i}!\\tag{1.56}考虑极限 $N \\to \\infty$ ，并且保持⽐值 $\\frac{n_i}{N}$ 固定，使⽤Stirling的估计： \\ln N! \\simeq N \\ln N - N\\tag{1.57}即可得： H = - \\lim_{N \\to \\infty} \\sum_{i} (\\frac{n_{i}}{N}) \\ln (\\frac{n_{i}}{N}) = - \\sum_{i} p_{i} \\ln p_{i}\\tag{1.58}在概率归⼀化的限制下，使⽤拉格朗⽇乘数法可以找到熵的最⼤值。因此，我们要最⼤化： \\tilde{H}=-\\sum_{i} p\\left(x_{i}\\right) \\ln p\\left(x_{i}\\right)+\\lambda\\left(\\sum_{i} p\\left(x_{i}\\right)-1\\right)\\tag{1.59}可以证明， 当所有的 $p(x_i)$ 都相等， 且值为 $p(x_i) = \\frac{1}{M}$ 时， 熵取得最⼤值。 其中，$M$ 是状态 $x_i$ 的总数。此时对应的熵值为 $H = \\ln M$ 。 2，关于连续变量 $\\boldsymbol{x}$ 的概率分布 $p(x)$ 的熵⾸先把 $x$ 切分成宽度为 $\\Delta$ 的箱⼦。然后假设 $p(x)$ 是连续的。均值定理（mean value theorem）（Weisstein, 1999）告诉我们，对于每个这样的箱⼦，⼀定存在⼀个值 $x_i$ 使得： \\int_{i \\Delta}^{(i+1) \\Delta}p(x_i)\\mathrm{d}x = p(x_i)\\Delta\\tag{1.60}现在可以这样量化连续变量 $x$：只要 $x$ 落在第 $i$ 个箱⼦中，我们就把 $x$ 赋值为 $x_i$ 。因此观察到值 $x_i$ 的概率为 $p(x_i) \\Delta$。这就变成了离散的分布，这种情形下熵的形式为： H \\Delta = - \\sum_{i}p(x_i) \\Delta \\ln(p(x_i) \\Delta) - \\ln \\Delta\\tag{1.61}其中， \\sum_{i}p(x_i) \\Delta = 1 \\tag{1.62}考察 $\\Delta \\to 0$ ，由以上公式即可推得微分熵（differential entropy）： \\lim_{\\Delta \\to 0}\\left\\{-\\sum_{i} p\\left(x_{i}\\right) \\Delta \\ln p(x_i) \\right\\} = - \\int p(x)\\ln p(x) \\mathrm{d}{x}\\tag{1.63}对于定义在多元连续变量（联合起来记作向量 $\\boldsymbol{x}$ ）上的概率密度，微分熵为： H[\\boldsymbol{x}] = - \\int p(x) \\ln p(x) \\mathrm{d}{x}\\tag{1.64}最⼤化微分熵的时候要遵循下⾯三个限制条件，即： \\int_{-\\infty}^{\\infty}p(x) \\mathrm{d}{x} = 1\\tag{1.65} \\int_{-\\infty}^{\\infty}x p(x) \\mathrm{d}{x} = \\mu\\tag{1.66} \\int_{-\\infty}^{\\infty}(x - \\mu)^{2} p(x) \\mathrm{d}{x} = \\delta^{2}\\tag{1.67}在上述条件的限制下，使⽤拉格朗⽇乘数法可以找到熵的最⼤值。最终结果化： p(x) = \\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{\\frac{1}{2}}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}(x-\\mu)^{2}\\right\\}\\tag{1.68}因此最⼤化微分熵的分布是⾼斯分布，其微分熵公式： H[x] = \\frac{1}{2}\\{ 1 + \\ln(2\\pi \\delta^{2}) \\}\\tag{1.69}3，关于连续变量 $(\\boldsymbol{x}, \\boldsymbol{y})$ 的联合概率分布 $p(\\boldsymbol{x}, \\boldsymbol{y})$ 的熵假设有⼀个联合概率分布 $p(\\boldsymbol{x}, \\boldsymbol{y})$ ，我们从这个概率分布中抽取了⼀对 $\\boldsymbol{x}$ 和 $\\boldsymbol{y}$ 。如果 $\\boldsymbol{x}$ 的值已知，那么需要确定对应的 $\\boldsymbol{y}$ 值所需的附加的信息就是 $− \\ln p(\\boldsymbol{y} | \\boldsymbol{x})$ 。因此，⽤来确定 $y$ 值的平均附加信息可以写成： H[\\boldsymbol{x}|\\boldsymbol{x}] = - \\iint p(\\boldsymbol{y}, \\boldsymbol{x})\\ln p(\\boldsymbol{y}|\\boldsymbol{x}) \\mathrm{d}{\\boldsymbol{y}} \\mathrm{d}{\\boldsymbol{x}}\\tag{1.70}这被称为给定 $\\boldsymbol{x}$ 的情况下，$\\boldsymbol{y}$ 的条件熵。使⽤乘积规则，很容易看出，条件熵满⾜下⾯的关系： H[\\boldsymbol{x}, \\boldsymbol{y}] = H[\\boldsymbol{y}|\\boldsymbol{x}] + H[\\boldsymbol{x}]\\tag{1.71}4，相对熵和互信息考虑某个未知的分布 $p(\\boldsymbol{x})$，假定我们已经使⽤⼀个近似的分布 $q(\\boldsymbol{x})$ 对它进⾏了建模。如果我们使⽤ $q(\\boldsymbol{x})$ 来建⽴⼀个编码体系，⽤来把 $\\boldsymbol{x}$ 的值传给接收者，那么，由于我们使⽤了 $q(\\boldsymbol{x})$ ⽽不是真实分布 $p(\\boldsymbol{x})$，因此在具体化 $\\boldsymbol{x}$ 的值（假定我们选择了⼀个⾼效的编码系统）时，我们需要⼀些附加的信息。我们需要的平均的附加信息量（单位是nat）： \\begin{aligned} \\mathrm{KL}(p \\| q) &=-\\int p(\\boldsymbol{x}) \\ln q(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}-\\left(-\\int p(\\boldsymbol{x}) \\ln p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\\right) \\\\ &=-\\int p(\\boldsymbol{x}) \\ln \\left\\{\\frac{q(\\boldsymbol{x})}{p(\\boldsymbol{x})}\\right\\} \\mathrm{d} \\boldsymbol{x} \\end{aligned}\\tag{1.72}这被称为分布 $p(\\boldsymbol{x})$ 和分布 $q(\\boldsymbol{x})$ 之间的相对熵（relative entropy） 或者 Kullback-Leibler散度 （Kullback-Leibler divergence）， 或者 KL散度（Kullback and Leibler, 1951）。 可以证明，Kullback-Leibler散度 满⾜ $\\mathrm{KL}(p | q) \\ge 0$，并且当且仅当 $p(\\boldsymbol{x}) = q(\\boldsymbol{x})$ 时等号成⽴。 如果⼀个函数具有如下性质：每条弦都位于函数图像或其上⽅，那么我们说这个函数是凸函数。其性质为： f(\\lambda a + (1-\\lambda)b) \\le \\lambda f(a) + (1-\\lambda)f(b)\\tag{1.73}由归纳法容易知，凸函数 $f(x)$ 满足 Jensen不等式（Jensen&#39;s inequality）： f \\left(\\sum_{i=1}^{M}\\lambda_{i} x_i\\right) \\le \\sum_{i=1}^{M}\\lambda_i f(x_i)\\tag{1.74}对于连续变量，Jensen不等式： f \\left(\\int \\boldsymbol{x} p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\\right) \\le \\int f(\\boldsymbol{x})p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\\tag{1.75}现在考虑由 $p(\\boldsymbol{x}, \\boldsymbol{y})$ 给出的两个变量 $\\boldsymbol{x}$ 和 $\\boldsymbol{y}$ 组成的数据集。如果变量的集合是独⽴的，那么他们的联合分布可以分解为边缘分布的乘积 $p(\\boldsymbol{x}, \\boldsymbol{y}) = p(\\boldsymbol{x})p(\\boldsymbol{y})$ 。如果变量不是独⽴的，那么我们可以通过考察联合概率分布与边缘概率分布乘积之间的 Kullback-Leibler散度来判断它们是否“接近”于相互独⽴。此时，Kullback-Leibler散度： \\begin{aligned} I[\\boldsymbol{x}, \\boldsymbol{y}] &\\equiv \\mathrm{KL}(p(\\boldsymbol{x}, \\boldsymbol{y}) \\|p(\\boldsymbol{x})q(\\boldsymbol{y}))\\\\ &=-\\iint p(\\boldsymbol{x}, \\boldsymbol{y}) \\ln \\left(\\frac{p(\\boldsymbol{x})q(\\boldsymbol{y})}{p(\\boldsymbol{x, y})}\\right) \\mathrm{d} \\boldsymbol{x} \\mathrm{d} \\boldsymbol{y} \\end{aligned}\\tag{1.76}这被称为变量 $\\boldsymbol{x}$ 和变量 $\\boldsymbol{y}$ 之间的互信息（mutual information）。 根据 Kullback-Leibler散度的性质，我们看到 $I[\\boldsymbol{x}, \\boldsymbol{y}] \\ge 0$ ，当且仅当 $\\boldsymbol{x}$ 和 $\\boldsymbol{y}$ 相互独⽴时等号成⽴。使⽤概率的加和规则和乘积规则，我们看到互信息和条件熵之间的关系： I[\\boldsymbol{x,y}] = H[\\boldsymbol{x}] - H[\\boldsymbol{x|y}] = H[\\boldsymbol{y}] - H[\\boldsymbol{y|x}]\\tag{1.77}","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【机器学习基础】从回归问题引基础：多项式曲线拟合","slug":"【机器学习基础】从回归问题引基础：多项式曲线拟合","date":"2019-09-17T13:52:56.000Z","updated":"2019-10-07T08:59:52.344Z","comments":true,"path":"2019/09/17/prml_01_polynomial_curve_fitting/","link":"","permalink":"https://zhangbc.github.io/2019/09/17/prml_01_polynomial_curve_fitting/","excerpt":"","text":"本系列为《模式识别与机器学习》的读书笔记。 一， 举例：多项式曲线拟合假设给定一个训练集。这个训练集由 $x$ 的 $N$ 次观测组成，写作 $\\mathbf{x}\\equiv(x_1,\\dots, x_N)^T$ ，伴随这对应的 $t$ 的观测值，记作 $\\mathbf{t}\\equiv (t_1,\\dots, t_N)^T$。其中，输入数据集合 $\\mathbf{x}$ 通过选择$x_n(n=1,\\dots,N)$ 的值来生成，这些 $x_n$ 均匀分布在区间[0, 1]，目标数据集 $\\mathbf{t}$ 的获得方式是：首先计算函数 $sin(2\\pi x)$ 的对应的值，然后给每个点增加一个小的符合高斯分布的随机噪声，从而得到对应的 $t_n$ 的值。 我们的目标是利用这个训练集预测对于输入变量的新值 $\\hat{x}$ 得到的目标变量的值 $\\hat{t}$。 如下图1.1，由 $N$ =10个数据点组成的训练集的图像，用蓝色圆圈表示。 如图1.2，误差函数对应于每个数据点与函数 $y(x, \\boldsymbol{w})$ 之间位移（绿⾊垂直线）的平⽅和（的⼀半）。 但是现在，我们要⽤⼀种相当⾮正式的、相当简单的⽅式来进⾏曲线拟合。特别地，将使⽤下⾯形式的多项式函数来拟合数据： y(x, \\boldsymbol{w})=w_{0}+w_{1} x+w_{2} x^{2}+\\ldots+w_{M} x^{M}=\\sum_{i=0}^{M} w_{j} x^{j}\\tag{1.1}其中 $M$ 是多项式的阶数（order），$x^j$ 表⽰ $x$ 的 $j$ 次幂。 多项式系数 $w_0 , \\dots , w_M$ 整体记作向量 $\\boldsymbol{w}$。 注意，虽然多项式函数 $y(x, \\boldsymbol{w})$ 是 $x$ 的⼀个⾮线性函数，它是系数 $\\boldsymbol{w}$ 的⼀个线性函数。类似多项式函数的这种关于未知参数满⾜线性关系的函数有着重要的性质，被叫做线性模型。 系数的值可以通过调整多项式函数拟合训练数据的⽅式确定。 这可以通过最⼩化误差函数 （error function）的⽅法实现。 E(\\boldsymbol{w})=\\frac{1}{2} \\sum_{n=1}^{N}\\left\\{y\\left(x_{n}, \\boldsymbol{w}\\right)-t_{n}\\right\\}^{2}\\tag{1.2}我们可以通过过选择使得 $E(\\boldsymbol{w})$ 尽量⼩的 $\\boldsymbol{w}$ 来解决曲线拟合问题。由于误差函数是系数 $\\boldsymbol{w}$ 的⼆次函数， 因此它关于系数的导数是 $\\boldsymbol{w}$ 的线性函数， 所以误差函数的最⼩值有⼀个唯⼀解， 记作 $\\boldsymbol{w}^*$ ，可以⽤解析的⽅式求出。最终的多项式函数由函数 $y\\left(x, \\boldsymbol{w}^*\\right)$ 给出。 如下图1.3～1.6，不同阶数的多项式曲线，⽤红⾊曲线表⽰，拟合了图1.1中的数据集。 当 $M=9$ 时，多项式函数精确地通过了每⼀个数据点，$E(\\boldsymbol{w}^*) = 0$。 然⽽， 拟合的曲线剧烈震荡，就表达函数 $sin(2\\pi x)$ ⽽⾔表现很差。这种⾏为叫做过拟合（over-fitting）。 通常用根均⽅（RMS）误差来计算： E_{R M S}=\\sqrt{2 E\\left(\\boldsymbol{w}^{*}\\right) / N}\\tag{1.3}如图1.7，当M 的取值为 $3 \\leq M \\leq 8$ 时， 测试误差较⼩， 对于⽣成函数 $sin(2\\pi x)$ 也能给出合理的模拟。 如图1.8，不同阶数的多项式的系数 $\\boldsymbol{w}^{*}$ 的值。观察随着多项式阶数的增加，系数的⼤⼩是如何剧烈增⼤的。 如图1.9～1.10，使⽤ $M = 9$ 的多项式对 $N = 15$ 个数据点和 $N = 100$ 个数据点通过最⼩化平⽅和误差函数的⽅法得到的解。 常⽤来控制过拟合现象的⼀种技术是正则化（regularization）。 这种技术涉及到给误差函数增加⼀个惩罚项，使得系数不会达到很⼤的值。这种惩罚项最简单的形式采⽤所有系数的平⽅和的形式。这推导出了误差函数的修改后的形式： \\tilde{E}(\\boldsymbol{w})=\\frac{1}{2} \\sum_{n=1}^{N}\\left\\{y\\left(x_{n}, \\boldsymbol{w}\\right)-t_{n}\\right\\}^{2}+\\frac{\\lambda}{2}\\|\\boldsymbol{w}\\|^{2}\\tag{1.4}其中，系数 $\\lambda$ 控制了正则化项相对于平⽅和误差项的重要性； \\|\\boldsymbol{w}\\|^{2} \\equiv \\boldsymbol{w}^{T} \\boldsymbol{w}=w_{0}^{2}+w_{1}^{2}+\\ldots+w_{M}^{2}通过把给定的数据中的⼀部分从测试集中分离出，来确定系数 $\\boldsymbol{w}$。这个分离出来的验证集（validation set），也被称为拿出集（hold-out set），⽤来最优化模型的复杂度（$M$ 或者 $\\lambda$）。 如图1.11～1.12，使⽤正则化的误差函数，⽤ $M = 9$ 的多项式拟合图中的数据集。其中正则化参数 $\\lambda$ 选择了两个值，分别对应于 $\\ln \\lambda=-18$ 和 $\\ln \\lambda=0$。 如图1.13，不同的正则化参数 $\\lambda$ 下，$M$ = 9的多项式的系数 $\\boldsymbol{w}^{*}$ 的值。观察随着 $\\lambda$ 的增大，系数的⼤⼩是逐渐变小的。 如图1.14，对于 $M = 9$ 的多项式，均⽅根误差与 $\\ln \\lambda$ 的关系。 二， 总结 本小节为机器学习的入门篇，主要通过一个多项式拟合具体实例引出了线性模型相关概念，训练集的意义，误差函数，根均方差，修正误差函数等公式，正则化参数概念。","categories":[{"name":"机器学习","slug":"machine-learning","permalink":"https://zhangbc.github.io/categories/machine-learning/"}],"tags":[{"name":"机器学习基础","slug":"machine-learning-foundation","permalink":"https://zhangbc.github.io/tags/machine-learning-foundation/"}]},{"title":"【资源共享】eBook分享大集合","slug":"【资源共享】eBook分享大集合","date":"2019-08-28T16:03:18.000Z","updated":"2019-09-29T06:16:03.059Z","comments":true,"path":"2019/08/29/eBooks_share/","link":"","permalink":"https://zhangbc.github.io/2019/08/29/eBooks_share/","excerpt":"","text":"eBook分享大集合 主要以IT领域经典书籍收藏，以备不时之需。 福利传送门：【GitHub】 欢迎各位指点，要是能补充更是感激不尽。 服务器系统类 Linux高性能服务器编程 Shell脚本学习指南 高级Bash脚本编程指南.3.9.1 (杨春敏 黄毅 译) 鸟哥的Linux私房菜基础篇(第3版) 深入理解计算机系统 机器学习类 吴恩达深度学习教程 deepLearning深度学习(开源版) python自然语言处理实战：核心技术与算法 机器学习方法 社交网站的数据挖掘与分析 统计学习方法 用Python进行自然语言处理 算法类 Java数据结构和算法(第2版) 编程之法面试和算法心得 编程珠玑(第2版) 编程珠玑2 大话数据结构 计算机程序设计艺术第1卷：基本算法（第3版） 计算机程序设计艺术第2卷：半数值算法（第3版） 计算机程序设计艺术第3卷：排序与查找（第2版） 剑指offer 数据结构(C语言版).严蔚敏_吴伟民.扫描版 数据结构与算法分析(C++描述)(第3版) 算法导论(第2版) 网络类 HTTP权威指南 TCP-IP详解卷1：协议 TCP-IP详解卷2：实现 TCP-IP详解卷3：TCP事务协议，HTTP，NNTP和UNIX域协议 图解TCP IP(第5版) 程序语言类 C/C++语言 C++ Primer(第5版)(中文版) C和指针 C语言程序设计 C语言的科学和艺术 modern-cpp-tutorial 大规模C++程序设计 深入体验C语言项目开发 实用C语言编程（第3版） Python语言 Django Web开发指南 Python.Cookbook(第2版)中文版 Python标准库中文版 Python高级编程（法莱德） Python核心编程(第2版) Python灰帽子 python基础教程(第2版) Python源码剖析 think in Python 编写高质量代码 改善Python程序的91个建议 利用Python进行数据分析 流畅的Python 深入浅出Python Java语言 Head First Java 中文高清版 Java编程思想(第4版) Java核心技术(第8版)卷I_基础知识 Java核心技术 Java入门经典 阿里巴巴Java开发手册终极版v1.3.0 设计模式之禅 秦晓波 PHP语言 Ajax与PHPWeb开发.pdf PHP高级程序设计_模式、框架与测试 PHP项目开发案例全程实录 PHP与MYSQL权威指南 C#/.NET语言 .NET本质论 .NET应用程序架构设计 原则 模式与实践 ASP.NET MVC4 WEB编程 ASP.NET MVC4高级编程 ASP.NET MVC4框架揭秘 ASP.NET.4.0 揭秘(卷1) ASP.NET.4.0 揭秘(卷2) ASP.NET本质论 ASP.NET设计模式 C#本质论 C#程序开发范例宝典 C#高级编程（第7版） C#入门经典(第3版) C#入门经典(第5版) C#与.NET程序员面试宝典 CLR.via.C#（第3版） IT企业必读的200个.NET面试题 WCF服务编程 WCF全面解析（上册） WCF全面解析（下册） 编写高质量代码改善C#程序的157个建议 大话设计模式 Web技术 CSS权威指南(第3版) HTML5程序设计(第2版) HTML5权威指南 JavaScript高级应用与实践 JavaScript权威指南(第4版) JavaScript权威指南(第6版) JavaScript入门经典(第4版) jQuery权威指南 WebKit技术内幕 高性能网站建设进阶指南 论道HTML5 认识与设计：理解UI设计准则 数据库类 Oracle Oracle高性能SQL引擎剖析-SQL优化与调优机制详解 PLSQL操作手册 编程艺术深入数据库体系结构 剑破冰山 Oracle开发艺术 收获，不止Oracle MySQL MySQL 5权威指南(第3版) MYSQL必知必会 MySQL技术内幕(第4版) MySQL技术内幕：SQL编程 MySQL技术内幕InnoDB存储引擎 MySQL性能调优与架构设计 高性能MySQL(第2版) 高性能MySQL(第3版) SQL Server SQL2005技术内幕： T-SQ程序设计 SQL2005技术内幕：存储引擎 SQL2008技术内幕：T-SQL查询 SQL2008技术内幕：T-SQL语言基础 SQLServer2008查询性能优化 SQLSERVER2008学习笔记：日常维护、深入管理、性能优化 SQL反模式 Transact-SQL权威指南 数据库索引设计与优化 数据库性能调优.原理与技术 大数据类 Hadoop权威指南(第2版) MongoDB权威指南 MongoDB实战 R与Hadoop大数据分析实战 Spark大数据处理：技术、应用与性能优化 Spark快速数据处理 其他系列 IT思维类 编码的奥秘 编码—隐匿在计算机软硬件背后的语言上 程序员的自我修养—链接、装载与库 程序员修炼之道 代码整洁之道 高效能人士的七个习惯 计算机程序的构造和解释 浪潮之巅 全栈增长工程师指南 人月神话 数学之美(第2版) 修改代码的艺术 一万小时天才理论 非书籍类 C语言学习资料.exe 架构设计类 GOF设计模式 UML和模式应用(第3版) 分布式JAVA应用 基础与实践 精通.NET企业项目开发：最新的模式、工具与方法 领域驱动设计C#2008实现 - 问题.设计.解决方案 领域驱动设计—软件核心复杂性应对之道 领域驱动设计与模式实战 企业应用架构模式 探索CQRS和事件源 微软应用技术架构(第2版) 重构_改善既有代码的设计 重构与模式 敏捷开发类 Scrum敏捷软件开发 Web开发敏捷之道 Web开发敏捷之道：应用Rails进行敏捷Web开发（第4版） 测试驱动开发的3项修炼：走出TDD丛林 大规模定制模式下的敏捷产品开发 高效程序员的45个习惯：敏捷开发修炼之道 敏捷估计与规划 敏捷技能修炼-敏捷软件开发与设计的最佳实践 敏捷开发：原则、模式与实践 敏捷开发的必要技巧 敏捷开发的艺术 敏捷开发回顾：使团队更强大 敏捷开发知识体系 敏捷软件开发：原则、模式与实践(C#版) 敏捷软件开发：原则、模式与实践 敏捷无敌 敏捷武士：看敏捷高手交付卓越软件 敏捷整合开发：更快改进性能的案例与实用技术 硝烟中的Scrum和XP 应用Rails进行敏捷Web开发(第3版) 用户故事与敏捷方法 LFS(100M+) 由于 GitHub 是gitLFS属于付费产品，免费空间有限，不作上传处理。 百度云传送门：【LFS_EBOOKS】 提取码：hpgp C#范例开发大全 C#核心开发技术从入门到精通 C语言程序设计_现代方法(第2版) C语言入门经典(第4版) Java核心技术(第10版)卷II_高级特性 Oracle+Database+11g数据库管理艺术 PHP 核心技术与最佳实践 SQL Server 2012编程入门经典(第4版) SQL Server企业级平台管理实践 大话设计模式 大话数据库 大数据Spark企业级实战版 代码大全(第2版) 代码重构(C# &amp; ASP.NET版) 锋利的jquery 软件设计精要与模式 实现领域驱动设计 数据结构与算法分析Java语言描述(第3版) 算法导论(第3版) GitHub上传100M以上文件解决方案 工具下载，详见【官网】 git-lfs-windowsgit-lfs-mac 基本步骤及其命令 123456789# 在项目中安装lfs$ git lfs install# 需要push的文件$ git lfs track \"程序语言类\\C&amp;C++语言\\C语言入门经典(第四版).（美）霍顿.pdf\"$ git add .gitattributes$ git lfs track \"程序语言类\\C&amp;C++语言\\C语言入门经典(第四版).（美）霍顿.pdf\"$ git add \"程序语言类\\C&amp;C++语言\\C语言入门经典(第四版).（美）霍顿.pdf\"$ git commit -m \"[add] add lfs ebook for C.\"$ git push origin master","categories":[{"name":"杂七杂八","slug":"others","permalink":"https://zhangbc.github.io/categories/others/"}],"tags":[{"name":"其他","slug":"others","permalink":"https://zhangbc.github.io/tags/others/"}]},{"title":"【经典算法】字符串转换成整数","slug":"【经典算法】字符串转换成整数","date":"2019-08-28T15:59:47.000Z","updated":"2019-08-28T16:12:53.702Z","comments":true,"path":"2019/08/28/algorithm_strings_02/","link":"","permalink":"https://zhangbc.github.io/2019/08/28/algorithm_strings_02/","excerpt":"","text":"本系列为《编程之法：面试和算法心得》的读书笔记。 算法1.3：字符串转换成整数 题目描述 输入一个由数字组成的字符串，把它转换成整数并输出。例如:输入字符串”123”，输出整数为123。给定函数原型 int StrToInt(const char *str)，实现字符串转换成整数的功能，不能使用库函数atoi。 分析与解法 思路分析：当扫描字符串的第一个字符“1”时，由于是第一位，故得到数字1；继续向后扫描到第二个字符”2“，之前已经得到数字1，在其后添加一个数字2，得到数字12，相当于前面的数字扩大了10倍然后加上刚扫描到的数字2，即：1×10+2=12。同理，扫描到第三个字符”3“，即可得到最终整数123为所求。故而，其基本思路就是：从左至右扫描字符串，把之前得到的数字乘以10，再加上当前字符表示的数字。 但是，在处理过程中，需要考虑以下问题： 1）空指针的输入：输入的是指针，在访问空指针时程序会崩溃，需要提前判空；2）正负符号：整数不仅包括数字，还有可能包括以“+”或“-”开头表示正负整数，遇到负号“-”需要做转换；3）非法字符：输入的字符串中可能有不是数字的字符（如误操作其他字符），需要预先判断，碰到非法字符程序应停止转换；4）整型溢出：输入的数字是以字符串的形式输入，若输入一个很长的字符串可能导致溢出。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 算法1.3：字符串转成整数int StrToInt(const char* str)&#123; static const int MAX_INT = (int)((unsigned)~0 &gt;&gt; 1); static const int MIN_INT = -(int)((unsigned)~0 &gt;&gt; 1); unsigned int n = 0; // 判空 if(str == 0) &#123; return 0; &#125; // 处理空格 while(isspace(*str)) &#123; ++str; &#125; // 处理正负 int sign = 1; if(*str == '+' || *str == '-') &#123; if(*str == '-') &#123; sign = -1; &#125; str++; &#125; while(isdigit(*str)) &#123; int c = *str - '0'; if(sign &gt; 0 &amp;&amp; (n &gt; MAX_INT/10 || (n == MAX_INT/10 &amp;&amp; c &gt; MAX_INT%10))) &#123; n = MAX_INT; break; &#125; else if(sign &lt; 0 &amp;&amp; (n &gt; (unsigned)MIN_INT/10 || (n == (unsigned)MIN_INT/10 &amp;&amp; c &gt; (unsigned)MIN_INT%10))) &#123; n = MIN_INT; break; &#125; n = n *10 + c; str++; &#125; return sign &gt; 0 ? n:-n;&#125; 算法分析：此算法难点在于处理数据溢出，其时间复杂度为 $O(n)$。 练习题 实现 string 到 double 的转换。","categories":[{"name":"C++","slug":"C","permalink":"https://zhangbc.github.io/categories/C/"}],"tags":[{"name":"数据结构与算法","slug":"data-structure-and-algorithms","permalink":"https://zhangbc.github.io/tags/data-structure-and-algorithms/"}]},{"title":"【经典算法】字符串旋转和包含算法","slug":"【经典算法】字符串旋转和包含算法","date":"2019-08-08T15:40:40.000Z","updated":"2019-08-08T15:48:32.238Z","comments":true,"path":"2019/08/08/algorithm_strings_01/","link":"","permalink":"https://zhangbc.github.io/2019/08/08/algorithm_strings_01/","excerpt":"","text":"本系列为《编程之法：面试和算法心得》的读书笔记。 作为一名大龄青年，为了即将踏入研究生之路，特此需要做一些计算机相关基础知识的积累，以弥补算法知识，谨以此开始自己的算法学习之路。 算法1.1：旋转字符串 题目描述 给定一个字符串，要求把字符串前面的若干个字符移动到字符串的尾部，如把字符串“abcdef”前面的2个字符’a’和’b’移动到字符串的尾部，使得原字符串变成字符串“cdefab”。请写一个函数完成此功能，要求对长度为n的字符串操作的时间复杂度为 $O(n)$，空间复杂度为 $O(1)$。 分析与解法 解法一：暴力移位法 1234567891011121314151617181920212223242526272829// 算法1.1：旋转字符串，暴力移位法void LeftShiftOne(char* strs, int number)&#123; int i = 0; char ch = strs[i]; for(i = 1; i &lt; number; i++) &#123; strs[i-1] = strs[i]; &#125; strs[i-1] = ch;&#125;void LeftRoatateString(char* strs, int n, int m)&#123; while(m--) &#123; LeftShiftOne(strs, n); &#125;&#125;// 测试函数int main(int argc, char* argv[])&#123; char strs[] = \"ABCDEFGH\"; LeftRoatateString(strs, 8, 3); cout &lt;&lt; strs &lt;&lt; endl; return 0;&#125; 算法分析：针对长度为n的字符串而言，假设需要移动m个字符到字符串的尾部，总共需要移动 m*n 次操作，同时设立一个变量存储第一个字符，故时间复杂度为 $O(n^2)$，空间复杂度为 $O(1)$，不合题意。 解法二：三步反转法 思路分析：将一个字符串分成X和Y两部分，在每个部分字符串上定义反转操作，如$X^T$，即把X的所有字符反转（例如X=”abc”，则 $X^T$=”cba”），于是得到：$(X^T Y^T)^T$=$YX$。 12345678910111213141516171819202122232425262728// 算法1.1：旋转字符串，三步反转法void ReverseString(char* str, int from, int to)&#123; while(from &lt; to) &#123; char ch = str[from]; str[from++] = str[to]; str[to--] = ch; &#125;&#125;void LeftReverseString(char* strs, int n, int m)&#123; m %= n; ReverseString(strs, 0, m-1); ReverseString(strs, m, n-1); ReverseString(strs, 0, n-1);&#125;// 测试函数int main(int argc, char* argv[])&#123; char strs[] = \"ABCDEFGH\"; LeftReverseString(strs, 8, 3); cout &lt;&lt; strs &lt;&lt; endl; return 0;&#125; 算法分析：针对长度为n的字符串而言，假设需要移动m个字符到字符串的尾部，总共需要移动 2*n 次操作，同时设立一个变量存储第一个字符，故时间复杂度为 $O(n)$，空间复杂度为 $O(1)$，符合题意。 练习题（自己动手） 链表翻转。例如给出一个链表和一个数k，链表为1—&gt;2—&gt;3—&gt;4—&gt;5—&gt;6，k=2，则翻转后为2—&gt;1—&gt;6—&gt;5—&gt;4—&gt;3；若k=3，翻转后3—&gt;2—&gt;1—&gt;6—&gt;5—&gt;4。 编写程序在原来字符串中把字符串尾部的m个字符移动到字符串的头部，要求：长度为n的字符串操作时间复杂度为 $O(n)$，空间复杂度为 $O(1)$。例如，源字符串为 “Ilovebaofeng”，m=7时输出为：“baofengIlove”。 单词翻转。输入一个英文句子，翻转句子中单词的顺序，但是单词内字符的顺序不变，句子中单词以空格符号隔开。为简单起见，标点符号和普通字符一样处理。例如，输入”I am a student.”，输出为 “student. a am I”。 算法1.2：字符串包含 题目描述 给定两个分别由字母组成的字符串A和字符串B，字符串B的长度比字符串A短。请问，如何快速地判断字符串B中的所有字符是否都在字符串A里面？为简单起见，我们规定输入的字符串只包含大写英文字母，请实现函数 bool StringContain(string &amp;A, string &amp;B)。示例一：string 1：ABCD，string 2： BAD，答案为true；示例二：string 1：ABCD，string 2： BCE，答案为false；示例三：string 1：ABCD，string 2： AA，答案为true。 分析与解法 解法一：常规解法 123456789101112131415161718192021222324// 算法1.2：字符串包含，常规方法bool StringContain(string &amp;a, string &amp;b)&#123; for(int i=0; i &lt; b.length(); i++) &#123; int j; for(j=0; (j &lt; a.length()) &amp;&amp; (a[j] != b[i]); j++); if(j &gt;= a.length()) &#123; return false; &#125; &#125; return true;&#125;// 测试函数int main(int argc, char* argv[])&#123; string a = \"ABCD\"; string b = \"AA\"; bool result = StringContain(a, b); cout &lt;&lt; result &lt;&lt; endl; return 0;&#125; 算法分析：这是一种最直观也是最简单的方法思路。此算法需要 $O（n*m）$ 次操作，时间开销较大。 解法二：排序方法 123456789101112131415161718// 算法1.2：字符串包含，排序方法bool StringContainSort(string &amp;a, string &amp;b)&#123; sort(a.begin(), a.end()); // 包含于&lt;algorithm&gt;模块内 sort(b.begin(), b.end()); for(int pa = 0, pb = 0; pb &lt; b.length(); pb++) &#123; while((pa &lt; a.length()) &amp;&amp; (a[pa] &lt; b[pb])) &#123; pa++; &#125; if(pa &gt;= a.length() || (a[pa] &gt; b[pb])) &#123; return false; &#125; &#125; return true;&#125; 算法分析：两个字符串的排序需要（常规情况）$O(m log m)+O(n log n)$ 次操作（快排算法），然后需要线性扫描 $O(m+n)$ 次操作。 解法三： 转换成素数 思路分析： 1）假定有一个仅由字母组成的字符串，按照从小到大的顺序，让每个字母与一个素数唯一对应，即用26个素数分别对应于A~Z；2）遍历长字符串。求得每个字符对应素数的乘积；3）遍历短字符串，判断乘积能否被短字符串中的字符对应的素数整除；4）输出结果。 12345678910111213141516171819202122232425// 算法1.2：字符串包含，转换成素数bool StringContainPrime(string &amp;a, string &amp;b)&#123; const int array[26] = &#123;2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101&#125;; int f = 1; for(int i = 0; i &lt; a.length(); i++) &#123; int x = array[a[i] - 'A']; if(f % x) &#123; f *= x; &#125; &#125; for(int j = 0; j &lt; b.length(); j++) &#123; int x = array[b[j] - 'A']; if(f % x) &#123; return false; &#125; &#125; return true;&#125; 算法分析：算法的时间复杂度为 $O(n)$ ，最好的情况为 $O(1)$（遍历短的字符串的第一个数，与长字符串素数的乘积相除，即出现余数，便可退出程序，返回 false）， n 为长字串的长度，空间复杂度为 $O(1)$。注意：此方法只有理论意义，因为整数乘积很大会造成溢出风险。 解法四：Hashtable方法 思路分析：先把长字符串 A中的所有字符都放入一个 Hashtable 里，然后轮询短字符串 B，看短字符串 B 的每个字符是否都在 Hashtable 里，如果都存在，说明长字符串 A 包含短字符串 B， 否则，说明不包含。 123456789101112131415161718// 算法1.2：字符串包含，Hashtable方法bool StringContainHash(string &amp;a, string &amp;b)&#123; int hash = 0; for(int i = 0; i &lt; a.length(); i++) &#123; hash |= (1 &lt;&lt; (a[i] - 'A')); &#125; for(int j = 0; j &lt; b.length(); j++) &#123; if(hash &amp; (1 &lt;&lt; (b[j] - 'A'))) &#123; return false; &#125; &#125; return true;&#125; 算法分析：此方法实质是用一个整数代替了Hashtable，空间复杂度为 $O(1)$，时间复杂度为 $O(n)$。 练习题（自己动手） 变位词：如果两个字符串的字符一样，但是顺序不一样，被认为是兄弟字符串，比如 bad 和 adb 即为兄弟字符串，现提供一个字符串，如何在字典中迅速找到它的兄弟字符串，请描述数据结构和查询过程。","categories":[{"name":"C++","slug":"C","permalink":"https://zhangbc.github.io/categories/C/"}],"tags":[{"name":"数据结构与算法","slug":"data-structure-and-algorithms","permalink":"https://zhangbc.github.io/tags/data-structure-and-algorithms/"}]},{"title":"【Python编码规范】设计模式","slug":"【Python编码规范】设计模式","date":"2019-06-13T14:55:38.000Z","updated":"2019-08-08T15:33:23.828Z","comments":true,"path":"2019/06/13/python_code91_05/","link":"","permalink":"https://zhangbc.github.io/2019/06/13/python_code91_05/","excerpt":"","text":"本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。 温馨提醒：在阅读本书之前，强烈建议先仔细阅读：PEP规范，增强代码的可阅读性，配合优雅的pycharm编辑器(开启pep8检查)写出规范代码，是Python入门的第一步。 建议50：利用模块实现单例模式1）所有的变量都会绑定到模块；2）模块只初始化一次；3）import机制是线程安全的。 建议51: 用mixin模式让程序更加灵活模板方法模式：在一个方法中定义一个算法的骨架，并将一些实现步骤延迟到子类中。 Python 中每一个类都有一个__base__属性，是一个元组，用来存放所有的基类，基类在运行中可以动态改变。 建议52：用发布订阅模式实现松耦合 发布订阅模式（publish/subscribe或pub/sub）是一种编程模式，消息的发送者（发布者）不会发送其消息给特定的接收者（订阅者），而是将发布的消息分为不同的类别直接发布，并不关注订阅者是谁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#!/usr/bin/env python# coding:utf-8​​\"\"\"发布订阅模式实现\"\"\"​​import messagefrom collections import defaultdict​​route_table = defaultdict(list)​​def sub(topic, callback): \"\"\"​ :param topic: :param callback: :return: \"\"\"​ if callback in route_table[topic]: return​ route_table[topic].append(callback)​​def pub(topic, *args, **kwargs): \"\"\"​ :param topic: :param args: :param kwargs: :return: \"\"\"​ for func in route_table[topic]: func(*args, **kwargs)​​def greeting(name): \"\"\"​ :param name: :return: \"\"\"​ print 'Hello, &#123;0&#125;.'.format(name)​​if __name__ == '__main__':​ sub('greet', greeting) pub('greet', 'LaiYonghao')​ message.sub('greet', greeting) message.pub('greet', 'Welcome to Python') 建议53：用状态模式美化代码状态模式：当一个对象的内在状态改变时允许改变其行为，但这个对象看起来像是改变了其类。主要用于控制一个对象状态的条件表达式过于复杂的情况，其可把状态的判断逻辑转移到表示不同状态的一系列类中，进而把复杂的判断逻辑简化。@stateful修饰函数，重载了被修饰类的getattr()方法从而使得类的实例方法能调用当前状态类的方法。被@stateful修饰后的类的实例是带有状态的，能够使用curr()查询当前状态，也可以使用switch()进行状态切换。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#!/usr/bin/env python# coding:utf-8​​\"\"\"状态模式实现\"\"\"​​from state import switch, stateful, State, behavior​​@statefulclass People(object): \"\"\"​ \"\"\"​ class Workday(State): \"\"\"​ \"\"\"​ default = True​ @behavior def day(self): print 'work hard.'​ class Weekend(State): \"\"\"​ \"\"\"​ @behavior def day(self): print 'play harder.'​​def main(): \"\"\"​ :return: \"\"\"​ people = People() for i in xrange(1, 8): if i == 6: switch(people, People.Weekend) if i == 1: switch(people, People.Workday) people.day()​​if __name__ == '__main__':​ main()","categories":[{"name":"Python","slug":"python","permalink":"https://zhangbc.github.io/categories/python/"}],"tags":[{"name":"Python编码规范","slug":"python-coding-convention","permalink":"https://zhangbc.github.io/tags/python-coding-convention/"}]},{"title":"【Python编码规范】库","slug":"【Python编码规范】库","date":"2019-05-12T15:01:16.000Z","updated":"2019-05-12T15:21:55.643Z","comments":true,"path":"2019/05/12/python_code91_04/","link":"","permalink":"https://zhangbc.github.io/2019/05/12/python_code91_04/","excerpt":"","text":"本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。 温馨提醒：在阅读本书之前，强烈建议先仔细阅读：PEP规范，增强代码的可阅读性，配合优雅的pycharm编辑器(开启pep8检查)写出规范代码，是Python入门的第一步。 建议36：掌握字符串的基本用法Python小技巧：Python遇到未闭合的小括号会自动将多行代码拼接为一行和把相邻的两个字符串字面量拼接在一起的。 12345&gt;&gt;&gt; st = ('select * '... 'from table '... 'whre field=\"value\";')&gt;&gt;&gt; st'select * from table whre field=\"value\";' 字符串用法举例： 12345678&gt;&gt;&gt; print isinstance('hello world', basestring) # basestring是str与unicode的基类True&gt;&gt;&gt; print isinstance('hello world', unicode)False&gt;&gt;&gt; print isinstance('hello world', str)True&gt;&gt;&gt; print isinstance(u'hello world', unicode)True split()的陷阱示例 123456&gt;&gt;&gt; ' Hello World'.split(' ')['', 'Hello', 'World']&gt;&gt;&gt; ' Hello World'.split()['Hello', 'World']&gt;&gt;&gt; ' Hello World'.split(' ')['', 'Hello', '', '', 'World'] title()应用示例 1234567&gt;&gt;&gt; import string&gt;&gt;&gt; string.capwords('hello wOrld')'Hello World'&gt;&gt;&gt; string.capwords(' hello wOrld ')'Hello World'&gt;&gt;&gt; ' hello wOrld '.title()' Hello World ' 建议37：按需选择sort()或者sorted()sorted(iterable[, cmp[, key[, reverse]]])：作用于任何可迭代对象，返回一个排序后的列表； sort(cmp[, key[, reverse]]])：一般作用于列表，直接修改原有列表，返回为None。 1）对字典进行排序 1234567&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; phone_book = &#123;'Linda': '775', 'Bob': '9349', 'Carol': '5834'&#125;&gt;&gt;&gt; sorted_pb = sorted(phone_book.iteritems(), key=itemgetter(1)) ​# 按照字典的value进行排序&gt;&gt;&gt; print phone_book&#123;'Linda': '775', 'Bob': '9349', 'Carol': '5834'&#125;&gt;&gt;&gt; print sorted_pb[('Carol', '5834'), ('Linda', '775'), ('Bob', '9349')] 2）多维list排序​1234567&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; game_result = [['Linda', 95, 'B'], ['Bob', 93, 'A'], ['Carol', 69, 'D'], ['zhangs', 95, 'A']]&gt;&gt;&gt; sorted_res = sorted(game_result, key=itemgetter(1, 2)) # 按照学生成绩排序，成绩相同的按照等级排序&gt;&gt;&gt; print game_result[['Linda', 95, 'B'], ['Bob', 93, 'A'], ['Carol', 69, 'D'], ['zhangs', 95, 'A']]&gt;&gt;&gt; print sorted_res[['Carol', 69, 'D'], ['Bob', 93, 'A'], ['zhangs', 95, 'A'], ['Linda', 95, 'B']] 3）字典中混合list排序 12345678910111213&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; list_dict = &#123;... 'Li': ['M', 7],... 'Zhang': ['E', 2],... 'Du': ['P', 3],... 'Ma': ['C', 9],... 'Zhe': ['H', 7]... &#125;&gt;&gt;&gt; sorted_ld = sorted(list_dict.iteritems(), key=lambda (k, v): itemgetter(1)(v)) # 按照字典的value[m,n]中的n值排序&gt;&gt;&gt; print list_dict&#123;'Zhe': ['H', 7], 'Zhang': ['E', 2], 'Ma': ['C', 9], 'Du': ['P', 3], 'Li': ['M', 7]&#125;&gt;&gt;&gt; print sorted_ld[('Zhang', ['E', 2]), ('Du', ['P', 3]), ('Zhe', ['H', 7]), ('Li', ['M', 7]), ('Ma', ['C', 9])] 4）list中混合字典排序 123456789101112&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; game_result = [... &#123;'name': 'Bob', 'wins': 10, 'losses': 3, 'rating': 75&#125;,... &#123;'name': 'David', 'wins': 3, 'losses': 5, 'rating': 57&#125;,... &#123;'name': 'Carol', 'wins': 4, 'losses': 5, 'rating': 57&#125;,... &#123;'name': 'Patty', 'wins': 9, 'losses': 3, 'rating': 71.48&#125;... ]&gt;&gt;&gt; sorted_res = sorted(game_result, key=itemgetter('rating', 'name')) # 按照name和rating排序&gt;&gt;&gt; print game_result[&#123;'wins': 10, 'losses': 3, 'name': 'Bob', 'rating': 75&#125;, &#123;'wins': 3, 'losses': 5, 'name': 'David', 'rating': 57&#125;, &#123;'wins': 4, 'losses': 5, 'name': 'Carol', 'rating': 57&#125;, &#123;'wins': 9, 'losses': 3, 'name': 'Patty', 'rating': 71.48&#125;]&gt;&gt;&gt; print sorted_res[&#123;'wins': 4, 'losses': 5, 'name': 'Carol', 'rating': 57&#125;, &#123;'wins': 3, 'losses': 5, 'name': 'David', 'rating': 57&#125;, &#123;'wins': 9, 'losses': 3, 'name': 'Patty', 'rating': 71.48&#125;, &#123;'wins': 10, 'losses': 3, 'name': 'Bob', 'rating': 75&#125;] 建议38：使用copy模块深拷贝对象 浅拷贝(shallow copy)：构造一个新的复合对象并将从原对象中发现的引用插入该对象中。实现方式有：工厂函数，切片操作，copy模块中copy操作等； 深拷贝(deep copy)：构造一个新的复合对象，但是遇到引用会继续递归拷贝其所指向的具体内容，也就是说它会针对引用所指向的对象继续进行拷贝，因此产生的对象不受其他引用对象操作的影响。实现方式有copy模块中的deepcopy()操作。 实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#!/usr/bin/env python# coding:utf-8from copy import copyfrom copy import deepcopyclass Pizza(object): \"\"\" \"\"\" def __init__(self, name, size, price): self.name = name self.size = size self.price = price def get_pizza_info(self): return self.name, self.size, self.price def show_pizza_info(self): print \"Pizza name: &#123;0&#125;, size: &#123;1&#125;, price: &#123;2&#125;\".format(self.name, self.size, self.price) def change_size(self, size): \"\"\" :param size: :return: \"\"\" self.size = size def change_price(self, price): \"\"\" :param price: :return: \"\"\" self.price = priceclass Order(object): \"\"\" \"\"\" def __init__(self, name): self.customer_name = name self.pizza_list = list() self.pizza_list.append(Pizza(\"Mushroom\", 12, 30)) def order_more(self, pizza): \"\"\" :param pizza: :return: \"\"\" self.pizza_list.append(pizza) def change_name(self, name): \"\"\" :param name: :return: \"\"\" self.customer_name = name def get_oder_detail(self): \"\"\" :return: \"\"\" print \"Customer name: &#123;0&#125;\".format(self.customer_name) for index, item in enumerate(self.pizza_list): item.show_pizza_info() def get_pizza(self, number): \"\"\" :param number: :return: \"\"\" return self.pizza_list[number]def customer_one(): c1 = Order(\"zhang San\") c1.order_more(Pizza(\"seafood\", 9, 40)) c1.order_more(Pizza(\"fruit\", 12, 35)) print \"==============Customer one order info=================\" c1.get_oder_detail() c2 = copy(c1) print \"==============Customer two order info(copy)=================\" c2.change_name(\"Li Si\") c2.get_pizza(2).change_size(9) c2.get_pizza(2).change_price(30) c2.get_oder_detail() c3 = deepcopy(c1) print \"==============Customer three order info(deepcopy)=================\" c3.change_name(\"Li Si\") c3.get_pizza(1).change_size(10) c3.get_pizza(1).change_price(50) c3.get_oder_detail() print \"==============Customer one order info=================\" c1.get_oder_detail()if __name__ == '__main__': customer_one() 运行结果如下： 1234567891011121314151617181920==============Customer one order info=================Customer name: zhang SanPizza name: Mushroom, size: 12, price: 30Pizza name: seafood, size: 9, price: 40Pizza name: fruit, size: 12, price: 35==============Customer two order info(copy)=================Customer name: Li SiPizza name: Mushroom, size: 12, price: 30Pizza name: seafood, size: 9, price: 40Pizza name: fruit, size: 9, price: 30==============Customer three order info(deepcopy)=================Customer name: Li SiPizza name: Mushroom, size: 12, price: 30Pizza name: seafood, size: 10, price: 50Pizza name: fruit, size: 9, price: 30==============Customer one order info=================Customer name: zhang SanPizza name: Mushroom, size: 12, price: 30Pizza name: seafood, size: 9, price: 40Pizza name: fruit, size: 9, price: 30 建议39：使用Counter进行计数统计 使用dict 12345678910&gt;&gt;&gt; some_data = ['a', 2, '2', 4, 5, '2', 'b', 7, 'a', 5, 'd', 'a', 'z']&gt;&gt;&gt; count_frq = dict()&gt;&gt;&gt; for index, item in enumerate(some_data):... if item in count_frq:... count_frq[item] += 1... else:... count_frq[item] = 1... &gt;&gt;&gt; print count_frq&#123;'a': 3, 2: 1, 'b': 1, 4: 1, 5: 2, 7: 1, '2': 2, 'z': 1, 'd': 1&#125; 使用defaultdict 12345678&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; some_data = ['a', 2, '2', 4, 5, '2', 'b', 7, 'a', 5, 'd', 'a', 'z']&gt;&gt;&gt; count_frq = defaultdict(int)&gt;&gt;&gt; for index, item in enumerate(some_data):... count_frq[item] += 1... &gt;&gt;&gt; print count_frqdefaultdict(&lt;type 'int'&gt;, &#123;'a': 3, 2: 1, 'b': 1, 4: 1, 5: 2, 7: 1, '2': 2, 'z': 1, 'd': 1&#125;) 使用set与list 12345678&gt;&gt;&gt; some_data = ['a', 2, '2', 4, 5, '2', 'b', 7, 'a', 5, 'd', 'a', 'z']&gt;&gt;&gt; count_set = set(some_data)&gt;&gt;&gt; count_list = list()&gt;&gt;&gt; for index, item in enumerate(some_data):... count_list.append((item, some_data.count(item)))... &gt;&gt;&gt; print count_list[('a', 3), (2, 1), ('2', 2), (4, 1), (5, 2), ('2', 2), ('b', 1), (7, 1), ('a', 3), (5, 2), ('d', 1), ('a', 3), ('z', 1)] 使用更为优雅的Pythonic方法—collections.Counter 1234567891011121314&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; some_data = ['a', 2, '2', 4, 5, '2', 'b', 7, 'a', 5, 'd', 'a', 'z']&gt;&gt;&gt; print Counter(some_data)Counter(&#123;'a': 3, 5: 2, '2': 2, 2: 1, 'b': 1, 4: 1, 7: 1, 'z': 1, 'd': 1&#125;)&gt;&gt;&gt; print Counter('success')Counter(&#123;'s': 3, 'c': 2, 'e': 1, 'u': 1&#125;)&gt;&gt;&gt; print Counter(s=3, c=2, e=1, u=1)Counter(&#123;'s': 3, 'c': 2, 'u': 1, 'e': 1&#125;)&gt;&gt;&gt; print Counter(&#123;'s': 3, 'c': 2, 'u': 1, 'e': 1&#125;)Counter(&#123;'s': 3, 'c': 2, 'u': 1, 'e': 1&#125;)&gt;&gt;&gt; print list(Counter(some_data).elements())['a', 'a', 'a', 2, 'b', 4, 5, 5, 7, '2', '2', 'z', 'd']&gt;&gt;&gt; print Counter(some_data).most_common(3) # 出现频次最高的前三个字符[('a', 3), (5, 2), ('2', 2)] 建议40：深入理解ConfigParser 实例 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# coding:utf-8import ConfigParserconf = ConfigParser.ConfigParser()conf.read('config.ini')print conf.get('default', 'host')conf = ConfigParser.ConfigParser()conf.read('config.ini')print conf.get('online', 'conn_str') # 仅在default下​====================config.ini=========================[default]conn_str = %(dbn)s://%(user)s:%(pw)s@%(host)s:%(port)s/%(db)sdbn = msyqlhost = 127.0.0.1user = rootport = 3306pw = xxxxxxdb = test[online]conn_str = %(dbn)s://%(user)s:%(pw)s@%(host)s:%(port)s/%(db)sdbn = msyqlhost = 127.0.0.1user = rootport = 3306pw = xxxxxxdb = test 建议41：使用argparese处理命令行参数12345import argparseparser = argparse.ArgumentParser()parser.add_argument('-v', dest='verbose', action='store_true')args = parser.parse_args()print args 建议42：使用pandas处理大型csv文件csv作为一种逗号分隔型值的纯文本格式文件，常见于数据库数据的导入导出、数据分析中记录的存储等。 以下列举几个与csv处理相关的API： csv.reader(csvfile[, dialect=&#39;excel&#39;][, fmtparam])：用于CSV文件的读取，返回一个reader对象用于在CSV文件中进行行迭代； csv.writer(csvfile, dialect=&#39;excel&#39;, **fmtparams)：用于写入CSV文件； csv.DictReader(csvfile, fieldnames=None, restKey=&#39;&#39;, restval=&#39;&#39;, dialect=&#39;excel&#39;, *args, **kwds)：用于支持字典的读取； csv.DictReader(csvfile, fieldnames=None, restval=&#39;&#39;, extrasaction=&#39;raise&#39;, dialect=&#39;excel&#39;, *args, **kwds)：用于支持字典的写入。 实例 12345678910111213141516171819import csv# 写入csvwith open('csv_test.csv', 'wb') as fp: fields = ['Tran_date', 'Product', 'Price', 'PaymentType'] writer = csv.DictWriter(fp, fieldnames=fields) writer.writerow(dict(zip(fields, fields))) data = &#123;'Tran_date': '1/2/09 6:17', 'Product': 'Nick', 'Price': '1200', 'PaymentType': 'Mastercard'&#125; writer.writerow(data)# 读取csvwith open('csv_test.csv', 'rb') as fp: for item in csv.DictReader(fp): print item csv使用非常简单，基本可以满足大部分需求，但是对于上百MB或G级别以上的文件处理无能为力。这种情况下，可以考虑使用pandas模块，它支持以下两种数据结构。 Series：是一种类似数组的带索引的一维数据结构，支持的类型与NumPy兼容。 123456789101112131415161718192021&gt;&gt;&gt; from pandas import Series&gt;&gt;&gt; obj = Series([1, 'a', (1, 2), 3], index=['a', 'b', 'c', 'd'])&gt;&gt;&gt; obja 1b ac (1, 2)d 3dtype: object&gt;&gt;&gt; obj_dic = Series(&#123;'Book': 'Python', 'Author': 'Dan', 'ISBN': '011334', 'Price': 25&#125;, index=['book', 'Author', 'ISBN', 'Price'])&gt;&gt;&gt; obj_dicbook NaN # 匹配失败，导致数据丢失Author DanISBN 011334Price 25dtype: object&gt;&gt;&gt; obj_dic.isnull()book TrueAuthor FalseISBN FalsePrice Falsedtype: bool DataFrame：类似于电子表格，其数据为排好序的数据列的集合，每一列都可以是不同的数据类型，类似一个二维数组，支持行和列的索引。 1234567891011&gt;&gt;&gt; from pandas import DataFrame&gt;&gt;&gt; data = &#123;'OrderDate': ['1-6-10', '1-23-10', '2-9-10', '2-26-10', '3-15-10'],... 'Region': ['East', 'Central', 'Central', 'West', 'East'],... 'Rep': ['Jones', 'Kivell', 'Jardine', 'Gill', 'Sorvino']&#125;&gt;&gt;&gt; DataFrame(data, columns=['OrderDate', 'Region', 'Rep']) OrderDate Region Rep0 1-6-10 East Jones1 1-23-10 Central Kivell2 2-9-10 Central Jardine3 2-26-10 West Gill4 3-15-10 East Sorvino pandas中处理CSV文件的函数主要为read_csv()和to_csv()。 指定读取部分列和文件的行数 123456789&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; df = pd.read_csv('/home/projects/pythoner/quality_code/csv_test.csv', nrows=5, usecols=['Tran_date', 'Product', 'Price'])&gt;&gt;&gt; df Tran_date Product Price0 1/2/09 6:17 Nick 12001 2/2/09 6:17 Nick 12002 3/2/09 6:17 Nick 12003 4/2/09 6:17 Nick 12004 5/2/09 6:17 Nick 1200 设置CSV文件与excel兼容 12345678910111213141516&gt;&gt;&gt; import csv&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; dia = csv.excel()&gt;&gt;&gt; dia.delimiter = \",\"&gt;&gt;&gt; pd.read_csv('/home/projects/pythoner/quality_code/csv_test.csv', dialect=dia, error_bad_lines=False) Tran_date Product Price PaymentType0 1/2/09 6:17 Nick 1200 Mastercard1 2/2/09 6:17 Nick 1200 Mastercard2 3/2/09 6:17 Nick 1200 Mastercard3 4/2/09 6:17 Nick 1200 Mastercard4 5/2/09 6:17 Nick 1200 Mastercard5 6/2/09 6:17 Nick 1200 Mastercard6 7/2/09 6:17 Nick 1200 Mastercard7 8/2/09 6:17 Nick 1200 Mastercard8 9/2/09 6:17 Nick 1200 Mastercard9 10/2/09 6:17 Nick 1200 Mastercard 对文件进行分块处理并返回一个可迭代的对象 123456789101112131415&gt;&gt;&gt; reader = pd.read_table('/home/projects/pythoner/quality_code/csv_test.csv', chunksize=5, iterator=True)&gt;&gt;&gt; iter(reader).next() Tran_date,Product,Price,PaymentType0 1/2/09 6:17,Nick,1200,Mastercard1 2/2/09 6:17,Nick,1200,Mastercard2 3/2/09 6:17,Nick,1200,Mastercard3 4/2/09 6:17,Nick,1200,Mastercard4 5/2/09 6:17,Nick,1200,Mastercard&gt;&gt;&gt; iter(reader).next() Tran_date,Product,Price,PaymentType5 6/2/09 6:17,Nick,1200,Mastercard6 7/2/09 6:17,Nick,1200,Mastercard7 8/2/09 6:17,Nick,1200,Mastercard8 9/2/09 6:17,Nick,1200,Mastercard9 10/2/09 6:17,Nick,1200,Mastercard 当文件格式相似时，支持多个文件合并处理 123456789101112131415&gt;&gt;&gt; file_list = ['/home/projects/pythoner/quality_code/csv_test1.csv', '/home/projects/pythoner/quality_code/csv_test2.csv']&gt;&gt;&gt; dfs = [pd.read_csv(f) for f in file_list]&gt;&gt;&gt; total_df = pd.concat(dfs)&gt;&gt;&gt; total_df Tran_date Product Price PaymentType0 1/2/09 6:17 Nick 1200 Mastercard1 2/2/09 6:17 Nick 1200 Mastercard2 3/2/09 6:17 Nick 1200 Mastercard3 4/2/09 6:17 Nick 1200 Mastercard4 5/2/09 6:17 Nick 1200 Mastercard0 6/2/09 6:17 Nick 1200 Mastercard1 7/2/09 6:17 Nick 1200 Mastercard2 8/2/09 6:17 Nick 1200 Mastercard3 9/2/09 6:17 Nick 1200 Mastercard4 10/2/09 6:17 Nick 1200 Mastercard 建议43：一般情况使用ElementTree解析XMLElementTree解析XML具有以下特性： 使用简单，将整个XML文件以树的形式展示，每一个元素的属性以字典的形式表示，非常方便处理； 内存上消耗明显低于DOM解析； 支持XPath查询，非常方便获取任意结点的值。 建议44：理解模块pickle优劣1）pickle.dump(obj,file[,protocol])：序列化数据到一个文件描述符。 其中：protocol为序列化使用的协议版本，0表示ASCII协议，为默认值；1表示老式的二进制协议；2表示2.3版本引入的新二进制协议。 2）pickle.load()：表示把文件中的对象恢复为原来的对象，这个过程也被称为反序列化。 123456789101112import cPickle as pickle​​my_data = &#123;\"name\": \"Python\", \"type\": \"language\", \"version\": \"2.7.6\"&#125;fp = open('pickle.dat', 'wb')pickle.dump(my_data, fp)fp.close()​fp = open('pickle.dat', 'rb')out = pickle.load(fp)print outfp.close() pickle的优点： 1）接口简单，容易使用；2）pickle的存储格式具有通用性，能够被不同平台的Python解析器共享；3）支持的数据类型广泛；4）pickle模块是可扩展的；5）能够自动维护对象间的引用。 pickle的缺点： 1）pickle不能保证操作的原子性；2）pickle存在安全性问题；3）pickle协议是Python特定的，不同语言之间的兼容性难以保证。 建议45：序列化的另一个不错的选择—JSONJSON具有以下优势： 使用简单，支持多种数据类型；仅存在两大数据结构：名称/值对的集合（对象，记录，结构，字典，散列表，键列表，关联数组等）；值的有序列表（数组，向量，列表，序列等）。 存储格式可读性好，容易修改； json支持跨平台跨语言操作，能够轻易被其他语言解析，存储格式较紧凑，所占空间较小； 具有较强的扩展性； json在序列化datetime时会抛出TypeError异常，需要对json本身的JSONEncoder进行扩展。 建议46：使用traceback获取栈信息 实例 1234567891011121314151617181920212223242526272829303132import trackbackg_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g']def f(): g_list[5] return g()def g(): return h()def h(): del g_list[2] return i()def i(): g_list.append('i') print g_list[7]if __name__ == '__main__': try: f() except IndexError as e: print 'Error: &#123;0&#125;'.format(e) traceback.print_exc() 输出结果如下： 12345678910111213Error: list index out of rangeTraceback (most recent call last): File \"/home/projects/pythoner/quality_code/configure_parser.py\", line 33, in &lt;module&gt; f() File \"/home/projects/pythoner/quality_code/configure_parser.py\", line 13, in f return g() File \"/home/projects/pythoner/quality_code/configure_parser.py\", line 17, in g return h() File \"/home/projects/pythoner/quality_code/configure_parser.py\", line 22, in h return i() File \"/home/projects/pythoner/quality_code/configure_parser.py\", line 27, in i print g_list[7]IndexError: list index out of range 建议47：使用logging记录日志信息1，日志级别 Level 使用情形 DEBUG 详细的信息，在追踪问题时使用 INFO 正常的信息 WARNING 一些不可预见的问题发生，或者将要发生，如磁盘空间低等，但不影响程序的运行 ERROR 由于某些严重的问题，程序中的一些功能受到影响 CRITICAL 严重的错误，或者程序本身不能继续运行 2， logging lib的四个主要对象 logger：程序信息输出的接口，分散在不同的代码中，使得程序可以在运行时记录相应的信息，并根据设置的日志级别或者filter来决定哪些信息需要输出，并将这些信息分发到其关联的handler。 Handler：用来处理信息的输出，可以将信息输出到控制台、文件或者网络。 Formatter：决定log信息的格式。 Filter：决定哪些信息需要输出，可以被handler和logger使用，支持层次关系。 logging.basicConfig([**kwargs]) 提供对日志系统的基本配置，默认使用StreamHandler和Formatter并添加到root logger。字典参数列表如下： 格式 描述 filename 指定FileHandler的文件名，而不是默认的StreamHandler filemode 打开文件的模式，默认为‘a’ format 输出格式字符串 datefmt 日期格式 level 设置root logger的日志级别 stream 指定StreamHandler，若与filename冲突，忽略stream 实例 1234567891011121314def get_logger(file_name, level=logging.INFO): \"\"\" 设置日志文件输出 :param file_name: 文件名称 :param level: 日志严重级别 ==&gt; CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG &gt; NOTSET :return: \"\"\" logger = logging.getLogger() logger.setLevel(level) file_handler = logging.FileHandler(file_name, encoding=\"utf-8\") file_handler.setLevel(level) formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s [line %(lineno)s]: %(message)s\") file_handler.setFormatter(formatter) logger.addHandler(file_handler) 3，使用建议 1）尽量为logging取一个名字而不是采用默认，eg：logger=logging.getLogger(__name__)；2）为了方便找出问题，logging的名字建议以模块或者class命名；3）logging只是线程安全的，不支持多进程写入同一个日志文件。 建议48：使用threading模块编写多线程程序 Python多线程支持两种方式创建： 1）通过继承Thread类，重写其run()方法(不是start()方法)；不支持守护线程；2）创建threading.Thread对象,在它的初始化函数（__init__()）中将可调用对象作为参数传入。 建议49：使用Queue使多线程编程更加安全Python中的Queue模块提供了以下队列： Queue.Queue(maxsize)：先进先出，maxsize为队列大小，其值为非正数时为无限循环队列； Queue.LifoQueue(maxsize)：后进先出，相当于栈； Queue.PriorityQueue(maxsize)：优先级队列。 生产-消费者模式实现demo： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#!/usr/bin/env python# coding:utf-8​​import Queueimport threadingimport random​​WRITE_LOCK = threading.Lock()​​class Producer(threading.Thread): \"\"\" 生产-消费者模式（生产者） \"\"\"​ def __init__(self, queue, condition, name): \"\"\"​ :param queue: :param condition: :param name: \"\"\"​ super(Producer, self).__init__() self.queue = queue self.name = name self.condition = condition print \"Producer &#123;0&#125; started.\".format(self.name)​ def run(self): \"\"\"​ :return: \"\"\"​ while True: global WRITE_LOCK self.condition.acquire() # 获取锁对象 if self.queue.full(): with WRITE_LOCK: print 'Queue is full, producer wait!' self.condition.wait() else: value = random.randint(0, 10) with WRITE_LOCK: print \"&#123;name&#125; put value: &#123;value&#125; into queue.\"\\ .format(name=self.name, value=value) self.queue.put(\"&#123;0&#125;: &#123;1&#125;\".format(self.name, value)) self.condition.notify() self.condition.release()​​class Consumer(threading.Thread): \"\"\" 生产-消费者模式（消费者） \"\"\"​ def __init__(self, queue, condition, name): \"\"\"​ :param queue: :param condition: :param name: \"\"\"​ super(Consumer, self).__init__() self.queue = queue self.name = name self.condition = condition print \"Consumer &#123;0&#125; started.\".format(self.name)​ def run(self): \"\"\"​ :return: \"\"\"​ while True: global WRITE_LOCK self.condition.acquire() # 获取锁对象 if self.queue.empty(): with WRITE_LOCK: print 'Queue is empty, consumer wait!' self.condition.wait() else: value = self.queue.get() with WRITE_LOCK: print \"&#123;name&#125; get value: &#123;value&#125; from queue.\"\\ .format(name=self.name, value=value) self.condition.notify() self.condition.release()​​if __name__ == '__main__':​ qe = Queue.Queue(10) con = threading.Condition() producer_1 = Producer(qe, con, \"P1\") producer_1.start() # producer_2 = Producer(qe, con, \"P2\") # producer_2.start()​ consumer_1 = Consumer(qe, con, \"C1\") consumer_1.start()","categories":[{"name":"Python","slug":"python","permalink":"https://zhangbc.github.io/categories/python/"}],"tags":[{"name":"Python编码规范","slug":"python-coding-convention","permalink":"https://zhangbc.github.io/tags/python-coding-convention/"}]},{"title":"【Python爬虫实例】Python解决521反爬方案","slug":"【Python爬虫实例】Python解决521反爬方案","date":"2019-05-05T15:49:14.000Z","updated":"2019-05-05T16:09:48.352Z","comments":true,"path":"2019/05/05/python_anti_spider_521/","link":"","permalink":"https://zhangbc.github.io/2019/05/05/python_anti_spider_521/","excerpt":"","text":"参考文献：https://github.com/xiantang/Spider/blob/master/Anti_Anti_Spider_521/pass_521.py 写在前面的话Python在爬虫方面的优势，想必业界无人不知，随着互联网信息时代的的发展，Python爬虫日益突出的地位越来越明显，爬虫与反爬虫愈演愈烈。下面分析一例关于返回HTTP状态码为521的案例。 案例准备 案例网站：【中国一带一路官网】， 以抓取文章【“一带一路”建设成果图鉴丨陆海内外联动，湖北推动产能合作纵深推进】为例，进行深度剖析。 案例剖析1） 浏览器访问【“一带一路”建设成果图鉴丨陆海内外联动，湖北推动产能合作纵深推进】： 2）写ython代码访问，查看http(s)返回状态 123456789101112131415161718# coding:utf-8import requestsfrom fake_useragent import UserAgentUSER_AGENT = UserAgent()ua = USER_AGENT.randomurl = r'https://www.yidaiyilu.gov.cn/xwzx/gnxw/87373.htm'headers = &#123; \"Host\": \"www.yidaiyilu.gov.cn\", \"User-Agent\": ua&#125;rs = requests.session()resp = rs.get(url)print(resp.status_code)print(resp.text) 不幸的是，返回的http的状态码却是501，text为一段混淆的js代码。 3）百度查资料，推荐为文首的【参考文献】 继续参照资料修改代码，Python执行JS首选execjs，pip安装如下：1pip install PyExecJS 将请求到的js执行： 1234text_521 = ''.join(re.findall('&lt;script&gt;(.*?)&lt;/script&gt;', resp.text))func_return = text_521.replace('eval', 'return')content = execjs.compile(func_return)print(content.call('f')) 将返回的结果print发现还是一段JS，标准格式化（【格式化Javascript工具】），结果如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859var _2i = function () &#123; setTimeout('location.href=location.pathname+location.search.replace(/[\\?|&amp;]captcha-challenge/,\\'\\')', 1500); document.cookie = '__jsl_clearance=1557019601.296|0|' + (function () &#123; var _2i = [([(-~[] &lt;&lt; -~[])] * (((+!+&#123;&#125;) + [(-~[] &lt;&lt; -~[])] &gt;&gt; (-~[] &lt;&lt; -~[]))) + []), (-~&#123;&#125; + [] + [ [] ][0]) + [3 - ~(+!+&#123;&#125;) - ~(+!+&#123;&#125;)], (-~&#123;&#125; + [] + [ [] ][0]) + [5], (-~&#123;&#125; + [] + [ [] ][0]) + [~~''], (-~&#123;&#125; + [] + [ [] ][0]), [-~~~!&#123;&#125; + [~~ [] ] - (-~~~!&#123;&#125;)], (-~&#123;&#125; + [] + [ [] ][0]) + [-~&#123;&#125; - ~[-~&#123;&#125; - ~&#123;&#125;]], (-~&#123;&#125; + [] + [ [] ][0]) + (-~&#123;&#125; + [] + [ [] ][0]), [-~(+!+&#123;&#125;)], (-~&#123;&#125; + [] + [ [] ][0]) + ([(-~[] &lt;&lt; -~[])] * (((+!+&#123;&#125;) + [(-~[] &lt;&lt; -~[])] &gt;&gt; (-~[] &lt;&lt; -~[]))) + []), (-~&#123;&#125; + [] + [ [] ][0]) + (-~[-~&#123;&#125; - ~&#123;&#125;] + [ [] ][0]), (((-~[] &lt;&lt; -~[]) &lt;&lt; (-~[] &lt;&lt; -~[])) + [ [] ][0]), [3 - ~(+!+&#123;&#125;) - ~(+!+&#123;&#125;)], (-~&#123;&#125; + [] + [ [] ][0]) + (((-~[] &lt;&lt; -~[]) &lt;&lt; (-~[] &lt;&lt; -~[])) + [ [] ][0]), [5], [-~&#123;&#125; - ~[-~&#123;&#125; - ~&#123;&#125;]], (-~&#123;&#125; + [] + [ [] ][0]) + [-~(+!+&#123;&#125;)], (-~[-~&#123;&#125; - ~&#123;&#125;] + [ [] ][0]), [~~''] ], _1d = Array(_2i.length); for (var _5 = 0; _5 &lt; _2i.length; _5++) &#123; _1d[_2i[_5]] = ['Bz', (-~[-~&#123;&#125; - ~&#123;&#125;] + [ [] ][0]), [&#123;&#125; + [] + [ [] ][0]][0].charAt(-~~~!&#123;&#125;), 'DR', ([(-~[] &lt;&lt; -~[])] / (+!/!/) + [] + [ [] ][0]).charAt(-~[-~~~!&#123;&#125; - ~(-~[] - ~&#123;&#125; - ~&#123;&#125;)]) + (+[(+!+&#123;&#125;), (+!+&#123;&#125;)] + []).charAt((+!+&#123;&#125;)), [ [][ [] ] + [] + [ [] ][0] ][0].charAt(-~&#123;&#125; - ~[-~&#123;&#125; - ~&#123;&#125;]), 'qM', (((-~[] &lt;&lt; -~[]) &lt;&lt; (-~[] &lt;&lt; -~[])) + [ [] ][0]) + (+[(+!+&#123;&#125;), (+!+&#123;&#125;)] + []).charAt((+!+&#123;&#125;)) + (-~&#123;&#125; /~~''+[]+[[]][0]).charAt((+!/!/)),'S','g%',(((-~[]&lt;&lt;-~[])&lt;&lt;(-~[]&lt;&lt;-~[]))+[[]][0]),'HxXL',[[][[]]+[]+[[]][0]][0].charAt(-~&#123;&#125;-~[-~&#123;&#125;-~&#123;&#125;]),'D',[-~(+!+&#123;&#125;)],'T%','YW',[&#123;&#125;+[]+[[]][0]][0].charAt(-~~~!&#123;&#125;),'vw'][_5]&#125;;return _1d.join('')&#125;)()+';Expires=Sun, 05-May-19 02:26:41 GMT;Path=/; '&#125;;if((function()&#123;try&#123;return !!window.addEventListener;&#125;catch(e)&#123;return false;&#125;&#125;)())&#123;document.addEventListener(' DOMContentLoaded ',_2i,false)&#125;else&#123;document.attachEvent(' onreadystatechange ',_2i)&#125; 4）修改与浏览器相关的代码，然后放入浏览器的console进行调试。 注意，在调试过程中，不难发现，js变量是动态生成的。最初还嵌套有document.createElement(&#39;div&#39;)，Python的execjs包不支持处理这类代码，需要做相应处理。 5）综上分析，完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#!/usr/bin/env python# coding:utf-8import sysimport reimport requestsimport execjsfrom fake_useragent import UserAgentreload(sys)sys.setdefaultencoding('utf8')class YiDaiYiLuSpider(object): \"\"\" 中国一带一路网（521反爬） \"\"\" USER_AGENT = UserAgent() ua = USER_AGENT.random url = r'https://www.yidaiyilu.gov.cn/xwzx/gnxw/87373.htm' headers = &#123; \"Host\": \"www.yidaiyilu.gov.cn\", \"User-Agent\": ua &#125; @classmethod def get_text521(cls): \"\"\" :return: \"\"\" rs = requests.session() resp = rs.get(url=cls.url, headers=cls.headers) text_521 = ''.join(re.findall('&lt;script&gt;(.*?)&lt;/script&gt;', resp.text)) cookie_id = '; '.join(['='.join(item) for item in resp.cookies.items()]) return cookie_id, text_521 @classmethod def generate_cookies(cls, func): \"\"\" :param func: :return: \"\"\" func_return = func.replace('eval', 'return') content = execjs.compile(func_return) eval_func = content.call('f') var = str(eval_func.split('=')[0]).split(' ')[1] rex = r\"&gt;(.*?)&lt;/a&gt;\" rex_var = re.findall(rex, eval_func)[0] mode_func = eval_func.replace('document.cookie=', 'return ').replace(';if((function()&#123;try&#123;return !!window.addEventListener;&#125;', ''). \\ replace(\"catch(e)&#123;return false;&#125;&#125;)())&#123;document.addEventListener('DOMContentLoaded',\" + var + \",false)&#125;\", ''). \\ replace(\"else&#123;document.attachEvent('onreadystatechange',\" + var + \")&#125;\", '').\\ replace(r\"setTimeout('location.href=location.pathname+location.search.replace(/[\\?|&amp;]captcha-challenge/,\\'\\')',1500);\", '').\\ replace('return return', 'return').\\ replace(\"document.createElement('div')\", '\"https://www.yidaiyilu.gov.cn/\"').\\ replace(r\"&#123;0&#125;.innerHTML='&lt;a href=\\'/\\'&gt;&#123;1&#125;&lt;/a&gt;';&#123;0&#125;=&#123;0&#125;.firstChild.href;\".format(var, rex_var), '') content = execjs.compile(mode_func) cookies_js = content.call(var) __jsl_clearance = cookies_js.split(';')[0] return __jsl_clearance @classmethod def crawler(cls): \"\"\" :return: \"\"\" url = r'https://www.yidaiyilu.gov.cn/zchj/sbwj/87255.htm' cookie_id, text_521 = cls.get_text521() __jsl_clearance = cls.generate_cookies(text_521) cookies = \"&#123;0&#125;;&#123;1&#125;;\".format(cookie_id, __jsl_clearance) cls.headers[\"Cookie\"] = cookies print(cls.headers) res = requests.get(url=url, headers=cls.headers) res.encoding = 'utf-8' print(res.text)if __name__ == '__main__': YiDaiYiLuSpider.crawler() 运行结果如下：","categories":[{"name":"Python","slug":"python","permalink":"https://zhangbc.github.io/categories/python/"}],"tags":[{"name":"Python爬虫实例","slug":"python-crawler","permalink":"https://zhangbc.github.io/tags/python-crawler/"}]},{"title":"【Python编码规范】基础语法","slug":"【Python编码规范】基础语法","date":"2019-05-04T23:59:26.000Z","updated":"2019-05-04T17:29:21.855Z","comments":true,"path":"2019/05/05/python_code91_03/","link":"","permalink":"https://zhangbc.github.io/2019/05/05/python_code91_03/","excerpt":"","text":"本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。 温馨提醒：在阅读本书之前，强烈建议先仔细阅读：PEP规范，增强代码的可阅读性，配合优雅的pycharm编辑器(开启pep8检查)写出规范代码，是Python入门的第一步。 Python 基础语法，即Python程序的基本要素，分为： 基本数据类型：数字、字符串、列表、字典、集合、元组等； 常见的语法：条件、循环、函数、列表解析等。 建议19：有节制地使用from…import语句Python提供了3种方式引入外部模块：import语句，from...import...及__import__函数。 __import__函数可以显式地将模块名称作为字符串传递并赋值给命名空间的变量。 在使用import时需要注意以下事项： 1）一般尽量优先使用import a形式，如果访问B时需要使用a.B的形式；2）有节制地使用from a import B形式，可以直接访问B；3）尽量避免使用from a import *，减少污染命名空间。 Python的import机制：Python在初始化运行环境的时候会预先加载一批内建模块到内存中，其相关信息被存放在sys.modules中。 from a import ...无节制的使用产生的问题： 1）命名空间的冲突； 文件a.py： 12def add(): print \"add in module A.\" 文件b.py： 12def add(): print \"add in module B.\" 测试文件importtest.py： 123456from a import addfrom b import addif __name__ == '__main__': add() 2）循环嵌套导入的问题。 可以考虑from...import的情况： 1）当只需要导入部分属性或方法时；2）模块中的这些属性和方法访问频率较高导致使用“模块名.名称”的形式进行访问过于烦琐时；3）模块的文档明确说明需要使用from...import形式，导入的是一个包下面的子模块，且使用from...import形式能够更为简单和便捷时。 建议20：优先使用absolute import来导入模块在Python2.4以前默认为隐式的relative import，局部范围的模块将覆盖同名的全局范围的模块。Python2.5后虽然默认的仍是relative import，但它为absolute import提供了一种新的机制，在模块中使用from __future__ import absolute_import语句进行说明后再进行导入。同时还通过点号.提供了一种显式进行relative import的方法。 相比于absolute import，relative import在实际应用中反馈的问题较多(Python3中已移除)，absolute import的可读性和出现问题后的可跟踪性更好，因此，推荐优先使用absolute import。 建议21：i+=1不等于++iPython解释器会将++i操作解释为+(+i)，其中+表示正数符号。对于--i也是类似。 实例一 123&gt;&gt;&gt; i=1&gt;&gt;&gt; ++i1 实例二：无限循环 12345i = 0ls = [1, 2, 3, 4, 5, 6]while i &lt; len(ls): print ls[0] ++i 建议22：使用with自动关闭资源 with 语句的语法： 12with 表达式 [as 目标]: 代码块 包含with语句的代码块执行过程如下： 1）计算表达式的值，返回一个上下文管理器对象；2）加载上下文管理器对象的__exit__()方法以备后用；3）调用上下文管理器对象的__enter__()方法；4）若with语句中设置了目标对象，则将__enter__()方法的返回值赋值给目标对象；5）执行with中的代码块；6）若步骤5)中的代码正常结束，调用上下文管理器对象的__exit__()方法，其返回值直接忽略；7）若步骤5)中的代码执行过程中发生异常，调用上下文管理器对象的__exit__()方法，并将异常类型，值及traceback信息作为参数传递给__exit__()方法。若__exit__()的返回值为false，则异常会被重新抛出；若__exit__()的返回值为true，则异常会被挂起，程序继续执行。 建议23：使用else子句简化循环（异常处理）12345678910111213def is_prime(number): \"\"\"​ :param number: :return: \"\"\"​ for i in xrange(2, number): for j in xrange(2, i): if i % j == 0: break else: print '&#123;0&#125; is a prime number.'.format(i) 当循环“自然”终结（循环条件为假）时else从句会被执行一次；当循环是由break语句得到中断时，else子句就不被执行。 建议24：遵循异常处理的几点原则Python中常用的异常处理语法是：try，except，else，finally，可以有多种组合。 异常处理流程图如下： 异常处理遵循的基本原则： 1）注意异常的粒度，不推荐在try中放入过多的代码；2）谨慎使用单独的except语句处理所有异常，最好能定位具体的异常；3）注意异常捕捉的顺序，在合适的层次处理异常；向上层传递的时候需要警惕异常被丢失的情况，可以使用不带参数的raise来传递；4）使用更为友好的异常信息，遵循异常参数的规范。 建议25：避免finally中可能发生的陷阱无论try语句中是否有异常抛出，finally语句总会被执行。 12345678910111213141516# -*-coding:UTF-8 -*-​def test(a): try: if a &lt;= 0: raise ValueError(\"data can not be negative.\") else: return a except ValueError as ex: print ex finally: print \"The end!\" return -1​print test(0)print test(2) 建议26：深入理解None，正确判断对象是否为空Python中以下数据会被当作空处理： 常量None； 常量False； 任何形式的数值类型零，如0，0L，0.0，0j； 空的序列，如‘’，()，[]； 空字典，如{}； 当用户定义的类中定义了nonzero()方法和len()方法，并且该方法返回整数0或者布尔值False。 注意：None的特殊性体现在它既不是0，False，也不是空字符串，它就是一个空值对象；其数据类型为NoneType，遵循单例模式，是唯一的，因而不能创建None对象。所有赋值为None的变量都相等，并且None与任何其他非None的对象比较结果都是False。 123456789101112&gt;&gt;&gt; id(None)140735411631784&gt;&gt;&gt; None == 0False&gt;&gt;&gt; None == \"\"False&gt;&gt;&gt; a = None&gt;&gt;&gt; id(a)140735411631784&gt;&gt;&gt; b = None&gt;&gt;&gt; a == b # 所有赋值为`None`的变量都相等True 实例：列表判空 12345ls = []if ls is not None: print \"ls is: \", lsesle: print \"ls is None\" 以上程序运行输出为：ls is: []，显然不是我们的预期结果。应修正为： 12345ls = []if ls: print \"ls is: \", lsesle: print \"ls is None\" 建议27：连接字符串优先使用join而不是+1）使用操作符+连接字符串的方法 123&gt;&gt;&gt; str1, str2, str3 = \"testing \", \"string \", \"concatenation \"&gt;&gt;&gt; str1 + str2 + str3'testing string concatenation ' 2）使用join方法连接字符串的方法 12&gt;&gt;&gt; ''.join([str1, str2, str3])'testing string concatenation ' 性能测试函数 123456789101112131415161718192021222324252627282930# -*-coding:UTF-8 -*-​import timeit​str_list = [\"It is a long value string will not keep in memory \" for n in xrange(10000)]​​def join_test(): return ''.join(str_list)​​def plus_test(): res = '' for i, v in enumerate(str_list): res += v​ return res​​if __name__ == '__main__':​ join_timer = timeit.Timer(\"join_test()\", \"from __main__ import join_test\") print join_timer.timeit(number=10) # 0.00255298614502 print join_timer.timeit(number=100000) # 13.4903669357 plus_timer = timeit.Timer(\"plus_test()\", \"from __main__ import plus_test\") print plus_timer.timeit(number=10) # 0.0193991661072 print plus_timer.timeit(number=100000) # 400.628134012 从以上测试效果看，join()方法的效率要高于+操作符，尤其是字符串规模较大时，两者的效率十分明显。 执行一次+，就会申请一块新的内存空间，并将上一次的操作结果和本次的右操作数复制到新申请的内存空间。时间复杂度为 $O(n^2)$;对于join()，会首先计算需要申请的总的内存空间，然后一次性申请所需内存并将字符序列中的每一个元素复制到内存中去，时间复杂度为$O(n)$。 建议28：格式化字符串时尽量使用.format方式而不是%Python中内置%操作符和.format方式都可以用作格式化字符串。 %转换说明符的基本形式为： 1%[转标记][宽度[.精确度]] 转换类型 常见用法 1）直接格式化字符或者数值 12&gt;&gt;&gt; print \"your score is %06.1f\" % 9.5your score is 0009.5 2）以元组的形式格式化 123456&gt;&gt;&gt; import math&gt;&gt;&gt; item_name = 'circumference'&gt;&gt;&gt; radius = 3&gt;&gt;&gt; print \"The %s of a circle with radius %f is %0.3f\" % \\... (item_name, radius, math.pi*radius*2)The circumference of a circle with radius 3.000000 is 18.850 3）以字典的形式格式化 123&gt;&gt;&gt; item_dict = &#123;'item_name': 'circumference', 'radius': 3, 'value': math.pi*radius*2&#125;&gt;&gt;&gt; print \"The %(item_name)s of a circle with radius %(radius)f is %(value)0.3f\" % (item_dict)The circumference of a circle with radius 3.000000 is 18.850 .format方式格式化字符串的基本语法为： 1.format([[填充符]对齐方式][符号][#][0][宽度][,][.精度][转换类型]) 常见用法 1）使用位置符号123&gt;&gt;&gt; print \"The number &#123;0:,&#125; in hex is: &#123;0:#x&#125;,\" \\... \"The number &#123;1&#125; in oct is: &#123;1:#o&#125;\".format(4746, 45)The number 4,746 in hex is: 0x128a,The number 45 in oct is: 0o55 2）使用名称 123&gt;&gt;&gt; print \"The max number is &#123;max&#125;, the min number is &#123;min&#125;, the average number is &#123;avg&#125;\"\\... .format(max=9, min=3, avg=6)The max number is 9, the min number is 3, the average number is 6 3）通过属性 1234567891011&gt;&gt;&gt; class Customer(object):... def __init__(self, name, sex, phone):... self.name = name... self.sex = sex... self.phone = phone... ... def __str__(self):... return 'Customer (&#123;self.name&#125;, &#123;self.sex&#125;, &#123;self.phone&#125;)'.format(self=self) ... &gt;&gt;&gt; print Customer(\"Lisa\", \"F\", \"13304634561\")Customer (Lisa, F, 13304634561) 4）格式化元组的具体项 123&gt;&gt;&gt; point = (1, 5)&gt;&gt;&gt; print 'X:&#123;0[0]&#125;; Y:&#123;0[1]&#125;'.format(point)X:1; Y:5 为什么要尽量使用format方式而不是%操作符来格式化字符串？ 1）format方式在使用上较%操作符更为灵活；使用format方式时，参数的顺序与格式化的顺序不必完全相同；2）format方式可以方便地作为参数传递；3）%最终会被.format方式替代；4）%方法在某些特殊情况下使用需要特别小心。如下例，特别小心 , 。 1234567&gt;&gt;&gt; items = (\"mouse\", \"mobilephone\", \"cup\")&gt;&gt;&gt; print \"items list are %s\" % (items)Traceback (most recent call last): File \"&lt;input&gt;\", line 1, in &lt;module&gt;TypeError: not all arguments converted during string formatting&gt;&gt;&gt; print \"items list are %s\" % (items,)items list are ('mouse', 'mobilephone', 'cup') 建议29：区别对待可变对象和不可变对象Python中一切皆对象，每一个对象都有一个唯一的标识符(id())，类型(type())以及值。 可变对象：字典，字节数组，列表； 不可变对象：数字，字符串，元组。 实例 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# coding=utf-8class Student(object): \"\"\" 区别可变对象与不可变对象 \"\"\" def __init__(self, name, course=list()): self.name = name self.course = course def add_course(self, course_name): self.course.append(course_name) def print_course(self): for index, item in enumerate(self.course): print item, ' ' print '\\n'if __name__ == '__main__': stu_a = Student(\"Wang Yi\") stu_a.add_course(\"English\") stu_a.add_course(\"Math\") print \"&#123;0&#125;'s course: \".format(stu_a.name) stu_a.print_course() print \"=================================\" stu_b = Student(\"Li san\") stu_b.add_course(\"Chinese\") stu_b.add_course(\"Physics\") print \"&#123;0&#125;'s course: \".format(stu_b.name) stu_b.print_course() 输出结果如下： 12345678910Wang Yi's course: English Math =================================Li san's course: English Math Chinese Physics 修正建议：传入None作为默认参数，在创建对象时动态生成列表。 12345def __init__(self, name, course=None): self.name = name if course is None: course = list() self.course = course 建议30：[]，()，{}：一致的容器初始化形式==&gt;列表解析 列表解析的语法为： 1[expr for iter_item in iterable if cond_expr] 列表解析的使用 1）支持多重嵌套 123&gt;&gt;&gt; nested_list = [['Hello', 'World'], ['Goodbye', 'World']]&gt;&gt;&gt; print [[s.upper() for s in xs] for xs in nested_list][['HELLO', 'WORLD'], ['GOODBYE', 'WORLD']] 2）支持多重迭代 12&gt;&gt;&gt; [(a, b) for a in ['a', '1', 1, 2] for b in ['1', 3, 4, 'b'] if a != b][('a', '1'), ('a', 3), ('a', 4), ('a', 'b'), ('1', 3), ('1', 4), ('1', 'b'), (1, '1'), (1, 3), (1, 4), (1, 'b'), (2, '1'), (2, 3), (2, 4), (2, 'b')] 3）列表解析语法中的表达式可以是简单表达式，也可以是复杂表达式，甚至函数。 1234567891011&gt;&gt;&gt; def f(v):... if v % 2 == 0:... v = v ** 2... else:... v = v + 1... return v... &gt;&gt;&gt; print [f(v) for v in [2, 3, 4, -1] if v &gt; 0][4, 4, 16]&gt;&gt;&gt; print [v ** 2 if v % 2 == 0 else v + 1 for v in [2, 3, 4, -1] if v &gt; 0][4, 4, 16] 4）列表解析语法中的iterable可以是任意可迭代对象。 建议31：记住函数传参既不是传值也不是传引用==&gt;而是传对象（的引用）1）传引用 1234567891011121314&gt;&gt;&gt; def inc(n):... print id(n)... n = n + 1... print id(n)... &gt;&gt;&gt; n = 3&gt;&gt;&gt; id(n)140407485781272&gt;&gt;&gt; &gt;&gt;&gt; inc(n)140407485781272140407485781248&gt;&gt;&gt; print n3 分析：按照传引用的观点，结果输出应为4，并且inc()函数里面执行操作n=n+1的前后n的id值应该是不变的。 2）传值 123456789101112131415&gt;&gt;&gt; def change_list(org_list):... print \"orginator list is: \", org_list... new_list = org_list... new_list.append(\"I am new.\")... print \"new list is: \", new_list ... return new_list... &gt;&gt;&gt; org_list = ['a', 'b', 'c']&gt;&gt;&gt; new_list = change_list(org_list)orginator list is: ['a', 'b', 'c']new list is: ['a', 'b', 'c', 'I am new.']&gt;&gt;&gt; print new_list['a', 'b', 'c', 'I am new.']&gt;&gt;&gt; print org_list['a', 'b', 'c', 'I am new.'] 分析：通过程序输出不难发现，在传值过程中，原来的列表对象随着新对象的变化随之发生变化。 3）可变对象传引用，不可变对象传值 12345678910111213141516171819202122232425262728&gt;&gt;&gt; def change(org_list):... print id(org_list)... new_list = org_list... print id(new_list)... if len(new_list) &gt; 5:... new_list = ['a', 'b', 'c']... for i, e in enumerate(new_list):... if isinstance(e, list):... new_list[i] = \"***\"... print new_list... print id(new_list)... &gt;&gt;&gt; test1 = [1, ['a', 1, 3], [2, 1], 6]&gt;&gt;&gt; change(test1)45124735284512473528[1, '***', '***', 6]4512473528&gt;&gt;&gt; print test1[1, '***', '***', 6]&gt;&gt;&gt; test2 = [1, 2, 3, 4, 5, 6, [1]]&gt;&gt;&gt; change(test2)45114667044511466704['a', 'b', 'c']4512476552&gt;&gt;&gt; print test2[1, 2, 3, 4, 5, 6, [1]] 分析：传入参数org_list为列表，属于可变对象，按照可变对象传引用的理解，new_list和org_list指向同一块内存，因此两者的id值输出一致，即修改new_list会导致org_list的直接修改；但是在test2中调用函数change()前后并没有发生改变。 Python中的赋值机制理解： 123a = 5b = ab = 7 验证上述过程 1234567891011&gt;&gt;&gt; a = 5&gt;&gt;&gt; id(a)140407485781224&gt;&gt;&gt; b = a&gt;&gt;&gt; id(b)140407485781224&gt;&gt;&gt; b = 7&gt;&gt;&gt; id(b)140407485781176&gt;&gt;&gt; id(a)140407485781224 小结：对于Python函数参数传递的正确说法是：传对象或者传对象的引用。函数参数在传递的过程中将整个对象传入，对可变对象对修改在函数外部以及内部都可见，调用者和被调用者之间共享这个对象；而对于不可变对象，由于不能真正被修改，因而修改往往是通过生成一个新对象然后赋值来实现的。 建议32：警惕默认参数潜在的问题 实例 123456789101112131415161718&gt;&gt;&gt; def test(new_item, list_a=list()):... print id(list_a)... list_a.append(new_item)... print id(list_a)... return list_a... &gt;&gt;&gt; test('a', ['b', 2, 4, [1, 2]])45114677124511467712['b', 2, 4, [1, 2], 'a']&gt;&gt;&gt; test(1)45124397604512439760[1]&gt;&gt;&gt; test('a')45124397604512439760[1, 'a'] 分析：在连续调用test(1)和test(‘a’)，结果和预想的完全不一样。 解决方案：在函数调用过程中动态生成，可以在定义时使用None对象作为占位符。 12345678910111213141516&gt;&gt;&gt; def test(new_item, list_a=None):... if list_a is None:... list_a = list()... print id(list_a)... list_a.append(new_item)... print id(list_a)... return list_a... &gt;&gt;&gt; test('a')45117940244511794024['a']&gt;&gt;&gt; test(1)45124401924512440192[1] 建议33：慎用变长参数Python支持可变长度的参数列表，可以通过函数定义时使用*args和**kwargs这两个特殊语法实现。 *args：实现可变参数列表； *args用于接收一个包装为元组形式的参数列表来传递非关键字参数，参数个数任意。 12345678910&gt;&gt;&gt; def summary(*args):... result = 0... for x in args[0:]:... result += x... return result... &gt;&gt;&gt; print summary(2, 4)6&gt;&gt;&gt; print summary(1, 2, 3, 4, 5)15 **kwargs：实现字典形式的关键字参数列表。 12345678&gt;&gt;&gt; def category_table(**kwargs):... for name, value in kwargs.items():... print \"&#123;0&#125; is a kind of &#123;1&#125;.\".format(name, value)... &gt;&gt;&gt; category_table(apple=\"fruit\", carrot=\"vegetable\", python=\"programming language\")python is a kind of programming language.carrot is a kind of vegetable.apple is a kind of fruit. 建议34：深入理解str()和repr()的区别str()和repr()的区别： 1）二者的目标不同：str()面向用户，其目的是可读性，返回字符串类型；repr()面向的Python解释器，或者说开发者，其目的是准确性，返回表示Python解释器内部的含义，常作为debug用途；2）在解释器中直接输入a时默认调用repr()，而print a则调用str()；3）repr()的返回值一般可用eval()函数还原对象，即：obj == eval(repr(obj))；4）二者分别调用__str__()和__repr__()方法，一般而言，在类中都应该定义__repr__()方法（默认方法）。 1234567891011&gt;&gt;&gt; s = \"' '\"&gt;&gt;&gt; str(s)\"' '\"&gt;&gt;&gt; repr(s)'\"\\' \\'\"'&gt;&gt;&gt; eval(repr(s)) == sTrue&gt;&gt;&gt; eval(str(s))' '&gt;&gt;&gt; eval(str(s)) == sFalse 建议35：分清staticmethod和classmethod的适用场景Python中的静态方法(staticmethod)和类方法(classmethod)都依赖于装饰器(decorator)来实现。 静态方法(staticmethod) 1234class C(object): @staticmethod def f(args1, args2, ...): pass 类方法(classmethod) 1234class C(object): @classmethod def f(cls,args1, args2, ...): pass 静态方法所带来的问题 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env python# coding=utf-8class Fruit(object): \"\"\" Fruit类 \"\"\" def __init__(self, area=\"\", category=\"\", batch=\"\"): self.area = area self.category = category self.batch = batch @staticmethod def init_product(product_info): area, category, batch = map(str, product_info.split('-')) fruit = Fruit(area, category, batch) return fruitclass Apple(Fruit): passclass Orange(Fruit): passif __name__ == '__main__': apple = Apple('2', '5', '10') orange = Orange.init_product(\"3-3-9\") print \"apple is instance of Apple: \", isinstance(apple, Apple) print \"orange is instance of Orange: \", isinstance(orange, Orange) 输出结果： 12apple is instance of Apple: Trueorange is instance of Orange: False 分析：静态方法实际上相当于一个定义在类中的函数，init_product()返回的实际是Fruit对象，所以不会是Orange对象。因而静态方法并不能获取期望的结果，类方法才是正确的解决方案。 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env python# coding=utf-8class Fruit(object): \"\"\" Fruit类 \"\"\" def __init__(self, area=\"\", category=\"\", batch=\"\"): self.area = area self.category = category self.batch = batch @classmethod def init_product(cls, product_info): area, category, batch = map(str, product_info.split('-')) fruit = cls(area, category, batch) return fruitclass Apple(Fruit): passclass Orange(Fruit): passif __name__ == '__main__': apple = Apple('2', '5', '10') orange = Orange.init_product(\"3-3-9\") print \"apple is instance of Apple: \", isinstance(apple, Apple) print \"orange is instance of Orange: \", isinstance(orange, Orange)","categories":[{"name":"Python","slug":"python","permalink":"https://zhangbc.github.io/categories/python/"}],"tags":[]},{"title":"【Python编码规范】编程惯用法","slug":"【Python编码规范】编程惯用法","date":"2019-04-27T16:09:25.000Z","updated":"2019-04-27T16:41:29.208Z","comments":true,"path":"2019/04/28/python_code91_02/","link":"","permalink":"https://zhangbc.github.io/2019/04/28/python_code91_02/","excerpt":"","text":"本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。 温馨提醒：在阅读本书之前，强烈建议先仔细阅读：PEP规范，增强代码的可阅读性，配合优雅的pycharm编辑器(开启pep8检查)写出规范代码，是Python入门的第一步。 建议8：利用assert语句来发现问题 断言(assert)基本语法如下： 1assert expression1 [\",\" expression2] assert用法举例： 123456&gt;&gt;&gt; x = 1&gt;&gt;&gt; y = 2&gt;&gt;&gt; assert x == y , \"not equals\"Traceback (most recent call last): File \"&lt;input&gt;\", line 1, in &lt;module&gt;AssertionError: not equals 关于assert的几点说明事项 1）__debug__的值默认为True，且只读，无法修改(Python2.7)。2）断言是有代价的，对性能产生一定影响。禁用断言的方法是在运行脚本的时候加上-O标记(不优化字节码，而是忽略与断言相关的语句)。 使用断言(assert)注意点： 1）不要滥用，这是使用断言最基本的原则；2）如果Python本身的异常能够处理就不要再使用断言；3）不要使用断言来检查用户的输入；4）在函数调用后，当需要确认返回值是否合理时可以使用断言；5）当条件时业务逻辑继续下去的先决条件时，可以使用断言。 建议9：数据交换值时不推荐使用中间交换变量12345&gt;&gt;&gt; from timeit import Timer&gt;&gt;&gt; Timer('temp=x;x=y;y=temp','x=2;y=3').timeit()0.03472399711608887&gt;&gt;&gt; Timer('x,y=y,x','x=2;y=3').timeit()0.031581878662109375 测试用例说明：不借助中间变量的方式耗费的时间更少，代码简洁，值得推荐。 建议10：充分利用Lazy evaluation的特性Lazy evaluation常被译作“延时计算”或“惰性计算”，指的是仅仅在真正需要执行的时候才计算表达式的值。典型例子：生成器表达式。 1）避免不必要的计算，带来性能上的提升；2）节省空间，使用无限循环的数据结构成为可能。 实例： 123456789101112131415161718#!/usr/bin/env python# coding:utf-8from itertools import islicedef fib(): a, b = 0, 1 while True: yield a a, b = b, a+bif __name__ == '__main__': print(list(islice(fib(), 5))) 建议11：理解枚举替代实现的缺陷1）替代方法 使用类属性 12345&gt;&gt;&gt; class Seasons(object):... Spring, Summer, Autumn, Winter = xrange(4)... &gt;&gt;&gt; print(Seasons.Spring)0 借助函数 12345678&gt;&gt;&gt; def enum(*args, **kwargs):... return type(\"Enum\", (object,), dict(zip(args, xrange(len(args))), **kwargs))... &gt;&gt;&gt; Seasons = enum(\"Spring\", \"Summer\", \"Autumn\", Winter=3)&gt;&gt;&gt; Seasons.Summer1&gt;&gt;&gt; Seasons.Winter3 使用collections.namedtuple 123456&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Seasons = namedtuple('Seasons','Spring Summer Autumn Winter')._make(xrange(4)) &gt;&gt;&gt; print SeasonsSeasons(Spring=0, Summer=1, Autumn=2, Winter=3)&gt;&gt;&gt; print Seasons.Autumn2 2）替代缺陷 允许枚举值重复 123456&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Seasons = namedtuple('Seasons','Spring Summer Autumn Winter')._make(xrange(4)) &gt;&gt;&gt; SeasonsSeasons(Spring=0, Summer=1, Autumn=2, Winter=3)&gt;&gt;&gt; Seasons._replace(Spring=2) # 不合理Seasons(Spring=2, Summer=1, Autumn=2, Winter=3) 支持无意义的操作 12&gt;&gt;&gt; Seasons.Summer + Seasons.Autumn == Seasons.Winter # 无意义True 3）Python2.7的替代方案(Python3.4后引入Enum类型)：flufl.enum 12345678910111213141516171819#!/usr/bin/env python# coding:utf-8from flufl.enum import Enum​​class Seasons(Enum):​ Spring = \"Spring\" Summer = 2 Autumn = 3 Winter = 4​​Seasons = Enum('Seasons', 'Spring Summer Autumn Winter')​print Seasonsprint Seasons.Summer.value 建议12：不推荐使用type来进行类型检查1）基于内建类型扩展的用户自定义类型，type函数并不能准确返回结果。 123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/env python# coding:utf-8import typesclass UserInt(int): \"\"\" 用户类UserInt继承int类实现定制化，不支持操作符（+=） \"\"\" def __init__(self, value=0): self._value = int(value) def __add__(self, other): if isinstance(other, UserInt): return UserInt(self._value + other._value) return self._value + other def __iadd__(self, other): raise NotImplementedError(\"not support operation.\") def __str__(self): return str(self._value) def __repr__(self): return \"Integer(&#123;0&#125;)\".format(self._value)if __name__ == '__main__': n = UserInt() print(n) # 输出 0 m = UserInt(2) print(m) # 输出 2 print(n+m) # 输出 2 print(type(n) is types.IntType) # 使用type进行类型判断，输出 False print(isinstance(n, int)) # 输出 True 2）在旧式类中，所有类的实例的type值都相等。 123456789101112&gt;&gt;&gt; class A:... pass... &gt;&gt;&gt; a = A()&gt;&gt;&gt; class B:... pass... &gt;&gt;&gt; b = B()&gt;&gt;&gt; type(a) == type(b)True&gt;&gt;&gt; type(a)&lt;type 'instance'&gt; 3）可以用isinstance()函数检查。 12345678&gt;&gt;&gt; isinstance(2, float)False&gt;&gt;&gt; isinstance(\"a\", (str, unicode))True&gt;&gt;&gt; isinstance((2,3), (str, list))False&gt;&gt;&gt; isinstance((2,3), (str, list, tuple))True 建议13：尽量转换为浮点类型再做除法当涉及除法运算的时候尽量先将操作数转换成浮点类型再做运算。 浮点数不精确性导致的无限循环： 1234&gt;&gt;&gt; i=1&gt;&gt;&gt; while i!=1.5:... i=i+0.1... print i 建议14：警惕eval()的安全漏洞 实例：根据用户的输入，计算Python表达式的值 12345678910111213141516171819# -*-coding:UTF-8 -*-​​import sysfrom math import *​def ExpCalcBot(string): try: print \"Your answer is\", eval(string) except NameError: print \"The expression you enter is not valid.\"​while True: print 'Please enter a number or operation. Enter e to complete. ' inputStr = raw_input() if inputStr == 'e': sys.exit() elif repr(inputStr) != ' ': ExpCalcBot(inputStr) 输入：__import__(&quot;os&quot;).system(&quot;dir&quot;)：显示当前目录下的所有文件； __import__(&quot;os&quot;).system(&quot;del */Q&quot;)：删除当前目录下的所有文件。 因此，在实际应用过程中，如果使用对象不是信任源，应该尽量避免使用eval，在需要使用eval的地方可以用安全性更好的ast.literal_eval替代。 建议15：使用enumerate()获取序列迭代的索引和值对序列进行迭代并获取序列中的元素进行处理的几种方法举例： 方法一 在每次循环中对索引变量进行自增 12345li = ['a', 'b', 'c', 'd', 'e']index = 0for i in li: print(\"index:\", index, \"element:\", i) index += 1 方法二 使用range()和len()方法结合 123li = ['a', 'b', 'c', 'd', 'e']for i in xrange(len(li)): print(\"index:\", i, \"element:\", li[i]) 方法三 使用while循环，用len获取循环次数 12345li = ['a', 'b', 'c', 'd', 'e']i = 0while i &lt; len(li): print(\"index:\", i, \"element:\", li[i]) i += 1 方法四 使用zip()方法 123li = ['a', 'b', 'c', 'd', 'e']for i, e in zip(range(len(li)), li): print(\"index:\", i, \"element:\", e) 方法五(推荐) 使用enumerate()获取序列迭代对索引和值 123li = ['a', 'b', 'c', 'd', 'e']for i, e in enumerate(li): print(\"index:\", i, \"element:\", e) 注意：在获取迭代过程中字典的key和value，应该使用如下iteritems()方法(Python3不再适用)。 123&gt;&gt;&gt; person=&#123;'name': 'Josn', 'age': 19, 'hobby': 'football'&#125;&gt;&gt;&gt; for k,v in person.iteritems():... print k, \":\", v 建议16：分清==与is的适用场景123456789101112&gt;&gt;&gt; a=\"Hi\"&gt;&gt;&gt; b=\"Hi\"&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a==b # is 和 == 结果是一样的True&gt;&gt;&gt; a1 =\"I am using long string for testing\" # 注意区分&gt;&gt;&gt; b1 =\"I am using long string for testing\"&gt;&gt;&gt; a1 is b1False&gt;&gt;&gt; a1==b1 # is 和 == 结果是不一样的True is：即object identity，表示的是对象标识符，检查对象的标识符是否一致，也就是比较两个对象在内存中是否拥有同一块内存空间； ==：即equal，表示的是值相等，用来判断两个对象的值是否相等，可以被重载。 字符串驻留(string interning)机制：对于较小的字符串，为了提高系统性能会保留其值的一个副本，当创建新的字符串时直接指向该副本即可。 注意：判断两个对象相等应该使用 == 而不是 is。 建议17：考虑兼容性，尽可能使用UnicodePython内建的字符串有两种类型：str和Unicode，共同祖先为basestring。 123456789&gt;&gt;&gt; str_uni = u'unicode字符串' # 前面加u表示Unicode&gt;&gt;&gt; str_uniu'unicode\\u5b57\\u7b26\\u4e32'&gt;&gt;&gt; print(str_uni)unicode字符串&gt;&gt;&gt; type(str_uni)&lt;type 'unicode'&gt;&gt;&gt;&gt; type(str_uni).__bases__(&lt;type 'basestring'&gt;,) Unicode：又称万国码，为每种语言设置了唯一的二进制编码表示方式，提供从数字代码到不同语言字符集之间的映射，从而满足跨平台、跨语言之间的文本处理要求。 Unicode编码系统分为编码方式和实现方式 在编码方式上，分为UCS-2和UCS-4，UCS-2用两个字节编码；UCS-4用四个字节编码。 实现方式又称为Unicode转换方式，简称UTF，包括UTF-7、UTF-8、UTF-16、UTF-32等。 UTF-8 较为常见，其特点是对不同范围的字符使用不同长度的编码，其中0x00～0x7F的字符UTF-8编码与ASCII编码完全相同；其最大长度是4个字节。 Windows本地默认编码是CP936。 解码：str.decode([编码参数[，错误处理]]) 编码：str.encode([编码参数[，错误处理]])错误处理参数有3种方式： （1）strict：默认值，抛出UnicodeError异常；（2）ignore：忽略不可转换的字符；（3）replace：将不可转换字符用?代替。 常见的编码参数 对于A、B两种编码系统之间的相互转换示意图如下： 有些软件在保存UTF-8编码时，会在文件最开始地方插入不可见的BOM(0xEF，0xBB，0xBF， 即BOM)，可以利用codecs模块解决。 123456import codecs​content = open('manage.py', 'r').read()if content[:3] == codecs.BOM_UTF8: content = content[:3]print content.decode(\"utf-8\") 编码声明的三种方式： 123456789# coding=&lt;encoding name&gt; #方式一​​#!/usr/bin/env python# -*- coding:&lt;encoding name&gt; -*- #方式二​​#!/usr/bin/env python# vim:set fileencoding=&lt;encoding name&gt; #方式三 建议18：构建合理的包层次来管理module本质上，每一个Python文件都是一个模块，使用模块可以增强代码的可维护性和可重用性。 包 即目录，包含一个__init__.py文件，允许嵌套。包中的模块通过“.”访问符进行访问，即“包名.模块名”。 直接导入一个包 1import package 导入子模块或者子包，包嵌套的情况下可以进行嵌套导入 1234567from package import moduleimport package.modulefrom package import subpackageimport package.subpackagefrom package.subpackage import moduleimport package.subpackage.module 包中__init__.py文件的作用 1）使包和普通目录区分；2）在该文件中声明模块级别的import语句，从而使其变成包级别可见；3）通过该文件中定义__all__变量，控制需要导入的子包或者模块。 使用包的好处 1）合理组织代码，便于维护和使用；2）能够有效地避免名称空间冲突。","categories":[{"name":"Python","slug":"python","permalink":"https://zhangbc.github.io/categories/python/"}],"tags":[{"name":"Python编码规范","slug":"python-coding-convention","permalink":"https://zhangbc.github.io/tags/python-coding-convention/"}]},{"title":"【Python编码规范】Python编码入门","slug":"【Python编码规范】Python编码入门","date":"2019-04-25T14:56:39.000Z","updated":"2019-04-26T16:09:59.802Z","comments":true,"path":"2019/04/25/python_code91_01/","link":"","permalink":"https://zhangbc.github.io/2019/04/25/python_code91_01/","excerpt":"","text":"本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。 温馨提醒：在阅读本书之前，强烈建议先仔细阅读：PEP规范，增强代码的可阅读性，配合优雅的pycharm编辑器(开启pep8检查)写出规范代码，是Python入门的第一步。 本书主要内容 1）容易被忽视的重要概念和常识，如代码的布局和编写函数的原则等；2）编写Python程序管用的方法，如利用assert语句去发现问题，使用enumerate()获取序列迭代的索引和值等；3）语法中的关键条款，如有节制地使用from…import语句，异常处理的几点基本原则等；4）常见库的使用，如按需选择sort()或者sorted()，使用Queue使多线程更安全等；5）Python设计模式的使用，如用发布订阅模式实现松耦合，用状态模式美化代码等；6）Python内部机制，如名字查找机制，描述符机制等；7）开发工具的使用，如pip等各种开发工具的使用，各种代码测试用具的使用等；8）Python代码的性能分析，优化的原则，工具，技巧，以及常见性能问题的解决等。 建议1：理解Pythonic概念1）Pythonic的定义：充分体现Python自身特色的代码风格。 The Zen of Python(Python之禅) 1234567891011121314151617181920212223&gt;&gt;&gt; import thisThe Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren't special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one-- and preferably only one --obvious way to do it.Although that way may not be obvious at first unless you're Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it's a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea -- let's do more of those!&gt;&gt;&gt; 快速排序 1234567891011121314151617181920def sort_quick(array): \"\"\" 快速排序 :param array: :return: \"\"\" less = list() greater = list() if len(array) &lt;= 1: return array pivot = array.pop() for index, item in enumerate(array): if item &lt;= pivot: less.append(item) else: greater.append(item) return sort_quick(less) + [pivot] + sort_quick(greater) 2）代码风格 交换两个变量的值，packaging/unpackaging机制 1234x = 2y = 3x, y = y, xprint x, y 容器遍历 12for index, item in enumerate(items): do_sth_with(item) 列表逆序 1234list_a = [1, 2, 3, 4, 5]str_c = 'abcdef'print(list(reversed(list_a)))print(list(reversed(str_c))) 标准库 12# 字符串格式化print 'Hello %(name)s!' % &#123;'name': 'Tom'&#125; 注解：%是非常影响可读性的，因为数量多了之后，很难清除哪一个占位符对应哪一个实参。 str.format()：Python最为推荐达到字符串格式化方法。 12# 字符串格式化, 替代%print 'Hello &#123;name&#125;!'.format(name='Tom') 3）Python的包和模块结构 (1) 包和模块的命名采用小写，单数形式且短小；(2)包通常作为命名空间，如只包含空的__init__.py文件。 建议2：编写pythonic代码1）要避免劣化代码 (1)避免只用大小写来区分不同的对象；(2)避免使用容易引起混淆的名称；(3)不要害怕过长的变量名。 实例1（函数名称，变量名意义均不明） 12345678910111213def funA(list_items, num): \"\"\" :param list_items: :param num: :return: \"\"\" for element in list_items: if num == element: return True return False 实例2（推荐） 12345678910111213def find_num(list_search, num): \"\"\" :param list_search: :param num: :return: \"\"\" for index, value in enumerate(list_search): if num == value: return True return False 2）pep8检测工具 1234C:\\&gt;pip install -U pep8C:\\Users\\Administrator\\Desktop\\zxt&gt;pep8 --first database.pydatabase.py:83:1: E302 expected 2 blank lines, found 1&gt;pep8 --show-source --show-pep8 waijiao.py 3）深入认识Python有助于编写Pythonic代码 掌握Python提供的所有特性，包括语言特性和库特性； 跟进学习Python的最新版本提供的新特性，掌握其变化趋势； 深入学习公认比较Pythonic的代码，例如Flask、gevent、requests等。 建议3：理解python与C语言的不同之处1）“缩进” 与 “{}“Python中使用严格的代码缩进方式分隔代码块，应养成良好的习惯，统一缩进风格，不要混用Tab键和空格。 2）&#39; 与 &quot;在C语言中，二者有严格的区分，但是在Python中，区别较小。 1234567Python 2.7.10 (default, Jul 15 2017, 17:16:57) &gt;&gt;&gt; str1 = \"He said, \\\"Hello!\\\"\"&gt;&gt;&gt; str2 = 'He said, \"Hello!\"'&gt;&gt;&gt; str1'He said, \"Hello!\"'&gt;&gt;&gt; str2'He said, \"Hello!\"' 3）三元操作符 ?: 1234x = 0y = -2print(x if x &lt; y else y)-2 4）switch...case 123456789n = raw_input(\"please input a number:\")if n == \"0\": print \"You typed zero.\"elif n == \"1\": print \"You are in top.\"elif n == \"2\": print \"N is an even number.\"else: print \"Error!\" 用跳转也可以实现： 123456def func(): return &#123; \"0\": \"You typed zero.\", \"1\": \"You are in top.\", \"2\": \"N is an even number.\" &#125;.get(n, \"Error!\") 建议4：在代码中适当添加注释Python有3种形式的代码注释：块注释，行注释，文档注释(docstring)。 (1）使用块或者行注释的时候仅注释复杂的操作，算法，难以理解的技巧或者不够一目了然的代码；(2）注释和代码隔开一定的距离；(3）给外部可访问的函数和方法添加文档注释(docstring)（&quot;&quot;&quot; &quot;&quot;&quot;）；(4）推荐文件头部包含copyright申明，模块描述等。 12345678910111213141516171819202122232425262728\"\"\"Requests HTTP library~~~~~~~~~~~~~~~~~~~~~Requests is an HTTP library, written in Python, for human beings. Basic GETusage: &gt;&gt;&gt; import requests &gt;&gt;&gt; r = requests.get('https://www.python.org') &gt;&gt;&gt; r.status_code 200 &gt;&gt;&gt; 'Python is a programming language' in r.content True... or POST: &gt;&gt;&gt; payload = dict(key1='value1', key2='value2') &gt;&gt;&gt; r = requests.post('http://httpbin.org/post', data=payload) &gt;&gt;&gt; print(r.text) &#123; ... \"form\": &#123; \"key2\": \"value2\", \"key1\": \"value1\" &#125;, ... &#125;The other HTTP methods are supported - see `requests.api`. Full documentationis at &lt;http://python-requests.org&gt;.:copyright: (c) 2015 by Kenneth Reitz.:license: Apache 2.0, see LICENSE for more details.\"\"\" 建议5：通过适当添加空行使代码布局更为优雅，合理Python代码布局应当遵循以下基本规则：1）在一组代码表达完一个完整的思路之后，应该用空白行进行间隔； 反例（多余空行） 12345if guess == number: print(\"Good job!\") else: print(\"Nope\") 2）尽量保持上下文语义的易理解性(如调用函数写在被调用函数之上)； 123456def A(): B()def B(): pass 3）避免过长的代码行，每行最好不要超过80个字符，超过的部分可以用圆括号、方括号、花括号等进行连接，并保存行连接的元素垂直对齐； 12x = ('This is a verey long string.' 'It is used for testing line limited characters') 4）不要为了保持水平对齐而使用多余的空格，同时也不要在一行有多个命令； 反例（多余的空格） 1234x = 5Year = 2013name = \"Jam\"d2 = &#123;'spam': 2, 'eggs': 3&#125; 反例（一行中多个命令） 1X = 1; Y = 2; 5）空格的使用要能在需要强调的时候警示读者：（1）二元运算符、比较、布尔运算的左右两边应该有空格； 1x == 1 （2）逗号和分号前不要使用空格； 推荐 1234if x == 4: print(x, y)x, y = y, x 反例（不推荐） 1234if x == 4 : print(x , y)x , y = y , x （3）函数名和左右括号之间，序列索引操作时序列名和[ ]之间不要空格，函数默认参数两侧不需要空格； 1234def sort_quick(array, if_print=0): ...arrays = [9, 8, 4, 5, 32, 64, 2, 1, 0, 10, 19, 27] （4）强调前面的操作符的时候使用空格。 12-2 - 5b*b + a*a 建议6：编写函数的4个原则函数 能够带来最大化的代码重用和最小化的代码冗余，不仅可以提高程序的健壮性，还可以增强可读性，减少维护成本。 1）函数设计尽量短小，嵌套层次不宜过深(最好控制在3层以内)； 2）函数声明应该做到合理，简单，易于使用； 3）函数参数设计应该考虑向下兼容； 4）一个函数只做一件事，尽量保证函数语句粒度的一致性。 建议7：将常量集中到一个文件Python使用常量： 通过命名风格来提醒使用者该变量代表的意义为常量，如常量名所有字母大写，用下画线连接各个单词；通过自定义的类实现常量功能。 示例：const.py 123456789101112131415161718192021222324#!/usr/bin/env python# coding:utf-8import sysclass _const(object): class ConstError(TypeError): pass class ConstCaseError(ConstError): pass def __setattr__(self, name, value): if self.__dict__.has_key(name): raise self.ConstError, \"Can't change const.&#123;name&#125;\".format(name=name) if not name.isupper(): raise self.ConstCaseError, 'const name \"&#123;name&#125;\" is not all uppercase'.format(name=name) self.__dict__[name] = valuesys.modules[__name__] = _const() 调用实例 1234567import constconst.COMPANY = \"IBM\"print(const.COMPANY)const.COMPANY = \"IBM2\" 上述调用会报错，因为代码中的常量一旦生成便不可更改 123456Traceback (most recent call last): File \"/home/projects/pythoner/quality_code/algorithm_sort.py\", line 40, in &lt;module&gt; const.COMPANY = \"IBM2\" File \"/home/projects/pythoner/quality_code/const.py\", line 18, in __setattr__ raise self.ConstError, \"Can't change const.&#123;name&#125;\".format(name=name)const.ConstError: Can't change const.COMPANY","categories":[{"name":"Python","slug":"python","permalink":"https://zhangbc.github.io/categories/python/"}],"tags":[{"name":"Python编码规范","slug":"python-coding-convention","permalink":"https://zhangbc.github.io/tags/python-coding-convention/"}]},{"title":"【心路历程】做好自己，面对现实（七）","slug":"【心路历程】做好自己，面对现实（七）","date":"2019-04-22T14:53:30.000Z","updated":"2019-04-22T15:00:36.255Z","comments":true,"path":"2019/04/22/myself_08/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_08/","excerpt":"","text":"弹指一瞬，已是四年没有更新这个系列的东西，想说的太多却欲言又止，生活的磨练快让我认不得从前的自己了，今日此时此刻翻阅了一下以前的关于自己的想法与历程，觉得有必要继续这个系列的话题了，以记录自己最真实的生活状态，人生旅程漫漫，不能没有梦，不能忘了原有的动力，即使穷生活也得还原生活的本真，揭开虚伪的面纱，还自己一个最真实的人生。 大学毕业快五年，我一直没有忘记思考自己想要什么样的生活？其实我还是没有想太明白，从不怕被人抛弃，也不怕人笑话，其实我对金钱一直是比较抵触的，但当听说很多朋友已买房的时候，我开始慢慢怀疑自己是不是有点另类了，多少人为了一套房子煞费苦心，得到了之后却并没觉得开心？虽然他们负债累累，谨小慎微的过着余生可数的日子，但是他们实现了人生一个小小的目标，而且时间不算太长。再回头审视自己，从学校出来，拿着微薄的薪水，带着一颗自卑的心，一直就这么撑着，因为身体的原因，还有倔强的性格，一丝丝不甘心的愿望，顿时觉得自己被时代抛弃了美好的青春年华。殊不知是自己缺少一颗闯进的心，缺少一个真正的规划的人生目标，2014年错失了一次从头再来的机会，那些所谓贷款的还款计划，今日想起来，着实应证了那句话，贫困限制了你的视野。穷怕了，连一个利息钱都不想多付一分，如果时时刻刻算着自己的经济账，那如何超脱自己，达到不受金钱的束缚？ 毕业就知道，三年一个小坎，五年一个大坎，跳过了就成功了，没有跳过注定平庸，至少一段相当长的时期会过着不是自己想要的生活。其实毕业这五年，我算是平庸得不能在平庸了，但是至始至终有一颗不甘平庸的心。2013年得力于同窗的举荐进了一家不大不小的医药物流企业做ERP技术支持，从零开始，从维护一个简易的电商平台再到一个分公司ERP业务系统再到整个公司ERP系统，不足一年就已完全掌控，对ERP流程优化，业务系统改造，与部门有效沟通均出了应有之力。尽管如此，对当时的生活状态还是不满意，工资低不说，主要感觉还是无法满足自己的学习欲望，于是乎，各种倒腾自己的业余生活尝试新技术的学习，还是想做一名合格的码农。2014年4月同事离职，两个人的工作推到了我一个人身上，经一个月的调整，也算平稳过渡，就这么撑到了2015年4月离开。其实对我来说得到了极大了锻炼价值，从中慢慢体会了许多东西，什么事情分轻重缓急，哪些任务需要主次分明，哪些工作项得有条不紊，只有亲身体验了方可知其中的奥妙。在这家公司，让我学会了做事，技术没有收获多少，做事的思维大有提升。2015年4月离职，去了上海，原本是拜师傅去的，仅仅半个月由于自己基础实在是太薄弱，最后夭折，开始寻下一份工作。这辈子一定要尝试自己想做的工作，想过的日子，哪怕没钱，哪怕遭无数人鄙视，一定要去做，必须去满足自己小小的虚荣心，哪怕只有一天，无论付出多大的代价。这是我拜师最大的收获，认识了一位开发大神（也是我同乡），后来离开也拒绝了他给我的半个月薪水，不为别的，就为一个承若：干不好，分文不取，拍屁股走人。他给了我很多建议，受益匪浅，时至今日，我们依然保持着联系，我依然能感受他对我无微不至的关怀。花了一个月的时间，拿到两个比较中意的offer，因不想重拾ERP技术，拒绝了业界许多好心人的推荐，当时的想法就是宁可无工作也不去走老路，一定要坚持自己的信念。经过两家工作性质对比思考，最终选择了薪资较低而且极为陌生的领域，开始决定摸爬滚打。技术用上了我业余的学习所得，当然也没有排除我之前掌握的核心技术栈。说是开发工程师，其实进来才发现也不是那么回事，各种杂活脏活还是由我一个人做，创业公司嘛，坑不少，想想能锻炼人，能感受一波创业的辛酸史，便留下了断了其他念头，老板在吃喝玩乐方面也比较阔气。在这家公司最大的感受就是，创业不易，且行且珍惜，要有危机意识，时时刻刻保持一颗清醒的头脑，一句话谨记，逆水行舟不进则退。其实生活工作亦如此。后来，感觉老板心态变了，不断画饼(平生最恶此举)，我就离开了。期间认识了不少人，见识了不少场合，逢场作戏太多，越来越觉得当初毕业拒绝考虑医院单位是明智之举。2015年5月至2017年4月，就这样结束了我的第二份工作，即将有幸迎来我的第三份工作—我在一家互联网金融的创业公司担任爬虫工程师，算是一个真正入门的coder，又一个全新的挑战。从上海转战武汉，回武汉也是我的一个想法—离家进了一小步，故人在，朋友常聚，情谊永存，工作之外，还需要倒腾一下生活，我是一个极度恋旧情的人，很难和新朋友结交为知己。很赞赏那句“人生得一知己足矣”，但是心里还是不乐意这么去想，我始终坚持人之初性本善的理念，只要敞开心扉，就能拥抱世界，拥抱未来。 面对一个真实的自己，细细回味着自己走过的路，顿时发现自己还是那么的倔强，尝试混迹于各种领域，生活却多了几分艰辛，坚决不留后路，至于生活到底过得如何，我还是靠着勇气战胜了自己，若能克服自卑的心理，或许我的各种尝试才会体现其应有的价值。浪费青春也罢，折腾生活也好，我无怨无悔，我会一如既往地带着人生最初的问题走下去。牛人，就是走到哪里都有可用武之地，都有金钱可挥霍，都有很多有意义的事情等着他去完成。我不是牛人，但我相信这样的人生理念，从一个小白做起，不怕丢失工作，唯恐没有勇气胜任工作。 by zhangbc2018-04-24","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【心路历程】做好自己，面对现实（六）","slug":"【心路历程】做好自己，面对现实（六）","date":"2019-04-22T14:39:55.000Z","updated":"2019-04-22T14:51:53.849Z","comments":true,"path":"2019/04/22/myself_07/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_07/","excerpt":"","text":"晃晃悠悠又是一年时，该到了总结与反思的时候了。想想当初的豪情壮志，如今却被时间磨平了，淡化了，可笑？可恨？可惜？可怜？这一年，不知道用什么言语表达，用什么词汇来描绘，单单是工作那么简单，抑或生活的平淡无奇，或许吧，生性贪婪而又好懒的我，真的不知道怎么总结这一年的得失？该是静下来好好思索思索的时候了，人无远虑，必有近忧。我始终认为人之所以与一般动物有别，就在于思维，靠着自己的思考实现自己应有的价值，思维的宽度决定了人的高度，思维的深度决定了人的价值。活着，本身就体现了一种非凡的价值。基于这个信念，我觉得有必要坦诚地面对现实，做好自己，别人的永远不会是自己的，但是世界是属于每个人的，不要忘却拼搏，终有一天会得到应有的回报。 这一年，从生活上得到的，工作中学到的，自己充电了解到的，都或多或少懂得了一些东西，少了一些面对未来的恐惧，多了一些生活的切身感悟，不再感怀生活，隐隐约约感到了一种无形的压力，迫使我继续前进，管他前面什么玩意儿，只要知道自己在干什么就足够了。规划赶不上变化，这不叫规划，凡是有成就的人，无不例外的实现了自己的既定目标，按着规划一步一步地去实践了，最后得以收获。所以，像我这样的小罗罗，注定成就不了所谓的事业，至少目前的这种态度，因为现实生活告诉我，成功容不得半点借口，今天计划的事情拖到明天，请问时间会倒流吗？对照一下13年的计划（参见13年终总结《2013与2014之流水》），顿时感慨良多。 1，补2013的读书空白。 这个做了一些，但还是严重不足，零零散散涉足了七八本（PDF，包括在读的），接触了Python，MySQL，学习不够深入。 2，每天坚持听一下VOA，抽点时间学习一下英语；(这一点做得非常差，逢考必败的根源所在) 这个做的稀烂，死活坚持不下来，不是没有用，订阅的SQLServerCentral没有研读过，稍微看过几篇，东西确实不错，老外就是老外，干货多多。IT不学真不行，永远了解不了最新的东西，最实在的东西。 3， 计划一次旅行。 玩嘛，我想这个毫无疑问实现了，时间虽短，但也算是达到了预期的效果，开开眼界，找找年轻疯狂的感觉，我想应该知足了，有游记见证，但是还欠一篇拙作，关于看电影引发的个人思考。 4，深入SQL学习，做一个SQL学习系列，这也是14年部门分下的培训任务。 这个系列没有做，多多少少学了一些，缺乏系统性，至于什么原因，我不想辩解，多说无益。部门人员异动，所谓的培训任务也就不了了之。 5，独立开发一到两个有价值的系统。 初生牛犊不怕邪，尽管雄心勃勃的做了个小系统（博客为证），由于种种原因，还是没有最终使用。这也没干好，那也没有做成，我究竟干了些什么呢？ 1，同事四月辞职，当时对业务系统还是没有多少头绪，交接的时候很多事情还是模棱两可，两个人的事情突然让我一个新手来做，顿时感到惶惶不可终日，不能怠慢了各路”神仙“，否则吃不了兜着走。 经过一个月的挣扎，算是基本搞定，平稳过渡，工作从开始的畏手畏脚到现在有的放矢。这或许就是所谓的成长吧，要想彻底的深入掌握ERP，我想还远远不够格，技术需要的不多，其实真正高大上的东西未必能普及到企业中去，ERP关键在于对业务的梳理，流程的理解，随业务的变化而变化，认识—&gt;理解—&gt;掌握—&gt;优化，看起来极其容易，做起来却百般不易。在维护的过程中，我逐渐明白一个道理，呆呆地坐在电脑前是开发不了用户满意的软件的，只有现场勘查，调研，充分理解用户实在的需求，站在用户的角度去探究解决问题的模型，我想即使做不到优秀，至少可以让用户用得满意，用得舒心。切忌，空对空的进行需求交流，分析，这是达不到效果的，最终的结果只会陷入bug—&gt;modify—&gt;bug—&gt;modify的泥潭。 2，生活，其实这没什么好说的，平淡无奇，掀不起半点涟漪。中秋，看了下同学，顺道和亲人过个团圆节，国庆，游了下西湖，走访了一位对我非常有影响力的叔叔，可以这么说，从小学到初中，我从他身上学到了不少东西，跟我讲作业，陪我玩象棋，与我谈未来，每逢寒暑假，只要他在家，我几乎是没有离开过他，对我的生活起着潜移默化的作用。 有这样一位指引者，我是多么的幸运啊！成年了，我们再也不是小孩子了，正因为如此，我们开始忙碌起来，忘记与亲人联系，忘记与朋友聊天，老婆老公，房子车子等一系列话题开始渐入我们的生活，心随之浮躁起来，开始了一些功利化的行为，从此联系似乎带了一种色彩，不再单纯了。疏于亲人，远于儿时最好的玩伴，只要不要同一个城市，作为朋友遗忘的概率极大，新建一个圈子，开始一种新的生活。或许我是一个比较喜欢恋旧的人的缘故吧，总感觉这样不太好，但是又不知如何是好？歌词唱得好：结交了新朋友，别忘了老朋友。感情，是个神奇的东西，它能牵动一个人的每根神经，或远或近，总有一种思念，偶尔总会情不自禁地拿起手中的电话拨拨最熟悉的号码，如果有一天不再联系，我无法想象我的生活有多么凄凉，不会聊谈了，嘴笨了，也许号码就不知不觉地没了，生活与感情，随缘吧。整个年度中，基本保证每周跟外公一次通话，跟父母一次通话，其他亲朋好友随机打，每个月电话费不低于100元，有些人常年未通电话，并非代表我们之间缘分已尽。 3，关于学习，这点确实做得很糟糕，博客数数几篇，寥寥几语，没有含金量，怎么能这样么？Python学习了基本语法，MySQL写点增删改查，多多接触，多多益善，这是个大忌。贪多必失， 用不到，学得多，忘得快，这个经典教训，一定要要牢记。多做笔记，做做分享，向大牛看齐，选一个切入点深入进去，真正学会融会贯通，做好知识储备，经历了一年的摸索，该是清醒的时候了。 4，关于经济目标，14年的规划是一年搞定助学贷款，今年实际完成是50%，弥补了去年的负债，再者向家里还了一笔债务，回家过年备点，基本上一年微薄的薪水就OK了， 存钱是没戏了，毕业三年内，我没有希望能存多少，但愿没有虚度光阴，实际还是虚度了一年。 2014年就这么过去了，总之是离目标有很大差距，主要原因是自己自制力不够，计划过于完美，难以实现。展望2015，希望有一个不一样的收获，所以，规划还是要有的。基于自己贪婪的性格，有必要重新审视一下，计划规划不能是一纸空文。走过的路，见过的人，做过的事，尽量留留痕迹，免得回头追问时间又去哪儿了？延续大四后期的坏习惯，看了大量的战争片，2015年再也不能这样了，否则真的玩完了。 谨记于此，以警示自己，误入歧途不能自拔，2015年应该没有什么大风浪，没有什么大动作，平平静静，充实365天，多多努力才是王道。 by zhangbc2014-12-30","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【心路历程】做好自己，面对现实（五）","slug":"【心路历程】做好自己，面对现实（五）","date":"2019-04-22T14:28:05.000Z","updated":"2019-04-22T14:35:43.822Z","comments":true,"path":"2019/04/22/myself_06/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_06/","excerpt":"","text":"上班已有几天了，可是思绪似乎还是停留在十一之行，也许是没有长大的表现，也许是该为这次的所见所闻记下点什么，一种发自内心的声音不停告诉自己，利用有限的时间，拿起一支拙劣的笔，为那个时刻，为已流失的光阴做一些补救脚印吧。此次杭州之行，收获还是蛮大的，且不说杭州风景，杭州美女，我只想说说杭州给我烙下的一个又一个印象。美丽杭州，说得一点也不过，或许在杭州，让我真真切切地感受到了中国梦的一步步逼近。当然，遇到的某些奇葩，我也就不提了，权且当他们小市民无知吧。 十一，我们花了一整天的时间，沿着西湖的河堤走了很长很长的一段路，西湖如西子，杭州佳人多，风景确实很美，感受在心灵，风景的描述及其感受，恕难从笔，我不擅长优美的散文故事。此篇所探讨的是国庆遇到的一系列的故事。早上，八点出发，坐7路公交，人多拥挤，这是每个城市的最大亮点。提到公交，我不得不感叹下武汉的公交，车猛，两车之间几乎可以保持零距离，人更猛，即使夹在门缝也不愿意下车，真正是做到了“敢为人先”的大无畏精神。相对于武汉，杭州公交略显宽松一点，我们在来回西湖的路上还能找个座位坐坐，恐怕这个待遇在武汉难得，至少我的印象很少，尤其在火车站赶坐公交，或者校门口坐公交，基本都是一路站到底。这或许就是城市生活，几许烦恼几多喜吧。这次出行，让我彻底感到无语的公交是在湖州长兴县城，一下子让我回到了高中时代的县城公交，破就不多说了，可恨的是25分钟一趟，沿途不报站，招手即停，更让你意想不到的是终点站还是高铁站。高铁都有了，还没钱改善一下公交么？从高铁站下来，遇上这样的公交，让我产生无限联想，县城企业无数，高大上的政府大楼（据说30亿建成），我还能说些什么呢？遇到这样的县城，只能是为该城百姓哀其不幸了。想想我们那个穷乡僻壤的小县城，这点还算是好的，年年在改善中，变化中……总体来说，杭州的公交还算不是太拥挤，或许我们赶上最疯狂的那一路吧，暂且留个好印象。 十一晚上，由于一切临时决定，算是做到了随心所欲，疯到了极点，到处招手打的，各路的士哥都把我们给拒绝了，原因是我们离我们所去之处比较近，不愿意送我们，还好心跟我们顺便指了一下方向。我们靠着导航瞎摸也不是个事儿，最后还是让两辆超破的摩的小坑了一把，以每人10元送我们去电影院，也在这里遇到了奇葩，姑且让我作个怪吧，建议出行尽量少做摩的，每个地方的摩的都不是省油的灯。十一点打的，由于是第一次，怕踩大坑，问了是打表还是一口价，说是打表，那没得说了，毕竟我们三个人都是第一次来这里，随便吧，反正我是抱着被坑的心态上车的，再说时候也不早了，离订房处还有一段相当的路程。为什么我是这种心态？说实在的，我对这些司机的故事早有耳闻，打表绕着走，一口价近道飞奔。四年前，在深圳，就这么深深地被坑了一次。上车和司机寒暄了几句，感觉司机人还算靠谱，下车20元，不算坑，毕竟快凌晨了，在某些地方，我想司机肯定会狠狠宰我们一次，毕竟一看我们就是外来主儿。相比摩的，我心里舒坦多了。接下来的三次的士都是这个价，路程不算太近，想到回家过年问起的士价格，真的感觉自己就是一个外乡人，被宰的主儿。从与司机的寒暄中，感受到了杭州人的生存压力，浙江富有，温州老板遍布神州大地，“5~6万的房价楼盘一天被抢空”，这话不管是不是含水分，含多少水分，但是能让我真正感受到杭州的生活成本不会太低，建筑极具特色。城市公共设施建设比较好，电影院随处可见，第一天费劲地找了两三家，第二天顺眼目睹了两三家。有住的就有吃的，有吃的就有乐的，有乐的就有游的，这或许就是杭州给我的一个印象吧。还有一点收获，是在钱塘江边上无意的收获，那就是漂流书屋，公共书屋，城市中能看到长廊边有这样的书亭设施，恐怕不多见，可以默默地感受一下杭州的城市文化建设。很多时候，谈及文化建设都感觉是知识分子的事情，老百姓能温饱就不错了。其实生活离不开文化，与人交流，感受生活，文化无处不在，与大师交流，心里豁然开明，与同龄人畅谈生，人生向往无限，与父辈交心，真真实实感受生活的酸甜苦辣，作为新一代的穷屌丝，文化不应该成为高大上的东西，我觉得有必要丰富一下自己的文化美餐，应该让文化实实在在地融入我们的日常生活，生活有向往，有期待，才有源源不断的奋斗的动力。但是杭州书亭美中不足的是，有书屋，书很陈旧，很少有人去翻阅，我随手翻看了一本书叫《麻雀》，屠格列夫著，这是当年小学四年级的一篇课文，讲述了一篇关于母爱的故事，印象非常深。记得当时读不懂，老师从教案中跟我们朗读课文中省去的那一部分时，我们更是一股脑的傻了。母爱，是一个伟大的话题，我很庆幸，我有一位勤恳和善仁慈，任劳任怨的母亲，只是感觉亏欠她太多太多……再次，翻开这篇课文，想到昨日的电影，心情顿时沉重了不少。 本次国庆之行，算是毕业后比较满意的一次旅行，杭州，长兴，两个不同的城市，两种不同的心境，收获却不是二的。浮浮躁躁的心，不知道又丢失了多少瞬间的灵感，不巧赶上工作的繁忙季，又不想丢失这些美好的记忆，所以还是记下来吧。写着写着，不曾想成了一篇回忆之作。 by zhangbc2014-10-09","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【心路历程】做好自己，面对现实（四）","slug":"【心路历程】做好自己，面对现实（四）","date":"2019-04-22T14:06:46.000Z","updated":"2019-04-22T14:36:41.331Z","comments":true,"path":"2019/04/22/myself_05/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_05/","excerpt":"","text":"水光潋滟晴方好，山色空蒙雨亦奇。欲把西湖比西子，浓妆淡抹总相宜。 ———-苏轼.《饮湖上初晴后雨》 一次疯狂的旅行，一次没有规划的旅行，一群不靠谱的靠谱的旅行， 除此之外，我实在是找不到什么适合的言语来表达这次国庆之行了。岁月总是在不经意的手指尖划过，用心去捕捉生活一瞬间的美，认认真真地感受其中的乐趣，我想这才是我所期盼的生活吧。写作的灵感也就来源于那一刹那之间，仔细回想学生时代，每当写作文的时候，真的就是这么一回事，其实生活亦如此，此次的国庆之行更是如此。 关于国庆之行的最初构想，是由大学班长何伟同志提出的，我只是从旁提提意见，准备十一回武汉聚聚，然后找个景点游哉悠哉。毕业一年了，大家是该聚聚的时候了，说长不长的一年，相信大家经历了很多，也收获了很多，相互叨叨，未必没人不期待吧。为此还特地，班长建了一个群，叫“十一去哪儿”，刚开始那会，激情四起，我心中甚喜，提前了一个月跟部门老大说十一想请假（要知道节前假后请个假有多么困难）。临近九月中旬，再次确定人数的时候，发现就不是原来那么回事了，各有各的忙，离别时难聚更难，相聚是多么不容易啊！没有闲人，既然如此，不必强求，也无须介怀，匆匆之间的生活，理解万岁。说实在话，我的内心依然不甘，计划的十一出行难道真的就要被不能班聚而葬送吗？思来想去，还是不甘心，十一不出行又能干嘛呢？他们都忙， 我闲人一个，那就一个人漂泊了，避远就近，人在苏州，苏杭苏杭，那就杭州了。所以一个人就默默地买好票了。原本不想打扰别人已有的安排，这实在不是我的风格，不强求于人，一切随心所愿的好。可我还是没有忍住，问了下何伟，令我意想不到的是他已经买好回家票了，三言两语，他也改签了去往杭州的高铁票，谁叫我们是好哥们呢？算是坑，也愿跳。9月13日，杭州的票就这么定了，最坏的打算是两个爷们独闯杭州，不为别的，只为西湖。期间，我们也各自咨询了很多亲们，问有没有意向去？回复都是模棱两可的，一句话，不靠谱。不靠谱就不靠谱吧，希望与现实总是存在差距的，那就算一次不靠谱的旅行吧。但是我依然没有放弃寻求伙伴的机会，直到9月27日，一条好友的说说振奋了我，平时少聊天，这次竟然答应得如此爽快，让我惊喜让我意外，此次旅行总算不是纯爷们的队伍了，振奋人心啊！杭州附近的城市，费费劲还是搞到了30日的火车票，算是尘埃落定了。 30日下午，三人分别从不同的三个城市（无锡，杭州，马鞍山）出发，经历火车时间也差不多（5H），就这么开始这次疯子旅行。说是一群疯子，其实也不过，列几个事实，疯不疯，看着说吧： 一，30日晚上相聚约九点，找个餐馆就餐，边吃边聊，彼此熟悉熟悉，然后开始找房子。都说人多不好找，我们也确实找了一阵子，十点多找到了。 二，第二天八点出发，临时看公交，上车，游西湖，随身没有一张地图，只是问了一下团价，觉得太贵，没报，索性两腿跑西湖，一天下来，跑的地点有：三潭印月，博物馆，雷峰塔（人多，没有买门票），钱王祠，苏堤（2.8公里，没走完），花巷观鱼，曲院风荷。小有遗憾，六点多出大门未见音乐喷泉。西湖之景，美不胜收，知足就行，开心便罢。开始寻找吃饭的地方，疯疯癫癫地走了不少路，耗时不少，总算找到一家快餐自助餐厅了。 三，原想着，吃罢饭，该找住的地方，十一游客特多，房子有限，没想到*来了一句，我们去看电影吧。快八点了，我们连电影院都不知道在哪儿，两部手机开始疯狂地搜索，好不容易找到号称浙江省最大的电影院——胜利剧院，话说今天没票了。不达目的决不罢休。又一次次探索，功夫不有心人，错过了《黄金时代》，迎来了《亲爱的》，九点25分准时上演了。 四，看完电影，十一点二十分了。我们还能找到住宿吗？我们真的要睡大街吗？我们真的要夜行杭州城吗？真的，这点着实让人担心，又是两部手机的开始漫无目标地搜寻，电话咨询，我的电话下午就关机了，电源准备不足（出行的亲们，引以为戒）。时至凌晨，我们找到价格不高不低的房子，说实在的，个人感觉杭州的租房价格有点奇葩，单间竟然和标准间一个价，或许我孤陋寡闻了吧。 五，第二天一觉睡到八九点，去了钱塘江，真不是时候，空荡荡的。然后，我们又开始找吃的，不能白来杭州，游的（西湖），乐的（亲爱的），就剩吃喝了。 找了一家比较远的自助餐人均价69元/人，对于我们这几个穷屌丝而言还是可以接受的。下午五点陆续离开杭州，杭州，我们不虚此行。 一篇流水记，即将尾声。写的总是没有亲身经历的来得实在，来得刺激，来得更有价值。许多事情看起来就那么回事，实际上做起来还真不是那么回事，其中的付出，艰辛与快乐并存，恐怕只有当事人才能真真切切地感受了。用朋友的一句话，那就是——一切尽在不言之中。十一行，算是圆满了，完成了我2014年的旅游计划，聚聚聊聊的目的也算达到了，在此特别感谢二位至亲的鼎力相助，我们共同实现了”西湖梦”。这一次疯子旅行，不靠谱的旅行，给我的收获远不止这些，疯疯狂狂的旅行，简简单单的生活，实实在在的人生，何忧之有？ 最后，来一张具有纪念意义而且大家都见过的小照，以此证明我们的西湖之行： by zhangbc2014-10-06","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【心路历程】做好自己，面对现实（三）","slug":"【心路历程】做好自己，面对现实（三）","date":"2019-04-22T14:02:55.000Z","updated":"2019-04-22T14:06:07.377Z","comments":true,"path":"2019/04/22/myself_04/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_04/","excerpt":"","text":"”露从今夜白，月是故乡明。“ 不知道多少个中秋没有和家人在一起过了，大概是童年的记忆了，也渐渐淡忘了这个团圆佳节，不过，或者一个人默默地过。每到中秋，端午，七夕这样的传统佳节，我都会不停地在打电话，或许只有这样才能弥补我思念的空缺，这也许就是我的生活，注定在寂寞中努力寻找属于自己的一丝丝慰藉，成为我平凡人生中一段看似不平凡的征程罢了。 毕业一年有余，每天除了工作之外几乎没有什么业余生活，偶尔心中不免有些内疚，生活为什么不能洒脱一些，主动一些，在物欲横流的今天，难道没有钱真的就得被逼上绝路吗？或许性格原因，或许五体不勤的缘故，我总是找不出一条合适的理由说服自己，活个范儿，管别人爱说咋说呢！ 在学校感受不到假期有多长，有多宝贵，所以大多数时间都是无为地被挥霍了，上班了，才渐渐明白，假期是多么地来之不易，春节从一个月或者更长一下子缩短到一周，尤其对一个回乡途中要耗上两天的漂泊浪子来说，其中的感受无以言表。又是一年中秋时，公司放假三天，所以我想不能再挥霍了，临时决定出去走走，向来没有长远计划，说干就干的劲儿还是有的，也许是上天可怜无规划之人吧，每次都能成事。去年十一回校探友也是9月28、29日临时买的票，一次简单的旅行，还是达到了预期的目的。要走的想法有了，可是接下来还是得仔细考虑考虑，去哪儿和谁做什么等等。仅仅一个想法还是不行的，周全规划一下，方使行动成功，所以一个想法只能算是一个成功的萌芽，并不是开始。三天，不可能走得太远，苏州附近的亲朋好友屈指可数，所以很快锁定了目标，探望一下朋友，随便找个地方玩玩，想法简简单单，仅此而已，人生亦应如此。 中秋三日假，总的来说，还是蛮愉快的。逛了一个景点——昆山周庄，看到古朴的建筑，虽然人少，缺少点氛围，但是总体印象还是不错的，七点回到朋友的住处，临时买菜做饭，两个人的饭菜不一会儿就上桌了，吃着香喷喷的饭菜，仿佛找到了一种久违的感觉。生活就应该这样的，自己动手，丰衣足食，不久的将来有一个新家，开始这样的生活，想想就是一种无比的期待。人的一生终究图个什么，我不知道，但是我应该很清楚地意识到，把握每一个稍纵即逝的幸福时刻，等到踏上黄泉路那一天能够微笑地离开，这或许就是我所理解的人生吧。买菜做饭，虽是一件普通的不能在普通的事，但在这个特殊的时刻，让我陷入了一次思考：生活不就是柴米油盐酱醋吗？为什么很多时候我们无法看清楚它的真面目，不会过日子呢？算一个小账，一顿饭，外面买一顿至少得15元吧，（炒饭另外）两顿就是30元，都说菜价米价上涨，其实去市场走一遭，30元买两顿的饭菜那是绰绰有余的，份量绝对够足，不像外面炒个肉丝，只见青椒不见肉块，其次如果手艺好点，味道也不差，吃着自己做的，心里本来就是美滋滋的。精打细算，学会过日子，否则就是有金山银山也是经不起折腾的。现在生活富裕了，我们的节约意识的确淡薄了不少，想不到这些生活的琐碎，每天只会不停地想，我的工资什么时候涨？或是我的工资都去哪儿了？想起读书那会，尤其是大一的时候，自己拿着一个小本子记录着自己的每一笔开销，小到一个笔记本，牙刷的钱，隔了一个月回头看看清晰的账目，多半会是露出满意的笑脸，俗语说得好，“用钱要用在刀口子”。并不是说要无限制地节约节省，记下来是为了让自己知道钱都花到哪儿去了，不至于花钱花得稀里糊涂，道不出所以然来。一直以来，我好像真没有吝啬过花钱，但是我比较清楚自己的钱都去哪儿了，管理自己的账目应该还算清楚吧。 完了一天，吃罢饭，九点有余，临时找住处，一切还算顺利，第二天一觉睡到十点半，天热没有安排出行，又做了一顿美食。中午就从朋友住处离开了，转战下一个目标。又是一个临时电话搞定一个新行程——去看看舅舅表哥一家人，打工在外，实属不易。从昆山转战苏北，车多行程非常顺利，虽然天气很热，但还是挡不住我心中的热火，想着第二天就是中秋佳节，应该去走走。去年就想着要去看看他们的，一直不想走动，所以未能如心所愿。和一大家子亲人过个中秋，也算是团圆了，看到舅舅舅母脸上的笑容，夹着额头一丝丝皱纹，顿感心中某种不快，上一辈为我们操劳得太多了，而我们呢？偶尔连他们一个小小的心愿都未能满足，这或许所谓的生活之苦吧，再一次让我深深体会带到：经济基础决定上层建筑，有什么样的工作就有什么样的生活。生活充满了太多的无奈，每一个人的境遇所有不同，但是生活的使命却是一致的，照顾上辈，操劳下辈。中华名族的传统美德，牵扯了太多太多的故事……今年的中秋，才让我知道了中秋的滋味，都说好散好聚，我不得不说散得容易，聚之不易。一个牵动无数家庭心的中秋，总是几多欢笑几多愁，看到的是笑脸，诉不出的却是悲愁，你又何谈一个“佳”字？中秋佳节倍思亲，十五的月亮十六圆，希望终有一天，我心中的月亮也能圆，生活虽不易，但是脚下的路还得继续，一条漫漫无尽头的路，路上有亲朋好友的陪伴，不曾寂寞，不曾落单，希望能加快步伐，不负青春，带着微笑向前去。中秋，往常一个人过，电话多多，今年也不另外，但和亲人相聚，私有时间自然有所减少，未能尽可能多的问候，或许这样的日子以后还会越来越多，各位至亲，理解万岁，不曾问候，思念依旧，没有前世的缘分，哪有现在的你们，又怎么会有今生我的福分？ 中秋过去已久，很久没有提笔，不知所云，以纪念今年不一样的中秋，以上次说说作结，算是给自己一个交代，“莫等闲，白了少年头，空悲切。”生活与责任，人生与使命，记住生活的美，才有源源不断的动力！ 记住以下五点，也不枉过一个美好的中秋：一，旅行是人生的必修课，规划很重要，随机应变往往决定了一件事能成与否;二，可口的饭菜是自己做的，靠山靠水不靠天，学会过日子，生活无处不精彩；三，经济基础决定上层建筑，有什么样的工作就有什么样的生活；四，身在他乡，和亲人过一个节，这才是家的感觉。五，节日的问候，时光渐变，我有可能慢慢疏忽这一点，各位至亲见谅。 by zhangbc2014-09-15","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【心路历程】做好自己，面对现实（二）","slug":"【心路历程】做好自己，面对现实（二）","date":"2019-04-22T13:54:02.000Z","updated":"2019-04-22T13:58:58.303Z","comments":true,"path":"2019/04/22/myself_03/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_03/","excerpt":"","text":"大学以来几乎没有碰过什么文学著作，心里想法太多，不能成为现实，这或许就是痴人说梦吧。梦，哦，对了，这是2013年整年最火的一个词儿，习总书记登台，让国人看到了希望，看到了一个中国梦，随着个人梦，青春梦，民族梦……应运而生。我也经常在追问自己：自己到底有没有一个梦，有没有一个明确的方向，该往何处去？ 记得刚毕业那会，找工作犹豫过，想做开发，能力明显不足，懂点语言的皮毛，偶尔也会在朋友们中间调侃几句；有人建议做实施，深入实践，懂得用户真正需要的东西，然后转做开发，由于种种原因，未能实现，后来转战到今天的这家不大的公司做运维（并非真正意义上的运维，算是简单的技术支持吧），很幸运，得到了同学的帮助，领导的赏识，慢慢成熟起来。叔叔曾问我：在公司感觉怎么样？我跟他详聊了一些情况，他说，还是要多学习，对一个行业的前景估量很重要，如果不是自己想要的，就得早作打算。很清楚地记得，他跟我说起过他一个关于对行业的前景估量的故事，坚持自己的正确选择，坚定不移的走下去。不知道为什么，时至今日，我都无法看到一个清晰的方向，IT，一个令人向往而又难以令人琢磨的东西，让人几分陶醉，也让人几分忧伤，陶醉的是自己可以在其中发现自己的兴趣，为她彻夜狂欢也不是不可能，新技术层层不穷，激发我们的求知欲，可以借以免去我们的生活烦忧；忧伤的是担心自己看不清方向，一股头脑热，摘了芝麻丢了西瓜，学不到真正的东西。技术是有助于人的，偶尔也会害了人。看到很多人对IT丧失了信心，开发的转测试，测试的转销售，IT的技术更新，IT人的不断更替，真的让我很难预料以后的道路如何？ 在这个部门，不到一年的时间里，我目睹了几拨人马，匆匆而来又匆匆而去，说句实话，心里不知道什么滋味，或许这就是IT界的一个缩影吧。跳槽，年轻人的专利，也是年轻的资本，出来拼的，图的不是稳定，而是快乐与生活的充实。如果我想图一份安逸的生活，我去年毕业就有可能呆在乡镇医院信息科，而且专业也对口，但是我没有，我不想这样的生活，至少不是我前半生想要的生活，给得了我安逸，却给不了我心灵的满足。根据自身的处境，身上的责任，我现在还不敢轻言跳槽，或许我资本不够，来之前，我就想过，我是来学习的，学不到东西，积累不到资本，我去何处都一样。记得有一次和朋友讨论过这个话题，跳槽，跳来跳去，手上没有筹码，在哪儿都一样。我认真思考过，我还是一个新手，什么都不会的新手，加强学习是我的第一要务。再说了，毕业前三年，我有两件事情要做：一，还贷，二，为未完成的梦做最后一搏。没有特殊情况，我是不会轻易的去改变的，即使我愿意付出更多的时间，我会为自己的想法付出行动，无论多么高的代价。这个五月应该是对我有着特殊意义的，感谢猴哥，让我搞掉了第一笔，作为低薪一族，我不觉得钱对我有多么重要，我对它从来不报任何希望，只要生活是自己想要的，没钱日子照样过。现在的生活虽不是很好，但足以让初生牛犊的我好好磨练磨练，值得熬一熬，抹掉我最初的梦想，也许会唤醒我另一个梦。 在中国梦大浪潮之下，追寻一个真实的自己，需要克服种种阻扰，来自心里的，来自家庭的，来自社会的，如果在乎多了，也就茫然了，不应该不尊重自己心中的想法，一个人如果没有强烈的金钱欲望，我想总会有平静的时刻。静心想想，我是否追随了中国梦而滋生了自己的个人梦，是否付出了行动？做一个会思考的人，在探索中前进，我想终有一天会过上属于自己想要的生活。 by zhangbc2014-05-17","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【心路历程】贫困，不能阻碍你的梦","slug":"【心路历程】贫困，不能阻碍你的梦","date":"2019-04-22T13:36:27.000Z","updated":"2019-04-22T13:58:38.745Z","comments":true,"path":"2019/04/22/myself_02/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_02/","excerpt":"","text":"在考研期间，遇到了几个比我低一届的同学存在的困惑，本想尽早写下这篇日志作为答复，由于考研，所以一直耽搁到了现在，现在想结合自己一路走过来的经历谈谈我个人的一些想法，由于涉及家庭因素，所以只能特殊处理。如果是本人看到的，请原谅我的直述，希望不要介怀。 一个土生土长的农村娃子，经历了太多太多的遭遇，时常会为没钱的日子发愁，是的，我不可否认，农村送出一个大学生的确不容易，本来经济来源相当有限，吃了这顿还得打算下顿。 清苦，节俭，朴素等这些耳熟能详的词语似乎一个一个不但扎根于我们的脑海中，而且我们也是这么走过来的。看看家庭经济比较宽裕的同伴的生活，我们曾经羡慕过，嫉妒过，也抱怨过上天的不公正待遇……因为贫困，眼见了一个个伙伴辍学，很早就迈入了残酷的现实的社会大门；因为贫困，无数父母背井离乡，过着听人使唤的卖苦力的生活，作为子女，当然不希望父母这样，于是一些“懂事”的孩子于心不忍，即使认为自己有能力的也过早地放弃了读书梦。在一次交流中，一位学弟告诉我，他想考研，但又不想读研，顾虑重重，我问其原因。他说主要是家里供不起。还有一位学弟告诉我，他为考研与否一直在纠结，因为家中有个弟弟在读书，家庭超负。这两位哥们其实面对的是同一个经济问题，在经济问题后面应该还有一个更重要的思想问题。烦不烦，很多时候不是取决问题的本身，而是看一个人会不会去思索，能不能去积极主动地面对现实，一味地抱怨只会让你更烦更纠结。 想考研，是为以后生活得更好，这无可厚非，大学生嘛，想实现自己的人生价值，有点野心未必不是好事；想考研，是为赚大钱，拿高薪，这是一种普众心理行为，在现行的社会制度下，晋级，职称无不与学历挂钩，每个人的职业生活压力都很大，也是完全可以理解的；想考研，是想做一项研究，追随自己的兴趣与爱好，立志于科研事业，这应该是考研的最高境界。单单是为赚钱而考研，我不太赞同，一，很多高薪职位并非需要很高的学历；二，考研应该是一笔物力，财力的巨大投资，是一笔青春代价的付出，而且这种投资与付出未必会收到你所期待的结果。如果你把考研的目的这个首要而且又具有决定性的问题想清楚了，那么对你后期的复习会有更大促进作用。考研，应该说一笔风险投资。你想以后要干什么？报什么专业？考什么档次的学校？这三个问题非常重要，关系着你的复习成效。报考的学校档次应该与投资的风险系数是呈正相关的。现在的研究生经费虽然非常高，但是各种补助也是相当丰厚的。关于经费，我想说的是，这不是重点，只要你足够优秀，上帝会偏袒你的，关键是你努力如何，有几分胜算？现在国家教育投资年年在增加，投资范围逐步扩大，你就认定自己有那么差吗？绿色通道，助学贷款，各种社会资助，就在你身边，就看你主不主动。想到四年的我，曾经也为此发愁过，选学校选专业时，优先挑选较便宜的。就在我感到无助的时候，一个从天而降的好消息来了，我们县城第一次提供生源地助学贷款，这对一个需要帮助的家庭来说，不能不说是雪中送炭啊！来到县教委办理手续时，又得到了一笔助学金，而后在大学里，在同学们的帮助与信任之下，得到了四年的高额助学金。四年的大学，你都走过来了，你还怕什么？又担心什么？你是贫困生，应该享受到了一定的资助，平时省吃节用一点点，有什么困难不能克服呢？ 0 贫困，不能阻碍我们的梦。鲤鱼跳龙门，农村出来的人都应该铭记这句话，这应该是我们读书初衷吧。如果我们不去尝试，怎么可能走出山旮旯？大山窝虽美，虽山清水秀，仁和心善，可是在经济与信息技术高速发展的今天，我们的经济从哪里来？难道还要死守农村自家的一亩三分地过日子吗？为了理想的生活，我们只有一条出路，那就是排除杂念，能走多久走多远，走得越远越好。起点高，不但可以改变生活，创造机会，更重要的是可以实现自己的人生价值。我们从农村走来，应该克服农村人比较狭隘的眼光，但不能说是轻视农民，不是他们无能，而是条件受限，应该用发展的、智慧的眼光正视我们的处境，没有优越感那就得靠自己的十二分努力去打拼、去创造，为不辜负乡亲们对我们的殷殷期望，必须拼。吃点苦算什么，我们又不是没有干过农活，比起插秧收割等田地间的耕作，我们要说有多幸福就有多幸福，不是吗？ 贫困，不是我们的错，不能阻碍我们的梦。贫困，这是个可以改变的事实，真心希望有经济困惑的后生们，不要顾虑太多，用你们的青春做资本去追梦吧！ by zhangbc2013-01-08","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【心路历程】面对现实，做好自己（一）","slug":"【心路历程】面对现实，做好自己（一）","date":"2019-04-22T13:23:38.000Z","updated":"2019-04-22T13:33:08.677Z","comments":true,"path":"2019/04/22/myself_01/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/myself_01/","excerpt":"","text":"毕业这么长时间，没有写下一则像样的日记，没有关注VOA的动态，没有规划学习，每天除了工作，简单不断重复的工作，余下的时间在双手之间悄悄溜走，从不觉得心痛，从不觉得自己有多么厚颜，感觉自己被某一种可怕的东西不断迁就着，不回头，也不想回头，偶尔还有一种可恶的念头：人活着，怎么活都是一辈子，干嘛非得拼命似的生活？勤一生，堕一生，两者之间无非就是别人的闲言碎语而已。当一个像我这样拥有此种邪恶念头的人，大概真的忘记了什么是厚颜无耻了？是的，人活着，这一生究竟为了什么，这是一个大话题，不是凭头论足就能下结论的，需要亲身实践，正所谓“纸上得来终觉浅，绝知此事要躬行”嘛。一介草民，无能把此话题说破抑或道明，但是真的有时候得想想，否则只会让自己越陷越深，越走越迷茫。 2013年7月踏出生活四年的大学校门，这次有着非凡的意义。母校，一个平时看不起眼的名称，在那一段时间，一些特定的场合，却总是让我的心在隐隐作痛。小学，初中，高中，大学，经历了这么多次，此次感受最深。这次，离开了母校或许就永远没有下一个母校了，离开虽不是生死离别，但却摆脱不了一种欲言又止的忧伤，轻轻而来，却又悄悄而去，何况还要拜别恩师，道别老乡，送别同窗，这一别就再也找到重逢的那一天了。我的确不知道重逢的那一天去哪儿了？ 这一次走出校门，意味着开始肩负起一份独立的使命，独立生活，独立经济，独立承担责任，来自家庭，事业，还有不远的未来。不远的未来，追逐爱情，抚育下一代，赡养父母……这不是归宿，仅仅是预设的开始，不是来与不来的问题，而是一个时间问题，为了这份使命，你准备了些什么呢？面对毕业，面对当下，我有点开始hold不住了，2013年毕业生699万，毕业不是就业，所以为了生计，解决就业难题才是关键，就业不成，何谈使命，连最起码的经济独立都做不到，还有什么资格空谈人生？一日三顿饱，这是最起码的生活保障。说实在的，当时有很多人为我的就业问题着急了，包括我的父母，我的亲属，我的朋友，我的恩师，社会很现实，将才干才多了去，不缺一个有生理缺陷的残疾人，我最终还是以平常之心坦然面对，走过了，也就习惯了，没有遗憾，没有怨言，从多次面试的经历来看，还是自己的所学甚少，残疾的头衔影响不是很大，毕竟社会还是有一定良知的，相信社会是美好的，你的心才会豁然开朗起来。在此处，也奉劝一些即将就业的后生们，人这一生，自信最重要，实力须有社会实践检测，找准方向，不可人云亦云尔。 毕业后，虽有一些小波折，但是不算太大，最终还是在苏州落定尘埃，高薪谈不上，至少自保了，也算是实现了经济上的基本独立。“上有天堂，下有苏杭”，说实在的，对我一个宅男而言，没有什么特别的感触，苏州的气候的确宜居，其实我所在的是苏州的一个小小的角落，距离苏州城区还有一定的距离。 毕业工作之余，偶尔回头想想自己的最初打算，顿时觉得有些不可思议，几乎没有办法实现，毕业了，如前所述，来自家庭的压力随之剧增，各种想法铺天盖地卷来，真的，有时候觉得无力从心，但又不得不为。对自己失言了，只能姑且一次次原谅而不了了之。一年内搞定助学贷款不是梦，可是预备的考研梦不得不破之，俗话说得好：鱼与熊掌不可兼得。一个人，不能把美事都想尽了，有得必有失，尊重现实，这才是生存的法则。不过不排除大牛们，若觉这是谬言，就权当过眼烟云。 2013年过去了很久，却又历历在目，经历了一些比较特殊的事情，真正从梦中把我拉回了现实，脚踏实地才是王道。毕业就业，工作是什么？我始终认为工作不应该成为一种累赘，即使一种累赘，无法改变之时，我们不妨换一种态度去坦然面对之。把痛苦变成一种乐趣，聊以自慰也未尝不可。工作的心态直接影响你的生活态度，与其对着宝马哭，还不如对着自行车笑。宝马显示了你的富有与尊贵，但是他也让你失去了锻炼身体的机会，脚踏车虽卑微，但偶尔也足可以让你感受兜风的乐趣。家中建设，让我开始感受做为家庭的一员，应尽家庭之义务，即使你未婚，谁说未婚的你就可以对家事不闻不问？压力再大，该承担的不该承担都应倾其所能地去承担，再说了，一家之事，何来你不担当之理？亲人的离世，或许你会说这有什么可说的，是的，生死离别，人之常事嘛。可是，这一次很不同，我的表弟，英年早逝，白发人送黑发人，你见得多吗？我是第二次经历，第一次发生2006年的夏天，不足周岁的亲妹妹的离世，那年，我没法忘记，估计是忘不了的，什么是残酷，这就是残酷，不是天意，而是现实。去年，接到舅舅的电话，听到噩耗的瞬间，我愕然了。不敢相信但又不得不相信 ，人死不能复生，这一既定的事实，就是所谓的上帝（本人不信佛，勿怪）也难以挽回，而且根本无法挽回。一个23岁的小伙子，说没就没了，怎么敢叫人轻言相信？23岁，我们都在干什么啊？在读书，正值青春，正当规划人生并为之奋斗的黄金时刻，说在此处，我不得不为每年高校的命案感到震惊。懂得珍惜生命，或许是活着的我们对已逝的人的最好的敬意吧。逝者已矣，活着美好，且行且珍惜。 以此作文，深知不是写作的料，警示自己误入歧途，越陷越深。 by zhangbc2014-04-15","categories":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/categories/mental-journey/"}],"tags":[{"name":"心路历程","slug":"mental-journey","permalink":"https://zhangbc.github.io/tags/mental-journey/"}]},{"title":"【数据库实践】T-SQL语言及其存储过程","slug":"【数据库实践】T-SQL语言及其存储过程","date":"2019-04-22T00:35:17.000Z","updated":"2019-04-22T11:37:46.818Z","comments":true,"path":"2019/04/22/db_tsql_procedure/","link":"","permalink":"https://zhangbc.github.io/2019/04/22/db_tsql_procedure/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 课本第11～13章主要知识点 一，T-SQL语言T-SQL语言是Microsoft公司在关系型数据库管理系统SQL Server中的SQL-3标准的实现，是Microsoft公司对结构化查询语言(SQL)的扩展。T-SQL语言是一种交互式的语言，具有功能强大，容易理解和掌握等特点。 1，数据定义语言(DDL)：DDL是指用于定义和管理数据库及数据库中各种对象的语句，包括create，alter，drop等。 创建表的语句格式为 1create table 表名 增加列的语句格式为 1alter table 表名 add 列名 列的描述 删除列的语句格式为 1alter table 表名 drop column 列名 修改列定义为 1alter table 表名 alter column 列名 列的描述 删除表的语句格式为 1drop table 表名 2，数据操纵语言(DML)：DML是指用于查询，添加，修改和删除数据库中数据的语句，包括select，insert，update，delete等。 3，数据控制语言(DCL)：DCL是指用于设置或更改数据库用户或者角色权限的语句，包括grant，revoke，deny等。 12345678910111213-- 给所有用户授予select权限grant select on student to publicgo ​-- 给指定用户授予特定权限grant insert,update,delete on student to LiMing,ZhangBingo​-- 联级授权grant select,update on student to user with [admin] option;revoke create table from ZhangWei;revoke select on sc from User;deny select, insert,update,delete on student to LiMing,ZhangBin; 4，其他语言元素(ALE) 1）注释：程序代码中不执行的文本字符串（也称注解），有：--和/* */。 2）变量(@)：局部变量（@），全局变量（@@）。 12345use stuSystemgo​declare @row int set @row=(select count(1) from student) 3）运算符 算术运算符：+，-，*，/，%； 赋值运算符：=； 位运算符：&amp;，|，^； 比较运算符：也称关系运算符，用于比较两个表达式的大小或者是否相同，其比较结果是布尔值TRUE，FALSE，UNKNOWN； 逻辑运算符：and，or，not。优先级别：not &gt; and &gt; or； 字符串串联运算符：允许通过加号(+)进行字符串串联，这个加号称为字符串串联运算符。 4）函数 在T-SQL语言中，函数用于执行一些特殊的运算以支持SQL Server的标准命令。 行集函数：在T-SQL语句中当成表引用。 1select * from openquery(local, 'select * from student;') ta; 聚合函数：用于对一组值进行计算并返回一个单一的值。除count之外，聚合函数忽略空值。 1select avg(score) as avg_score,sum(score) as total_score from sc; Ranking函数：为查询结果数据集分区中的每行返回一个序列值。有：rank，dense_rank，ntile，row_number。 标量函数：用于对传递给它的一个或多个参数值进行处理和计算，并返回一个单一的值。 标量函数 说明 配置函数 返回当前的配置信息 游标函数 返回有关游标的信息 日期和时间函数 对日期和时间输入值进行处理 数学函数 对作为函数参数提供的输入值执行计算 元数据函数 返回有关数据库和数据库对象的信息 安全函数 返回有关用户和角色的信息 字符串函数 对字符串(char或者varchar)输入值执行操作 系统函数 执行操作并返回有关SQL Server中的值、对象和设置的信息 系统统计函数 返回系统的统计信息 文本和图像函数 对文本或者图像输入值或者列执行操作，返回有关这些值的信息 5）流程控制语句 if-else语句 123456use stuSystemselect avg(score) from sc;if (select avg(score) from sc) &lt; 60print '很抱歉，你没有通过考试！'elseprint '祝贺你，考试通过了！' begin-end语句：能够将多个T-SQL语句组合成一个语句块，并将它们视为一个单元处理。 go语句：批的结束语句。批是一起提交并作为一个组执行的若干T-SQL语句。 123456use stuSystemgodeclare @msg varchar(50)set @msg='Hello world!'go case语句 123456789101112use stuSystemgo​select 'score' = case when score is null then '没有成绩' when score &lt; 60 then '不及格' when score &lt; 85 and score &gt;= 60 then '良好' else '优秀' end, cast(sno as varchar(20)) as sno from sc where cno = 'C03' order by score; while-continue-break语句 goto语句 123goto label...label: waitfor语句 1waitfor [delay 'time'|time 'time'] return语句 二，存储过程1，存储过程定义：几乎包含了所有的T-SQL语句，是为了完成特定功能而汇集在一起的一组SQL程序语句，经编译后存储在数据库中。 2，存储过程的调用方法 12exec proc 存储过程名exec proc 存储过程名 参数值[,参数值...] 3，存储过程分类 i）系统存储过程（前缀为SP_）；ii）扩展存储过程（前缀为XP_）；iii）用户自定义存储过程。 4，存储过程的创建和执行 创建存储过程 1234567create procedure dbo.pro_sc_insert@sno char(10),@cno char(2),@score realasbegin insert into sc(sno, cno, score) values(@sno, @cno, @score)end​ 执行存储过程 1exec pro_sc_insert '2010009', 'C1', 88 5，存储过程中的游标 1）游标的定义：可以把游标看成一种数据类型，用于遍历结果集，相当于指针，或是数组中的下标，分为局部游标(local)和全局游标(global)。 2）游标的使用方法 创建游标 123456-- 默认为global；-- forward_only(默认值)：游标只能前进的，只能从头到尾提取记录；-- scoll：可以在行间来回跳转。declare 游标名 cursor [local|global] [forward_only|scoll]for select 查询语句 使用游标：增加了服务器的负担，使用游标的效果远没有使用默认结果集的效率高，因此，能不用游标尽量不要用。 12345678910111213declare cur_select_name cursor for select sname from student;​declare @name varchar(20)open cur_select_namefetch next from cur_select_name into @name-- fetch_status取值：0正常执行；-1超出了结果集；-2所指向的行已不存在。 while(@@fetch_status = 0) begin print '姓名：' + @name fetch next from cur_select_name into @name endclose cur_select_namedeallocate cur_select_name 6，自动执行的存储过程 12use masterexec sp_procoption '存储过程名', 'startup', 'on' 7，存储过程的查看，修改和删除 1）查看存储过程 12345678910111213-- 显示存储过程的参数及其数据类型sp_help[[@name=]name]​-- 显示存储过程的源代码sp_helptext[[@objname=]name]​-- 显示和存储过程相关的数据库对象sp_depends[@objname=]'object'​-- 返回当前数据库中的存储过程列表sp_stored_procedures[[@sp_name=]'name'] [,[@sp_owner=]'owner'] [,[@sp_qualifier=]'sp_qualifier'] 2）修改存储过程 12345alter procedure stu_infowith encryptionas select sno, age from student order by age desc;go 3）删除存储过程 1drop procedure &#123;procedure&#125;[, ...n] 8，扩展存储过程：SQL Server 动态装载并执行的动态链接库（DDL），只能添加到master数据库中。 三，触发器及其应用1，触发器的概念和工作原理 1）触发器的概念：触发器是一种特殊类型的存储过程，在执行语言事件时自动生效。其特殊性表现：它是在执行某些T-SQL语句时自动生效的。 2）DML触发器：在数据库中发生DML事件时启动。将触发器和触发它的语句作为可在触发器内回滚的单个事务对待，如果检测到错误，则整个事务即自动回滚。 3）DDL触发器：是SQL Server 2005的新功能，当服务器或者数据库中发生DDL事件时将调用这些触发器。 2，创建触发器 1）DML触发器主要优点 i）DML触发器可通过数据库中相关表实现联级更改；ii）DML触发器可以防止恶意或者错误的INSERT，UPDATE及DELETE操作，并强制执行比CHECK约束定义的限制更为复杂的其他限制；iii）DML触发器可以评估数据修改前后表的状态，并根据该差异采取措施。 2）insert型DML触发器：通常用于更新时间标记字段，或者验证被触发器监控的字段中数据满足要求的标准，以确保数据的完整性。 12345678910/* 检测sc表添加数据的合法性，即添加的数据与student表的数据不匹配时，将删除此数据. */create trigger tri_sc_ins on scfor insert asbegin declare @sno char(10) select @sno=inserted.sno from inserted if not exists(select sno from student where student.sno=@sno) delete from sc where sno=@snoend 3）update型DML触发器：当在一个有update触发器的表中修改记录时，表中原来的记录被移动到删除表中，修改过的记录插入到了插入表中，触发器可以参考删除表和插入表及被修改的表，以确定如何完成数据库操作。 12345678910/* 防止用户修改SC表的成绩 */create trigger tri_sc_update on scfor updateasif update(scorce)begin raiserror('不能修改成绩！', 16, 10) rollback transactionendgo 4）delete型DML触发器 为了防止确实需要删除但会引起数据一致性问题的记录的删除； 执行可删除主记录的子记录的级联删除操作。 12345678910/* 当删除student表中的记录时，自动删除sc表对应学号的记录. */create trigger tri_del_sc on studentfor deleteasbegin delete @sno char(10) select @sno=deleted.sno from deleted delete from sc where sno=@snoend 5）DDL触发器 DDL触发器目的： i）防止对数据库架构进行某些更改；ii）希望数据库中发生某种情况以响应数据库架构中的更改；iii）要记录数据库架构中的更改或者事件。 1234567891011121314151617/* 防止数据库中的任意表被修改或者删除. */create trigger tri_safetyon databasefor drop_table, alter_tableasprint 'You must disable trigger \"tri_safety\" to drop or alter tables!'rollback/* 防止在数据库中创建表 */create trigger tri_ban_createin databasefor create_tableasprint 'create table issued.'select eventdata().value('(/event_instance/TSQLCommand/CommandText)[1]', 'nvarchar(max)')raiserror('New tables cannot be created in this database.', 16, 1)rollback 3，查看，修改和删除触发器 1）查看触发器 123456789/* 查看触发器的一般信息，如触发器的名称，属性，类型和创建时间 */sp_help 'trigger_name'/* 查看触发器的正文信息 */sp_helptext 'trigger_name'/* 查看指定触发器所引用的表或者指定的表涉及的所有触发器 */sp_depends 'trigger_name'sp_depends 'table_name' 2）修改触发器1sp_rename oldname, newname 3）删除触发器1drop trigger &#123;trigger&#125;[, ...n] 4，触发器的用途 1）可以实现比约束更为复杂的数据约束；2）可以检查SQL所做的操作是否被允许；3）当一个SQL语句对数据表进行操作时，触发器可以根据该SQL语句的操作情况对另一个数据表进行操作；4）约束的本身是不能调用存储过程的，但是触发器可以调用一个或者多个存储过程；5）在执行完SQL语句之后，触发器可以判断更改过的记录是否达到一定条件，如果达到，触发器可以自动调用SQL Mail来发邮件；6）约束不能返回信息，而触发器可以返回信息；7）可以修改原本要操作的SQL语句；8）为了保护已经建好的数据表，触发器可以在接收到以drop和alter开头的SQL语句中，不进行对数据表的操作。 四，嵌入式SQL1，嵌入式SQL简介 1）嵌入式SQL定义：嵌入式SQL是一种将SQL语句直接写入C语言，COLBOL，FORTRAN，Ada等编程语言的源代码中的方法。将SQL语句嵌入的目标源码的语言称为宿主语言。 2，嵌入式SQL的工作原理 提供对于嵌入式SQL的支持，需要数据库厂商除了提供DBMS之外，还必须提供一些工具。为了实现对于嵌入式SQL的支持，技术上必须解决以下4个问题: 1.宿主语言的编译器不可能识别和接受SQL文，需要解决如何将SQL的宿主语言源代码编译成可执行码;2.宿主语言的应用程序如何与DBMS之间传递数据和消息;3.如何把对数据的查询结果逐次赋值给宿主语言程序中的变量以供其处理;4.数据库的数据类型与宿主语言的数据类型有时不完全对应或等价，如何解决必要的数据类型转换问题。嵌入式SQL源码的处理流程 为了解决上述这些问题，数据库厂商需要提供一个嵌入式SQL的预编译器，把包含有嵌入式SQL文的宿主语言源码转换成纯宿主语言的代码。这样一来，源码即可使用宿主语言对应的编译器进行编译。通常情况下，经过嵌入式SQL的预编译之后，原有的嵌入式SQL会被转换成一系列函数调用。因此，数据库厂商还需要提供一系列函数库，以确保链接器能够把代码中的函数调用与对应的实现链接起来。 3，嵌入式SQL的一般形式 预编译 修改和扩充主语言使之能处理SQL语句","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库实践","slug":"db-practice","permalink":"https://zhangbc.github.io/tags/db-practice/"}]},{"title":"【数据库实践】 数据表及其SQL基本操作","slug":"【数据库实践】数据表及其SQL基本操作","date":"2019-04-21T12:05:44.000Z","updated":"2019-04-21T12:36:46.444Z","comments":true,"path":"2019/04/21/db_table_sql/","link":"","permalink":"https://zhangbc.github.io/2019/04/21/db_table_sql/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 课本第09～10章主要知识点 一，表的概述1，表的定义：表是包含 SQL Server 2005数据库中的所有数据的对象。表定义是一个列集合。 2，表的类型 1）分区表：将数据水平划分为多个单元的表，这些单元可以分布到数据库中的多个文件组中。2）系统表：存储服务器配置及其所有表的数据。3）用户表：用户自己创建的数据表和表示示例数据表，用于存储用户的信息，用户可以随意更改。4）临时表：分为本地临时表和全局临时表，存储在tempdb中，当不再使用临时表时会自动将其删除。 二，创建表1，表列的数据类型 1）精确数字型 整数类型（bigint，int，smallint，tinyint） bigint：存储范围为 $-2^{63}$~$2^{63}-1$；占用8个字节。 int：存储范围为 $-2^{31}$~$2^{31}-1$；占用4个字节。 smallint：存储范围为 $-2^{15}$~$2^{15}-1$；占用2个字节。 tinyint：存储范围为 0~255；占用1个字节。 位数据类型（bit） bit：存储范围是0和1，占用1个字节。常作为逻辑变量使用，用于表示真、假或者是、否等二值选择。 数值类型（decimal，numeric） decimal：和numeric一样，可以用2～17个字节来存储$-10^{38}+1$到$10^{38}-1$之间但是固定精度和小数位数位的数字。表示形式decimal(p,s)，其中p确定精确的总位数，默认为18位；s确定小数位数，默认为0。 货币类型（money，smallmoney）：必须在有效位置前加一个货币单位符号。 money：用于存储货币值，数值分为一个正数和一个小数分别存储在两个4字节的整型值中，存储范围为$-2^{63}$~$2^{63}-1$，精确到货币单位的千分之一。 smallmoney：与money数据类型相似，但是存储范为$-2^{31}$~$2^{31}-1$。 2）近似数据类型：real，float。 real：存储十进制数值，最大可以为7位精确位数。存储范围为$-3.40 \\times 10^{-38}$ ~ $3.40 \\times 10^{38}$，占用4个字节。 float：可以精确到15位小数。存储范围为$-1.79 \\times 10^{-308}$ ~ $1.79 \\times 10^{308}$，占用8个字节。float(n)：n指定float数据的精度，n为1～15的整数值。当n为1～7时，实际上是定义了一个real类型的数据，占用4个字节；当n为8～15时，系统认为其是float类型，占用8个字节。 3）日期和时间数据类型：datetime和smalldatetime。 datetime：存储日期和时间的结合体，存储从1753年1月1日0时到9999年12月31日23时59分59秒，其精确度可以达到三百分之一秒，即3.33ms。占用8个字节，日期和时间分别占用4个字节。默认格式为 MM DD YYYY hh:mm A.M./P.M.。 smalldatetime：与datetime类型相似，但是存储范围为1900年1月1日至2079年6月6日。占用4个字节，时间和日期分别占用2个字节，精确度为1min。 4）字符数据类型：char，varchar，text。 char：定义形式为char(n)，n表示所有字符占用的存储空间，其取值为1～8000，默认值为1。如果输入数据的字符串长度小于n，则系统自动在其后面添加空格来填充；如果输入的数据过长，将会截掉其超出部分。如果定义一个char数据类型，且允许该列为空，则该字段被当成varchar来处理。 varchar：定义形式为varchar(n)，可存储长达8000个字符的客人变长度字符串。其存储空间是根据存储在表的每列值的字符数变化的。 text：用于存储文本数据，其容量理论上为 1~$2^{31}-1$，实际应用根据硬盘的存储空间而定。 5）unicode字符数据类型：nchar，nvarchar，ntext。 nchar：定义形式为nchar(n)，n的取值为1～4000。与char类似，但采用unicode标准字符集，unicode标准用2个字节为1个存储单位。 nvarchar：定义形式为nvarchar(n)，n的取值为1～4000。与varchar类似，但采用unicode标准字符集。 ntext：理论上容量为$2^{30}-1$，与text类似，但采用unicode标准字符集。 6）二进制数据类型：binary，varbinary，image。 binary：定义形式为binary(n)，数据存储长度是固定的，即n+4个字节。当输入的二进制数据长度小于n时，余下部分填充0。二进制数据类型的最大长度为8000，常用于存储图像等数据。 varbinary：定义形式为varbinary(n)，数据存储长度是变化的，为实际输入数据的长度加上4个字节，其他类似binary。 image：用于存储图像数据，其理论容量为$2^{31}-1$个字节。 7）其他数据类型：sql_variant，table，timestamp，uniqueidentifier，xml，cursor等。 sql_variant：用于存储除文本、图形数据和timestamp类型数据外的其他任何合法的SQL Server 2005数据。 table：用于存储对表或者视图处理后的结果集。 timestamp：时间戳数据类型，提供数据库范围内的唯一值。 uniqueidentifier：用于存储一个16字节长的二进制数据类型，是SQL Server 2005根据计算机网络适配器地址和CPU时钟产生的全局唯一标识符代码（GUID），通过调用 SQL Server 2005 的NEWID()函数获取。 xml：存储xml类型的数据，其数据容量不能超过2GB。 cursor：是变量或者存储过程OUTPUT参数的一种数据类型，这些参数包含对游标的引用。 8）用户自定义数据类型 语法格式如下： 1sp_addtype [@typename=]type, [@phystype=]system_data_type[, [@nulltype=]'null_type'][, [@owner=]'owner_name'] 举例：自定义一个地址(address)数据类型。 1exec sp_addtype address, 'varchar(80)', 'not null' 2，列的其他属性 1）NULL，NOT NULL和默认值在数据库中，NULL是一个特殊值，表示未知值的概念。默认值是指如果插入行时没有为列指定值，默认值则指定列中使用的值。 2）IDENTITY属性：实现标识符列。 一个表只能有一个使用IDENTITY属性定义的列，且必须通过使用bigint，int，smallint，tinyint或者decimal，numeric数据类型类定义该列； 可指定种子和增量，两者的默认值均为1； 标识符列不允许为空值，也不能包含default定义或者对象； 在设置IDENTITY属性后，可以使用$IDENTITY关键字在选择列表中引用该列，也可以通过名称引用该列； objectproperty函数可以用于确定一个表是否具有IDENTITY列，columnproperty函数可以确定IDENTITY列的名称； 通过使值能显示插入，set identity_insert可以用于禁用该列的IDENTITY属性。 3，表的创建 12345678910use stuSystemgocreate table student( studentID int not null primary key, studentName char(10) not null, class char(10) not null, depart char(16) not null, yearClass char(6) not null) 三，维护表1，修改表名与表结构 1234567-- 修改表名，将studentcourse修改为courseexec sp_rename 'studentcourse', 'course';​-- 修改表结构alter table student add sex char(2);alter table student drop column yearClass;alter table student alter column studentName char(8); 2，删除表 1drop table course; 3，表数据的维护 1）添加数据（insert） 1insert into course values('10015', '计算机网络', '张三', 2, null); 2）更新表数据（update） 1update course set cname='计算机组成原理' where cno='10015'; 3）删除表数据（delete） 1delete from student where sno=200901020023; 四，表数据完整性1，SQL Server提供的数据类型完整性组件 完整性类型 组件 实体完整性 索引，unique约束，primary key约束和identity属性 域完整性 foreign key约束，check约束，default定义，not null定义和规则 参照完整性 foreign key约束，check约束和触发器 用户定义完整性 create table中的所有列级和表级约束，存储过程和触发器 2，primary key约束 表通常具有一列或者一组列可以用于唯一标识表中的一行，这样的一列或者多列称为表的主键（PK），用于强制表的实体完整性。 3，foreign key约束 外键是用于建立和加强两个表数据之间的链接的一列或者多列，用于强制参照完整性。 一个表 中最多可以有253个参照表，因此每个表最多可以有253个foreign key约束； 在foreign key约束中，只能参照同一个数据库中的表； foreign key子句中的列数目和每个列指定的数据类型必须和reference子句中的相同； foreign key约束不能自动创建索引； 参照同一个表中的列时，必须只使用reference子句，而不能使用foreign key子句； 在临时表中，不能使用foreign key约束。 4，check约束：通过限制列可接受的值，check约束可以强制域的完整性。 5，表关系：显示某个表中的列如何链接到另一个表的列；可以防止出现冗余数据。 五，视图1，视图概述 1）视图是一个虚拟表，是由若干个表或者视图中导出的表，其结构和数据是建立在对表的查询基础上的，其内容由查询定义。 2）视图的主要优点和作用： i）着重于特定数据：视图使用户能够着重于他们所感兴趣的特定数据和所负责的特定任务，不必要的数据或者敏感数据可以不出现在视图中；ii）简化数据操作：视图可以简化用户处理数据的方式；iii）提供向后兼容性：视图能够在表的架构更改时为表创建向后兼容接口；iv）自定义数据：视图允许用户以不同方式查看数据，即使在他们同时使用相同的数据时也是如此；v）导出和导入数据：可使用视图将数据导出到其他应用程序；vi）跨服务器组合分区数据：使用分区视图，可以使用多个服务器对数据进行分区。 3）视图分类： i）标准视图：组合了一个或者多个表中的数据，可以获得使用视图的大多数好处，包括将重点放在特定数据上及简化数据操作。ii）索引视图：被具体化了的视图，即它已经经过计算并存储，可以为视图创建唯一的聚集索引。iii）分区视图：在一台或者多台服务器间水平连接一组成员表的分区数据。 2，创建视图 1）创建视图原则 i) 只能在当前数据库中创建视图；ii) 视图名称必须遵循标志符的规则，且对每个用户必须唯一；iii) 可以在其他视图和引用视图的过程之上创建视图；iv) 定义视图的查询不能包括order by，compute，compute by子句或者into关键字；v) 不能在视图上定义全文索引定义；vi) 不能创建临时视图，也不能在临时表上创建视图；vii) 不能对视图执行全文查询，但是如果查询所引用的表被配置为支持全文索引，就可以在视图定义中包含全文查询。 举例： 12345use stuSystemgo​create view view_teacher_choice as select b.tname, a.chsu from course a, teacher b where a.tid=b.tid; 3，使用视图 1）使用视图进行数据检索 1select * from view_teacher_choice; 2）通过视图修改数据 如果在视图定义中使用了with check option子句，则所有在视图上执行的数据修改语句都必须符合定义视图的select语句中所设定的条件。 SQL Server 必须能够明确地解析对视图所引用基表中的特定行所做的修改操作。 对基表中须更新而又不允许空值的所有列，其值在insert语句或者default定义中指定。 如果在视图删除数据，在视图定义的from子句中只能列出一个表。 视图修图数据通过insert，update，delete语句来完成。 4，修改视图 12345use stuSystemgo​alter view view_teacher_choice as select b.tname, a.chsu from course a, teacher b where a.tid=b.tid and a.chsu &gt; 30; 5，重命名视图 1exec sp_rename 'view_teacher_choice', 'view_teacher_choice_total'; 6，查看视图 1234use stuSystemgo​exec sp_helptext 'view_teacher_choice_total'; 7，删除视图 1234use stuSystemgo​drop view view_teacher_choice_total; 六，索引1，索引概述 1）索引定义：索引是对数据库表中一个或者多个列的值进行排序而创建的一种存储结构。 2）索引分类： i）聚集索引(clustered)：保证数据库表中记录的物理顺序与索引顺序相同，一个表只能有一个聚集索引。ii）非聚集索引(nonclustered)：数据库表中记录的物理顺序与索引顺序可以不相同，一个表可以有多个非聚集索引。iii）唯一索引(unique)：表示表中的任何两个记录的索引值都不相同，与表的主键类似，确保索引列不包括重复的值；iv）组合索引：将两个或者多个字段组合起来的索引，而单独的字段允许不是唯一的值。 2，创建索引 1234use stuSystemgo​create index idx_time in course(choice_time asc); 3，查看索引 1exec sp_helpindex idx_time; 4，删除索引 1drop index course.idx_time; 七，SQL操作查询1，简单查询 123456select sno,sn,age from student;select * from student;select distinct sno from course;select sn as sname,sno,age from student;select sn,age-5 from student;select avg(age) from student; 2，带条件的列查询 比较大小和确定范围 部分匹配查询 查询的排序 123456789101112select sno,score from sc where cno='C01';select sno,cno,score from sc where score&gt;80;select sno,cno,score from sc where (cno='C01' or cno='C02') and score&gt;80;select sno,sn from student where age between 18 and 20;select sno,sn,cno from student where score is null;select sno,cno,score from sc where cno in('C01', 'C02');select sno,sn from student where sn like '李%';select sno,score from sc where cno='C01' order by score desc;select sum(score) as total_score, avg(score) as avg_score from sc where sno='0001';select max(score) as max_score,min(score) as min_score,max(score)-min(score) as diff from sc where cno='C01';select count(distinct dept) as dept_num from student;select sno,sum(score) aas total_score from sc where score &gt;= 60 group by sno having(*) &gt;= 3 order bu sum(score) desc; 3，多表查询 所谓多表查询，即在两个或者两个以上的表中进行的查询操作，分为：连接查询和子查询（嵌套查询）。 连接查询 1）内连接 等值连接：在连接条件中使用等于号(=)运算符，与被连接列的列值进行比较，在查询结果中列出被连接表中的所有列，包括其中的重复列。 不等连接：在连接条件中使用除等于号(=)以外的其他比较运算符，与被连接列的列值进行比较，这些运算符包括&gt;，&gt;=，&lt;，&lt;=，!&gt;，!&lt;，&lt;&gt;。 自然连接：在连接条件中使用等于号(=)运算符，与被连接列的列值进行比较，在查询结果中列出被连接表中的所有列，但会删除其中的重复列。 1select * from student a inner join sc b on a.sno=b.sno; 2）交叉连接（笛卡尔积）：两个关系中所有元组的任意组合。 1select * from student cross join sc; 3）自连接：如果在一个连接查询中，涉及的两个表都是同一张表，这种查询称为自连接查询。 1select a.* from student a inner join student b on a.cno=b.cno and b.sno='20090701027'; 4）外连接：其查询结果既包含那些满足条件的行，又包含其中某个表的全部行。 左外连接（left join） 右外连接（right join） 全外连接（full join） 123select a.sno,a.sname,a.class,a.cno,b.score from student a left join sc b on a.sno=b.sno;select a.sno,a.sname,a.class,a.cno,b.score from student a right join sc b on a.sno=b.sno;select a.sno,a.sname,a.class,a.cno,b.score from student a full join sc b on a.sno=b.sno; 子查询 在where子句中包含一个形如select-from-where的查询块，此查询称为子查询或者嵌套查询，包含子查询的语句称为父查询或者外部查询。基本关键字：any，in，all，exists 1234select tname from teacher where tno=any(select tno from tc where cno='C05');select tname from teacher where tno in (select tno from tc where cno='C05');select tname,sal from teacher where sal&gt;all(select sal from teacher where dept='电力系') and dept!='电力系';select tname from teacher where exists (select 1 from tc where teacher.tno=tc.tno and cno='C05');","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库实践","slug":"db-practice","permalink":"https://zhangbc.github.io/tags/db-practice/"}]},{"title":"【数据库实践】 数据库及其管理","slug":"【数据库实践】数据库及其表管理","date":"2019-04-17T15:32:38.000Z","updated":"2019-04-18T11:37:54.594Z","comments":true,"path":"2019/04/17/db_table_manager/","link":"","permalink":"https://zhangbc.github.io/2019/04/17/db_table_manager/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 课本第07～08章主要知识点 一，SQL Server 2005概述1，SQL Server 2005 新特性 SQL Server 是一个全面的，集成的数据库解决方案，为企业中的用户提供了用于企业数据管理和商业智能的一个安全，可靠和高效的平台。 1）Service Broker 提供了一个功能强大的，异步编程的新模型。数据库应用程序通常使用异步编程来缩短交互式响应时间，并增加应用程序总吞吐量。 帮助数据库开发人员生成可靠且可扩展的应用程序。 在SQL Server实例之间提供可靠的消息传递服务。 2）CLR集成3）专用管理员DAC：SQL Server 2005为管理员提供了一种特殊的诊断连接，以供在无法与服务器建立标准连接时使用。4）用户与架构分离 简化删除数据库用户的操作 多个用户可以用户同一个架构 5）分区：分区是大型数据表和索引采取的优化存储性能和数据存储的技术。 2，SQL Server 2005 体系结构 1）数据库引擎组件：为数据存储、处理和安全实现高可伸缩性和高可用性服务。2）报表组件服务组件：提供了全面的报表解决方案，可创建、管理和发布传统的、可打印的报表和交互性的、基于Web的报表。3）分析服务组件：提供数据分析服务。利用分析服务开发人员设计、创建和管理多种数据源和多种数据结构，通过数据分析规律，获取数据知识。4）集成服务组件：用于构建高性能数据集成解决方案（包括为数据仓库提取、转换和加载（ETL）包）的平台。5）其他组件：复制服务组件，通知服务组件，服务代理组件，全文搜索。 3，SQL Server 2005安装 1）SQL Server 2005版本选择 SQL Server 2005企业版：支持32位和64位系统，适合超大型企业。包括联机事务处理，高度复杂的数据分析，数据仓库系统和网站所需的基本功能。 SQL Server 2005标准版：支持32位和64位系统，适合中小型企业。包括电子商务，数据仓库和业务流解决方案所需的基本功能。 SQL Server 2005工作组版：仅支持32位系统，适合小型企业使用。包括SQL Server产品系列的核心数据库功能，可升级至标准版或者企业版。 SQL Server 2005开发版：支持32位和64位系统，仅适合于开发和测试系统使用。具备和企业版完全一样的功能，但有许可限制，只能用于开发和测试。 SQL Server 2005简易版：仅支持32位系统，是一个免费的，使用简单，易于管理的数据库。 SQL Server 2005精简版：仅支持32位系统，适合在移动智能设备上使用。 二，数据库的类型1，系统数据库 数据库是表、视图、索引、存储过程等对象的集合，是数据库管理系统的核心，数据库与管理它的数据库管理系统统一组成数据库服务器。 1）master：记录了SQL Server系统的所有服务器的系统信息，包括实例范围的元数据，端点，链接服务器和系统配置设置。 2）model：用于在SQL Server实例上创建的所有数据库的模板。 3）msdb：提供给SQL Server代理服务器使用的数据库，主要用于为警报，作业，任务调度及记录操作员的操作提供相应的支持。 4）tempdb：连接到SQL Server实例的所有用户都可用的全局资源，保存所有临时表和临时存储过程。 5）resource：是一个隐藏的只读数据库，包含了SQL Server中的所有系统对象，但不包含用户数据或者用户元数据。 2，用户数据库 数据库命名规则如下： 第一个字符必须是字母a～z和A~Z、汉字或者下画线(_)、符号@，#； 后续字符可以是字母a～z和A~Z、汉字、数字或者下画线(_)、符号@，$、数字符号； 标识符不能是T-SQL的保留字； 长度不能超过128。 3，数据库快照 1）数据库快照的定义：数据库快照是用户数据库的只读、静态视图，不包括未提交的事务。 2）数据库快照的特点： 反映某个时刻（完成数据库快照创建的时刻）数据库的数据； 不允许更新； 一个用户数据库可以创建多个数据库快照，并且必须与数据库在同一个服务器实例上。 三，数据库存储文件SQL Server 2005数据库文件名称包括： 逻辑文件名：在所有 T-SQL 语句中引用物理文件时所使用的名称，必须符合SQL Server 标识符规则，而且是唯一的。 物理文件名：包括目录路径的物理文件名称，必须符合操作系统文件命名规则。 SQL Server 2005数据库操作系统文件包括： 数据文件：包括数据和对象，如表、索引、存储过程和视图。 日志文件：包含恢复数据库中的所有事务所需的信息。 1，数据库文件 主要数据库文件（.mdf）：数据库的起点，包括数据库的启动信息，并指向数据库中其他文件。 次要数据库文件（.ndf）：是可选的，由用户定义并存储用户数据。 事务日志文件（.ldf）：用于恢复数据库的日志信息。 2，数据库文件组 主文件组：包含主要数据文件和任何没有明确分配给其他文件组的其他文件。系统表的所有页均分配在主文件组中。 用户定义文件组：通过在 create database 或者 alter database 语句中使用 filegroup 关键字指定的任何文件组。 默认文件组（primary文件组）：如果在数据库中创建对象时没有指定对象所属的文件组，对象将被分配给默认文件组。 注意：日志文件包括在文件组内。 四，数据库的对象1，数据库关系图：以图形方式来表示表之间的关系。 2，表：组织和存储数据。 3，索引：提高数据检索速度，但增加了系统存储空间的开销。 4，视图：实现用户对数据对查询，但是视图的结构和数据是建立在对表的查询基础上的。 5，存储过程和触发器：数据库中的对编程对象。 存储过程独立于表，存储在服务器上，供客户端调用，提供应用程序的效率。 触发器是一种特殊的存储过程，可以大大增强应用程序的健壮性，数据的可恢复性和可管理性。 6，规则和约束：对能够放入表中的内容进行限定。 规则：用于在用户定义数据类型上加以限制。 约束：本身并非实际的对象，而只是描述特定表的元数据。 7，默认值： 默认值有两种类型： 默认值默认其本身是一个对象； 默认值不是实际的对象，只描述表特定列的元数据。 8，全文目录：全文目录是数据的映射，以加速对启用了全文搜索的列中特定文本块的搜索。 五，创建数据库 一个SQL Server实例，最多可以创建32767个数据库。 1，使用SQL Server管理工具创建 2，使用T-SQL创建 1234567891011121314151617create database xsxk on primary -- 建立主要数据文件( name = 'xsxk_data', -- 逻辑名称 filename = 'E:\\students\\xsxk_data.mdf', -- 物理文件路径和名称 size = 1024KB, -- 初始大小 maxsize = unlimited, -- 最大尺寸为无限大 filegrowth = 10% -- 增长速度)log on( name = 'xsxk_log', -- 建立日志文件 filename = 'E:\\students\\xsxk_log.ldf', -- 物理文件路径和名称 size = 1024KB, -- 初始大小 maxsize = 5120KB, filegrowth = 1024KB ) 六，维护数据库1，查看数据库状态 1234use master;go​select name,state,state_desc from sys.databases; 2，修改数据库 1234567891011121314151617181920alter database xsxk modify file( name = 'xsxk_data', -- 逻辑名称 size = 5MB, -- 初始大小 maxsize = 50MB, filegrowth = 2% -- 增长速度)goalter database xsxklog on( name = 'xsxk_log', -- 建立日志文件 filename = 'E:\\students\\xsxk_log.ldf', -- 物理文件路径和名称 size = 1024KB, -- 初始大小 maxsize = 5120KB, filegrowth = 2MB ) go 3，删除数据库 1drop database xsxk; 4，分离和附加数据库 123456789101112-- 分离数据库sp_detach_db xsxk;​-- 附加数据库create database xsxk on( filename = 'E:\\students\\xsxk_data.mdf'),( filename = 'E:\\students\\xsxk_log.ldf') for attach;go 5，其他操作 1）脱机用户数据库 2）联机用户数据库 3）重命名数据库 1alter database xsxk modify name = xsxks 4）收缩数据库 12345-- 将userDB用户数据库中的文件减小，以使userDB中的文件有10%的可用空间dbcc sharinkdatabases(userDB, 10)​-- 将userDB用户数据库中名为DataFile的文件大小收缩到7MDBCC shrinkfile(DataFile, 7)","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库实践","slug":"db-practice","permalink":"https://zhangbc.github.io/tags/db-practice/"}]},{"title":"【数据库理论】数据库的安全和保护","slug":"【数据库理论】数据库的安全和保护","date":"2019-04-14T11:05:17.000Z","updated":"2019-04-14T16:43:26.612Z","comments":true,"path":"2019/04/14/db_security/","link":"","permalink":"https://zhangbc.github.io/2019/04/14/db_security/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 一，安全与保护概述1）数据安全性控制：防止未经授权的用户和存取数据库中的数据，避免数据的泄露，更改或破坏； 2）数据完整性控制：保证数据库中数据及语义的正确性和有效性，防止任何对数据库造成错误的操作； 3）数据库的并发控制：在多用户同时对一个个数据进行操作时，系统应能够加以控制，防止破坏数据库中的数据； 4）数据库的恢复：在数据库遭到破坏或者数据不正确时，系统有能力把数据库恢复到正确的状态。 二，数据库的安全性1，数据库安全性控制 1）用户标识与鉴别：系统提供的最外层的安全保护措施，其方法是由系统提供一定的方式让用户标识自己的名字或身份。 2）存取控制： （1）定义用户权限，并将用户权限等存储在数据字典中。用户权限是指不同等用户对不同对数据对象允许执行对操作权限，这些定义经过编译后存放在数据字典中，这些定义称为安全规则或授权规则；（2）合法权限检查。用户权限定义和合法权限检查机制组成类DBMS的安全子系统。 3）试图机制：数据安全性，逻辑数据独立性和操作简便性。 4）审计：审计追踪是一个对数据库进行更新对日志，还包括一些其他信息，如哪个用户执行了更新和什么时候执行的更新等。 5）数据加密：防止数据库中数据在存储和传输中失密的有效手段。 2，SQL Server的安全性措施 1）SQL Server安全控制概述 （1）操作系统安全验证（网络层）：通过设置安全模式来实现。（2）SQL Server安全验证（服务器）：通过SQL Server服务器登录名管理来实现。（3）SQL Server数据库安全验证：通过SQL Server数据库用户管理来实现。（4）SQL Server数据库对象安全验证（处理权限）：通过权限管理来实现。 2）SQL Server的安全认证模式 （1）身份验证阶段：Windows身份验证，混合模式验证。（2）权限认证阶段（3）设置身份验证 3）登录名和用户管理1234USE &lt;DATABASE NAME&gt;goCREATE USER &lt;new user_name&gt; FOR LOGIN &lt;login name&gt;; 4）权限管理 （1）权限分类：语句权限，对象权限，隐含权限；（2）角色分类：在SQL Server中，组是通过角色来实现的。角色分为服务器角色和数据库角色（预定义的数据库角色，用户自定义数据库角色和应用程序角色）。 三，数据库的完整性1，数据库的完整性定义：数据库的完整性是指数据的正确性（Correctness），有效性（Validity）和相容性（Consistency）。正确性是指数据的合法性；有效性是指数据是否属于所定义的有效范围；相容性是指表示同一事实的两个数据应一致，不一致就是不相容。 完整性检查：在DBMS中，检查数据库中的数据是否满足语义规定的条件。 2，完整性约束条件 1）完整性约束作用的对象可以是列，元组，关系。其中：列的约束主要是列的类型，取值范围，精度，排序等约束条件；元组的约束是元组中各个字段间的联系约束；关系的约束是若干元组间，关系集合上及关系之间的联系约束。 2）静态约束：指数据库在每个确定状态时的数据对象所应满足的约束条件，是反映数据库状态合理性的约束。 （1）静态列级约束：对一个列的取值范围的说明，即对数据类型的约束（数据的类型，长度，单位，精度等），对数据格式的约束，对取值范围或者取值集合的约束，对空值的约束，其他约束。（2）静态元组约束：规定元组的各个列之间的约束关系。（3）静态关系约束：实体完整性约束，参照完整性约束，函数依赖约束，统计约束。 3）动态约束：指数据库从一种状态转变为另一种状态时，新旧值之间所应满足的约束条件，是反映数据库状态变迁的约束。 （1）动态列级约束：修改列定义或者列值时应满足的约束条件。（2）动态元组约束：修改元组中各个字段间需要满足某种约束条件。（3）动态关系约束：加在关系变化前后状态上的限制条件，如事务一致性，原子性等。 3，完整性控制 1）定义功能：提供定义完整性约束条件的机制。 2）检查功能：检查用户发出的操作请求是否违背列完整性约束条件。 3）如果发现用户的操作请求使数据违背了完整性约束条件，则采取恰当的操作。 4，SQL Server的完整性实现 1）声明型数据完整性约束：在 create table 和 alter table 定义中使用约束限制表中的值。 2）过程型数据完整性约束：由缺省，规则和触发器实现，由视图和存储过程支持。 （1）约束：是SQL Server提供的自动保持数据库完整性的一种方法。分为：i）空值约束1[CONSTRAINT constraint_name] [NULL/NOT NULL] ii）主键约束1234-- 列级[CONSTRAINT constraint_name] PRIMARY KEY -- 表级[CONSTRAINT constraint_name] PRIMARY KEY(&lt;column_name&gt;[&#123;,&lt;column_name&gt;&#125;]) iii）唯一约束1234-- 列级[CONSTRAINT constraint_name] UNIQUE -- 表级[CONSTRAINT constraint_name] UNIQUE(&lt;column_name&gt;[&#123;,&lt;column_name&gt;&#125;]) 唯一约束和主键约束的区别： （1）在一个基本表中，只能定义在一个主键约束，但可以定义多个唯一约束；（2）两者都为指定但列建立唯一索引，但主键约束限制更为严格，不但不允许有重复值，而且也不允许由空值；（3）唯一约束与主键约束产生但约束可以是聚集索引，也可以是非聚集索引，在缺省情况下，唯一约束产生非聚集索引，主键约束产生聚集索引；（4）不能同时为同一列或者一组列既定义唯一约束，又定义主键约束。 iv）外键约束和参照约束 1[CONSTRAINT constraint_name] [FOREIGN KEY] REFERENCES ref_table (ref_column[&#123;,&lt;trf_column&gt;&#125;]) v）缺省值约束1[CONSTRAINT constraint_name] DEFAULT constraint_expression vi）检查约束1[CONSTRAINT constraint_name] CHECK(logical_expression) 例如：12345678910111213use mastergo ​create table Titles ( title_id varchar(6) constraint pk_title_id primary key, title varchar(80) not null constraint uniq_title unique, [type] char(12) not null constraint def_type default 'UNDECIDED', pub_id char(4), price money constraint chk_price check(price between 5 and 100), ytd_sales int, pub_date datetime not null constraint def_pub_date default getdate()) （2）规则：当向表当某列（或使用与该规则绑定的用户定义数据类型的所有列）插入列或更新数据时，它指定限制输入新值的取值范围。 12345678use mastergo​create rule rule_price as @price &gt;= 5 and @price &lt;= 100go​exec sp_bindrule 'rule_price', 'Titles.price'go （3）缺省：它指定在向数据库中的表插入数据时，如果用户没有明确给出某列的值，SQL Server自动为该列（使用与该缺省绑定的用户定义数据类型的所有列）输入值。 12345678use mastergo​create default def_price as 50go​exec sp_bindefault def_price, 'Titles.price'go 四，事务1，事务的概念 1）事务定义：用户定义的一个数据库操作序列，这些操作要么可全部成功执行，否则，将不执行其中任何一个操作。事务是一个不可分割的工作单元。 2）事务基本性质(ACID) （1）原子性(Atomicity)：事务中包含的所有操作要么全做，要么一个也不做。（2）一致性(Consistency)：定义在数据库上的各种完整性约束。（3）隔离性(Ioslation)：确保事务并发执行后的系统状态与这些事务以某种次序串行执行后的状态是等价的。（4）持久性(Durability)：一个事务一旦成功完成，它对数据库的改变必须是永久的，即使是在系统遇到故障的情况下也不会丢失。 2，事务调度 1）事务调度的定义：在一个大型的DBMS中，可能会同时存在多个事务处理请求，系统需要确定这组事务的执行次序，即每个事务的指令在系统中执行的时间顺序，这称为事务的调度。 2）合法调度须满足以下条件： i）调度必须包含所有的事务的指令；ii）一个事务中指令的顺序在调度中必须保持不变。 3）调度的基本形式 i）串行调度 串行调度：在前一个事务完成之后，在开始另外一个事务，类似与操作系统中的单道批处理作业。 可串行化调度：定义多个事务的并发执行是正确的，当且仅当其结果与按某一次序串行地执行它们时的结果相同，这种调度策略称为可串行化调度。 ii）并行调度 并行调度可串行化：如果一组事务并行调度的执行结果等价于这组事务中所有提交事务的某个串行调度，则称该并行调度可串行化。 级联回滚：由于一个事务的故障而导致一系列其他事务的回滚。 无级联调度：应该对调度做出某种限制以避免级联回滚发生，这样的调度称为无级联调度。 可恢复调度：对每对事务$T_i$和$T_j$，如果$T_j$读取了由$T_i$所写的数据项，则$T_i$必须先于$T_j$提交。 3，事务隔离级别 1）并发操作带来的问题 i）丢失修改：又称写-写错误，两个事务$T_1$和$T_2$读入同一数据并修改，$T_2$提交的结果破坏了$T_1$提交的结果，导致$T_1$的修改被丢失。ii）脏读：又称写-读错误，事务$T_1$修改某一数据，并将其写回磁盘，事务$T_2$读取同一数据后，$T_1$由于某种原因被撤销，这时$T_1$已修改过的数据恢复原值，$T_2$读到的数据就与数据库中的数据不一致，则$T_2$读到的数据即为“脏”数据，即不正确的数据。iii）不可重复读：又称读-写错误，事务$T_1$读取某一数据后，事务$T_2$对其做了修改，当$T_1$再次读取该数据时，得到与前次不同的值。iv）幻想读：事务$T_2$按一定条件读取了某些数据后，事务$T_1$插入（删除）了一些满足这些条件的数据，当$T_2$再次按相同条件读取数据时，发现多（少）了一些记录。 2）事务隔离级别的定义 i）未提交读：又称脏读，允许运行在该隔离级别上的事务读取当前数据页上的任何数据，而不管该数据是否已提交，解决了丢失修改问题。ii）提交读：保证运行在该隔离级别上的事务不会读取其他未提交事务所修改的数据，解决了丢失修改和脏读的问题。iii）可重复读：保证一个事务如果再次访问同一数据，与此前访问相比，数据不会发生修改，解决了丢失修改，脏读和不可重复读问题。iv）可串行化：在这个级别上的一组事务的并发执行与它们的某个串行调度是等价的，解决了并发操作带来的四个不一致问题。 4，SQL Server中的事务定义 1）事务定义模式123456789101112-- 定义BEGIN TRAN[SACTION] [事务名[WITH MARK['事务描述']]]​-- 提交1COMMIT [TRAN[SACTION] [事务名]]-- 提交2COMMIT[WORK]​-- 回滚1ROLLBACK [TRAN[SACTION] [事务名|保存点名]]-- 回滚2COMMIT[WORK] 2）事务执行模式 i）显式事务：每个事务均以 BEGIN TRANSACTION 语句显式开始，以 COMMIT 或者 ROLLBACK 语句显式结束。ii）隐性事务：每个事务无须描述事务的开始，但仍以 COMMIT 或 ROLLBACK语句显式完成。iii）自动提交事务：SQL Server 的默认事务管理模式，意指每条单独的语句都是一个事务。在完成每个 T-SQL 语句时，都被提交或者回滚。 12-- 隐性事务设置方法SET IMPLICIT_TRANSACTIONS &#123;ON|OFF&#125; 3）事务隔离级别的定义1234567set transaction isolation level read uncommitted;​set transaction isolation level read committed;​set transaction isolation level repeatable read;​set transaction isolation level serializable; 4）批处理，触发器的事务 批处理是包含一个或者多个SQL语句的组，从应用程序一次性地发送到服务器执行。服务器将批处理语句编译成一个可执行单元，此单元称为执行计划。 五，并发控制1，相关概念 事务是并发控制的基本单位，事务最基本的特征之一是隔离性。为保证事务的隔离性，系统必须对并发事务之间的相互作用加以控制，这称为并发控制。并发控制的主要技术是封锁。 2，封锁技术 1）封锁：事务$T$在对某个数据库对象操作之前，先向系统发出请求，对其加锁。最基本的封锁模式有排他锁(X锁)和共享锁(S锁)。 i）排他锁：又称写锁，如果事务$T$对数据对象$A$加上$X$锁，则只允许$T$读取和修改$A$，其他任何事务都不能再对$A$加任何类型的锁，直到$T$释放$A$上的锁。申请对$A$的排他锁可表示为$XLOCK(A)$。ii）共享锁：又称读锁，如果事务$T$对数据对象$A$加上$S$锁，则只允许$T$读取$A$但不允许修改$A$，其他事务只能再对$A$加$S$锁而不能加$X$锁，直到$T$释放$A$上的$S$锁。申请对$A$的共享锁可表示为$SLOCK(A)$。 3，事务隔离级别与封锁规则 1）封锁协议（Locking Protocol）：在运用$X$锁和$S$锁这两种基本封锁对数据对象加锁时，还需要约定规则，如何时申请$X$锁或$S$锁，持锁时间，何时释放等，这些规则称为封锁协议。 2）长锁：保持到事务结束的锁；短锁：用完就释放的锁。 4，封锁粒度（MGL） 1）封锁粒度定义：封锁对象的大小称为粒度。 i）多粒度封锁：数据库中被封锁的资源按粒度大小会呈现处一种层次关系，元组隶属于关系，关系隶属于数据库，称为粒度树。当为某结点加上意向锁（$I$锁）时，就表明某些内层结点已发生事实上的封锁，防止其他事务再去封锁该结点，这种封锁方式称为多粒度封锁（Multi Granularity Lock）。ii）意向锁：如果对一个结点加意向锁，则说明该节点的下层结点正在加锁；对任意节点加锁时，必须先对它所在的上层结点加意向锁。 意向共享锁（IS锁）：如果对一个数据对象加IS锁，表示它的后裔结点拟（意向）加S锁。 意向排他锁（IX锁）：如果对一个数据对象加IX锁，表示它的后裔结点拟（意向）加X锁。 共享意向排他锁（SIX锁）：如果对一个数据对象加SIX锁，表示对它加S锁，再加IX锁。 5，并发控制 1）SQL Server锁模式：共享锁（S锁），更新锁（U锁），排他锁（X锁），意向共享锁（IS锁）， 意向排他锁（IX锁），共享意向排他锁（SIX锁）。 2）SQL Server锁粒度：行级（Row），页面级（Page），表级（Table） 3）强制封锁类型在通常情况下，数据封锁由DBMS控制，对用户是透明的，但可以在SQL语句中加入锁定提示来强制 SQL Server 使用特定类型但锁。 六，数据库恢复技术1，恢复的概念：负责将数据库从故障所造成的错误状态中恢复到某一已知的正确状态（也称为一致性状态或者完整状态）。 2，故障的种类 1）事务故障：指事务的运行没达到预期对象终点就终止，有两种错误可能造成事务执行失败。 i）非预期故障：是指不能由应用程序处理的故障，如运算溢出，但该事务可以在以后但某个时间重新执行。ii）可预期故障：指应用程序可以发现的事务故障，并且可以控制让事务回滚。 2）系统故障：又称软故障，指在硬件故障，软件错误的影响下，导致内存中的数据丢失，并使得事务处理终止，但未破坏外存中数据库。由于硬件错误和软件漏洞致使系统终止，而不破坏外存内容但故障又称为故障-停止假设。 3）介质故障：又称硬故障，指由于磁盘的磁头碰撞，瞬时的强磁场干扰等造成磁盘的损坏，破坏外存上的数据库，并影响正在存取的这部分数据的所有事务。 4）恢复的基本原理是冗余，即数据库中任意部分的数据可以根据存储在系统别处的冗余数据来重建。一般的冗余形式：副本和日志。 3，恢复的实现技术 1）数据转储：DBA定期将整个数据库复制到磁带或者在另一个磁盘上保存起来的过程。 i）静态转储：在系统中无运行事务时进行的转储操作，即转储操作开始的时刻，数据库处于一致性状态，而转储期间不允许对数据库的任何存取，修改活动。ii）动态转储：指在转储期间允许对数据库进行存取或修改，即转储与用户事务可以并发执行。iii）全量转储：每次转储全部数据库。iv）增量转储：每次只转储上一次转储后更新过的数据。 2）登记日志文件 日志是以事务为单位记录数据库每次更新活动的文件，由系统自动记录。遵循以下原则： i）登记的次序严格按并发事务执行的时间次序；ii）必须先写日志文件，然后写数据库。 前像：要撤消事务，日志中必须包含数据库发生变化前的所有记录的备份，这些记录称为前像(Before-Images)。 后像：为了恢复事务，日志中必须包含数据库改变之后的所有记录的备份，这些记录称为后像(After-Images)。 3）基本日志结构 日志是日志记录(Log Records)的序列，主要包含： （1）事务开始标识，如&lt;$T_i$ start&gt;；（2）更新日志记录，描述一次数据库写操作，如&lt;$T_i,X_i,V_1,V_2$&gt; 事务标识$T_i$是执行WRITE操作的事务的唯一标识；数据项标识$X_i$是所写数据项的唯一标识，通常是数据项在磁盘上的位置；更新前数据的旧值$V_1$ (对插入操作而言此项为空值)； 更新后数据的新值$V_2$ (对删除操作而言此项为空值)。（3）事务结束标识&lt;$T_i$ COMMIT&gt;，表示事务$T_i$提交；&lt;$T_i$ ABORT&gt;，表示事务$T_i$中止。 4，SQL Server基于日志的恢复策略 1）事务分类 i）圆满事务：指日志文件中记录了事务的COMMIT标识，说明日志中已经完整地记录下事务所有的更新活动。ii）夭折事务：指日志文件只有事务的开始标识，而无COMMIT标识，说明对事务更新活动的记录是不完整的，无法根据日志来重现事务。 2）基本的恢复操作 i）重做：对圆满事务所做过的修改操作应执行REDO操作，即重新执行该操作，修改对象赋予其新记录值，这种方法称为前滚。ii）撤销：对夭折事务所做过的修改操作应执行UNDO操作，即撤销该操作，修改对象赋予其旧记录值，这种方法又称为回滚。 3）事务故障恢复 i）反向扫描日志文件，查找该事务的更新操作；ii）对该事务的更新操作执行逆操作，即将事务更新前的旧值写入数据库；iii）继续反向扫描日志文件，查找其他事务的其他更新操作，做同样处理；iv）如此处理下去，直至读到该事务的开始标识，事务恢复故障完成。 4）系统故障恢复 i）正向扫描日志文件，找出圆满事务，将其事务标识记入重做队列；找出夭折事务，将其事务标识记入撤销队列；ii）对撤销队列中的各个事务进行撤销处理，即反向扫描日志文件，对每个撤销事务对更新操作执行逆操作；iii）对重做队列中的各个事务进行重做处理，即正向扫描日志文件，对每个重做事务重新执行日志文件登记的操作。 5）介质故障恢复 i）装入最新的数据库后备副本，将数据库恢复到最近一次转储时的一致性状态；ii）装入相应的日志文件副本，重做已完成的事务。 5，SQL Server的备份与恢复 1）SQL Server的备份 i）数据库备份：即完全备份。 设置简单恢复模式： 1alter database master set recovery simple; 执行完全备份： 12345use master;go​-- init:如果已存在bak文件，则首先删除，后执行backup database master to disk='E:\\mater_full.bak' with init; ii）差异备份：只存储上一次完备之后发生改变的数据。 12345use master;go​-- init:如果已存在bak文件，则首先删除，后执行backup database master to disk='E:\\mater_diff.bak' with init, differential; iii）日志备份：数据库的恢复模式必须设为完整恢复模式并且必须在数据更改为完整恢复模式后至少执行一次完整数据库备份。 设置完整恢复模式： 1alter database master set recovery full; 执行日志备份： 1234use master;go​backup log master to disk='E:\\master_log.bak' 2）SQL Server恢复 i）使用SQL Server管理工具还原数据库ii）使用T-SQL语句从简单备份策略还原数据库1restore database master from disk='E:\\master_full.bak'; iii）使用T-SQL语句还原差异备份12restore database master from disk='E:\\master_full.bak' with norecovery;restore database master from disk='E:\\master_diff.bak'; iv）使用T-SQL语句从完整备份策略还原数据库123restore database master from disk='E:\\master_full.bak' with replace, norecovery;restore log master from disk='E:\\master_log1.bak' with norecovery;restore log master from disk='E:\\master_log2.bak';","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库理论","slug":"db-theory","permalink":"https://zhangbc.github.io/tags/db-theory/"}]},{"title":"【数据库理论】数据库的设计与实施","slug":"【数据库理论】数据库的设计与实施","date":"2019-04-14T10:44:12.000Z","updated":"2019-04-14T10:46:42.548Z","comments":true,"path":"2019/04/14/db_design/","link":"","permalink":"https://zhangbc.github.io/2019/04/14/db_design/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 一，数据库设计概述数据库技术是信息资源开发，管理和服务的最有效手段，从小型 单项事务处理系统到大型的信息系统都利用了数据库技术来保证系统数据的整体性，完整性和共享性。 1，数据库设计的内容与特点 1）数据库设计包括结构特性设计与行为特性设计。 结构特性设计即数据库框架和数据库结构设计，其结果是得到一个合理的数据模型，以反映真实的事务间的联系，目的是汇总各用户的视图，尽量减少冗余，实现数据共享。结构特性是静态的。 行为特性设计是指应用程序设计，如查询，报表处理等，结构特性必须适应行为特性。 2，数据库设计方法 1）新奥尔良方法：将数据库设计分为需求分析，概念设计，逻辑设计，物理设计。 2）从本质上，规范设计法的基本思想是“反复探寻，逐步求精”。 3，数据库设计的步骤 1）数据库设计的过程：三大阶段六步骤，即：数据库规范设计，需求分析，概念结构设计，逻辑结构设计，物理结构设计，数据库实施与维护（总体规划阶段，系统开发设计阶段，系统运行与维护阶段）。 2）六个步骤： （1）数据规划设计：明确数据库建设的总体目标和技术路线，得出数据库设计项目的可行性分析报告，对数据库设计的进度和人员分工做出安排。（2）需求分析：准确弄清用户要求是数据库设计的基础。（3）概念结构设计：数据库逻辑结构依赖于具体的DBMS。概念结构是各用户关心的系统信息结构，是对现实世界的第一层抽象。（4）逻辑结构设计：使概念结构转换为某个DBMS所支持的数据模型，并进行优化。（5）物理结构设计：设计目标是从一个满足用户要求的已确定的逻辑模型出发，设计一个在限定的软件，硬件条件和应用环境下可实现的，运行效率高的数据库结构。（6）数据库实施与维护 二，数据库规划1，系统调查：搞清楚企业的组织层次，得到企业的组织结构图。 2，可行性分析：分析数据库建设是否具有可行性，即从经济，法律，技术等多方面进行可行性论证分析，在此基础上得到可行性报告。 3，数据库建设的总体目标和数据库建设的实施总安排 三，需求分析1，需求分析的任务 1）需求分析的任务：通过详细调查现实世界要处理的对象，充分了解原系统工作概况，明确各用户需求，在此基础上确定新的功能。 2）需求分析的重点：调查，搜集用户在数据管理中的信息要求，处理要求，安全性与完整性要求。 信息要求是指用户需要从数据库中获取信息的内容和性质，由用户的信息要求可以导出数据要求，即在数据库中需要存储哪些数据。 处理要求是指用户要求完成什么样的处理功能，对处理的响应时间有什么要求，处理方式是批处理还是联机处理。 安全性要求是指保护数据不被未授权的用户破坏。 完整性要求是指保护数据不被授权的用户破坏。 2，需求分析的方法 1）常用的调查方法 （1）跟班作业（2）开调查会（3）查阅档案资料（4）询问（5）设计调查用表并请用户填写 3，需求分析的步骤 1）分析用户的活动 2）确定系统的边界 3）分析用户活动所设计的数据数据流图（DFD）是描述各处理活动之间数据流动的有力工具，是一种从数据流的角度描述一个组织业务活动的图示。 4）分析系统数据 数据字典（DD）是描述每个数据流，每个文件，每个加工的集合，是对数据流图中出现的所有数据元素给出逻辑定义和描述。 数据字典包括数据项，数据文件，数据流，数据加工处理。 数据项描述={数据项名，别名，数据项含义，数据类型，字节长度，取值范围，取值含义，与其他数据项的逻辑关系} 数据文件描述={数据文件名，所有数据项名，数据存取频度，存取方式} 数据流描述={数据流名称，所有数据项名，数据流来源，数据流去向，平均流量，峰值流量} 数据加工处理描述={加工处理名，说明，输入的数据流名，输出的数据流名，处理要求} 四，概念结构设计概念结构设计阶段是将用户需求抽象为信息结构（概念模型）的过程。 1，局部E-R图的设计 2，全局E-R图的设计 五，逻辑结构设计逻辑模式设计的主要目标是产生一个具体DBMS可处理的数据模型和数据库模式，即把概念设计阶段的全局E-R图转换成DBMS支持的数据模型。一般步骤： （1）将概念结构转换为一般的关系模型，网状模型或层次模型。（2）将转换来的关系模型，网状模型，层次模型向DBMS支持的数据模型转换，变成合适的数据库模式。（3）对模式进行挑战和优化。 六，物理结构设计1）在进行数据库的物理结构设计时，首先确定数据库的物理结构，然后是对所设计的物理结构设计进行评价。2）物理结构设计的重要目标是满足主要应用的性能要求。3）就RDBMS而言，物理结构设计主要内容有：为关系模式选取存取方法，设计关系，索引等数据库文件的物理存储结构。","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库理论","slug":"db-theory","permalink":"https://zhangbc.github.io/tags/db-theory/"}]},{"title":"【数据库理论】关系模式的规范化与查询优化","slug":"【数据库理论】关系模式的规范化与查询优化","date":"2019-04-13T14:31:38.000Z","updated":"2019-04-14T06:10:16.477Z","comments":true,"path":"2019/04/13/db_query_opt/","link":"","permalink":"https://zhangbc.github.io/2019/04/13/db_query_opt/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 一，问题的提出1，关系模式 关系模式定义：一个关系模式是一个系统，它是有一个五元组$R(U, D, DOM, I, F)$组成，其中，$R$为关系名，$U$是$R$的一组属性集合$\\{ A_1,A_2,A_3,\\dots,A_n \\}$，$D$是$U$中属性的域集合$\\{ D_1,D_2,D_3,\\dots,D_n \\}$，$DOM$为属性$U$到域$D$的映射，$I$为完整性约束集合，$F$为属性间的函数依赖关系。 2，关系 1）关系定义：在关系模式$R( U, D, DOM, I, F )$中，当且仅当$U$上的一个关系$r$足$F$时，$r$称为关系模式$R$的一个关系，记作$R(U)$或$R(U,F)$。 2）关系数据库对关系有一个最起码的要求： 每个属性必须是不可分割的数据项。满足列这个条件的关系模式就属于第一范式（1NF）。 3）数据依赖：通过一个关系中属性间值的相等与否以体现数据间的相互关系，是现实世界属性间相互联系的抽象，是数据内在的性质，是语义的体现。主要有：函数依赖FD，多值依赖MVD。 3，插入异常：表示数据插入时出现问题，即无法在缺少另一个实体实例或者关系实例的情况下表示实体或者实体的信息。 4，删除异常：删除表的某一行来反映某个实体实例或者关系实例消失，会导致丢失另一个不同实例实体或者关系实例的信息。 5，更新异常：更改表所对应的某个实体实例或者关系实例的单个属性，会将多行的相关信息全部更新。 二，关系模式的函数依赖1，函数依赖（FD） 1）函数依赖（FD）定义：设$R(U)$是属性集$U$上的关系模式，$X$，$Y \\subseteq U$。若对$R(U)$的任意一个可能的关系$r$，$r$中有任意两个元组$t_1$和$t_2$，如果$t_1 [ X ] = t_2 [ X ]$，有$t_1[ Y ] = t_2[ Y ]$，则称$X$函数确定$Y$，或者说$Y$函数依赖$X$，记为$X \\rightarrow Y$。 （1）如果$X \\rightarrow Y$，但是$Y \\nsubseteq X$，则称$X \\rightarrow Y$是非平凡的函数依赖；（2）如果$X \\rightarrow Y$，但是$Y \\subseteq X$，则称$X \\rightarrow Y$是平凡的函数依赖；（3）如果$X \\rightarrow Y$，则$X$为这个函数依赖的决定属性集(Determinant)；（4）如果$X \\rightarrow Y$，$Y \\rightarrow X$，则记为$X \\longleftrightarrow Y$；（5） 如果$Y$不函数依赖于$X$，则记为$X \\nrightarrow Y$。 2）完全函数依赖和部分函数依赖设$R(U)$是属性集$U$上的关系模式，如果$X \\rightarrow Y$，并且对于$X$的任何一个真子集$Z$，都有$Z \\nrightarrow Y$，则称$Y$完全依赖于$X$，记$X \\stackrel{f}{\\rightarrow} Y$；若$X \\rightarrow Y$，但$Y$不完全函数依赖于$X$，则称$Y$部分函数依赖于$X$，记$X \\stackrel{p}{\\rightarrow} Y$。 3）传递函数依赖设$R(U)$是属性集$U$上的关系模式，$X \\subseteq U$，$Y \\subseteq U$，$Z \\subseteq U$，$Z - X$，$Z - Y$，$Y - X$均非空，如果$X \\rightarrow Y(Y \\nsubseteq X)$，$Y \\nrightarrow X$，$Y \\rightarrow Z$，则称$Z$传递依赖于$X$。 2，键 1）候选键：设$R(U)$是属性集$U$上的关系模式，$K \\subseteq U$，如果$K \\stackrel{f}{\\rightarrow} U$，则$K$为$R$的候选键。候选键包含了关系模式的所有属性，称为全键。2）主属性：包含在任意一个候选键中的属性称为主属性。3）非主属性：不包含在任意候选键中的属性称为非主属性或非键属性。4）外键：在关系模式$R$中属性或者属性组$X$并非$R$的候选键，但$X$是另一个关系模式的候选键，则称$X$是$R$的外部键，也称外键。 3，函数依赖的逻辑蕴含 1）阿姆斯特朗公理体系 （1）包含规则：设$R(U)$是属性集$U$上的关系模式，$X \\subseteq U$，$Y \\subseteq U$，且$Y \\subseteq X$，则$X \\rightarrow Y$。 （2）平凡依赖：由包含规则得到的函数依赖都是平凡函数依赖。 （3）逻辑蕴含：设$R(U)$是属性集$U$上的关系模式，$F$是$R$上函数依赖集合，如果$R$的任意关系实例$r$使$F$成立的，函数依赖$X \\rightarrow Y$均成立，则称$F$逻辑蕴含$X \\rightarrow Y$。 （4）阿姆斯特朗公理：设$R$是一个具有属性集合$U$的关系模式，$F$是$R$的一个函数依赖集合，$X \\subseteq U$，$Y \\subseteq U$，$Z \\subseteq U$。包含如下规则： i）包含规则：又称自反律，如果$Y \\subseteq X \\subseteq Z$，则$X \\rightarrow Y$为$F$所蕴含；ii）传递规则：如果$F$蕴含$X \\rightarrow Y$，$Y \\rightarrow Z$，则$X \\rightarrow Z$为$F$所蕴含；iii）增广规则：如果$F$蕴含$X \\rightarrow Y$，且$Z \\subseteq U$，则$XZ \\rightarrow YZ$为$F$所蕴含。 阿姆斯特朗公理包含蕴含规则如下： i）合并规则：如果$X \\rightarrow Y$，$X \\rightarrow Z$，则$X \\rightarrow YZ$；ii）伪传递规则：如果$X \\rightarrow Y$，$WY \\rightarrow Z$，则$WX \\rightarrow Z$；iii）分解规则：如果$X \\rightarrow Y$，$Z \\subseteq Y$，则$X \\rightarrow Z$；iv）集合累积规则：如果$X \\rightarrow YZ$且$Z \\rightarrow W$，则$X \\rightarrow YZW$。 【引理4-1】 $X \\rightarrow A_1A_2A_3 \\dots A_n$ 成立的充分必要条件是$X \\rightarrow A_i（i=1，2，\\dots，n）$成立。 2）闭包，覆盖和最小覆盖 （1）函数依赖的闭包：设$R$是一个具有属性集合$U$的关系模式，$F$是给定的函数依赖的集合，由$F$推导出的所有函数依赖的集合称为$F$的闭包，记作$F^{+}$。 （2）函数依赖集的覆盖：$R$表上的两个函数依赖集合$F$和$G$，如果函数依赖集$G$可以从$F$用蕴含规则推导出来，换言之，如果 $G \\subset F^{+}$，则称$F$ 覆盖 $G$，如果$F$ 覆盖 $G$ 且 $G$ 覆盖 $F$，则称这两个函数依赖集等价，记作$F \\equiv G$。 （3）属性集的闭包：设$R$是一个具有属性集合$U$的关系模式，$F$是$R$上的函数依赖集，$X \\subseteq U$，定义$X$的闭包 $X^+$，作为$X$函数决定的最大属性集$Y$，则最大属性集满足 $X \\rightarrow Y$ 存在于 $F^+$ 中。 【算法4-1】 属性集 $X$ 的闭包 $X^+$ 的迭代算法： i）选$X$作为闭包$X^+$的初始值$X[0]$；ii）由$X[i]$计算$X[i+1]$时，它是由 $X[i]$ 并上属性集合$A$所组成的，其中$A$为$F$中存在的函数依赖 $Y \\rightarrow Z$，而$A \\subseteq Z$，$Y \\subseteq X[i]$。因为$U$是有穷的所以上述过程经过有限步后会达到$X[i] = X[i+1]$，此时$X[i]$为所求的$X^+$。 （4）最小覆盖 i）$F$中任意函数依赖的右部只包含一个属性；ii）不存在这样的函数依赖 $X \\rightarrow A$，使得$F$与 $F- \\{ X \\rightarrow A \\}$ 等价；iii）不存在这样的函数依赖$X \\rightarrow A$，$X$包含真子集$Z(Z \\subset X)$，使得 $(F-\\{ X \\rightarrow A \\} \\cup \\{ Z \\rightarrow A \\})$与$F$等价。如果$F$满足上述条件，则函数依赖$F$称为极小或者最小函数依赖集。 （5）最小覆盖集算法 从函数依赖集$F$构造最小覆盖$M$的算法如下： i）从函数依赖集$F$，创建函数依赖的一个等价集$H$，它的函数依赖的右边只有单个属性（使用分解规则）ii）从函数依赖集$H$，顺次去掉在$H$中非关键的单个函数依赖。一个函数依赖$X \\rightarrow Y$在一个函数依赖集中是非关键的，指如果 $X \\rightarrow Y$ 从$H$中去掉，得到结果$J$，仍然满足$H^+ = J^+$，或者说$H \\equiv J$。iii）从函数依赖集$J$，顺次用左边具有更少属性的函数依赖替换原来的函数依赖，只要不会导致$J^+$改变。iv）从剩下的函数依赖集中收集所有左边相同的函数依赖，使用合并规则创建一个等价的函数依赖集$M$，它的所有依赖的左边是唯一的。 （6）每个函数依赖集$F$都等价于一个极小函数依赖集。 三，关系模式的规范化规范化定义：把一个给定规范模式转化为某种范式的过程称为关系模式的规范过程，简称规范化。 1，第一范式 【定义4-1】 设$R$是一个关系模式，如果$R$的每个属性的值域都是不可分割的简单数据项的集合，则这个模式称为第一范式关系模式，记作1NF。 2，第二范式 【定义4-2】 如果关系模式$R$是第一范式，而且每个非主属性都完全函数依赖于$R$的键，则$R$称为第二范式的关系模式，记作2NF。 3，第三范式 【定义4-3】 设关系模式$R$满足2NF，而且它的任意一个非键属性都不传递依赖于任何候选键，则$R$称为第三范式的关系模式，记作3NF。 4，BCNF 【定义4-4】 设关系模式$R$满足1NF，如果对$R$的每个函数依赖 $X \\rightarrow Y$ 且 $Y \\nsubseteq X$，$X$必为候选键，则$R$满足BCNF，即：在关系模式 $R(U,F)$ 中，如果每个决定因素都包含键，则$R(U,F) \\in BCNF$。 一个满足BCNF的关系模式有如下 条件： i）所有非键属性对每个键都是完全函数依赖；ii）所有的键属性对每个不包含它的键，也是完全函数依赖；iii）没有任何属性完全函数依赖于非键属性的任意一组属性。 5，多值依赖与第四范式 1）多值依赖 （1）多值依赖定义：设 $R(U)$ 是属性集$U$上的一个关系模式，$X$，$Y$，$Z$是$U$的子集，并且 $Z=U-X-Y$，关系模式$R(U)$中多值依赖 $X \\rightarrow \\rightarrow Y$ 成立，当且仅当对$R(U)$的任意关系$r$，给定的一对 $(x，z)$ 值，有一组$Y$的值，这组值仅仅取决于$x$值，而与$z$值无关。 （2）如果 $X \\rightarrow \\rightarrow Y$，而 $Z=\\varnothing$，即$Z$为空，则 $X \\rightarrow \\rightarrow Y$ 称为平凡的多值依赖。 （3）多值依赖的公理（设$U$是一个关系模式的属性集，$X，Y，Z，W，V$都是集合$U$的子集。） i）对称性规则：如果 $X \\rightarrow \\rightarrow Y$，则 $X \\rightarrow \\rightarrow U-X-Y$；ii）传递性规则：如果 $X \\rightarrow \\rightarrow Y$，$Y \\rightarrow \\rightarrow Z$，则 $X \\rightarrow \\rightarrow Z-Y$；iii）增广规则：如果 $X \\rightarrow \\rightarrow Y$，$V \\subseteq W$，则 $WX \\rightarrow \\rightarrow VY$；iv）替代规则：如果 $X \\rightarrow Y$，则$X \\rightarrow \\rightarrow Y$；v）聚集规则：如果$X \\rightarrow \\rightarrow Y$，$Z \\subseteq Y$，$W \\cap Z = \\varnothing$，$W \\rightarrow Z$，则 $X \\rightarrow Z$。 （4）多值依赖的推导规则（设$U$是一个关系模式的属性集，$X，Y，Z，W，V$都是集合$U$的子集。） i）合并规则：如果 $X \\rightarrow \\rightarrow Y$， $X \\rightarrow \\rightarrow Z$，则 $X \\rightarrow \\rightarrow YZ$；ii）分解规则：如果 $X \\rightarrow \\rightarrow Y$， $X \\rightarrow \\rightarrow Z$，则 $X \\rightarrow \\rightarrow Y \\cap Z$，$X \\rightarrow \\rightarrow Y-Z$，$X \\rightarrow \\rightarrow Z-Y$；iii）伪传递规则：如果 $X \\rightarrow \\rightarrow Y$，$WY \\rightarrow \\rightarrow Z$，则 $WX \\rightarrow \\rightarrow (Z-WY)$；iv）混合伪传递规则：如果 $X \\rightarrow \\rightarrow Y$，$XY \\rightarrow Z$，则 $X \\rightarrow (Z-Y)$。 （5）在$R(U)$上，如果有 $X \\rightarrow \\rightarrow Y$ 在 $W(W \\subseteq U)$ 上成立，则 $X \\rightarrow \\rightarrow Y$ 称为$R(U)$的嵌入型多值依赖。 2）第四范式 【定义4-5】 设关系模式 $R(U,F) \\in 1NF$，$F$是$R$上的多值依赖集，如果$R$的每个非平凡多值依赖 $X \\rightarrow \\rightarrow Y$（$Y-X \\ne \\varnothing$，$XY$未包含$R$的全部属性），$X$都含有$R$的候选键，则称$R$是第四范式，记为4NF。 6，各范式之间的关系 1）各范式之间的关系 4NF \\subset BCNF \\subset 3NF \\subset 2NF \\subset 1NF2）各范式小结 i）$4NF$：限制关系模式的属性之间不允许有非平凡且非函数依赖的多值依赖；ii）$3NF \\rightarrow BCNF$：消除主属性对候选关键字的部分和传递函数依赖；iii）$2NF \\rightarrow 3NF$：消除非主属性对候选关键字的传递函数依赖；iv）$1NF \\rightarrow 2NF$：消除非主属性对候选关键字的部分函数依赖。 四，关系模式的分解特性1，关系模式的分解 1）关系模式的分解定义：把一个关系模式分解成若干个关系模式的过程，称为关系模式的分解。 【定义4-6】 关系模式$R(U, F)$ 的分解是指$R$为它的一组子集 $\\rho=\\{R_1(U_1, F_1),R_2(U_2, F_2),\\dots,R_k(U_k, F_k)\\}$ 所代替的过程。其中，$U=U_1 \\cup U_2 \\dots \\cup U_k$，并且设 $U_i \\subseteq U_j(1 \\le i,j \\le k)$，$F_i$是$F$在$U$上的投影，即 $F_i = \\{X \\rightarrow Y \\in F^+ \\wedge XY \\subseteq U_i \\}$。 分解后表的连接丢失或者多余元组的分解称为有损分解，或称有损连接分解。 2）关系模式的分解遵守原则： i）无损连接性：信息不失真（不增减信息）；ii）函数依赖保持性：不破坏属性间存在的依赖关系。 2，分解的无损连接性 1）无损连接的概念 【定义4-7】 设$F$是关系模式$R$的函数依赖集，$\\rho=\\{R_1(U_1, F_1),R_2(U_2, F_2),\\dots,R_k(U_k, F_k)\\}$ 是$R$ 的一个分解，$r$是$R$的一个关系，定义 m_{\\rho}(r)=\\pi_{U_{1}}(r) \\infty \\pi_{U_{2}}(r) \\infty \\cdots \\infty_{\\pi_{U_{k}}}(r)如果$R$满足$F$的任意关系$r$均有则$r=m_\\rho(r)$，则称分解 $\\rho$ 具有无损连接性。 【引理4-2】 设 $\\rho=\\{R_1(U_1, F_1),R_2(U_2, F_2),\\dots,R_k(U_k, F_k)\\}$ 为关系模式$R$的一个分解， $r$是$R$的任一个关系，有 (1) $r \\subseteq m_\\rho(r)$；(2) 如果 $s = m_\\rho(r)$， 则 $\\pi_{U_{i}}(r) = \\pi_{U_{i}}(s)$；(3) $m_\\rho(m_\\rho(r)) = m_\\rho(r)$。 2）进行关系分解的必要性 一个关系模式分解后，可以存放原来所不能存放的信息，通常称为“悬挂”的元组，这是实际所需要的，也是分解的优点。 3）无损连接判定方法 【算法4-2】 (矩阵法)判别一个分解的无损连接性的算法。 (1) 构造初始表：构造一个$k$行$n$列的初始表，其中，每列对应于$R$的一个属性，每行用于表示分解后的一个模式组成。如果属性$A_j$属于关系模式$R_i$，则表示在表的第$i$行第$j$列置符号$a_j$，否则置符号$b_{ij}$。(2) 根据$F$中的函数依赖修改表的内容：考察$F$中的每个函数依赖 $X \\rightarrow Y$，在属性组$X$所在的那些列上寻找具有相同符号的行，如果找到这样的两行或者更多行，则修改这些行，使这些行上属性组$Y$所在的列上元素相同。修改规则是：如果$Y$所在的要修改的行有一个为$a_j$，则这些元素均变成$a_j$；否则改为$b_{mj}$（其中$m$为这些行的最小行号）。(3) 判断分解是否为无损连接：如果通过修改，发现表中有一行变为 $a_1,a_2,\\dots,a_n$，则分解是无损连接的，否则分解不具有无损连接性。 【定理4-1】 （定理法）设 $\\rho = \\{ R_1,R_2 \\}$ 是关系模式$R$的一个分解，$F$是$R$的函数依赖集，$U_1$，$U_2$和$U$分别是$R_1$，$R_2$和$R$的属性集合，那么$\\rho$是$R$(关于$F$)的无损分解的充分必要条件为 (U_1\\cap U_2) \\rightarrow U_1 - U_2 \\in F^+或者 (U_1\\cap U_2) \\rightarrow U_2 - U_1 \\in F^+【定理4-2】 （逐步分解定理）设$F$是关系模式$R$的函数依赖集，$\\rho = \\{ R_1,R_2,\\dots,R_k \\}$ 是$R$关于$F$的一个无损连接。 （1）如果 $\\sigma = \\{ S_1,S_2,\\dots,S_m \\}$ 是 $R_i$ 关于 $F_i$ 的一个无损连接分解，则 $\\varepsilon = \\{ R_1,R_2,\\dots,R_{i-1},S_1,S_2,\\dots,S_m,R_{i+1},\\dots,R_k \\}$ 是$R$关于$F$的无损连接分解，其中，$F_i = \\pi_{R_i}(F)$。（2）设 $\\tau = \\{ R_1,\\dots,R_{k},R_{k+1},\\dots,R_n \\}$ 是 $R$ 的一个分解，其中，$\\tau \\supseteq \\rho$，$\\tau$ 也是$R$关于$F$的无损连接分解。 3）分解的函数依赖保持性 【定义4-8】 设$F$是关系模式$R$的函数依赖集， $\\rho=\\{R_1(U_1, F_1),R_2(U_2, F_2),\\dots,R_k(U_k, F_k)\\}$ 为$R$的一个分解，如果 $F_i = \\pi_{R_i}(F)(i=1,2,\\dots,k)$ 的并集 $(F_1 \\cup F_2 \\cup \\dots \\cup F_k)^+ \\equiv F^+$，则称分解 $\\rho$ 具有函数依赖保持性。 3，关系模式分解算法 1）分解的基本要求：分解后的关系模式与分解前的关系模式等价，即分解必须具有无损连接和函数依赖保持性。 2）分解算法的结论 i）如果要求分解具有无损连接性，则分解一定可以达到BCNF； ii）如果要求分解保持函数依赖，则分解可以达到3NF，但不一定能够达到BCNF； iii）如果要求分解既具有无损连接性，又保持函数依赖，则分解可以达到3NF，但不一定能够达到BCNF。 五，关系模式的优化1，水平分解 1）水平分解的定义：水平分解是把关系元组分为若干个子集合，每个子集合定义为一个子关系，以提高系统效率的过程。 2）水平分解的规则： （1）根据“80%与20%原则”，在一个大型关系中，经常使用的数据只是很有限的一部分，可以把经常使用的数据分解出来形成一个子关系。（2）如果关系$R$上具有$n$个事务，而且多数事务存取的数据不相交，则$R$可分解为不大于$n$个子关系，使每个事务存取的数据形成一个关系。 2，垂直分解 1）垂直分解的定义设 $R(A_1,A_2,\\dots,A_k)$ 是关系模式，$R$的一个垂直分解是$n$个关系的集合$\\{ R_1(B_1,B_2,\\dots,B_v),\\dots,R_n(D_1,D_2,\\dots,D_m) \\}$，其中，$\\{B_1,B_2,\\dots,B_v \\},\\dots,\\{ D_1,D_2,\\dots,D_m \\}$ 是 $\\{ A_1,A_2,\\dots,A_k \\}$ 的子集合。 2）垂直分解的基本原则：经常在一起使用的属性从$R$中分解出来形成一个独立的关系。 六，关系查询优化1，关系系统及其查询优化 1）查询优化工作： i）关系数据库内部提供的优化机制；ii）用户通过改变查询的运算次序和建立索引等机制进行优化。 2）关系数据库查询优化的总目标：选择有效的策略，快速求得给定关系表达式的值，以减少查询执行的总开销。 3）查询执行的开销计算 i）在集中式数据库中，总代价=I/O代价+CPU代价ii）在多用户环境下，总代价=I/O代价+CPU代价+内存代价 2，查询优化的一般策略 1）尽量先执行选择运算2）在执行连接前对关系适当地预处理： i）索引连接；ii）排序合并连接 3，关系代数等价变换规则 1）连接，笛卡尔积交换律 E_1 \\times E_2 \\equiv E_2 \\times E_1E_{1} \\infty E_{2} \\equiv E_{2} \\infty E_{1}\\begin{array}{c}{E_{1} \\infty E_{2}} \\\\ {F}\\end{array} \\equiv \\begin{array}{c}{E_{2} \\infty E_{1}} \\\\ {F}\\end{array}2）连接，笛卡尔积结合律 (E_1 \\times E_2) \\times E_3 \\equiv E_1 \\times (E_2 \\times E_3)(E_{1} \\infty E_{2}) \\infty E_{3} \\equiv E_{1} \\infty (E_{2} \\infty E_{3})\\begin{array}{c}{(E_{1} \\infty E_{2}) \\infty E_{3}} \\\\ {F_1} \\quad \\quad {F_2} \\end{array} \\equiv \\begin{array}{c}{E_{1} \\infty (E_{2} \\infty E_{3})} \\\\ {F_1} \\quad \\quad {F_2} \\end{array}3）投影的串接定律 \\pi A_1,A_2,\\dots,A_n(\\pi B_1,B_2,\\dots.B_m(E)) = \\pi A_1,A_2,\\dots,A_n(E)4）选择的串接定律 \\sigma_{F_1}(\\sigma_{F_2}(E)) \\equiv \\sigma_{F_1 \\wedge F_2}(E)5）选择与投影的交换律 F(\\pi A_1,A_2,\\dots,A_n(E)) \\equiv \\pi A_1,A_2,\\dots,A_n(\\sigma_F(E))6）选择与笛卡尔积的交换律 \\sigma_F(E_1 \\times E_2) \\equiv \\sigma_F(E_1) \\times E_2\\sigma_F(E_1 \\times E_2) \\equiv \\sigma_{F_1}(E_1) \\times \\sigma_{F_2}(E_2) （其中F = F_1 \\wedge F_2）\\sigma_F(E_1 \\times E_2) \\equiv \\sigma_{F_2}(\\sigma_{F_1} (E_1) \\times E_2)7）选择与并运算的交换设$E = E_1 \\cup E_2$，$E_1$、$E_2$有相同的属性名，则 \\sigma_F(E_1 \\cup E_2) \\equiv \\sigma_F(E_1) \\cup \\sigma_F(E_2)8）选择与差运算的交换 \\sigma_F(E_1 - E_2) \\equiv \\sigma_F(E_1) - \\sigma_F(E_2)9）投影与笛卡尔积的交换 \\pi A_1,A_2,\\dots,A_n,B_1,B_2,\\dots,B_m(E_1 \\times E_2) \\equiv \\pi A_1,A_2,\\dots,A_n(E_1) \\cup \\pi B_1,B_2,\\dots,B_m(E_2)10）投影与并运算的交换 \\pi A_1,A_2,\\dots,A_n(E_1 \\cup E_2) \\equiv \\pi A_1,A_2,\\dots,A_n(E_1) \\cup \\pi A_1,A_2,\\dots,A_m(E_2)4，关系代数表达式的优化算法 关系系统的查询优化步骤： （1）把查询转换成某种内部表示（2）把语法树转换成标准（优化）形式（3）选择底层的存取路径（4）生成查询计划，选择代价最小的查询计划","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库理论","slug":"db-theory","permalink":"https://zhangbc.github.io/tags/db-theory/"}]},{"title":"【Java基础】Java基础100实例","slug":"【Java基础】Java基础100实例","date":"2019-04-12T14:52:13.000Z","updated":"2019-04-12T15:03:08.080Z","comments":true,"path":"2019/04/12/java_code_100/","link":"","permalink":"https://zhangbc.github.io/2019/04/12/java_code_100/","excerpt":"","text":"【学习参考资料】：菜鸟教程-Java教程 通过菜鸟教程-Java教程的初步学习，现将其教程训练代码汇聚成篇。 菜鸟教程-Java Coding学习笔记 Applet应用程序实例 文档注释演示实例 序列化和反序列化 Socket编程—服务端实例 Socket编程—客户端实例 Java进阶知识 遍历演示 Map遍历实例 泛型方法实例 泛型的有界类型参数实例 泛型类实例 类型通配符实例 发邮件(纯文本，HTML文本，附件) 图片二进制转换 JAVA8新特性实例 JAVA操作MYSQL实例 菜鸟教程-Java实例Java 环境设置实例 Java实例 – 如何编译一个Java文件？ Java实例 – Java如何运行一个编译过的类文件？ Java实例 – 如何执行指定class文件目录（classpath）？ Java实例 – 如何查看当前Java运行的版本？ 12$ javac -d . HelloWorld.java$ java com.runoob.HelloWorld Java 字符串 Java 实例 – 字符串比较 Java 实例 - 查找字符串最后一次出现的位置 Java 实例 - 删除字符串中的一个字符 Java 实例 - 字符串替换 Java 实例 - 字符串反转 Java 实例 - 字符串查找 Java 实例 - 字符串分割 Java 实例 - 字符串分割(StringTokenizer) Java 实例 - 字符串大小写转换 Java 实例 - 测试两个字符串区域是否相等 Java 实例 - 字符串性能比较测试 Java 实例 - 字符串优化 Java 实例 - 字符串格式化 Java 实例 - 连接字符串 Java 数组 Java 实例 – 数组排序及元素查找 Java 实例 – 数组添加元素 Java 实例 – 获取数组长度 Java 实例 – 数组反转 Java 实例 – 数组输出 Java 实例 – 数组获取最大和最小值 Java 实例 – 数组合并 Java 实例 – 数组填充 Java 实例 – 数组扩容 Java 实例 – 查找数组中的重复元素 Java 实例 – 删除数组元素 Java 实例 – 数组差集 Java 实例 – 数组交集 Java 实例 – 在数组中查找指定元素 Java 实例 – 判断数组是否相等 Java 实例 - 数组并集 Java 时间处理 Java 实例 - 格式化时间（SimpleDateFormat） Java 实例 - 获取当前时间 Java 实例 - 获取年份、月份等 Java 实例 - 时间戳转换成时间 Java 方法 Java 实例 – 方法重载 Java 实例 – 输出数组元素 Java 实例 – 汉诺塔算法 Java 实例 – 斐波那契数列 Java 实例 – 阶乘 Java 实例 – 方法覆盖 Java 实例 – instanceOf 关键字用法 Java 实例 – break 关键字用法 Java 实例 – continue 关键字用法 Java 实例 – 标签(Label) Java 实例 – enum 和 switch 语句使用 Java 实例 – Enum（枚举）构造函数及方法的使用 Java 实例 – for 和 foreach循环使用 Java 实例 – Varargs 可变参数使用 Java 实例 – 重载(overloading)方法中使用 Varargs 打印图形 Java 实例 – 打印菱形 Java 实例 – 九九乘法表 Java 实例 – 打印三角形 Java 实例 – 打印倒立的三角形 Java 实例 – 打印平行四边形 Java 实例 – 打印矩形 Java 文件操作 Java 实例 - 文件写入 Java 实例 - 读取文件内容 Java 实例 - 删除文件 Java 实例 - 将文件内容复制到另一个文件 Java 实例 - 向文件中追加数据 Java 实例 - 创建临时文件 Java 实例 - 修改文件最后的修改日期 Java 实例 - 获取文件大小 Java 实例 - 文件重命名 Java 实例 - 设置文件只读 Java 实例 - 检测文件是否存在 Java 实例 - 在指定目录中创建文件 Java 实例 - 获取文件修改时间 Java 实例 - 创建文件 Java 实例 - 文件路径比较 Java 目录操作 Java 实例 - 递归创建目录 Java 实例 - 删除目录 Java 实例 - 判断目录是否为空 Java 实例 - 判断文件是否隐藏 Java 实例 - 获取目录大小 Java 实例 - 在指定目录中查找文件 Java 实例 - 获取文件的上级目录 Java 实例 - 获取目录最后修改时间 Java 实例 - 打印目录结构 Java 实例 - 遍历指定目录下的所有目录 Java 实例 - 遍历指定目录下的所有文件 Java 实例 - 在指定目录中查找文件 Java 实例 - 遍历系统根目录 Java 实例 - 查看当前工作目录 Java 实例 - 遍历目录 Java 异常处理 Java 实例 - 异常处理方法 Java 实例 - 多个异常处理（多个catch） Java 实例 - Finally的用法 Java 实例 - 使用 catch 处理异常 Java 实例 - 多线程异常处理 Java 实例 - 获取异常的堆栈信息 Java 实例 - 重载方法异常处理 Java 实例 - 链试异常 Java 实例 - 自定义异常 Java 数据结构 Java 实例 – 数字求和运算 Java 实例 – 利用堆栈将中缀表达式转换成后缀 Java 实例 – 在链表（LinkedList）的开头和结 Java 实例 – 获取链表（LinkedList）的第一个 Java 实例 – 删除链表中的元素 Java 实例 – 获取链表的元素 Java 实例 – 获取向量元素的索引值 Java 实例 – 栈的实现 Java 实例 – 链表元素查找 Java 实例 – 压栈出栈的方法实现字符串反转 Java 实例 – 队列（Queue）用法 Java 实例 – 获取向量的最大元素 Java 实例 – 链表修改 Java 实例 – 旋转向量 Java 集合 Java 实例 – 数组转集合 Java 实例 – 集合比较 Java 实例 – HashMap遍历 Java 实例 – 集合长度 Java 实例 – 集合打乱顺序 Java 实例 – 集合遍历 Java 实例 – 集合反转 Java 实例 – 删除集合中指定元素 Java 实例 – 只读集合 Java 实例 – 集合输出 Java 实例 – 集合转数组 Java 实例 – List 循环移动元素 Java 实例 – 查找 List 中的最大最小值 Java 实例 – 遍历 HashTable 的键值 Java 实例 – 使用 Enumeration 遍历 HashTable Java 实例 – 集合中添加不同类型元素 Java 实例 – List 元素替换 Java 实例 – List 截取 Java 网络实例 Java 实例 – 获取指定主机的IP地址 Java 实例 – 查看端口是否已使用 Java 实例 – 获取本机ip地址及主机名 Java 实例 – 获取远程文件大小 Java 实例 – Socket 实现多线程服务器程序 Java 实例 – 查看主机指定文件的最后修改时间 Java 实例 – 使用 Socket 连接到指定主机 Java 实例 – 网页抓取 Java 实例 – 获取 URL响应头的日期信息 Java 实例 – 获取 URL 响应头信息 Java 实例 – 解析 URL Java 实例 – ServerSocket 和 Socket 通信实例 Java 线程 Java 实例 – 查看线程是否存活 Java 实例 – 获取当前线程名称 Java 实例 – 状态监测 Java 实例 – 线程优先级设置 Java 实例 – 死锁及解决方法 Java 实例 – 获取线程id Java 实例 – 线程挂起 Java 实例 – 终止线程 Java 实例 – 生产者/消费者问题 Java 实例 – 获取线程状态 Java 实例 – 获取所有线程 Java 实例 – 查看线程优先级 Java 实例 – 中断线程 github代码送门: java_projects/runoob 腾讯云coding代码送门: java_project/dev","categories":[{"name":"Java","slug":"java","permalink":"https://zhangbc.github.io/categories/java/"}],"tags":[{"name":"Java基础","slug":"java-basic","permalink":"https://zhangbc.github.io/tags/java-basic/"}]},{"title":"【Java基础】Java扩展知识","slug":"【Java基础】Java扩展知识","date":"2019-04-12T14:19:34.000Z","updated":"2019-04-12T14:24:20.104Z","comments":true,"path":"2019/04/12/java_extend_knowledge/","link":"","permalink":"https://zhangbc.github.io/2019/04/12/java_extend_knowledge/","excerpt":"","text":"【学习参考资料】：菜鸟教程-Java教程 1，Java文档注释1）Java支持三种注释方式，分别是//、/* */、/** */(说明注释)。 2）javadoc标签 标签 描述 示例 @author 标识一个类的作者 @author description @deprecated 指名一个过期的类或成员 @deprecated description {@docRoot} 指明当前文档根目录的路径 Directory Path @exception 标志一个类抛出的异常 @exception exception-name explanation {@inheritDoc} 从直接父类继承的注释 Inherits a comment from the immediate surperclass. {@link} 插入一个到另一个主题的链接 {@link name text} {@linkplain} 插入一个到另一个主题的链接，但是该链接显示纯文本字体 Inserts an in-line link to another topic. @param 说明一个方法的参数 @param parameter-name explanation @return 说明返回值类型 @return explanation @see 指定一个到另一个主题的链接 @see anchor @serial 说明一个序列化属性 @serial description @serialData 明通过writeObject( ) 和 writeExternal( )方法写的数据 @serialData description @serialField 说明一个ObjectStreamField组件 @serialField name type description @since 标记当引入一个特定的变化时 @since release @throws 和 @exception标签一样. The @throws tag has the same meaning as the @exception tag. {@value} 显示常量的值，该常量必须是static属性。 Displays the value of a constant, which must be a static field. @version 指定类的版本 @version info 示例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.runoob;​​import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;​/** * 文档注释演示实例 * @author zhangbc * @version 1.0 */public class SquareNumber &#123; /** * This method returns the square of number. * This is a multiline description. You can use as many lines as you like. * @param number The value to be squared. * @return number squared. */ public double square(double number) &#123; return number * number; &#125;​ /** * This method input a number from the user. * @return The value input as a double. * @throws IOException in input error * @see IOException */ public double getNumber() throws IOException &#123; InputStreamReader isr = new InputStreamReader(System.in); BufferedReader inData = new BufferedReader(isr); String str; str = inData.readLine(); return Double.parseDouble(str); &#125;​ /** * This method demonstrates square(). * @param args args unused. * @throws IOException on input error. * @see IOException */ public static void main(String[] args) throws IOException &#123; SquareNumber sn = new SquareNumber(); double val; System.out.print(\"Enter value to be squared:\"); val = sn.getNumber(); val = sn.square(val); System.out.println(\"Squared value is : \" + val); &#125;&#125; 2，Java 8 新特性1）Java8(即jdk1.8)新特性 （1）Lambda 表达式：Lambda允许把函数作为一个方法的参数（函数作为参数传递进方法中。（2）方法引用：可以直接引用已有Java类或对象（实例）的方法或构造器。与lambda联合使用，方法引用可以使语言的构造更紧凑简洁，减少冗余代码。（3）默认方法：默认方法就是一个在接口里面有了一个实现的方法。（4）新工具：新的编译工具，如：Nashorn引擎jjs、 类依赖分析器jdeps。（5）Stream API：把真正的函数式编程风格引入到Java中。（6）Date Time API：加强对日期与时间的处理。（7）Optional 类：Optional 类已经成为 Java 8 类库的一部分，用来解决空指针异常。（8）Nashorn, JavaScript 引擎：允许我们在JVM上运行特定的javascript应用。 3，Java MySQL连接 MysqlDemo.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.runoob;​import java.sql.*;​/** * 连接数据库实例 * @author zhangbc * @version v1.0 * @date 2019/3/28 22:14 */public class MysqlDemo &#123;​ /** * JDBC驱动名及其数据库URL */ static final String JDBC_DRIVER = \"com.mysql.jdbc.Driver\"; static final String DB_URL = \"jdbc:mysql://127.0.0.1:3306/pyspider_db\";​ /** * 数据库的用户与密码 */ static final String USER = \"root\"; static final String PASSWORD = \"xxxxxx\";​ public static void main(String[] args) &#123; Connection conn = null; Statement stmt = null; try &#123; Class.forName(JDBC_DRIVER); System.out.println(\"连接数据库...\"); conn = DriverManager.getConnection(DB_URL, USER, PASSWORD);​ System.out.println(\"实例化Statement对象...\"); stmt = conn.createStatement(); String sql; sql = \"select id, name, url from websites;\"; ResultSet rs = stmt.executeQuery(sql);​ while (rs.next()) &#123; int id = rs.getInt(\"id\"); String name = rs.getString(\"name\"); String url = rs.getString(\"url\");​ System.out.printf(\"ID: %d\\t站点名称：%s\\t站点URL：%s\\n\", id, name, url); &#125;​ rs.close(); stmt.close(); conn.close(); &#125; catch (SQLException se) &#123; se.printStackTrace(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (stmt != null) &#123; stmt.close(); &#125; &#125; catch (SQLException se) &#123; se.printStackTrace(); &#125;​ try &#123; if (conn != null) &#123; conn.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 4，Java 9 新特性详情参见：Java 9 新特性","categories":[{"name":"Java","slug":"java","permalink":"https://zhangbc.github.io/categories/java/"}],"tags":[{"name":"Java基础","slug":"java-basic","permalink":"https://zhangbc.github.io/tags/java-basic/"}]},{"title":"【Java基础】Java网络编程","slug":"【Java基础】Java网络编程","date":"2019-04-12T14:13:28.000Z","updated":"2019-04-12T14:21:49.206Z","comments":true,"path":"2019/04/12/java_net_program/","link":"","permalink":"https://zhangbc.github.io/2019/04/12/java_net_program/","excerpt":"","text":"【学习参考资料】：菜鸟教程-Java教程 1，Java网络编程1）概述 网络编程：编写运行在多个设备（计算机）的程序，这些设备都通过网络连接起来。java.net包中J2EE的API包含有类和接口，他们提供低层次的通信细节。主要有： TCP：传输控制协议，保障了两个应用程序之间的可靠通信，通常用于互联网协议，称为TCP/IP； UDP：用户数据报协议，一个无连接的协议，提供了应用程序之间要发送的数据的数据包。 2）Socket编程 套接字使用TCP提供了两台计算机之间的通信机制。 客户端程序创建一个套接字，并尝试连接服务器的套接字。 当连接建立时，服务器会创建一个 Socket 对象。客户端和服务器现在可以通过对 Socket 对象的写入和读取来进行通信。 java.net.Socket 类代表一个套接字，并且 java.net.ServerSocket 类为服务器程序提供了一种来监听客户端，并与他们建立连接的机制。 以下步骤在两台计算机之间使用套接字建立TCP连接时会出现： 服务器实例化一个 ServerSocket 对象，表示通过服务器上的端口通信。 服务器调用 ServerSocket 类的 accept() 方法，该方法将一直等待，直到客户端连接到服务器上给定的端口。 服务器正在等待时，一个客户端实例化一个 Socket 对象，指定服务器名称和端口号来请求连接。 Socket 类的构造函数试图将客户端连接到指定的服务器和端口号。如果通信被建立，则在客户端创建一个 Socket 对象能够与服务器进行通信。 在服务器端，accept() 方法返回服务器上一个新的 socket 引用，该 socket 连接到客户端的 socket。 3）ServerSocket类的方法：服务器应用程序通过使用 java.net.ServerSocket 类以获取一个端口,并且侦听客户端请求。 4）Socket类的方法：java.net.Socket 类代表客户端和服务器都用来互相沟通的套接字。客户端要获取一个 Socket 对象通过实例化 ，而 服务器获得一个 Socket 对象则通过 accept() 方法的返回值。 5）InetAddress类的方法：表示互联网协议（IP）地址。 6）demo实例 GreetingClient.java 1234567891011121314151617181920212223242526272829import java.io.*;import java.net.Socket;/** * Socket编程--客户端实例 * @author zhangbc * @date 2019/3/7 14:26 */public class GreetingClient &#123; public static void main(String[] args) &#123; String serverName = args[0]; int port = Integer.parseInt(args[1]); try &#123; System.out.println(\"连接到主机：\" + serverName + \", 端口号：\" + port); Socket client = new Socket(serverName, port); System.out.println(\"远程主机地址：\" + client.getRemoteSocketAddress()); OutputStream outServer = client.getOutputStream(); DataOutputStream outData = new DataOutputStream(outServer); outData.writeUTF(\"Hello from \" + client.getLocalSocketAddress()); InputStream inFromServer = client.getInputStream(); DataInputStream inData = new DataInputStream(inFromServer); System.out.println(\"服务器响应：\" + inData.readUTF()); client.close(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; GreetingServer.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.io.DataInputStream;import java.io.DataOutputStream;import java.io.IOException;import java.net.ServerSocket;import java.net.Socket;import java.net.SocketTimeoutException;import java.lang.Thread;/** * @author zhangbc * @date 2019/3/7 15:09 */public class GreetingServer extends Thread &#123; private ServerSocket serverSocket; public static void main(String[] args) &#123; int port = Integer.parseInt(args[0]); try &#123; Thread thread = new GreetingServer(port); thread.run(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; public GreetingServer(int port) throws IOException &#123; serverSocket = new ServerSocket(port); serverSocket.setSoTimeout(10000); &#125; public void run() &#123; while (true) &#123; try &#123; System.out.println(\"等待远程连接，端口号为：\" + serverSocket.getLocalPort() + \"...\"); Socket server = serverSocket.accept(); DataInputStream inData = new DataInputStream(server.getInputStream()); System.out.println(inData.readUTF()); DataOutputStream outData = new DataOutputStream(server.getOutputStream()); outData.writeUTF(\"谢谢连接我：\" + server.getLocalSocketAddress() + \"\\nGoodbye!\"); server.close(); &#125; catch (SocketTimeoutException es) &#123; System.out.println(\"Socket timed out!\"); break; &#125; catch (IOException ex) &#123; ex.printStackTrace(); break; &#125; &#125; &#125;&#125; 2，Java发送邮件 SendEmail.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.runoob;import javax.activation.DataHandler;import javax.activation.DataSource;import javax.activation.FileDataSource;import javax.mail.*;import javax.mail.internet.*;import javax.mail.Message.RecipientType;import java.util.Properties;/** * 发邮件(纯文本，HTML文本，附件) * @author zhangbocheng * @version v1.0 * @date 2019/3/7 20:40 */public class SendEmail &#123; public static void main(String[] args) throws NullPointerException &#123; // 收件人邮箱 String to = \"xxxxxxxxxxxxxxxxx@qq.com\"; // 发件人邮箱 final String from = \"xxxxxxxxxxxxxxxxx@163.com\"; final String pwd = \"xxxxxxxx\"; // 指定发邮件的主机 String host = \"smtp.163.com\"; // 获取系统属性 Properties properties = System.getProperties(); // 设置邮件服务器 properties.setProperty(\"mail.host\", host); properties.put(\"mail.smtp.auth\", \"true\"); // 获取默认Session对象 Session session = Session.getDefaultInstance(properties, new Authenticator() &#123; @Override protected PasswordAuthentication getPasswordAuthentication() &#123; return new PasswordAuthentication(from, pwd); &#125; &#125;); try &#123; // 创建默认的MimeMessage对象 MimeMessage message = new MimeMessage(session); // Set From：头部头字段 message.setFrom(new InternetAddress(from)); // Set To：头部头字段 message.addRecipient(RecipientType.TO, new InternetAddress(to)); // Set Subject：头部头字段 message.setSubject(\"This is the Subject Line!\"); // 设置消息体 message.setText(\"This is test text.\"); // 发送HTML消息，可以插入html标签 message.setContent(\"&lt;h1&gt;This is actual message&lt;/h1&gt;\", \"text/html;charset=utf-8\"); // 创建消息部分 BodyPart messageBodyPart = new MimeBodyPart(); // 消息 messageBodyPart.setText(\"This is message body.\"); // 创建多重消息 Multipart multipart = new MimeMultipart(); // 设置文本消息 multipart.addBodyPart(messageBodyPart); // 附件部分 messageBodyPart = new MimeBodyPart(); String fileName = \"/home/projects/java_pro/java_instances_demo/src/main/java/com/runoob/SendEmail.java\"; DataSource source = new FileDataSource(fileName); messageBodyPart.setDataHandler(new DataHandler(source)); messageBodyPart.setFileName(fileName); multipart.addBodyPart(messageBodyPart); // 发送完整部分 message.setContent(multipart); // 发送消息 Transport.send(message); System.out.println(\"Sent message successfully.\"); &#125; catch (MessagingException mex) &#123; mex.printStackTrace(); &#125; &#125;&#125; 3，Java Applet基础1）Applet基础 Applet是一种Java程序，一般运行在支持Java的Web浏览器内，是一个全功能的Java应用程序。 Java 中 Applet 类继承了 java.applet.Applet 类。 Applet 类没有定义 main()，所以一个 Applet 程序不会调用 main() 方法。 Applet 被设计为嵌入在一个 HTML 页面。 当用户浏览包含 Applet 的 HTML 页面，Applet 的代码就被下载到用户的机器上。 要查看一个 Applet 需要 JVM。 JVM 可以是 Web 浏览器的一个插件，或一个独立的运行时环境。 用户机器上的 JVM 创建一个 Applet 类的实例，并调用 Applet 生命周期过程中的各种方法。 Applet 有 Web 浏览器强制执行的严格的安全规则，Applet 的安全机制被称为沙箱安全。 Applet 需要的其他类可以用 Java 归档（JAR）文件的形式下载下来。 2）Applet的生命周期 Applet 类中的四个方法给我们提供了一个框架： init: 提供所需的任何初始化。在 Applet 标记内的 param 标签被处理后调用该方法。 start: 浏览器调用 init 方法后，该方法被自动调用。每当用户从其他页面返回到包含 Applet 的页面时，则调用该方法。 stop: 当用户从包含 Applet 的页面移除的时候，该方法自动被调用。 destroy: 此方法仅当浏览器正常关闭时调用。 paint: 该方法在 start() 方法之后立即被调用，或者在 Applet 需要重绘在浏览器的时候调用。paint() 方法实际上继承于 java.awt。 3）Applet类 每一个 Applet 都是 java.applet.Applet 类的子类，基础的 Applet 类提供了供衍生类调用的方法,以此来得到浏览器上下文的信息和服务。 4）Applet的调用 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Hello World, Applet&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;hr&gt;&lt;applet code=\"HelloWorldApplet.class\" width=\"320\" height=\"120\"&gt;&lt;/applet&gt;&lt;hr&gt;&lt;/body&gt;&lt;/html&gt;","categories":[{"name":"Java","slug":"java","permalink":"https://zhangbc.github.io/categories/java/"}],"tags":[{"name":"Java基础","slug":"java-basic","permalink":"https://zhangbc.github.io/tags/java-basic/"}]},{"title":"【Java基础】Java进阶编程","slug":"【Java基础】Java进阶编程","date":"2019-04-11T14:05:07.000Z","updated":"2019-04-11T16:14:43.141Z","comments":true,"path":"2019/04/11/java_advance_program/","link":"","permalink":"https://zhangbc.github.io/2019/04/11/java_advance_program/","excerpt":"","text":"【学习参考资料】：菜鸟教程-Java教程 1，Java数据结构Java工具包提供了强大的数据结构，在Java中的数据结构主要包括以下接口和类：枚举（Enumeration），位集合（BitSet），向量（Vector），栈（Stack），字典（Dictionary），哈希表（Hashtable），属性（Properties）。 1）枚举（Enumeration）：该接口定义了一种从数据结构中取回连续元素的方式。 2）位集合（BitSet）：该类实现了一组可以单独设置和清除的位或标志。 3）向量（Vector）：对象的元素通过索引访问，在创建时不必给对象指定大小，其大小会根据需要动态的变化。 4）栈（Stack）：实现了一个后进先出的数据结构，是Vector的一个子类。 5）字典（Dictionary）：是一个抽象类，定义了键映射到值的数据结构。当想要通过特定的键而不是整数索引来访问数据时，应使用Dictionary。注意：Dictionary类已经过时了，在实际开发中，你可以实现Map接口来获取键/值的存储功能。 6）哈希表（Hashtable）：提供了一种在用户定义键结构的基础上来组织数据的手段。Hashtable是原始的java.util的一部分， 是一个Dictionary具体的实现 。 7）属性（Properties）：继承于 Hashtable.Properties 类表示了一个持久的属性集；属性列表中每个键及其对应值都是一个字符串。 2，Java集合框架1）集合框架设计目标 （1）必须是高性能的，基本集合（动态数组，链表，树，哈希表）的实现也必须是高效的；（2）允许不同类型的集合，以类似的方式工作，具有高度的互操作性；（3）对一个集合的扩展和适应必须是简单的。 2）Java集合框架图 3）集合框架是一个用来代表和操纵集合的统一架构，包含如下内容： （1）接口：是代表集合的抽象数据类型。例：Collection，List，Set，Map等。（2）实现（类）：是集合接口的具体实现。从本质上，他们是可重复使用的数据结构，例：ArrayList，LinkedList，HashSet，HashMap。（3）算法：是实现集合接口的对象里的方法执行的一些有用的计算，例：搜索和排序，这些算法被称为多态，因为相同的方法可以在相似的接口上有着不同的实现。 集合框架的类和接口均在java.util包中。任何对象加入集合类后，自动转变为Object类型，所以在取出的时候，需要进行强制类型转换。 4）集合接口 Set和List的区别 Set 接口实例存储的是无序的，不重复的数据。List 接口实例存储的是有序的，可以重复的元素。 Set 检索效率低下，删除和插入效率高，插入和删除不会引起元素位置改变 &lt;实现类有HashSet,TreeSet&gt;。 List和数组类似，可以动态增长，根据实际存储的数据的长度自动增长List的长度。查找元素效率高，插入删除效率低，因为会引起其他元素位置改变 &lt;实现类有ArrayList,LinkedList,Vector&gt; 。 5）集合实现类（集合类）：Java提供了一套实现了Collection接口的标准集合类。 6）集合算法：集合定义了三个不可改变的静态变量：EMPTY_SET，EMPTY_LIST，EMPTY_MAP。Collection Algorithms是一个列表中的所有算法实现。 7）迭代：使用Java Iterator，通过实例列出Iterator和listIterator接口提供的所有方法。 ArrayListDemo.java 123456789101112131415161718192021222324252627282930public class ArrayListDemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(\"Hello\"); list.add(\"World\"); list.add(\"Maven\"); list.add(\"Demo\");​ // 遍历1：使用foreach遍历List System.out.println(\"使用foreach遍历List:\"); for (String str: list) &#123; System.out.print(str + \" \"); &#125;​ // 遍历2：把链表变为数组相关的内容进行遍历List String[] strArray = new String[list.size()]; list.toArray(strArray); System.out.println(\"\\n把链表变为数组相关的内容进行遍历List:\"); for (int i = 0; i &lt; strArray.length; i++) &#123; System.out.print(strArray[i] + \" \"); &#125;​ // 遍历3：使用迭代器进行相关遍历List Iterator&lt;String&gt; iterator = list.iterator(); System.out.println(\"\\n使用迭代器进行相关遍历List:\"); while (iterator.hasNext()) &#123; System.out.print(iterator.next() + \" \"); &#125; &#125;&#125; MapDemo.java 1234567891011121314151617181920212223242526272829303132333435public class MapDemo &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put(\"key1\", \"value1\"); map.put(\"key2\", \"value2\"); map.put(\"key3\", \"value3\");​ // 遍历1：通过Map.KeySet遍历 System.out.println(\"通过Map.KeySet遍历key与value：\"); for (String key: map.keySet()) &#123; System.out.println(\"key=\" + key + \", value=\" + map.get(key)); &#125;​ // 遍历2：通过Map.entrySet使用iterator遍历 System.out.println(\"通过Map.entrySet使用iterator遍历key和value：\"); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = map.entrySet().iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = iterator.next(); System.out.println(\"key=\" + entry.getKey() + \", value=\" + entry.getValue()); &#125;​ // 遍历3：推荐，通过Map.entrySet遍历 System.out.println(\"通过Map.entrySet遍历key和value:\"); for (Map.Entry&lt;String, String&gt; entry: map.entrySet()) &#123; System.out.println(\"key=\" + entry.getKey() + \", value=\" + entry.getValue()); &#125;​ // 遍历4：通过Map.values()遍历 System.out.println(\"通过Map.values()遍历所有的value，但不能遍历key：\"); for (String val: map.values()) &#123; System.out.println(\"value=\" + val); &#125; &#125;&#125; 8）比较器：使用 Java Comparator，通过实例列出Comparator接口提供的所有方法。 3，Java泛型1）Java泛型（generics） 是JDK5中引入的一个新特性，提供了编译时类型安全检测机制，该机制允许程序员在编译时检测到非法的类型。泛型的本质是参数化类型，即：所操作的数据类型被指定为一个参数。 2）定义泛型方法的规则 所有泛型方法声明都有一个类型参数声明部分（由尖括号分隔），该类型参数声明部分在方法返回类型之前（在下面例子中的&lt;E&gt;）。 每一个类型参数声明部分包含一个或多个类型参数，参数间用逗号隔开。一个泛型参数，也被称为一个类型变量，是用于指定一个泛型类型名称的标识符。 类型参数能被用来声明返回值类型，并且能作为泛型方法得到的实际参数类型的占位符。 泛型方法体的声明和其他方法一样。注意类型参数只能代表引用型类型，不能是原始类型（如int,double,char的等）。 123456789101112131415161718192021222324252627282930/** * 泛型方法实例 */class GenericMethod &#123; public static void main(String[] args) &#123; // 创建不同类型的数组：Integer，Double和Character Integer[] intArray = &#123;1, 2, 3, 4, 5&#125;; Double[] doubleArray = &#123;1.1, 2.2, 3.3, 4.4, 5.5&#125;; Character[] charArray = &#123;'H', 'E', 'L', 'L', '0'&#125;;​ System.out.println(\"整型数组元素为：\"); printArray(intArray); System.out.println(\"双精度小数数组元素为：\"); printArray(doubleArray); System.out.println(\"字符型数组元素为：\"); printArray(charArray); &#125;​ /** * 泛型方法printArray * @param inputArray * @param &lt;E&gt; */ public static &lt;E&gt; void printArray(E[] inputArray) &#123; for (E element: inputArray) &#123; System.out.printf(\"%s \", element); &#125; System.out.println(); &#125;&#125; 3）有界的类型参数：限制那些被允许传递到一个类型参数的类型种类范围。声明首先要列出类型参数的名称，后跟 extends 关键字，最后紧跟它的上界。 123456789101112131415161718192021222324252627/** * 泛型的有界类型参数实例 */class MaxGenericMethod &#123; public static void main(String[] args) &#123; System.out.printf(\"%d,%d和%d中最大的数为%d.\\n\\n\", 3, 4, 5, maximum(3, 4, 5));​ System.out.printf(\"%.2f,%.2f和%.2f中最大的数为%.2f.\\n\\n\", 6.6, 7.7, 8.8, maximum(6.6, 7.7, 8.8));​ System.out.printf(\"%s,%s和%s中最大的数为%s.\\n\\n\", \"pear\", \"apple\", \"orange\", maximum(\"pear\", \"apple\", \"orange\")); &#125;​ public static &lt;T extends Comparable&lt;T&gt;&gt; T maximum(T x, T y, T z) &#123; T max = x; if (y.compareTo(max) &gt; 0) &#123; max = y; &#125; if (z.compareTo(max) &gt; 0) &#123; max = z; &#125; return max; &#125;&#125; 4）泛型类：在类名后面添加类型参数声明部分。泛型类的类型参数声明部分也包含一个或多个类型参数，参数间用逗号隔开。一个泛型参数，也被称为一个类型变量，是用于指定一个泛型类型名称的标识符。因为他们接受一个或多个参数，这些类被称为参数化的类或参数化的类型。 123456789101112131415161718192021222324/** * 泛型类实例 * @param &lt;T&gt; */class Box&lt;T&gt; &#123; private T t; public void add(T t) &#123; this.t = t; &#125;​ public T get() &#123; return t; &#125;​ public static void main(String[] args) &#123; Box&lt;Integer&gt; integerBox = new Box&lt;&gt;(); Box&lt;String&gt; stringBox = new Box&lt;&gt;(); integerBox.add(10); stringBox.add(\"菜鸟教程\");​ System.out.printf(\"整型值为：%d\\n\", integerBox.get()); System.out.printf(\"字符串为：%s\\n\", stringBox.get()); &#125;&#125; 5）类型通配符：一般使用?代替具体的类型参数。 （1）类型通配符上限通过形如List来定义，如此定义就是通配符泛型值接受Number及其下层子类类型；（2）类型通配符下限通过形如List&lt;? super Number&gt;来定义，表示类型只能接受Number及其三层父类类型。 1234567891011121314151617181920212223242526/** * 类型通配符实例 */class Wildcard &#123; public static void main(String[] args) &#123; List&lt;String&gt; name = new ArrayList&lt;&gt;(); List&lt;Integer&gt; age = new ArrayList&lt;&gt;(); List&lt;Number&gt; number = new ArrayList&lt;&gt;();​ name.add(\"icon\"); age.add(18); number.add(314);​ getData(name); getUpperNumber(age); getUpperNumber(number); &#125;​ public static void getData(List&lt;?&gt; data) &#123; System.out.printf(\"data: %s\\n\", data.get(0)); &#125;​ public static void getUpperNumber(List&lt;? extends Number&gt; data) &#123; System.out.println(\"data: \" + data.get(0)); &#125;&#125; 4，Java序列化1）Java序列化：Java提供了一种对象序列化的机制，该机制中，一个对象可以表示为一个字节序列，该字节序列包括该对象的数据，有关对象的类型的信息和存储在对象中数据的类型。 （1）序列化一个对象，并将它发送到输出流。1public final void writeObject(Object x) throws IOException （2）从流中取出下一个对象，并将对象反序列化。1public final void readObject(Object x) throws IOException, ClassNotFundException 2）完整demo实例： Employee.java 12345678910111213141516171819package runoob;​import java.io.*;​/** * Employee类实现Serializable接口 * @author 张伯成 * @date 2019/3/7 12:30 */public class Employee implements Serializable &#123; public String name; public String address; public transient int SSN; public int number;​ public void mailCheck() &#123; System.out.println(\"Mailing a check to \" + name + \" \" + address); &#125;&#125; SerializeDemo.java 12345678910111213141516171819202122232425/** * SerializeDemo类，序列化对象 */class SerializeDemo &#123; public static void main(String[] args) &#123; Employee employee = new Employee(); employee.name = \"Reyan Ali\"; employee.address = \"Phonkka kuan, Ambehta Peer.\"; employee.SSN = 11122333; employee.number = 101;​ try &#123; FileOutputStream fileOut = new FileOutputStream(\"employee.ser\"); ObjectOutputStream out = new ObjectOutputStream(fileOut); out.writeObject(employee); out.close(); fileOut.close(); System.out.println(\"Serialized data is saved in employee.ser.\"); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125;&#125;​ DeserializeDemo.java 123456789101112131415161718192021222324252627/** * DeserializeDemo类，反序列化对象 */class DeserializeDemo &#123; public static void main(String[] args) &#123; Employee employee; try &#123; FileInputStream fileIn = new FileInputStream(\"employee.ser\"); ObjectInputStream in = new ObjectInputStream(fileIn); employee = (Employee) in.readObject(); in.close(); fileIn.close(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); return; &#125; catch (ClassNotFoundException ex) &#123; System.out.println(\"Employee class not found.\"); ex.printStackTrace(); return; &#125; System.out.println(\"Deserialize Employee...\"); System.out.println(\"Name: \" + employee.name); System.out.println(\"Address: \" + employee.address); System.out.println(\"SSN: \" + employee.SSN); System.out.println(\"Number: \" + employee.number); &#125;&#125;","categories":[{"name":"Java","slug":"java","permalink":"https://zhangbc.github.io/categories/java/"}],"tags":[{"name":"Java基础","slug":"java-basic","permalink":"https://zhangbc.github.io/tags/java-basic/"}]},{"title":"【Java基础】Java面向对象","slug":"【Java基础】Java面向对象","date":"2019-04-11T11:57:07.000Z","updated":"2019-04-11T12:07:56.393Z","comments":true,"path":"2019/04/11/java_object_oriented/","link":"","permalink":"https://zhangbc.github.io/2019/04/11/java_object_oriented/","excerpt":"","text":"【学习参考资料】：菜鸟教程-Java教程 1，Java 继承1）Java继承的概念：继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。 2）Java继承类型：Java不支持多继承，但是支持多重继承。3）Java继承的特性 子类拥有父类非 private 的属性、方法。 子类可以拥有自己的属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。 Java 的继承是单继承，但是可以多重继承，单继承就是一个子类只能继承一个父类，多重继承就是，例如 A 类继承 B 类，B 类继承 C 类，所以按照关系就是 C 类是 B 类的父类，B 类是 A 类的父类，这是 Java 继承区别于 C++ 继承的一个特性。 提高了类之间的耦合性（继承的缺点，耦合度高就会造成代码之间的联系越紧密，代码独立性越差）。 4）Java继承关键字 继承可以使用extends和implements这两个关键字来实现继承，都继承于java.lang.Object，默认继承object祖先类。 （1）extends：只能继承一个类。（2）implements：可以变相地使java具有多继承的特性，使用范围为类继承接口的情况，可以同时继承多个接口。（3）super：通过super关键字来实现对父类成员的访问，用来引用当前对象的父类。（4）this：指向自己的引用。（5）finally：声明类可以把类定义为不能继承的，即最终类；或用于修饰方法，该方法不能被子类重写。 5）Java构造器 子类是不继承父类的构造器（构造方法或者构造函数）的，它只是调用（隐式或显式）。如果父类的构造器带有参数，则必须在子类的构造器中显式地通过 super 关键字调用父类的构造器并配以适当的参数列表。 ExtendsDemo.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.example.springboot;​/** * @function: * @author: 张伯成 * @date: 2019/3/3 */public class ExtendsDemo &#123; public static void main(String[] args) &#123; System.out.println(\"==========SubClassOne 类继承===========\"); SubClassOne sc1 = new SubClassOne(); SubClassOne sc2 = new SubClassOne(100); System.out.println(\"==========SubClassTwo 类继承===========\"); SubClassTwo sc3 = new SubClassTwo(); SubClassTwo sc4 = new SubClassTwo(200); &#125;&#125;​​/** * SuperClass 祖先类 */class SuperClass &#123; private int number; SuperClass() &#123; System.out.println(\"SuperClass()\"); &#125;​ SuperClass(int number) &#123; this.number = number; System.out.println(\"SuperClass(int number)\"); &#125;&#125;​​/** * SubClassOne 类继承 */class SubClassOne extends SuperClass &#123; private int number; SubClassOne() &#123; System.out.println(\"SubClassOne()\"); &#125;​ SubClassOne(int number) &#123; super(300); this.number = number; System.out.println(\"SubClassOne(int number): \" + number); &#125;&#125;​​/** * SubClassTwo 类继承 */class SubClassTwo extends SuperClass &#123; private int number; SubClassTwo() &#123; super(300); System.out.println(\"SubClassTwo()\"); &#125;​ SubClassTwo(int number) &#123; this.number = number; System.out.println(\"SubClassTwo(int number): \" + number); &#125;&#125; 2，Java Override/Overload1）重写（Override）重写是子类对父类对允许访问的方法的实现过程进行重新编写，返回值和形参都不能改变，即外壳不变，核心重写。重写方法不能抛出新的检查异常或者比被重写方法声明更加宽泛的异常。1234567891011121314151617181920212223242526272829/** * Animal类，祖先类 */class Animal &#123; public void move() &#123; System.out.println(\"动物可以移动...\"); &#125;&#125;​​/** * AnimalDog类，继承Animal，并重写父类的move()方法 */class AnimalDog extends Animal &#123; public void move() &#123; super.move(); System.out.println(\"狗可以跑和走.\"); &#125;&#125;​​class DogTest &#123; public static void main(String[] args) &#123; Animal animal = new Animal(); Animal dog = new AnimalDog(); animal.move(); dog.move(); &#125;&#125; 2）方法的重写规则 参数列表必须完全与被重写方法的相同。 返回类型必须完全与被重写方法的返回类型相同。 访问权限不能比父类中被重写的方法的访问权限更低。例如：如果父类的一个方法被声明为public，那么在子类中重写该方法就不能声明为protected。 父类的成员方法只能被它的子类重写。 声明为final的方法不能被重写。 声明为static的方法不能被重写，但是能够被再次声明。 子类和父类在同一个包中，那么子类可以重写父类所有方法，除了声明为private和final的方法。 子类和父类不在同一个包中，那么子类只能够重写父类的声明为public和protected的非final方法。 重写的方法能够抛出任何非强制异常，无论被重写的方法是否抛出异常。但是，重写的方法不能抛出新的强制性异常，或者比被重写方法声明的更广泛的强制性异常，反之则可以。 构造方法不能被重写。 如果不能继承一个方法，则不能重写这个方法。 3）super：当需要在子类中调用父类的被重写方法时，要使用super关键字。 4）重载（Overload）：在一个类里面，方法名字相同，而参数不同，返回类型可同可不同。 5）重载规则 被重载的方法必须改变参数列表(参数个数或类型不一样)； 被重载的方法可以改变返回类型； 被重载的方法可以改变访问修饰符； 被重载的方法可以声明新的或更广的检查异常； 方法能够在同一个类中或者在一个子类中被重载； 无法以返回值类型作为重载函数的区分标准。 OverLoading.java12345678910111213141516171819202122232425262728public class OverLoading &#123; public int test() &#123; System.out.println(\"test()\"); return 1; &#125;​ public void test(int number) &#123; System.out.println(\"test(int)\"); &#125;​ public String test(int number, String str) &#123; System.out.println(\"test(int, String)\"); return \"test(int, String)\"; &#125;​ public String test(String str, int number) &#123; System.out.println(\"test(String,int)\"); return \"test(String,int)\"; &#125;​ public static void main(String[] args) &#123; OverLoading overLoad = new OverLoading(); System.out.println(overLoad.test()); overLoad.test(100); System.out.println(overLoad.test(100, \"test3\")); System.out.println(overLoad.test(\"test\", 100)); &#125;&#125; 6）重写和重载的区别 区别点 重载方法 重写方法 参数列表 必须修改 一定不能修改 返回类型 可以修改 一定不能修改 异常 可以修改 可以减少或删除，一定不能抛出新的或更广的异常 访问 可以修改 一定不能做更严格的限制（可以降低限制） 方法的重写(Overriding)和重载(Overloading)是java多态性的不同表现，重写是父类与子类之间多态性的一种表现，重载可以理解成多态的具体表现形式。 方法重载是一个类中定义了多个方法名相同,而他们的参数的数量不同或数量相同而类型和次序不同,则称为方法的重载(Overloading)。 方法重写是在子类存在方法与父类的方法的名字相同,而且参数的个数与类型一样,返回值也一样的方法,就称为重写(Overriding)。 方法重载是一个类的多态性表现,而方法重写是子类与父类的一种多态性表现。 3，Java 多态多态是同一个行为具有多个不同表现形式或形态的能力。多态的好处：可以使程序有良好的扩展，并可以对所有类的对象进行通用处理。 1）多态的优点：消除类型之间的耦合关系；可替换性；可扩充性；接口性；灵活性；简化性。 2）多态存在的三个必要条件：继承；重写；父类引用指向子类对象。 PolymorphicDemo.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.example.springboot;​/** * @function: * @author: 张伯成 * @date: 2019/3/3 */public class PolymorphicDemo &#123; public static void main(String[] args) &#123; show(new Cat()); show(new Dog());​ Animals animal = new Cat(); animal.eat();​ Cat cat = (Cat) animal; cat.work(); &#125;​ public static void show(Animals animal) &#123; animal.eat();​ if (animal instanceof Cat) &#123; Cat cat = (Cat) animal; cat.work(); &#125; else if (animal instanceof Dog) &#123; Dog dog = (Dog) animal; dog.work(); &#125; &#125;&#125;​​abstract class Animals &#123; abstract void eat();&#125;​​class Cat extends Animals &#123; public void eat() &#123; System.out.println(\"吃鱼\"); &#125;​ public void work() &#123; System.out.println(\"抓老鼠\"); &#125;&#125;class Dog extends Animals &#123; public void eat() &#123; System.out.println(\"吃骨头\"); &#125;​ public void work() &#123; System.out.println(\"看家\"); &#125;&#125; 3）虚函数：虚函数的存在是为了多态。Java 中其实没有虚函数的概念，它的普通函数就相当于 C++ 的虚函数，动态绑定是Java的默认行为。 4）多态的实现方式：重写；接口；抽象类和抽象方法。 4，Java 抽象类在Java中抽象类表示的是一种继承关系，一个类只能继承一个抽象类，而一个类却可以实现多个接口。 1）抽象类：在Java语言中使用abstract class定义抽象类。 2）继承抽象类 3）抽象方法：abstract 关键字可以用来声明抽象方法，抽象方法只包含一个方法名，而没有方法体；抽象方法没有定义，方法名后面直接跟一个分号，而不是花括号。 4）抽象类总结规定 （1）抽象类不能被实例化；只有抽象类的非抽象子类可以创建对象；（2）抽象类中不一定包含抽象方法，但是有抽象方法的类必定是抽象类；（3）抽象类中的抽象方法只是声明，不包含方法体，就是不给出方法的具体实现也就是方法的具体功能；（4）构造方法，类方法（用 static 修饰的方法）不能声明为抽象方法；（5）抽象类的子类必须给出抽象类中的抽象方法的具体实现，除非该子类也是抽象类。 5，Java 封装1）在面向对象程式设计方法中，封装（Encapsulation）是指一种将抽象性函式接口的实现细节部分包装、隐藏起来的方法。 封装可以被认为是一个保护屏障，防止该类的代码和数据被外部类定义的代码随机访问。 要访问该类的代码和数据，必须通过严格的接口控制。 封装 最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 2）封装的优点 （1）两个的封装能够减少耦合；（2）类内部的结构可以自由修改；（3）可以对成员变量进行更精确的控制；（4）隐藏信息，实现细节。 6，Java 接口1）接口概念 接口（Interface），在JAVA编程语言中是一个抽象类型，是抽象方法的集合，接口通常以interface来声明。一个类通过继承接口的方式，从而来继承接口的抽象方法。 类描述对象的属性和方法。接口则包含类要实现的方法。 除非实现接口的类是抽象类，否则该类要定义接口中的所有方法。 接口无法被实例化，但是可以被实现。一个实现接口的类，必须实现接口内所描述的所有方法，否则就必须声明为抽象类。另外，在 Java 中，接口类型可用来声明一个变量，他们可以成为一个空指针，或是被绑定在一个以此接口实现的对象。 2）接口与类相似点 一个接口可以有多个方法。 接口文件保存在.java 结尾的文件中，文件名使用接口名。 接口的字节码文件保存在 .class 结尾的文件中。 接口相应的字节码文件必须在与包名称相匹配的目录结构中。 3）接口与类的区别 接口不能用于实例化对象。 接口没有构造方法。 接口中所有的方法必须是抽象方法。 接口不能包含成员变量，除了 static 和 final 变量。 接口不是被类继承了，而是要被类实现。 接口支持多继承。 4）接口特性 接口是隐式的，接口中每一个方法也是隐式抽象的，接口中的方法会被隐式的指定为 public abstract。 接口中可以含有变量，但是接口中的变量会被隐式的指定为 public static final 变量。 接口中的方法是不能在接口中实现的，只能由实现接口的类来实现接口中的方法。 接口的方法都是公有的。 5）抽象类和接口的区别 抽象类中的方法可以有方法体，就是能实现方法的具体功能，但是接口中的方法不行。 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是 public static final 类型的。 接口中不能含有静态代码块以及静态方法(用 static 修饰的方法)，而抽象类是可以有静态代码块和静态方法。 一个类只能继承一个抽象类，而一个类却可以实现多个接口。 6）接口的声明1234[可见度] interface 接口名称 [extends 其他的接口名名] &#123; // 声明变量 // 抽象方法&#125; 7）接口的实现：当类实现接口的时候，类要实现接口中所有的方法，否则，类必须声明为抽象的类；类使用implements关键字实现接口，在类声明中，implements关键字放在class声明后面。 （1）重写接口中声明的方法时，需要注意以下规则： 类在实现接口的方法时，不能抛出强制性异常，只能在接口中，或者继承接口的抽象类中抛出该强制性异常。 类在重写方法时要保持一致的方法名，并且应该保持相同或者相兼容的返回值类型。 如果实现接口的类是抽象类，那么就没必要实现该接口的方法。 （2）在实现接口的时候，需要注意以下规则： 一个类可以同时实现多个接口。 一个类只能继承一个类，但是能实现多个接口。 一个接口能继承另一个接口，这和类之间的继承比较相似。 8）接口的继承 9）接口的多继承：在Java中，类的多继承是不合法，但接口允许多继承；在接口的多继承中extenfs关键字只需要用一次，在其后跟着继承接口。 10）标记接口 标记接口是没有任何方法和属性的接口，它仅仅表明它的类属于一个特定的类型，供其他代码来测试允许做一些事情。 标记接口的作用：简单形象的说就是给某个对象打个标（盖个戳），使对象拥有某个或某些特权。 标记接口的主要目的：建立一个公共的父接口；向一个类添加数据类型。 7，Java 包（Package） 1）包的作用 （1）把功能相似或相关的类或接口组织在同一个包中，方便类的查找和使用。（2）如同文件夹一样，包也采用了树形目录的存储方式。同一个包中的类名字是不同的，不同的包中的类的名字是可以相同的，当同时调用两个不同包中相同类名的类时，应该加上包名加以区别。因此，包可以避免名字冲突。（3）包也限定了访问权限，拥有包访问权限的类才能访问某个包中的类。 总之，Java 使用包（package）这种机制是为了防止命名冲突，访问控制，提供搜索和定位类（class）、接口、枚举（enumerations）和注释（annotation）等。 2）创建包 3）import语句：为了能够使用某一个包的成员，需要在Java程序中明确导入该包。 4）package的目录结构 类目录的绝对路径叫做 class path。设置在系统变量 CLASSPATH 中。编译器和 java 虚拟机通过将 package 名字加到 class path 后来构造 .class 文件的路径。 5）设置CLASSPATH系统变量 用下面的命令显示当前的CLASSPATH变量： 12345# Windows 平台（DOS 命令行下）$ C:\\&gt; set CLASSPATH# UNIX 平台（Bourne shell 下）$ echo $CLASSPATH 删除当前CLASSPATH变量内容： 1234# Windows 平台（DOS 命令行下）$ C:\\&gt; set CLASSPATH=# UNIX 平台（Bourne shell 下）$ unset CLASSPATH; export CLASSPATH 设置CLASSPATH变量： 1234# Windows 平台（DOS 命令行下）$ C:\\&gt; set CLASSPATH=C:\\users\\jack\\java\\classes# UNIX 平台（Bourne shell 下）$ CLASSPATH=/home/jack/java/classes; export CLASSPATH","categories":[{"name":"Java","slug":"java","permalink":"https://zhangbc.github.io/categories/java/"}],"tags":[{"name":"Java基础","slug":"java-basic","permalink":"https://zhangbc.github.io/tags/java-basic/"}]},{"title":"【Java基础】Java基础知识","slug":"【Java基础】Java基础知识","date":"2019-04-10T16:39:20.000Z","updated":"2019-04-10T17:10:49.729Z","comments":true,"path":"2019/04/11/java_basic_knowledge/","link":"","permalink":"https://zhangbc.github.io/2019/04/11/java_basic_knowledge/","excerpt":"","text":"【学习参考资料】：菜鸟教程-Java教程 1，Java基本数据类型变量就是申请内存来存储值。内存管理系统根据变量的类型为变量分配存储空间，分配的空间只能用来储存该类型数据。 Java有两大数据类型：内置数据类型和引用数据类型。 1）内置数据类型 名称 描述 取值范围 默认值 主要用途 byte 8位，有符号的，以二进制补码表示的整数 - $2^7$ ~ $2^7$ -1 0 在大型数组中节约空间，代替整数 short 16位，有符号的，以二进制补码表示的整数 -$2^{15}$~$2^{15}$-1 0 节省空间 int 32位，有符号的，以二进制补码表示的整数 -$2^{31}$~$2^{31}$-1 0 整型变量的默认类型 long 64位，有符号的，以二进制补码表示的整数 -$2^{63}$~$2^{63}$-1 0L 使用在需要比较大整数的系统上 float 单精度、32位、符合IEEE 754标准的浮点数 -$2^{31}$~$2^{31}$-1 0.0f 在存储大型浮点数组时可以节省空间；不能用来表示精确的值 double 双精度、64位、符合IEEE 754标准的浮点数 -$2^{63}$~$2^{63}$-1 0.0d 浮点数变量的默认类型 boolean 表示一位的信息 true,false false 作为一种标记来记录true/false情况 char 单一的 16 位 Unicode 字符 \\u0000~\\uffff(0~$2^{16}$-1) 存储任何字符 2）引用类型 在Java中，引用类型的变量非常类似于C/C++的指针。引用类型指向一个对象，指向对象的变量是引用变量。 对象、数组都是引用数据类型。 所有引用类型的默认值都是null。 一个引用变量可以用来引用与任何与之兼容的类型。 3）Java常量：常量在程序运行时，不会被修改，关键字final。通常使用大写字母表示常量。 4）Java类型转换 自动类型转换：转换从低级到高级。 数据类型转换满足的原则：（1）不能对boolean类型进行类型转换；（2）不能把对象类型转换成不相关类的对象；（3）在把容量大的类型转换为容量小的类型时必须使用强制类型转换；（4）转换过程中可能导致溢出或损失精度；（5）浮点数到整数的转换是通过舍弃小数得到，而不是四舍五入。 强制类型转换：条件是转换的数据类型必须是兼容的。 隐含强制类型转换 2，Java语句类型1）循环结构 while do...while for foreach 增强型for循环：主要用于数组 demo：1234567891011121314151617public class Test &#123; public static void main(String args[])&#123; int [] numbers = &#123;10, 20, 30, 40, 50&#125;; for(int x : numbers )&#123; System.out.print( x ); System.out.print(\",\"); &#125; System.out.print(\"\\n\"); String [] names =&#123;\"James\", \"Larry\", \"Tom\", \"Lacy\"&#125;; # 增强型for循环 for( String name : names ) &#123; System.out.print( name ); System.out.print(\",\"); &#125; &#125;&#125; 2）break语句：主要用于循环语句或者switch语句中；跳出最里层的循环，并且继续执行该循环下面的语句。 3）continue语句：适用于任何循环控制结构中，作用是让程序立刻跳转到下一次循环的迭代。 4）分支结构 if：一个if语句包含一个布尔表达式和一条或多条语句。 switch case语句：判断一个变量与一系列值中某个值是否相等，每个值称为一个分支。 3，Java基础类1）Number &amp; Math类 2）Math类：包含了用于执行基本数学运算的属性和方法，如初等指数、对数、平方根和三角函数。 3）Character类：对单个字符进行操作。 将一个char类型的参数传递给需要一个Character类型参数的方法时，那么编译器会自动地将char类型参数转换为Character对象。 这种特征称为装箱，反过来称为拆箱。 4）String类 用于获取有关对象的信息的方法称为访问器方法。 5）StringBuffer和StringBuilder类 和String类不同的是，StringBuffer和StringBuilder类的对象能够被多次的修改，并且不产生新的未使用对象。 StringBuilder类在Java 5中被提出，它和StringBuffer之间的最大不同在于 StringBuilder的方法不是线程安全的（不能同步访问）。 由于StringBuilder相较于StringBuffer有速度优势，所以多数情况下建议使用 StringBuilder类。 在应用程序要求线程安全的情况下，则必须使用 StringBuffer类。 4，Java数组1）声明数组123dataType[] arrayRefVar; // 首选的方法 # ordataType arrayRefVar[]; // 效果相同，但不是首选方法 2）创建数组 1arrayRefVar = new dataType[arraySize]; 3）多维数组1234567891011type[][] typeName = new type[typeLength1][typeLength2]; // 直接为每一维分配空间​// 从最高维开始，分别为每一维分配空间String s[][] = new String[2][];s[0] = new String[2];s[1] = new String[3];s[0][0] = new String(\"Good\");s[0][1] = new String(\"Luck\");s[1][0] = new String(\"to\");s[1][1] = new String(\"you\");s[1][2] = new String(\"!\"); 4）Arrays类java.util.Arrays 类能方便地操作数组，它提供的所有方法都是静态的。具有以下功能： 给数组赋值：通过 fill 方法。 对数组排序：通过 sort 方法，按升序。 比较数组：通过 equals 方法比较数组中元素值是否相等。 查找数组元素：通过 binarySearch 方法能对排序好的数组进行二分查找法操作。 5，Java日期时间java.util包提供了Date类来封装当前的日期和时间。 Date类提供两个构造函数来实例化 Date 对象。第一个构造函数使用当前日期和时间来初始化对象。1Date( ) 第二个构造函数接收一个参数，该参数是从1970年1月1日起的毫秒数。1Date(long millisec) 1）获取当前日期时间12345678import java.util.Date; public class DateDemo &#123; public static void main(String args[]) &#123; Date date = new Date(); System.out.println(date.toString()); &#125;&#125; 2）日期比较 getTime()：获取两个日期（自1970年1月1日经历的毫秒数值），然后比较这两个值。 before(), after(), equals() compareTo()：由Comparable接口定义的，Date类实现了这个接口。 3）格式化日期时间 SimpleDataFormat1234567public class DateTestDemo &#123; public static void main(String[] args) &#123; Date now = new Date(); SimpleDateFormat ft = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); System.out.println(\"当前时间为：\"+ft.format(now)); &#125;&#125; printf1234567891011121314151617public class DateTestDemo &#123; public static void main(String[] args) &#123; Date now = new Date(); //c的使用 System.out.printf(\"全部日期和时间信息：%tc%n\", now); //f的使用 System.out.printf(\"年-月-日格式：%tF%n\", now); //d的使用 System.out.printf(\"月/日/年格式：%tD%n\", now); //r的使用 System.out.printf(\"HH:MM:SS PM格式（12时制）：%tr%n\", now); //t的使用 System.out.printf(\"HH:MM:SS格式（24时制）：%tT%n\", now); //R的使用 System.out.printf(\"HH:MM格式（24时制）：%tR\", now); &#125;&#125; 4）Java休眠（Sleep）1234567891011public class SleepDemo &#123; public static void main(String[] args) &#123; try &#123; System.out.println(new Date()+\"\\n\"); Thread.sleep(1000*5); System.out.println(new Date()+\"\\n\"); &#125; catch (Exception ex)&#123; System.out.println(\"Got an exception: \"+ex); &#125; &#125;&#125; 5）Calendar类123456789101112131415161718192021public class DateTestDemo &#123; public static void main(String[] args) &#123; // 创建一个代表系统当前日期的Calendar对象 Calendar now = Calendar.getInstance(); int year = now.get(Calendar.YEAR); int month = now.get(Calendar.MONTH); int day = now.get(Calendar.DAY_OF_WEEK); int date = now.get(Calendar.DATE); int hour = now.get(Calendar.HOUR_OF_DAY); int minute = now.get(Calendar.MINUTE); int second = now.get(Calendar.SECOND);​ System.out.println(now.getTime()); System.out.printf(\"%d-%d-%d %d:%d:%d\\n%d\\n\", year, month, day, hour, minute, second, date);​ // 创建一个表示2009年3月12日的Calendar对象 now.set(2009, 3-1, 12); System.out.println(now.getTime()); &#125;&#125; 6）GregorianCalendar类：实现了公历日历，是Calendar类的一个具体实现。1234567891011public class DateTestDemo &#123; public static void main(String[] args) &#123; GregorianCalendar gCalender = new GregorianCalendar(); int year = gCalender.get(Calendar.YEAR); if (gCalender.isLeapYear(year)) &#123; System.out.println(\"当前年份是闰年！\"); &#125; else &#123; System.out.println(\"当前年份不是闰年！\"); &#125; &#125;&#125; 6，正则表达式1） java.util.regex包主要包括以下三个类： Pattern类：是一个正则表达式的编译表示。Pattern类没有公共构造方法。要创建一个Pattern对象，你必须首先调用其公共静态编译方法，它返回一个Pattern 对象。该方法接受一个正则表达式作为它的第一个参数。 Matcher类：是对输入字符串进行解释和匹配操作的引擎。 PatternSyntaxException：是一个非强制异常类，它表示一个正则表达式模式中的语法错误。 12345678public class RegexDemo &#123; public static void main(String[] args) &#123; String content = \"I am a student, graduated from HuBei University.\"; String pattern = \".*HuBei.*\"; boolean isMath = Pattern.matches(pattern, content); System.out.println(\"字符串是否包含类'HuBei'子字符串？\"+isMath); &#125;&#125; 2）捕获组捕获组是把多个字符当一个单独单元进行处理的方法，它通过对括号内的字符分组来创建。特殊的组（group(0)），它总是代表整个表达式。该组不包括在 groupCount 的返回值中。1234567891011121314151617181920public class RegexDemo &#123; public static void main(String[] args) &#123; String content = \"I am a student, graduated from HuBei University in 2013. \"; String pattern = \"(\\\\D*)(\\\\d+)(.*)\";​ // 创建Pattern对象 Pattern rex = Pattern.compile(pattern);​ // 创建matcher对象 Matcher matcher = rex.matcher(content); if (matcher.find()) &#123; System.out.println(\"Found value: \" + matcher.group(0)); System.out.println(\"Found value: \" + matcher.group(1)); System.out.println(\"Found value: \" + matcher.group(2)); System.out.println(\"Found value: \" + matcher.group(3)); &#125; else &#123; System.out.println(\"No match!!!\"); &#125; &#125;&#125; 7，Java方法1）Java方法定义：Java方法是语句的集合，他们在一起执行一个功能。 方法是解决一类问题的步骤的有序组合 方法包含于类或对象中 方法在程序中被创建，在其他地方被引用 2）Java方法的优点 使程序变得更简短而清晰 有利于程序维护 可以提高程序开发的效率 提高了代码的重用性 3）Java方法的命名规则 必须以字母、&#39;_&#39;或&#39;＄&#39;开头（方法名第一个单词应以小写字母开头，后面的单词则用大写字母开头写，不使用连接符）； 可以包括数字，但不能以它开头； 下划线可能出现在JUnit测试方法名称中用以分隔名称的逻辑组件。 4）Java方法的定义123456修饰符 返回值类型 方法名(参数类型 参数名)&#123; ... 方法体 ... return 返回值;&#125; 5）Java方法调用1234567891011121314151617public class MaxTest &#123; public static void main(String[] args) &#123; int i = 2, j = 4; int max = getMax(i, j); System.out.printf(\"%d和%d比较，最大值是：%d.\", i, j, max); &#125;​ private static int getMax(int num1, int num2) &#123; int result; if (num1 &gt; num2) result = num1; else result = num2;​ return result; &#125;&#125; 6）void关键字 7）Java方法重载：一个类的两个方法拥有相同的名字，但是有不同的参数列表。 8）变量作用域 9）命令行参数：命令行参数是在执行程序时候紧跟在程序名字后面的信息。 10）构造方法：当一个对象被创建时候，构造方法用来初始化该对象。构造方法和它所在类的名字相同，但构造方法没有返回值。 11）可变参数：JDK1.5开始，Java支持传递同类型的可变参数给一个方法。1typeName... parameterName 在方法声明中，在指定参数类型后加一个省略号(...) 。 一个方法中只能指定一个可变参数，它必须是方法的最后一个参数。任何普通的参数必须在它之前声明。12345678910111213141516171819202122public class MaxTest &#123; public static void main(String[] args) &#123;​ double max = getMax(1, 2, 3, 4, 5, 6, 7, 8, 0, 1); System.out.printf(\"...numbers中，最大值是：%.2f.\", max); &#125;​ private static double getMax(double... numbers) &#123; if (numbers.length == 0) &#123; System.out.println(\"No argument passed.\"); return -10000000000L; &#125;​ double result = numbers[0]; for (double var: numbers) &#123; if (var &gt; result) &#123; result = var; &#125; &#125; return result; &#125;&#125; 12）finalize()方法：在对象被垃圾收集器析构(回收)之前调用，用来清除回收对象。 Java 的内存回收可以由 JVM 来自动完成。 FinalizationDemo.java如下：1234567891011121314151617181920212223public class FinalizationDemo &#123; public static void main(String[] args) &#123; Cake c1 = new Cake(1); Cake c2 = new Cake(2); Cake c3 = new Cake(3);​ c2 = c3 = null; System.gc(); // 调用Java垃圾回收集器 &#125;&#125;​class Cake extends Object &#123; private int id; public Cake(int id) &#123; this.id = id; System.out.println(\"Cake Object \" + id + \" is created.\"); &#125;​ protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(\"Cake object \" + id + \" is disposed.\"); &#125;&#125; 8，Java流（Stream），文件（File）和IOJava.io包几乎包含了所有操作输入、输出需要的类。一个流可以理解为一个数据的序列。输入流表示从一个源读取数据，输出流表示向一个目标写数据。 1）读取控制台输入：Java的控制台输入由System.in完成。1BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); 2）从控制台读取多字符输入：从BufferedReader对象读取一个字符要使用 read()方法。1int read() throws IOException BRReadLine.java1234567891011public class BRReadLine &#123; public static void main(String[] args) throws IOException &#123; char ch; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); System.out.println(\"输入字符，按下'q'键退出.\"); do &#123; ch = (char) br.read(); System.out.println(ch); &#125; while (ch != 'q'); &#125;&#125; 3）从控制台读取字符串：从 标准输入读取一个字符要使用BufferedReader的 readLine()方法。1String readLine() throws IOException BRReadLines.java123456789101112public class BRReadLines &#123; public static void main(String[] args) throws IOException &#123; String str; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); System.out.println(\"Enter lines of text;\"); System.out.println(\"Enter 'end' to quit.\"); do &#123; str = br.readLine(); System.out.println(str); &#125; while (!str.equals(\"end\")); &#125;&#125; 4）控制台输出：控制台的输出由 print() 和 println() 完成。12345678public class WriteDemo &#123; public static void main(String[] args) &#123; int num; num = 'A'; System.out.write(num); System.out.write('\\n'); &#125;&#125; 5）读写文件：一个流被定义为一个数据序列。输入流用于从源读取数据，输出流用于向目标写数据。 6）FileInputStream：该流用于从文件读取数据，它的对象可以用关键字new来创建。12345678910111213141516171819202122232425/** * 把给定的context以二进制写进文件，同时输出控制台 * 但是存在乱码问题 */public class FileStreamTest &#123; public static void main(String[] args) throws IOException &#123; try &#123; byte[] bWrite = &#123;'a', 11, 21, 32, 40, 54&#125;; OutputStream os = new FileOutputStream(\"file_test.txt\"); for (byte var: bWrite) &#123; os.write(var); &#125; os.close();​ InputStream is = new FileInputStream(\"file_test.txt\"); int size = is.available(); for (int i = 0; i &lt; size; i++) &#123; System.out.println((char) is.read() + \" \"); &#125; is.close(); &#125; catch (IOException ex) &#123; System.out.println(\"File InputStream error: \" + ex.toString()); &#125; &#125;&#125; 7）FileOutputStream：该流用来创建一个文件并向文件中写数据，它的对象可以用关键字 new 来创建。1234567891011121314151617181920212223242526272829303132333435/** * 把给定的context以二进制写进文件，同时输出控制台 */public class FileIOStreamTest &#123; public static void main(String[] args) throws IOException &#123; File file = new File(\"file_test.txt\");​ // 构建FileOutputStream对象，文件不存在会自动新建 FileOutputStream fop = new FileOutputStream(file);​ // 构建对象，参数可以指定编码，默认为操作系统默认编码，windows是gbk OutputStreamWriter writer = new OutputStreamWriter(fop, \"UTF-8\");​ // 写入到缓冲区 writer.append(\"中文输入\"); writer.append(\"\\r\\n\"); writer.append(\"English input\");​ // 关闭写入流，同时会把缓冲区的内容写入文件 writer.close();​ // 关闭输出流，释放系统资源 fop.close();​ FileInputStream fip = new FileInputStream(file); InputStreamReader reader = new InputStreamReader(fip, \"UTF-8\"); StringBuilder sb = new StringBuilder(); while (reader.ready()) &#123; sb.append((char) reader.read()); &#125; System.out.println(sb.toString()); reader.close(); fip.close(); &#125;&#125; 关于文件和I/O的一些其他类：File Class；FileReader Class；FileWriter Class。 9，Java中的目录1）创建目录File类中有两个方法可以用来创建文件夹： mkdir( )方法创建一个文件夹，成功则返回true，失败则返回false。 mkdirs()方法创建一个文件夹和它的所有父文件夹。 1234567public class CreateDir &#123; public static void main(String[] args) &#123; String dirName = \"/home/share/java\"; File file = new File(dirName); file.mkdirs(); &#125;&#125; 2）读取目录12345678910111213141516171819202122public class ReadDir &#123; public static void main(String[] args) throws IOException &#123; String dirName = \"/home/share\"; File fp = new File(dirName); if (fp.isDirectory()) &#123; System.out.println(\"目录：\" + dirName); // 提取包含的文件和文件夹的列表 String[] strings = fp.list(); for (int i = 0; i &lt; strings.length; i++) &#123; File tmp = new File(dirName + \"/\" + strings[i]); if (tmp.isDirectory()) &#123; System.out.println(strings[i] + \" 是一个目录.\"); &#125; else &#123; System.out.println(strings[i] + \" 是一个文件.\"); &#125; &#125; &#125; else &#123; System.out.println(dirName + \" 不是一个目录.\"); &#125; &#125;&#125; 3）删除目录或文件：删除文件可以使用java.io.File.delete()方法。123456789101112131415161718192021public class DeleteDir &#123; public static void main(String[] args) &#123; File folder = new File(\"/home/share/java\"); deleteFolder(folder); &#125;​ // 删除文件及目录 private static void deleteFolder(File folder) &#123; File[] files = folder.listFiles(); if (files != null) &#123; for (File fp: files) &#123; if (fp.isDirectory()) &#123; deleteFolder(fp); &#125; else &#123; fp.delete(); &#125; &#125; &#125; folder.delete(); &#125;&#125; 10，Java Scanner类java.util.Scanner 是 Java5 的新特征，我们可以通过 Scanner 类来获取用户的输入。基本语法如下：1Scanner scanner = new Scanner(System.in); 1）使用next方法 一定要读取到有效字符后才可以结束输入。 对输入有效字符之前遇到的空白，next() 方法会自动将其去掉。 只有输入有效字符后才将其后面输入的空白作为分隔符或者结束符。 next()不能得到带有空格的字符串。 ScannerNext.java12345678910111213public class ScannerNext &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); System.out.println(\"Next方式接收：\");​ // 判断是否还有输入 if (scan.hasNext()) &#123; String str = scan.next(); System.out.println(\"输入的数据为：\" + str); &#125; scan.close(); &#125;&#125; 2）使用nextLine方法 以Enter为结束符,也就是说 nextLine()方法返回的是输入回车之前的所有字符。 可以获得空白。 ScannerNextLines.java12345678910111213class ScannerNextLines &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); System.out.println(\"NextLine方式接收：\");​ // 判断是否还有输入 if (scan.hasNextLine()) &#123; String str = scan.nextLine(); System.out.println(\"输入的数据为：\" + str); &#125; scan.close(); &#125;&#125; 注意：如果要输入 int 或 float 类型的数据，在 Scanner 类中也有支持，但是在输入之前最好先使用 hasNextXxx() 方法进行验证，再使用 nextXxx() 来读取。 ScannerNumbers.java12345678910111213141516171819202122232425262728293031323334public class ScannerNumbers &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in);​ int numberI; float numberF; double numberD; System.out.println(\"输入整数：\"); if (scan.hasNextInt()) &#123; numberI = scan.nextInt(); System.out.println(\"整数是：\" + numberI); &#125; else &#123; System.out.println(\"输入的不是整数！\"); &#125;​ System.out.println(\"输入浮点数：\"); if (scan.hasNextFloat()) &#123; numberF = scan.nextFloat(); System.out.println(\"浮点数是：\" + numberF); &#125; else &#123; System.out.println(\"输入的不是浮点数！\"); &#125;​ System.out.println(\"输入双精度小数：\"); if (scan.hasNextDouble()) &#123; numberD = scan.nextDouble(); System.out.println(\"双精度小数是：\" + numberD); &#125; else &#123; System.out.println(\"输入的不是双精度小数！\"); &#125;​ scan.close(); &#125;&#125; 11，Java异常处理1）理解Java异常处理是如何工作的，需掌握以下三种类型的异常： 检查性异常：最具代表的检查性异常是用户错误或问题引起的异常，这是程序员无法预见的。 运行时异常： 运行时异常是可能被程序员避免的异常。与检查性异常相反，运行时异常可以在编译时被忽略。 错误： 错误不是异常，而是脱离程序员控制的问题。错误在代码中通常被忽略。 2）Exception 类的层次 所有的异常类是从 java.lang.Exception 类继承的子类。 Exception 类是 Throwable 类的子类。除了Exception类外，Throwable 类还有一个子类Error 。 Error 用来指示运行时环境发生的错误。例如，JVM 内存溢出。一般地，程序不会从错误中恢复。 异常类有两个主要的子类：IOException 类和 RuntimeException 类。 在 Java 内置类中，有大部分常用检查性和非检查性异常。 3）Java内置类4）异常方法：主要是Throwable的方法。5）捕获异常123456789101112public class ExceptionDemo &#123; public static void main(String[] args) &#123; try &#123; int[] array = new int[2]; array[0] = 1; System.out.println(\"Access elements three: \" + array[2]); &#125; catch (ArrayIndexOutOfBoundsException ex) &#123; System.out.println(\"Exception: \" + ex.toString()); &#125; System.out.println(\"Out of the block.\"); &#125;&#125; 6）多重捕获块：一个 try 代码块后面跟随多个 catch 代码块的情况就叫多重捕获。 7）throws/throw关键字：如果一个方法没有捕获到一个检查性异常，那么该方法必须使用 throws 关键字来声明。throws 关键字放在方法签名的尾部。 8）finally关键字：finally 关键字用来创建在 try 代码块后面执行的代码块；无论是否发生异常，finally 代码块中的代码总会被执行；在 finally 代码块中，可以运行清理类型等收尾善后性质的语句。 ExceptionDemo.java1234567891011121314public class ExceptionDemo &#123; public static void main(String[] args) &#123; int[] array = new int[2]; try &#123; System.out.println(\"Access elements three: \" + array[2]); &#125; catch (ArrayIndexOutOfBoundsException ex) &#123; System.out.println(\"Exception throw: \" + ex.toString()); &#125; finally &#123; array[0] = 20; System.out.println(\"First element value: \" + array[0]); System.out.println(\"The finally statement is executed.\"); &#125; &#125;&#125; 9）声明自定义异常 所有异常都必须是 Throwable 的子类。 如写一个检查性异常类，则需要继承 Exception 类。 如写一个运行时异常类，那么需要继承 RuntimeException 类。 综合实例 InsufficientFundException.java： 123456789101112/** * 自定义异常，继承Exception类 */public class InsufficientFundException extends Exception &#123; private double amount; public InsufficientFundException(double amount) &#123; this.amount = amount; &#125; public double getAmount() &#123; return amount; &#125;&#125; CheckingAccount.java： 1234567891011121314151617181920212223242526272829303132333435/** * 银行账户基本操作类 */public class CheckingAccount &#123; private double balance; private int number; public CheckingAccount(int number) &#123; this.number = number; &#125;​ // 存钱 public void deposit(double amount) &#123; balance += amount; &#125;​ // 取钱 public void withdraw(double amount) throws InsufficientFundException &#123; if (amount &lt;= balance) &#123; balance -= amount; &#125; else &#123; double needs = amount - balance; throw new InsufficientFundException(needs); &#125; &#125;​ // 返回余额 public double getBalance() &#123; return balance; &#125;​ // 返回账号 public int getNumber() &#123; return number; &#125;&#125; BankDemo.java： 123456789101112131415161718192021** * 模拟银行账户基本操作实例 */class BankDemo &#123; public static void main(String[] args) &#123; CheckingAccount account = new CheckingAccount(6228481); System.out.println(\"Deposit $500...\"); account.deposit(500.00);​ try &#123; System.out.println(\"Withdrawing $100...\"); account.withdraw(100);​ System.out.println(\"Withdrawing $600...\"); account.withdraw(600); &#125; catch (InsufficientFundException ex) &#123; System.out.println(\"Sorry, but you are short $\"+ex.getAmount()); ex.printStackTrace(); &#125; &#125;&#125; 10）通用异常在Java中定义了两种类型的异常和错误： JVM(Java虚拟机) 异常：由 JVM 抛出的异常或错误。例如：NullPointerException 类，ArrayIndexOutOfBoundsException 类，ClassCastException 类。 程序级异常：由程序或者API程序抛出的异常。例如 IllegalArgumentException 类，IllegalStateException 类。","categories":[{"name":"Java","slug":"java","permalink":"https://zhangbc.github.io/categories/java/"}],"tags":[{"name":"Java基础","slug":"java-basic","permalink":"https://zhangbc.github.io/tags/java-basic/"}]},{"title":"【数据库理论】关系数据库","slug":"【数据库理论】关系数据库","date":"2019-04-10T13:22:51.000Z","updated":"2019-04-10T16:13:32.275Z","comments":true,"path":"2019/04/10/db_rdb/","link":"","permalink":"https://zhangbc.github.io/2019/04/10/db_rdb/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 一，关系模型的基本概念 关系及基本术语 在关系模型中，表格的第一行称为关系框架，是属性$A_1$，$A_2$，$A_3$，$\\dots$，$A_k$的有限集合。 表中的每行称为关系的一个元组；每列称为属性，它在某个值域上的取值，不同的属性可以在相同的值域上取值。 关系中的属性个数称为元数（Arity），元组个数称为基数。 关键字 超关键字(Super Key)：在关系中能够唯一标识元组的属性集合。 候选关键字(Candidate Key)：如果某一属性集合是超关键字，但去掉其中任意属性后就不再是超关键字，这样的属性称为候选关键字。 候选关键字的诸属性称为主属性，不包含在任何候选关键字中的属性称为非主属性（非码属性）。 主关键字(Primary Key)：如果关系中存在多个候选关键字，用户可选作元组标识的一个候选关键字为主关键字。 合成关键字(Composite Key)：当某个候选关键字包含多个属性时，该候选关键字称为合成关键字。 外部关键字(Foreign Key)：如果关系R的某个（些）属性K不是R中的候选关键字，而是另一个关系S的候选关键字，则K称为R的外部关键字。 二，关系模式 在关系数据库中，关系模式是型，关系是值。 【定义3-1】关系的描述称为关系模式，形式化表示为$R(U, D, DOM, I, F)$其中，$R$为关系名，$U$为组成该关系的属性名集合，$D$是$U$中属性的域，$DOM$为属性到域的映像集合，$I$为完整性约束集合，$F$为属性间数据的依赖关系集合。 ⚠️关系模式中带有下画线的属性集为主关键字。 三，关系模型的完整性 域完整性约束：主要规定属性值必须取自值域，一个属性能否为空值由其语义决定。 实体完整性约束：规定基本关系的所有主属性都不能取空值，而不仅是主属性整体不能取空值。 参照完整性约束：要求“不引用不存在的实体”，考虑的是不同关系之间的或同一关系的不同元组之间的制约。形式定义： 如果属性集K是关系R的主关键字，K也是关系S的外关键字，那么在关系S中，K的取值只允许两种可能，要么为空值，要么等于关系R中某个主关键字的值。关系R称为“参照关系”模式，关系S称为“依赖关系”模式。 用户自定义完整性约束：针对某个具体关系数据库的约束条件。 四，关系代数 关系查询语言分类 关系代数语言：查询操作是以集合操作为基础运算的DML语言。 关系演算语言：查询操作是以谓词演算为基础运算的DML语言。 关系代数的五种基本操作 相等定义：设有同类关系$r_1$和$r_2$，若$r_1$的任何一个元组都是$r_2$的一个元组，则称关系$r_2$包含关系$r1$，记作$r_1$ $\\subseteq$ $r_2$或 $r_2$ $\\supseteq$ $r_1$，如果$r_1$ $\\subseteq$ $r_2$且$r_1$ $\\supseteq$ $r_2$，则称关系$r_1$等于关系$r_2$，记作$r_1$=$r_2$。 【定义3-2】并(Union)：设有同类关系$r_1$[R]和$r_2$[R]，两者的并(Union)运算定义 $r_1$ $\\bigcup$ $r_2$ = { $t$ $\\mid$ $t$ $\\in$ $r_1$ $\\vee$ $t$ $\\in$ $r_2$ }式中，$\\bigcup$为并运算符。$r_1$ $\\bigcup$ $r_2$的结果关系是$r_1$的所有元组与$r_2$的所有元组的并集（去掉重复元组）。 【定义3-3】差(Difference)：设有同类关系$r_1$[R]和$r_2$[R]，两者的差(Difference)运算定义为 $r_1$ - $r_2$ = { $t$ $\\mid$ $t$ $\\in$ $r_1$ $\\wedge$ $t$ $\\notin$ $r_2$ }式中，- 为相减运算符。$r_1$ - $r_2$的结果关系是$r_1$的所有元组减去$r_1$与$r_2$相同的元组所剩下的元组的集合。 【定义3-4】笛卡儿积(Difference)：设$r$[R]为$k_1$元关系，$s$[S]为$k_2$元关系，两者的笛卡儿积(Difference)运算定义为 $r$ $\\times$ $s$ = { $t$ $\\mid$ $t$ = &lt;$u,v$&gt; $\\wedge$ $u$ $\\in$ $r$ $\\wedge$ $v$ $\\in$ $s$ }。 【定义3-5】投影(Projection)：是对一个关系进行垂直分割，消去某些列，并重新安排列的顺序的操作。设有$r$[R]为$k$元关系，其关系框架R={$A_1$,$A_2$,$\\dots$,$A_k$}，$A_{j_1}$，$A_{j_2}$，$\\dots$，$A_{j_n}$ 为R中互不相同的属性，那么关系$r$在属性（分量）$A_{j_1}$，$A_{j_2}$，$\\dots$，$A_{j_n}$ 上的投影运算定义为 $\\Pi$ $A_{j_1}$,$A_{j_2}$,$\\dots$,$A_{j_n}$ $($ $r$ $)$ = { $u$ $\\mid$ $u$ = &lt; $t$ [$A_{j_1}$],$t$[$A_{j_2}$],$\\dots$,$t$[$A_{j_n}$] &gt; $\\wedge$ $t$ $\\in$ $r$ }式中，$\\Pi$为投影运算符。 【定义3-6】选择(Selection)：根据某些条件对关系进行水平分割，即选取符合条件的元组的操作。条件可用命题公式F表示，由运算对象和运算符组成： 运算对象：常数（用引号括起来）、元组分量（属性名或列的序号） 运算符：算术比较运算符（$\\lt$,$\\le$,$\\gt$,$\\ge$,=,$\\ne$，也称$\\theta$符），逻辑运算符（$\\vee$,$\\wedge$,$\\neg$） 关系R关于公式F的选择操作用$\\sigma$F$($R$)$表示，其定义为：$\\sigma$F $($ R $)$ $\\equiv$ { $t$ $\\mid$ $t$ $\\in$ R $\\wedge$ F($t$)=true}式中，$\\sigma$为选择运算符。$\\sigma$F$($R$)$表示从R中挑选满足公式F为真的元组所构成的关系。 关系代数的其他操作 【定义3-7】交(Intersection)：设有同类关系$r_1$[R]和$r_2$[R]，两者的交(Intersection)运算定义 $r_1$ $\\bigcap$ $r_2$ = { $t$ $\\mid$ $t$ $\\in$ $r_1$ $\\wedge$ $t$ $\\in$ $r_2$}式中，$\\bigcap$为交运算符。$r_1\\bigcapr_2$的结果关系是$r_1$与$r_2$的所有相同元组构成的集合，显然，$r_1\\bigcapr_2$ 等于$r_1$ - ($r_1$ - $r_2$ )或者$r_2$ - ($r_2$ - $r_1$ )。 【定义3-8】$\\theta$-连接：设$r$[R]、$s$[S]关系框架分别为R = {$A_1$,$A_2$,$\\dots$,$A_n$} 和 {$B_1$,$B_2$,$\\dots$,$B_m$}，那么关系$r$和$s$的$\\theta$-连接（$\\theta$-Join）运算定义为：$r$ $\\Join$ $s$ = { $t$ $\\mid$ $t$ = &lt; $u, v$ &gt;$\\wedge$ $u$ $\\in$ $r$ $\\wedge$ $v$ $\\in$ $u$[$A_i$]$\\theta$ $v$[$B_j$]} 【定义3-9】F-连接 ：设$r$[R]、$s$[S]关系框架分别为R = { $A_1$,$A_2$,$\\dots$,$A_n$ }，{ $B_1$,$B_2$,$\\dots$,$B_m$ }，F($A_1$,$A_2$,$\\dots$,$A_n$,$B_1$,$B_2$,$\\dots$,$B_m$)为一公式，那么关系$r$和$s$的F-连接（F-Join）运算定义为：$r$ $\\Join$ $s$ = { $t$ $\\mid$ $t$ = &lt; $u, v$&gt;$\\wedge$ $u$ $\\in$ $r$ $\\wedge$ $v$ $\\in$ $s$ $\\wedge$F($u$[$A_1$],$\\dots$,$u$[$A_n$]),$u$[$B_1$],$\\dots$,$u$[$B_m$]) }即：$r$ $\\Join$ $s$ = $\\sigma$F $(r$ $\\times$ $s)$ 【定义3-10】自然连接： Natural Jion是一种特殊的等值连接，它要求关系R和关系S具有相同的属性组B($b_1$,$b_2$,$b_3$,$\\dots$ $\\dots$)，这些属性组的取值是相等的，在最后生成的关系中去掉属性重复的列。其计算过程如下： (1)计算$r$ $\\times$ $s$；(2)设$r$和$s$的公共属性是$A_1$,$A_2$,$\\dots$,$A_m$，选出$r$ $\\times$ $s$中满足$r.A_1$=$s.A_1$,$r.A_2$=$s.A_2$, $\\dots$, $r.A_m$=$s.A_m$的那些元组；(3)去掉$s.A_1$,$s.A_2$,$\\dots$,$s.A_m$这些列。 【定义3-11】除(Division)：给定关系$r$(X,Y)和$s$(Y,Z)，其中，X，Y，Z为属性组，$r$中的Y与$s$中的Y可以有不同的属性名，但必须出自相同的域集。R与S的除(Division)运算得到一个新的关系$p$(X)，$p$是r中满足下列条件的元组在X属性列上的投影，即元组在X上的分量值$x$的像集$Y_x$包含s在Y上投影的集合，记为$r$ $\\div$ $s$ = { $t_r$[X]$\\mid$ $t_r$ $\\in$ $r$ $\\wedge$ $\\pi_y$($s$) $\\subseteq$ $Y_x$ }式中，$Y_x$为$x$在$r$中的像集，$x$=$t_r$[X]。 五，关系演算关系演算是以数理逻辑中的谓词为基础的，按谓词变元的不同，关系演算可以分为元组关系演算和域关系演算。 元组关系演算：以元组为变量。1） 在元组关系演算中，元组关系演算表达式的一般形式为：$\\{t|P(t)\\}$式中，t是元组变量，表示一个元数确定的元组，P是满足一定逻辑条件的公式，公式可以分解为一些原子公式，$\\{t|P(t)\\}$表示满足公式P的所有元组$t$的集合。2）在一个演算公式中，未用存在量词$\\exists$或全称量词$\\forall$符号定义的元组变量，称为自由元组变量，否则称为约束元组变量。 域关系演算：以属性(域)为变量，简称域演算。","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库理论","slug":"db-theory","permalink":"https://zhangbc.github.io/tags/db-theory/"}]},{"title":"【Java基础】Java入门知识","slug":"【Java基础】Java入门知识","date":"2019-04-08T15:52:05.000Z","updated":"2019-08-28T16:05:24.707Z","comments":true,"path":"2019/04/08/java_introductory_knowledge/","link":"","permalink":"https://zhangbc.github.io/2019/04/08/java_introductory_knowledge/","excerpt":"","text":"【学习参考资料】：菜鸟教程-Java教程 1，java简介 Java是由Sun Microsystems公司于1995年5月推出的Java面向对象程序设计语言和Java平台的总称。 1）Java分为三个体系： JavaSE(J2SE)Java2 Platform Standard Edition，java平台标准版） JavaEE(J2EE)(Java 2 Platform,Enterprise Edition，java平台企业版) JavaME(J2ME)(Java 2 Platform Micro Edition，java平台微型版) 2）Java的主要特性： java语言是简单的； java语言是面向对象的（纯面向对象）； java语言的分布式的； java语言是健壮的（丢弃指针，强类型机制，异常处理，垃圾的自动收集）； java语言是安全的（安全防范机制（类ClassLoader），安全管理机制（类SecurityManager））； java语言是可移植的； java语言是解释型的； java是高性能的； java语言多线程的； java语言是动态的。 3）Java的发展史：诞生于1995年；2014年3月18日，Oracle公司发表Java SE8。 4）Java工具：Java语言尽量保证系统内存在1G以上。 2，Java开发环境配置1）下载JDK工具解压安装，对应不同的系统选择适合的版本。 2）变量设置参数如下： 变量名：JAVA_HOME 变量值：C:\\Program Files (x86)\\Java\\jdk1.8.0_91 // 要根据自己的实际路径配置 变量名：CLASSPATH 变量值：.;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar; //记得前面有个”.“ 变量名：Path 变量值：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 3）测试JDK是否安装成功12~ java -version # 输出java安装的版本~ javac -version # 输出javac安装的版本 注意：如果使用1.5以上版本的JDK，不用设置CLASSPATH环境变量，也可以正常编译和运行Java程序。 4）Java开发工具选择 Eclipse IntelliJ IDEA(推荐) 3，Java基础语法1）相关概念 类：类是一个模板，描述一个对象的行为和状态。 java中的类： 局部变量：在方法、构造方法或者语句块中定义的变量被称为局部变量。变量声明和初始化都是在方法中，方法结束后，变量就会自动销毁。 成员变量：成员变量是定义在类中，方法体之外的变量。这种变量在创建对象的时候实例化。成员变量可以被类中方法、构造方法和特定类的语句块访问。 类变量：类变量也声明在类中，方法体之外，但必须声明为static类型。 构造方法：每个类都有构造方法。如果没有显式地为类定义构造方法，Java编译器将会为该类提供一个默认构造方法。构造方法的名称必须与类同名，一个类可以有多个构造方法。 创建对象：在Java中，使用关键字new创建一个新的对象，主要有三步：声明，实例化（new），初始化。 对象：对象是一个类的实例，有状态和行为。 在软件开发中，方法操作对象内部状态的改变，对象的相互调用也是通过方法来完成。 方法：方法即行为，一个类可以有很多方法。 实例变量：每个对象都有独特的实例变量，对象的状态由这些实例变量的值决定。 2）编程注意点 大小写敏感 类名每个单词的首字母大写（帕斯卡命名法） 方法名以小写字母开头，之后每个单词首字母大写（驼峰命名法） 源文件名必须和类名相同 主方法入口，所有的Java程序由public static void main(String []args)方法开始执行 3）Java标识符 标识符：类名，变量名及方法名都称为标识符。 所有的标识符都应该以字母（A-Z或者a-z）,美元符（$）、或者下划线（_）开始； 首字符之后可以是字母（A-Z或者a-z）,美元符（$）、下划线（_）或数字的任何字符组合； 关键字不能用作标识符； 标识符是大小写敏感的； 合法标识符举例：age、$salary、_value、__1_value； 非法标识符举例：123abc、-salary； 4）Java修饰符 （1）访问控制： default, public, protected, private 修饰符 当前类 同一包内 子孙类（同一包） 子孙类（不同包） 其他包 public Y Y Y Y Y protected Y Y Y Y /N N default Y Y Y N N private Y N N N N protected说明： 子类与基类在同一包中：被声明为protected的变量、方法和构造器能被同一个包中的任何其他类访问； 子类与基类不在同一包中：那么在子类中，子类实例可以访问其从基类继承而来的 protected 方法，而不能访问基类实例的protected方法。 （2）访问控制和继承，注意以下原则： 父类中声明为public的方法在子类中也必须为public。 父类中声明为protected的方法在子类中要么声明为protected，要么声明为 public，不能声明为private。 父类中声明为private的方法，不能够被继承。 （3）非访问控制：final, abstract,static, synchronized,volatile static：创建类方法和类变量； final：修饰类，方法和变量。修饰的类不可被继承；修饰的方法不能被继承的类重新定义；修饰的变量为常量，不可修改。 abstract：创建抽象类和抽象方法； synchronized：用于线程编程，synchronized声明的方法同一时间只能被一个线程访问； transient：序列化的对象包含被 transient修饰的实例变量时，Java虚拟机(JVM)跳过该特定的变量；该修饰符包含在定义变量的语句中，用来预处理类和变量的数据类型。 volatile：用于线程编程， 修饰的成员变量在每次被线程访问时，都强制从共享内存中重新读取该成员变量的值，当成员变量发生变化时，会强制线程将变化值回写到共享内 5）Java变量 局部变量：类的方法中的变量。 局部变量声明在方法、构造方法或者语句块中； 局部变量在方法、构造方法、或者语句块被执行的时候创建，当它们执行完成后，变量将会被销毁； 访问修饰符不能用于局部变量； 局部变量只在声明它的方法、构造方法或者语句块中可见； 局部变量是在栈上分配的。 局部变量没有默认值，所以局部变量被声明后，必须经过初始化，才可以使用。 类变量（静态变量）：独立于方法之外的变量，用static修饰。成员变量（非静态变量）：独立于方法之外的变量，不用static修饰。 6）Java数组：储存在堆上的对象，可以保存多个同类型变量。 7）Java枚举：Java 5.0引入了枚举，枚举限制变量只能是预先设定好的值。使用枚举可以减少代码中的bug。 8）Java关键字：参见Java关键字列表 9）Java注释 10）Java空行：空白行，或者有注释的行，Java编译器都会忽略掉。 11）Java继承在Java中，一个类可以由其他类派生。被继承的类称为超类（super class），派生类称为子类（subclass）。 12）Java接口：在Java中，接口可以理解为对象间相互通信的协议。 13）源文件声明规则 一个源文件中只能有一个public类 一个源文件可以有多个非public类 源文件的名称应该和public类的类名保持一致 如果一个类定义在某个包中，那么package语句应该在源文件的首行 如果源文件包含import语句，那么应该放在package语句和类定义之间；如果没有package语句，那么import语句应该在源文件中最前面。 import语句和package语句对源文件中定义的所有类都有效。在同一源文件中，不能给不同的类不同的包声明。 14）Java包：包主要用来对类和接口进行分类。 15）Import语句 16）Java运算符 算术运算符 关系运算符 位运算符：Java定义类位运算符，应用于整数类型（int），长整型（long），短整型（short），字符型（char）和字节型（byte）等类型。 逻辑运算符 赋值运算符 其他运算符（instanceof，自增，自减，条件运算符） 17）Java 源程序与编译型运行区别","categories":[{"name":"Java","slug":"java","permalink":"https://zhangbc.github.io/categories/java/"}],"tags":[{"name":"Java基础","slug":"java-basic","permalink":"https://zhangbc.github.io/tags/java-basic/"}]},{"title":"【数据库实战】SQL Server数据库常用脚本","slug":"【数据库实战】SQL-Server数据库常用脚本","date":"2019-04-07T08:40:27.000Z","updated":"2019-04-09T13:11:32.517Z","comments":true,"path":"2019/04/07/sql_used_script/","link":"","permalink":"https://zhangbc.github.io/2019/04/07/sql_used_script/","excerpt":"","text":"1，创建链接远程服务器及其删除12345exec sp_addlinkedserver 'web','','SQLOLEDB','192.168.10.106'exec sp_addlinkedsrvlogin 'web','false',null,'sa','123'--删除链接服务器exec sp_dropserver 'web','droplogins' 2，重置SQLSERVER表的自增列，让自增列重新计数语法：123456DBCC CHECKIDENT( table_name [, &#123; NORESEED | &#123; RESEED [,new_reseed_value ] &#125; &#125; ] ) [ WITH NO_INFOMSGS ] 参数： table_name:是要对其当前标识值进行检查的表名。指定的表必须包含标识列。表名必须符合标识符规则。 NORESEED:指定不应更改当前标识值。 RESEED:指定应该更改当前标识值。 new_reseed_value:用作标识列的当前值的新值。 WITH NO_INFOMSGS:取消显示所有信息性消息。 查看某表当前的种子值，示例：1dbcc checkident('mainTable',noreseed); 123-------------显示如下----------------检查标识信息: 当前标识值 '2707'，当前列值 '2707'。--DBCC 执行完毕。如果 DBCC 输出了错误信息，请与系统管理员联系。 重置表mainTable的当前标识值为1，示例：1dbcc checkident('mainTable',reseed,1); 123-------------显示如下----------------检查标识信息: 当前标识值 'NULL'，当前列值 '1'。--DBCC 执行完毕。如果 DBCC 输出了错误信息，请与系统管理员联系。 3，几个有用的存储过程 修改xx表中所有值null 123456789101112131415161718192021222324252627282930313233/************************* 功能：修改xx表中所有列为NULL='' 作者：by zhangbc 时间：2015-10-19*************************/if (OBJECT_ID('modifyNull','P') is not null) drop procedure modifyNullgocreate procedure modifyNull(@table char(100))asbegin --定义游标 declare col_cur cursor scroll dynamic --scroll表示可以向前或向后移动 dynamic：表示可写也可读 for select b.name from sysobjects a inner join syscolumns b on a.id=b.id where a.name=@table --打开游标 open col_cur declare @columnName nvarchar(100) fetch next from col_cur into @columnName declare @sql nvarchar(1000) while (@@FETCH_STATUS=0) begin set @sql='update ' + @table + ' set ' + @columnName + ' = ISNULL(' + @columnName + ', '''')' exec(@sql) fetch next from col_cur into @columnName end --关闭游标 close col_cur --释放游标 deallocate col_curend 修改数据库中所有表的所有列为null 123456789101112131415161718192021222324252627/************************* 功能：修改数据库中所有表的所有列为NULL='' 作者：by zhangbc 时间：2015-10-19*************************/create procedure [dbo].[modifyAllNull]asbegin declare tab_cur cursor scroll dynamic --scroll表示可以向前或向后移动 dynamic：表示可写也可读 for select name from sysobjects where xtype='U' --打开游标 open tab_cur declare @tableName nvarchar(100) fetch next from tab_cur into @tableName declare @sql nvarchar(1000) while (@@FETCH_STATUS=0) begin set @sql='exec dbo.modifyNull ' +'''' + @tableName + '''' exec(@sql) fetch next from tab_cur into @tableName end --关闭游标 close tab_cur --释放游标 deallocate tab_curend","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库实战","slug":"db-actual-combat","permalink":"https://zhangbc.github.io/tags/db-actual-combat/"}]},{"title":"【数据库理论】绪论","slug":"【数据库理论】绪论","date":"2019-04-06T10:42:18.000Z","updated":"2019-04-10T16:56:20.697Z","comments":true,"path":"2019/04/06/db_introduction/","link":"","permalink":"https://zhangbc.github.io/2019/04/06/db_introduction/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 一，数据库的概念 数据库定义：数据库（DB）是长期保存在计算机的存储设备上并按照某种模型组织起来的，可以被各种用户或应用共享的数据集合。 数据库分类：关系数据库，层次数据库，网状数据库. 数据库基本特征：1）具有较高的数据独立性；数据独立性是指数据的组织方法和储存方法与应用程序互不依赖，彼此独立的特性，包括物理独立性和逻辑独立性。2）数据库用综合的方法组织数据，保证尽可能高的访问效率；3）具有较小的数据冗余，可以供多个用户共享；4）具有安全控制机制，能够保证数据的安全，可靠；5）数据允许多用户共享，能够有效，及时地处理数据，并能够保证数据的一致性和完整性。 二，数据库管理系统（DBMS） DBMS的定义：DBMS是位于用户与操作系统之间的数据管理软件，数据库在建立，运用和维护时由数据库管理系统统一管理，控制。 DBMS的目标： 1）用户界面友好； 2）功能完备； 3）效率高，DBMS应该具有较高的系统效率和高的用户生产率，其中系统效率包括： （1）计算机内部资源的利用率，即能够充分利用磁盘空间，CPU，设备等资源，并注意使各种资源的负载均衡以提高整个系统的效率；（2）DBMS本身的运行效率，根据系统目标确定恰当的体系结构，数据结构和算法，保证DBMS运行的高效率；用户生产率是指用户设计和开发应用程序的效率； 4）结构清晰：DBMS内部结构清新，层次分明既便于支持其外层开发环境的构造，也便于自身的设计、开发与维护； 5）开放性：DBMS的开放性是指符合标准和规范，如ODBC标准，SQL标准等。 DBMS的基本功能： 1）数据库定义功能：DDL可以定义数据库中数据之间的联系，可以定义数据的完整性约束条件和保证完整性的触发机制等，包括全局逻辑数据结构（模式）的定义，局部逻辑数据结构（外模式）的定义，保密定义等；2）数据库操纵功能：DML可以接收，分析和执行用户提出的访问数据库的各种要求，完成对数据库的各种基本操作，如对数据库的检索，插入，删除和修改等操作，可以重新组织数据的存储结构，可以完成数据库的备份和恢复等操作；3）数据库的控制功能：DCL包括整个数据库系统的运行控制，数据库的完整性控制，数据库的安全性控制及多用户环境下的数据库并发访问控制等；4）数据库的运行管理功能：指DBMS运行机制和管理功能；5）数据库组织和存储管理功能6）数据库的建立和维护功能7）数据库通信功能 DBMS的组成： 1）数据库定义语言及其翻译程序： DDL用于描述数据库中要存储的现实世界实体的语言，包括数据字典中数据库的逻辑结构，完整性约束，物理存储结构的表述，数据库的各种数据操作和数据库的维护管理的各种依据。包括： 模式DDL：定义全局逻辑数据结构(包括所有字段的名称，特征及其相互关系)，数据的完整性，安全性约束；外模式DDL：为用户定义所用的局部逻辑数据结构(包括与用户的应用程序有关的字段名称，特征及其相互关系)，描述外模式到模式之间的映射关系；内模式DDL：用于描述数据在存储介质上的安排和存放方式，描述模式到内模式之间的映射关系； 2）数据库操纵语言及其编译（或解释）程序： DML用于实现对数据库的一些基本操作，如数据检索，数据插入，数据修改和数据删除，其中数据插入，数据修改和数据删除操作又称为数据更新操作。分为： 宿主型DML：本身不能独立使用，必须嵌入到宿主语言中，如C，COLBOL，PASCAL等，因此也称嵌入型DML，仅负责对数据库数据的操纵，其他工作都由宿主语言完成；自主型DML，又称自含型DML，可以独立进行数据查询，数据更新等操作，语法简单，使用方便，适合终端用户使用； 3）数据库运行控制程序：包括系统初启程序(DBMS的神经中枢)，访问控制程序，安全性控制程序，完整性检查程序，并发控制程序，数据存取/更新程序，通信控制程序； 4）数据库服务实用程序：包括数据装入程序，工作日志程序，性能监督程序，数据库重新组织程序，系统恢复程序，转储/编辑/打印程序。 数据库管理与数据库管理员(DBA)： 1）DBA：从事数据库管理工作的人员，不是数据库的“占有者”，而是数据库的“保护者”。 2) DBA职责 在数据库设计开始之前，DBA首先调查数据库的用户需求。 在数据库设计阶段，DBA要负责数据库标准的制定和功用数据字典的研制，要负责各级数据库模式的设计，负责数据库安全，可靠方面的设计，决定文件组织的方法。 在数据库运行阶段，DBA要负责对用户进行数据库方面的培训，负责数据库的转储和恢复，数据维护，用户的使用权限等，负责监控数据库的性能。 三，数据库系统（DBS） DBS定义：指在计算机系统中引入数据库后的系统构成，一般由数据库，数据库系统运行环境，数据库管理系统及其开发工具，数据库管理员和用户组成。 数据库系统的三级模式结构： 1）模式定义：数据库中全体数据的逻辑结构和特征的描述，仅仅涉及型的描述，而不涉及具体的值。 2）三级模式结构： 外模式：也称子模式或者用户模式，是数据库用户看见和使用的局部数据的逻辑结构和特征的描述，是数据库的用户视图，是和某个应用相关的数据逻辑表示；一个数据库可以有多个外模式； 模式：也称逻辑模式，是数据库中全体数据的逻辑结构和特征的描述，是所有用户的公共视图，是数据库的整个逻辑描述，并说明一个数据库所采用的数据模型；一个数据库只有一个模式； 内模式：也称存储模式，是数据物理结构和存储方式的描述，是数据库的内部表示方式；一个数据库只有一个内模式。 二级映像功能和数据独立性： 1）外模式/模式映像：当模式改变时，DBA对各个外模式/模式的映像做出相应改变，使外模式保持不变，从而使应用程序不改变，保证了数据的逻辑独立性； 2）模式/内模式映像：唯一定义了数据全局逻辑结构和存储方式之间的对应关系。当数据库的存储方式改变时，DBA将对模式/内模式映像做出相应的改变，使模式保持不变，从而保证了数据的物理独立性。 数据库系统的体系结构： 1）单用户结构的数据库系统：最早最简单，不能共享数据； 2）主从式结构的数据库系统：结构简单，数据易于维护维护和管理，但系统的可靠性不高； 3）分布式结构的数据库系统：数据在逻辑上上一个整体，但是分布在计算机网络的不同结点上。 4）客户端/服务器结构的数据库系统 数据库系统的工作流程： 1）DBA建立并维护数据库； 2）用户编写应用程序； 3）应用程序在DBMS支持下运行，在模式，外模式，内模式，用户源程序翻译为目标代码后，即可启动目标程序。 四，数据库的发展 人工管理阶段： 1）特点：数据不保存；应用程序管理数据；数据不共享；数据不具有独立性。 文件系统阶段： 1）优点：数据可以长期保存；有专门的软件即文件系统管理数据，文件系统把数据组织成相互独立的数据文件；文件的形式多样化； 2）缺点：数据共享性差，冗余度大；数据独立性差；数据联系弱。 数据库系统阶段 数据库技术的研究，应用领域和发展方向 数据模型 数据库管理系统软件的研制 数据操作 数据库理论：主要集中在关系的规范化理论，关系数据理论等。 代表性的数据库应用领域和发展方向： 因特网上的Web数据库 面向对象数据库 多媒体数据库 并行数据库 人工智能领域的知识库和主动数据库 模糊数据库系统","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库理论","slug":"db-theory","permalink":"https://zhangbc.github.io/tags/db-theory/"}]},{"title":"【数据库理论】数据模型","slug":"【数据库理论】数据模型","date":"2019-04-06T04:48:52.000Z","updated":"2019-04-09T13:11:32.520Z","comments":true,"path":"2019/04/06/db_data_model/","link":"","permalink":"https://zhangbc.github.io/2019/04/06/db_data_model/","excerpt":"","text":"本系列为《数据库系统原理与应用（刘先锋等著）》的读书笔记。 一，数据描述 数据的三种范畴 1）现实世界（客观世界）：现实世界所反映的客观存在的事物及其相互之间的联系，指数处理对象最原始的表现形式。 2）信息世界（观念世界）：是现实世界在人们头脑中的反映，经过一定的选择，命名和分类而形成的。以下为相关概念： 实体（Entity）：客观存在的事物在人们头脑中的反映，或说，客观存在并可相互区别的客观事物或抽象事件。 属性（Attribute）：实体所具有的某一方面的特性。 域（Domain）：一个属性可能取的所有属性值的范围称为该属性的域。 码（Key）：唯一标识实体的属性集。 实体型（Entity Type）：用实体名及其属性名集合来抽象和刻画同类实体，称为实体型。 实体集（Entity Set）：同一类型实体的集合。 3）机器世界（数据世界或存储世界）数据化后的信息称为数据，所以说数据是信息的符号表示。以下为相关概念： 数据项（字段，Field）：对应于信息世界中的属性。 记录（Record）：对应于每个实体所对应的数据。 记录型（Record Type）：对应于信息世界中的实体型。 文件（File）：对应于信息世界中的实体集。 关键字（Key）：对应于能够唯一标识一个记录的字段集。 实体间的联系 一对一联系：如果实体集A中每个实体，实体集B中至多有一个实体与之联系，反之亦然，则称实体集A与实体集B具有一对一联系，记作1:1； 一对多联系：如果实体集A中每个实体，实体集B中有n（n&gt;=0）个实体与之联系，反之，实体集B中每个实体，实体集A中至多有一个实体与之联系，则称实体集A与实体集B具有一对多联系，记作1:n； 多对多联系：如果实体集A中每个实体，实体集B中有n（n&gt;=0）个实体与之联系，反之，实体集B中每个实体，实体集A中有m（m&gt;=0）个实体与之联系，则称实体集A与实体集B具有多对多联系，记作n:m。 二，概念模型与E-R方法 数据模型 1）数据模型应满足以下要求： i）能够比较真实地模拟现实世界；ii）容易为人所理解；iii）便于在计算机上实现。 2）数据模型分类： i）概念模型：即信息模型，是按用户的观点来对数据和信息建模的，主要用于数据设计；ii）基本数据模型：主要包括网状模型，层次模型，关系模型等，是按计算机系统的观点对数据建模的，主要用于DBMS的实现。 数据模型的三要素 1）数据结构：用于描述系统的静态特性。数据结构是所有研究对象类型的集合，这些对象是数据库的组成部分，分为两大类： 与数据类型，内容，性质有关的对象； 与数据之间联系有关的对象。 2）数据操作：用于描述系统的动态特征。数据操作是指对数据库中各种对象（型）的实例（值）允许执行的操作的集合，包括操作及有关的操作规则。数据库主要有检索和修改（插入，删除，更新）两大数据操作。 3）数据完整性约束：是一组完整性规则的集合。完整性规则是给定的数据模型中的数据及其联系所具有的制约和存储规则，用于限制符合数据模型的数据库状态及状态的变化，用于确保数据的正确、有效和相容。 概念数据模型 1）概念模型的定义：按用户的观点对现实世界进行数据建模而形成的，是一种独立于计算机系统的模型，完全不涉及信息在计算机系统中的表示，也不依赖于具体的数据库管理系统，用于描述某个特定组织所关心的信息结构。 2）概念模型的相关基本概念：实体、属性、域、码、实体型和实体集。 3）概念模型的基本关系：在概念模型中主要解决问题是实体间的联系。 E-R图表示法 1）实体型：用矩形表示，矩形框内写明实体名； 2）属性：用椭圆表示，椭圆内写明属性名； 3）联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标注联系的类型（1:1，1:n，n:m）。 E-R模型优点： i）接近人的思想，容易理解；ii）与计算机无关，用户容易接受。 三，传统的三大数据模型 层次模型 1）层次模型定义：用树形结构表示实体之间的联系的模型称为层次模型。 2）建立数据的层次模型需要满足以下条件： i）有且仅有一个结点没有父结点，这个结点即为树根结点；ii）其他数据记录有且仅有一个父结点。 3）层次模型的基本特点：任何一个给定的记录值只有按其路径查看，才能体现它的全部含义，没有一个子记录值能够脱离父记录值而独立存在的。 4）层次模型的最明显特点：层次清晰，构造简单，易于实现，可以很方便地表示一对一和一对多这两种实体之间的联系。 5）层次模型的主要优点： i）层次模型本身比较简单；ii）实体间联系是固定的，且预先定义好的应用系统采用层次模型来实现，其性能优于关系模型的性能，不低于网状模型的性能；iii）层次模型提供了良好的完整性支持。 6）层次模型的主要缺点： i）现实世界中很多联系是非层次性的，层次模型表示这类联系的方法很笨拙，只能通过引入冗余数据或创建非自然组织（引入虚结点）来解决；ii）对插入和删除操作的限制比较多；iii）查询子结点必须通过父结点；iv）由于结构严谨，层次命令趋于程序化。 网状模型 1）网状模型的定义： 网状模型是一种有向图，在数据库中，把满足以下两个条件的基本层次联系集合称为网状模型： i）允许一个以上的结点无父结点；ii）一个结点可以有多于一个的父结点。 2）为了描述网状模型的记录之间的联系，引进了系（Set）的概念，所谓系可以理解为命名了的联系，它由一个父记录型和一个或多个子记录型构成。 3）网状数据库的定义：用网状模型设计出来的数据库称为网状数据库。 4）网状模型的主要优点： i）能够更为直接地描述现实世界，如一个结点可以有多个父结点；ii）具有良好的性能，存取效率较高。 5）网状模型的主要缺点： i）结构比较复杂，而且随着应用环境的扩大，数据库的结构就变得越来越复杂，不利于用户最终掌握；ii）其DDL，DML语言复杂，用户不易使用。 关系模型 1）关系模型的主要优点： i）数据结构比较简单；ii）具有很高的数据独立性；iii）可以直接处理多对多联系；iv）坚实的理论基础。 2）值域的定义：在关系模型中，一个n元关系有n个属性，属性的取值范围称为值域。 3）关系模型主要缺点：存取路径对用户透明，查询效率往往不如非关系数据模型。 四，数据独立与三层结构 三级模式结构 外模式：又称为用户模式，是数据库用户和数据库系统的接口，是数据库用户的数据视图，是数据库用户可以看得见和使用的局部数据的逻辑结构和特征描述，是与某一个应用有关的数据的逻辑表示。 模式：可分为概念模式和逻辑模式，是所有数据库用户的公共数据视图，是数据库中全部数据的逻辑结构和特征的描述。 一个数据库只有一个模式，其中概念模式可以用实体-联系模型模型来描述，逻辑模式以某种数据模型为基础，综合考虑所有用户的需求，并将其形成全局逻辑结构。 内模式：又称为存储模式，是数据库物理结构和存储方式的描述，是数据在数据库内部的表示方式。 数据的独立性 逻辑数据独立性：当模式改变时，只要对外模式/模式映像做相应的改变，就可以使外模式保持不变，以外模式为依据编写的应用程序就不受影响，从而应用程序不必修改，保证了数据与程序之间的逻辑独立性。 存储数据独立性：当内模式改变时，只要对模式/内模式映像做相应的改变，使模式保持不变，应用程序就不受影响，从而保证了数据与程序之间的物理独立性，称为存储数据独立性。 五，数据库管理系统 DBMS的主要功能 数据库的主要职责就是有效地实现数据库三级模式之间的转换，即把用户或应用程序对数据库的一次访问，从用户级带到概念级，再导向物理级，转换为对存储数据的操作。 1）数据库定义2）数据库操作及查询优化3）数据库控制运行管理4）数据组织，存储和管理5）数据库的恢复和维护6）数据库的多种接口7）其他功能 DBMS的组成 1）DBMS由查询处理器和存储管理器两大部分组成。其中： (1) 查询处理器主要有DDL编译器，DML编译器，嵌入式DML的预编译器及查询运行核心程序；(2) 存储管理器有授权和完整性管理器，事务管理器，文件管理器及缓冲区管理器。 2）查询处理程序：把用较高级的语言所表示的数据库操作（查询、更新等）转换成一系列对数据库的请求。 3）存储管理程序：包括文件管理程序和缓冲区管理程序。 4）事务管理程序：保证数据库中所有事务全部都能正确执行。 用户访问数据库的过程（略）","categories":[{"name":"数据库技术","slug":"database","permalink":"https://zhangbc.github.io/categories/database/"}],"tags":[{"name":"数据库理论","slug":"db-theory","permalink":"https://zhangbc.github.io/tags/db-theory/"}]}]}