---
title: 【机器学习基础】最大边缘分类器
type: categories
copyright: false
date: 2019-10-24 19:13:37
tags: 机器学习基础
categories: 机器学习
title_en: prml_07_01
mathjax: true
---


> 本系列为《模式识别与机器学习》的读书笔记。


# 一，最大边缘分类器

考察线性模型的⼆分类问题，线性模型的形式为

$$
y(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x})+b\tag{7.1}
$$

其中 $\boldsymbol{\phi}(\boldsymbol{x})$ 表⽰⼀个固定的特征空间变换，并且显式地写出了偏置参数 $b$ 。训练数据集由 $N$ 个输⼊向量 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ 组成，对应的⽬标值为 $t_1,\dots,t_N$ ，其中 $t_n\in\{−1, 1\}$ ， 新的数据点 $\boldsymbol{x}$ 根据 $y(\boldsymbol{x})$ 的符号进⾏分类。

现阶段，假设训练数据集在特征空间中是线性可分的，即根据定义，存在⾄少⼀个参数 $\boldsymbol{w}$ 和 $b$ 的选择⽅式，使得对于 $t_n = +1$ 的点，函数(7.1)都满⾜ $y(\boldsymbol{x}_n)>0$ ，对于 $t_n = −1$ 的点，都有 $y(\boldsymbol{x}_n)<0$ ，从⽽对于所有训练数据点，都有 $t_ny(\boldsymbol{x}_n)>0$ 。

如果有多个能够精确分类训练数据点的解，那么应该尝试寻找泛化错误最⼩的那个解。 **⽀持向量机**解决这个问题的⽅法是：引⼊**边缘（`margin`）**的概念，这个概念被**定义**为决策边界与任意样本之间的最⼩距离，如图7.1所⽰。

![边缘](/images/prml_20191023102328.png)

如图7.2，最⼤化边缘会⽣成对决策边界的⼀个特定的选择，这个决策边界的位置由数据点的⼀个⼦集确定，被称为**⽀持向量**，⽤圆圈表⽰。

![⽀持向量](/images/prml_20191023102337.png)

在⽀持向量机中，决策边界被选为使边缘最⼤化的那个决策边界。

点 $\boldsymbol{x}$ 距离由 $y(\boldsymbol{x})=0$ 定义的超平⾯的垂直距离为 $\frac{|y(\boldsymbol{x})|}{\|\|\boldsymbol{w}\|\|}$ ，其中 $y(\boldsymbol{x})$ 的函数形式由公式(7.1)给出，我们感兴趣的是那些能够正确分类所有数据点的解，即对于所有的 $n$ 都有 $t_ny(\boldsymbol{x}_n)>0$ ，因此点 $\boldsymbol{x}_n$ 距离决策⾯的距离为
$$
\frac{t_ny(\boldsymbol{x}_n)}{\|\boldsymbol{w}\|}=\frac{t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)}{\|\boldsymbol{w}\|}\tag{7.2}
$$

边缘由数据集⾥垂直距离最近的点 $\boldsymbol{x}_n$ 给出，希望最优化参数 $\boldsymbol{w}$ 和 $b$ ，使得这个距离能够最⼤化。因此最⼤边缘解可以通过下式得到：

$$
\underset{\boldsymbol{w}, b}{\arg \max}\left\{\frac{1}{\|\boldsymbol{w}\|} \min _{n}\left[t_{n}\left(\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)+b\right)\right]\right\}\tag{7.3}
$$

注意到如果进⾏重新标度 $\boldsymbol{w}\to\kappa\boldsymbol{w}$ 以及 $b\to\kappa{b}$ ， 那么任意点 $\boldsymbol{x}_n$ 距离决策⾯的距离 $\frac{t_ny(\boldsymbol{x}_n)}{\|\|\boldsymbol{w}\|\|}$ 不会发⽣改变。利用这个性质，对于距离决策⾯最近的点，令

$$
t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)=1
$$

在这种情况下，所有的数据点会满⾜限制

$$
t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)\ge1,    n=1\dots,N
$$

这被称为**决策超平⾯的标准表⽰**。 对于使上式取得等号的数据点，我们说**限制被激活**（`active`），对于其他的数据点，我们说**限制未激活**（`inactive`）。根据定义，总会存在⾄少⼀个激活限制，因为总会有⼀个距离最近的点，并且⼀旦边缘被最⼤化，会有⾄少两个激活的限制。这样，最优化问题就简化为了最⼤化 $\|\|\boldsymbol{w}\|\|^{-1}$ ，这等价于最⼩化 $\|\|\boldsymbol{w}\|\|^2$ ，因此我们要在上述限制条件下，求解最优化问题

$$
\underset{\boldsymbol{w},b}{\arg\min}\frac{1}{2}\|\boldsymbol{w}\|^{2}
$$

为了解决这个限制的最优化问题，引⼊拉格朗⽇乘数 $a_n\ge0$ 。每个限制条件都对应着⼀个乘数 $a_n$ ，从⽽可得下⾯的拉格朗⽇函数

$$
L(\boldsymbol{w},b,\boldsymbol{a})=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{n=1}^{N}a_n\{t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)-1\}\tag{7.4}
$$

其中 $\boldsymbol{a} = (a_1,\dots,a_N)^{T}$ 。令 $L(\boldsymbol{w},b,\boldsymbol{a})$ 关于 $\boldsymbol{w}$ 和 $b$ 的导数等于零，有
$$
\boldsymbol{w}=\sum_{n=1}^{N}a_nt_n\boldsymbol{\phi})(\boldsymbol{x}_n)\\
\sum_{n=1}^{N}a_nt_n=0
$$

使⽤这两个条件从 $L(\boldsymbol{w},b,\boldsymbol{a})$ 中消去 $\boldsymbol{a}$ 和 $b$ ，就得到了最⼤化边缘问题的对偶表⽰（`dual representation`），其中要关于 $\boldsymbol{a}$ 最⼤化

$$
\tilde{L}(\boldsymbol{a})=\sum_{n=1}^{N}a_n-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_na_mt_nt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\tag{7.5}
$$

其中 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})$，限制条件为

$$
a_n\ge0,n=1\dots,N\\
\sum_{n=1}^{N}a_nt_n=0
$$

通过使⽤公式消去 $\boldsymbol{w}$ ，$y(\boldsymbol{x})$ 可以根据参数 $\{a_n\}$ 和核函数表⽰，即

$$
y(\boldsymbol{x})=\sum_{n=1}^{N}a_nt_nk(\boldsymbol{x},\boldsymbol{x}_n)+b\tag{7.6}
$$

满足如下性质：

$$
a_n\ge0\\
t_ny(\boldsymbol{x}_n)-1\ge0\\
a_n\{t_ny(\boldsymbol{x}_n)-1\}=0
$$

因此对于每个数据点，要么 $a_n = 0$ ，要么 $t_n y(\boldsymbol{x}_n) = 1$ 。任何使得 $a_n = 0$ 的数据点都不会出现在公式(7.5)的求和式中，因此对新数据点的预测没有作⽤。剩下的数据点被称为**⽀持向量**（`support vector`）。

解决了⼆次规划问题，找到了 $\boldsymbol{a}$ 的值之后，注意到⽀持向量  $\boldsymbol{x}_n$ 满⾜ $t_ny(\boldsymbol{x}_n)=1$，就可以确定阈值参数 $b$ 的值，可得
$$
t_n\left(\sum_{m\in{\mathcal{S}}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)+b\right)=1\tag{7.7}
$$

其中 $\mathcal{S}$ 表⽰⽀持向量的下标集合。 ⾸先乘以 $t_n$ ，使⽤ $t_n^2=1$ 的性质，然后对于所有的⽀持向量，整理⽅程，解出 $b$ ，可得

$$
b=\frac{1}{N_\mathcal{S}}\sum_{n\in{\mathcal{S}}}\left(t_n-\sum_{m\in{\mathcal{S}}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\right)\tag{7.8}
$$

其中 $N_\mathcal{S}$ 是⽀持向量的总数。

 对于接下来的模型⽐较，可以将最⼤边缘分类器⽤带有简单⼆次正则化项的最⼩化误差函数表⽰，形式为
$$
\sum_{n=1}^{N}E_{\propto}(y(\boldsymbol{x}_n)t_n-1)+\lambda\|\boldsymbol{w}\|^{2}
$$

其中$E_{\infty}(z)$ 是⼀个函数，当 $z\ge0$ 时，函数值为零，其他情况下函数值为 $\infty$ 。

如图7.3，⼆维空间中来⾃两个类别的⼈⼯⽣成数据的例⼦。图中画出了具有⾼斯核函数的⽀持向量机的得到的常数 $y(\boldsymbol{x}_n)$ 的轮廓线。同时给出的时决策边界、边缘边界以及⽀持向量。

![具有⾼斯核函数的⽀持向量机](/images/prml_20191024104707.png)

# 二，重叠类分布

在实际中，类条件分布可能重叠，这种情况下对训练数据的精确划分会导致较差的泛化能⼒。

引⼊**松弛变量**（`slack variable`）$\xi_n\ge 0$ ，其中 $n = 1,\dots, N$ ，每个训练数据点都有⼀个**松弛变量**（`Bennett`, 1992; `Cortes and Vapnik`, 1995）。对于位于正确的边缘边界内部的点或者边界上的点，$\xi_n=0$ ，对于其他点，$\xi_n=|t_n−y(\boldsymbol{x}_n)|$ 。 因此，对于位于决策边界 $y(\boldsymbol{x}n)=0$ 上的点，$\xi_n=1$ ，并且 $\xi_n>1$ 的点就是被误分类的点。 从而分类的限制条件为
$$
t_ny(\boldsymbol{x}_n)\ge1-\xi_n,n=1,\dots,N
$$

其中松弛变量被限制为满⾜ $\xi_n \ge 0$ 。$\xi_n = 0$ 的数据点被正确分类，要么位于边缘上，要么在边缘的正确⼀侧。$0 < \xi_n\le 1$ 的点位于边缘内部，但是在决策边界的正确⼀侧。$\xi_n > 1$ 的点位于决策边界的错误⼀侧，是被错误分类的点。这种⽅法有时被描述成放宽边缘的硬限制，得到⼀个**软边缘**（`soft margin`），并且允许⼀些训练数据点被错分。

如图7.4，松弛变量 $\xi_n \ge 0$ 的说明。圆圈标记的数据点是⽀持向量。

![松弛变量](/images/prml_20191023105522.png)

 现在的⽬标是最⼤化边缘，同时以⼀种⽐较柔和的⽅式惩罚位于边缘边界错误⼀侧的点。于是最⼩化
$$
C\sum_{n=1}^{N}\xi_n+\frac{1}{2}\|\boldsymbol{w}_n\|^{2}
$$

其中参数 $C>0$ 控制了松弛变量惩罚与边缘之间的折中。由于任何被误分类的数据点都有 $\xi_n>1$ ，因此 $\sum_{n}\xi_n$ 是误分类数据点数量的上界。于是，参数 $C$ 类似于（作⽤相反的）正则化系数，因为它控制了最⼩化训练误差与模型复杂度之间的折中。

现在想要在限制条件以及 $\xi_n \ge 0$ 的条件下最⼩化式，对应的拉格朗⽇函数为
$$
L(\boldsymbol{w},b,\boldsymbol{\xi},\boldsymbol{a},\boldsymbol{\mu})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C\sum_{n=1}^{N}\xi_n-\sum_{n=1}^{N}a_n\{t_ny(\boldsymbol{x}_n)-1+\xi_n\}-\sum_{n=1}^{N}\mu_n\xi_n\tag{7.9}
$$

其中 $\{a_n \ge 0\}$ 和 $\{\mu_n \ge 0\}$ 是拉格朗⽇乘数。对应的 **`KKT`** 条件为

$$
a_n\ge0\\
t_ny(\boldsymbol{x}_n)-1+\xi_n\ge0\\
a_n(t_ny(\boldsymbol{x}_n)-1+\xi_n)=0\\
\mu_n\ge0\\
\xi_n\ge0\\
\mu_n\xi_n=0
$$

其中 $n = 1,\dots, N$ 。

现在对 $\boldsymbol{w}$ , $b$ 和 $\{\xi_n\}$ 进⾏最优化，有
$$
\frac{\partial{L}}{\partial\boldsymbol{w}}=0\Rightarrow\boldsymbol{w}=\sum_{n=1}^{N}a_nt_n\boldsymbol{\phi}(\boldsymbol{x}_n)\\
\frac{\partial{L}}{\partial{b}}=0\Rightarrow\sum_{n=1}^{N}a_nt_n=0\\
\frac{\partial{L}}{\partial{\xi_n}}=0\Rightarrow{a_n}=C-\mu_n\tag{7.10}
$$

从而，

$$
\tilde{L}(\boldsymbol{a})=\sum_{n=1}^{N}a_n-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_na_mt_nt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\tag{7.11}
$$

关于对偶变量 $\{a_n\}$ 最⼤化公式(7.11)时必须要满⾜以下限制

$$
0\le{a_n}\le{C}\\
\sum_{n=1}^{N}a_nt_n=0
$$

其中 $n = 1,\dots, N$ 。 第一个公式被称为**盒限制**（`box constraint`）。

对于数据点的⼀个⼦集，有 $a_n = 0$ ，在这种情况下这些数据点对于预测模型没有贡献；剩余的数据点组成了⽀持向量。这些数据点满⾜ $a_n > 0$ ，必须满⾜
$$
t_ny(\boldsymbol{x}_n)=1-\xi_n\tag{7.12}
$$

如果 $a_n < C$ ，那么 $\mu_n > 0$ ，则有 $\xi_n = 0$ ，从⽽这些点位于边缘上；$a_n = C$ 的点位于边缘内部，并且如果 $\xi_n \le 1$ 则被正确分类，如果 $\xi_n > 1$ 则分类错误。

 为确定公式(7.1)中的参数 $b$ ，注意到 $0<a_n<C$ 的⽀持向量满⾜ $\xi_n = 0$ 即 $t_ny(\boldsymbol{x}_n)=1$ ，因此就满⾜
$$
t_n\left(\sum_{m\in{S}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)+b\right)=1\tag{7.13}
$$

通过求平均的⽅式得

$$
b=\frac{1}{N_\mathcal{M}}\sum_{n\in{\mathcal{M}}}\left(t_n-\sum_{m\in{\mathcal{S}}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\right)\tag{7.14}
$$

其中 $\mathcal{M}$ 表⽰满⾜ $0 < a_n < C$ 的数据点的下标的集合。

⽀持向量机的另⼀种等价形式， 被称为 $\nu-SVM$，由`Schölkopf et al.`（2000）提出。它涉及到最⼩化

$$
\tilde{L}(\boldsymbol{a})=-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_na_mt_nt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\tag{7.15}
$$

限制条件为

$$
0\le{a_n}\le\frac{1}{N}\\
\sum_{n=1}^{N}a_nt_n=0\\
\sum_{n=1}^{N}a_n\ge\nu
$$

这种⽅法的**优点**是，参数 $\nu$ 代替了参数 $C$ ，它既可以被看做**边缘错误**（`margin error`）（$\xi_n > 0$ 的点，因此就是位于边缘边界错误⼀侧的数据点，它可能被误分类也可能没被误分类）的上界， 也可以被看做⽀持向量⽐例的下界。

如图7.5，$\nu-SVM$ 应⽤于⼆维不可分数据集的例⼦，圆圈表⽰⽀持向量。这⾥使⽤了形如 $\exp(-\gamma\| \boldsymbol{x}−\boldsymbol{x}^{\prime}\|^{2})$ 的⾼斯核，且 $\gamma = 0.45$ 。

![ν-SVM](/images/prml_20191024113744.png)

⼀种最流⾏的训练⽀持向量机的⽅法被称为**顺序最⼩化优化**（`sequential minimal optimization`），或者称为  **`SMO`**（`Platt`, 1999），这种⽅法考虑了分块⽅法的极限情况，每次只考虑两个拉格朗⽇乘数。

考虑⼀个简单的⼆阶多项式核，⽤它的分量进⾏展开
$$
\begin{aligned}k(\boldsymbol{x},\boldsymbol{z})&=(1+\boldsymbol{x}^{T}\boldsymbol{z})^{2}\\&=(1+x_1z_1+x_2z_2)^{2}\\&=1+2x_1z_1+2x_2z_2+x_1^2z_1^2+2x_1z_1x_2z_2+x_2^2z_2^2\\&=(1,\sqrt{2}x_1,\sqrt{2}x_2,x_1^2,\sqrt{2}x_1x_2,x_2^2)(1,\sqrt{2}z_1,\sqrt{2}z_2,z_1^2,\sqrt{2}z_1z_2,z_2^2)^{T}\\&=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{z})\end{aligned}\tag{7.16}
$$

于是这个核函数表⽰六维特征空间中的⼀个内积， 其中输⼊空间到特征空间的映射由向量函数 $\boldsymbol{\phi}(\boldsymbol{x})$ 描述，然⽽对这些特征加权的系数被限制为具体的形式。因此，原始⼆维空间  $\boldsymbol{x}$ 中的 任意点集都会被限制到这个六维特征空间中的⼆维⾮线性流形中。

**⽀持向量机不提供概率输出，⽽是对新的输⼊进⾏分类决策。** `Veropoulos et al.`（1999）讨论了对 **`SVM`** 的修改，使其能控制假阳性和假阴性之间的折中。然⽽，如果希望把 **`SVM`** ⽤作较⼤的概率系统中的⼀个模块，那么需要对于新的输 ⼊ $\boldsymbol{x}$ 的类别标签 $t$ 的概率预测。为了解决这个问题，`Platt`（2000）提出了使⽤ `logistic sigmoid` 函数拟合训练过的⽀持向量机的输出的⽅法。具体来说，需要求解的条件概率被假设具有下⾯的形式

$$
p(t=1|\boldsymbol{x})=\sigma(Ay(\boldsymbol{x})+B)\tag{7.17}
$$

其中 $y(\boldsymbol{x})$ 由公式(7.1)定义， 参数 $A$ 和 $B$ 的值通过最⼩化交叉熵误差函数的⽅式确定。 交叉熵误差函数根据由 $y(\boldsymbol{x}_n)$ 和 $t_n$ 组成的训练数据集定义。⽤于拟合`sigmoid`函数的数据需要独⽴于训练原始 **`SVM`** 的数据，为了避免严重的过拟合现象。

# 三，与 **`logistic`** 回归的关系

对于位于边缘边界正确⼀侧的数据点，即满⾜ $y_nt_n\ge1$ 的数据点，有 $\xi_n = 0$ ，对于剩余的数据点，有 $\xi_n = 1 − y_nt_n$ 。因此⽬标函数可以写成（忽略整体的具有可乘性的常数）

$$
\sum_{n=1}^{N}E_{SV}(y_nt_n)+\lambda\|\boldsymbol{w}\|^{2}
$$

其中 $\lambda = (2C)^{−1}$ ，$E_{SV} (·)$ 是**铰链（`hinge`）误差函数**，定义为

$$
E_{SV}(y_nt_n)=[1-y_nt_n]_{+}
$$

其中 $[·]_+$ 表⽰正数部分。

如图7.6，⽀持向量机使⽤的“铰链”误差函数的图像，⽤蓝⾊表⽰。同时画出的还有`logistic`回归的误差函数，使⽤因⼦ $\frac{1}{\ln(2)}$ 重新放缩，从⽽通过点 $(0, 1)$ ，⽤红⾊表⽰，还画出了误分类误差函数（⿊⾊）和平⽅误差函数（绿⾊）。

![铰链误差函数](/images/prml_20191023112729.png)

考虑`logistic`回归模型，发现⽐较⽅便的做法是对⽬标变量 $t\in\{0, 1\}$ 进⾏操作。为了与⽀持向量机进⾏对⽐，⾸先使⽤⽬标变量 $t\in\{−1,1\}$ 重写最⼤似然`logistic`回归函数。注意到 $p(t=1|y)=\sigma(y)$ ，其中 $y(\boldsymbol{x})$ 由公式(7.1)给出，$\sigma(y)$ 是的 `logistic sigmoid`函数。因此有 $p(t=−1|y)=1−\sigma(y)=\sigma(−y)$ ，从而

$$
p(t|y)=\sigma(yt)\tag{7.18}
$$

从这个式⼦中可以通过对似然函数取负对数的⽅式构造⼀个误差函数。带有正则化项的误差函数的形式为

$$
\sum_{n=1}^{N}E_{LR}(y_nt_n)+\lambda\|\boldsymbol{w}\|^{2}
$$

其中

$$
E_{LR}(y_nt_n)=\ln(1+\exp(-yt))
$$

`logistic`误差函数与铰链损失都可以看成对误分类误差函数的连续近似。有时⽤于解决分类问题的另⼀个连续近似的误差函数是平⽅和误差函数。但是，它具有下⾯的**性质**：它会着重强调那些被正确分类的在正确的⼀侧距离决策边界较远的点。


# 四，多类 **`SVM`**

将多个两类 **`SVM`** 组合构造多类分类器的⼀种常⽤的⽅法（`Vapnik`, 1998）是构建 $K$ 个独⽴的 **`SVM`** ，其中第 $k$ 个模型 $y_k(\boldsymbol{x})$ 在训练时，使⽤来⾃类别 $\mathcal{C}_k$ 的数据作为正例，使⽤来⾃剩余的 $K − 1$ 个类别的数据作为负例。这被称为“**1对剩余**”（`one-versus-the-rest`）⽅法。然⽽使⽤独⽴的分类器进⾏决策会产⽣不相容的结果，其中⼀个输⼊会同时被分配到多个类别中，这个问题有时可以这样解决：对于新的输⼊ $\boldsymbol{x}$ ，使⽤下式做预测

$$
y(\boldsymbol{x})=\underset{k}{\max}y_k(\boldsymbol{x})\tag{7.19}
$$

不幸的是，这种启发式的⽅法会产⽣⼀个**问题**：不同的分类器是在不同的任务上进⾏训练的，⽆法保证不同分类器产⽣的实数值 $y_k(\boldsymbol{x})$ 具有恰当的标度。

“**1对剩余**”⽅法的另⼀个**问题**是训练集合不平衡。

`Lee et al.`（2001）提出了“1对剩余”⽅法的⼀种变体。这种变体修改了⽬标值，使得正例类别的⽬标值为 $+1$，负例类别的⽬标值为 $−\frac{1}{K−1}$ 。

`Weston and Watkins`（1999）定义了⼀个单⼀⽬标函数⽤来同时训练所有的 $K$ 个 **`SVM`** ，基于的是最⼤化每个类别与其余剩余类别的边缘。然⽽，这会导致训练过程变慢，因为这种⽅法需要求解的不是 $N$ 个数据点上的 $K$ 个独⽴的最优化问题（整体代价为 $O(KN^2)$），⽽是要求解⼀个规模为 $(K − 1)N$ 的单⼀的最优化问题，整体代价为 $O(K^2N^2)$ 。

另⼀种⽅法是在所有可能的类别对之间训练 $\frac{K(K−1)}{2}$ 个不同的⼆分类 **`SVM`** ，然后将测试数据点分到具有最⾼“投票数”的类别中去。这种⽅法有时被称为“**1对1**”（`one-versus-one`）。

后⼀个问题可以通过将每对分类器组织成有向⽆环图的⽅式解决，这就产⽣了 **`DAGSVM`** （`Platt et al.`, 2000）。对于 $K$ 个类别， **`DAGSVM`** 共有 $\frac{K(K−1)}{2}$ 个分类器。每次对新的测试点分类时，只需要 $K − 1$ 对分类器进⾏计算。选定的分类器是根据遍历图的路径确定的。

`Dietterich and Bakiri`（1995）提出了⼀种不同的⽅法解决多分类问题。这种⽅法基于的是误差-修正输出编码，并且被`Allwein et al.`（2000）⽤到⽀持向量机中，这种⽅法可以被看做“1对1”投票⽅法的⼀个推⼴。这种⽅法中，⽤来训练各个分类器的类别划分的⽅式更加⼀般，$K$ 个类别本⾝被表⽰为选定的两类分类器产⽣的响应的集合。结合⼀套合适的解码⽅法，这种⽅法对于错误以及各个分类器的输出的歧义性具有鲁棒性。

# 五，回归问题的 **`SVM`**

在简单的线性回归模型中， 最⼩化⼀个正则化的误差函数

$$
\frac{1}{2}\sum_{n=1}^{N}\{y_n-t_n\}^{2}+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}
$$

为了得到稀疏解，⼆次误差函数被替换为⼀个 $\epsilon$ **-不敏感误差函数**（`ϵ-insensitive error function`） （`Vapnik`, 1995）。如果预测 $y(\boldsymbol{x})$ 和⽬标 $t$ 之间的差的绝对值⼩于 $\epsilon$，那么这个误差函数给出的误差等于零，其中 $\epsilon > 0$ 。 $\epsilon$ **-不敏感误差函数** 的⼀个简单的例⼦

$$
E_{\epsilon}(y(\boldsymbol{x})-t) =
\begin{cases} 
0,  & 如果|y(\boldsymbol{w})-t|<\epsilon \\
|y(\boldsymbol{x})-t|-\epsilon, & 其他情况
\end{cases}\tag{7.20}
$$

它在不敏感区域之外，会有⼀个与误差相关联的线性代价。

如图7.7，$\epsilon$ -不敏感误差函数（红⾊）的图像。在不敏感区域之外，误差函数值随着距离线性增⼤。作为对⽐，同时给出了⼆次误差函数（绿⾊）。

![不敏感误差函数](/images/prml_20191023113908.png)

于是最⼩化正则化的误差函数，形式为

$$
C\sum_{n=1}^{N}E_{\epsilon}(y(\boldsymbol{x})-t_n)+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}
$$

其中 $y(\boldsymbol{x})$ 由公式(7.1)给出。

通过引⼊松弛变量的⽅式，可以重新表达最优化问题。对于每个数据点 $\boldsymbol{x}_n$ ，现在需要两个松弛变量 $\xi_n\ge0$ 和 $\hat{\xi}_n\ge0$ ， 其中 $\xi_n > 0$ 对应于 $t_n > y(\boldsymbol{x}_n)+\epsilon$ 的数据点，$\hat{\xi}>0$ 对应于 $t_n < y(\boldsymbol{x}_n)−\epsilon$ 的数据点。

如图7.8，**`SVM`** 回归的说明。图中画出了回归曲线以及 $\epsilon$-不敏感“管道”。同时给出的是松弛变量 $\xi$ 和 $\hat{\xi}$ 的例⼦。 对于 $\epsilon$ -管道上⽅的点，$\xi>0$ 且 $\hat{\xi}=0$ ，对于 $\epsilon$ -管道下⽅的点，$\xi=0$ 且 $\hat{\xi}>0$， 对于 $\epsilon$ -管道内部的点，$\xi=0$ 且 $\hat{\xi}=0$ 。

![SVM回归的说明](/images/prml_20191023114837.png)

⽬标点位于 $\epsilon$-管道内的条件是 $y_n−\epsilon \le t_n \le y_n+\epsilon$ ，其中 $y_n=y(\boldsymbol{x}_n)$ 。引⼊松弛变量使得数据点能够位于管道之外，只要松弛变量不为零即可。对应的条件变为
$$
t_n\le y(\boldsymbol{x}_n)+\epsilon+\xi_n\\
t_n\ge y(\boldsymbol{x}_n)-\epsilon-\hat{\xi}_n
$$

⽀持向量回归的误差函数可以写成

$$
C\sum_{n=1}^{N}(\xi_n+\hat{\xi}_n)+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}
$$

引⼊拉格朗⽇乘数 $a_n\ge0$ , $\hat{a}_n\ge0$ , $\mu_n\ge0$ 以及 $\hat{\mu}_n\ge0$ ，然后最优化拉格朗⽇函数

$$
\begin{aligned}L&=C\sum_{n=1}^{N}(\xi_n+\hat{\xi}_n)+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}-\sum_{n=1}^{N}(\mu_n\xi_n+\hat{\mu}_n\hat{\xi}_n)\\&-\sum_{n=1}^{N}a_n(\epsilon+\xi_n+y_n-t_n)-\sum_{n=1}^{N}\hat{a}_n(\epsilon+\hat{\xi}_n-y_n+t_n)\end{aligned}\tag{7.21}
$$

令拉格朗⽇函数关于 $\boldsymbol{w}$ , $b$ , $\xi_n$ 和 $\hat{\xi}_n$ 的导数为零，有

$$
\frac{\partial{L}}{\partial\boldsymbol{w}}=0\Rightarrow\boldsymbol{w}=\sum_{n=1}^{N}(a_n-\hat{a}_n)\boldsymbol{\phi}(\boldsymbol{x}_n)\\
\frac{\partial{L}}{\partial{b}}=0\Rightarrow\sum_{n=1}^{N}(a_n-\hat{a}_n)=0\\
\frac{\partial{L}}{\partial{\xi_n}}=0\Rightarrow{a_n}=C-\mu_n\\
\frac{\partial{L}}{\partial{\hat{\xi}_n}}=0\Rightarrow{\hat{a}_n}=C-\hat{\mu}_n
$$

对偶问题涉及到关于 $\{a_n\}$ 和 $\{\hat{a}_n\}$ 最⼤化

$$
\begin{aligned}\tilde{L}(\boldsymbol{a},\hat{\boldsymbol{a}})&=-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(a_n-\hat{a}_n)(a_m-\hat{a}_m)k(\boldsymbol{x}_n,\boldsymbol{x}_m)\\&-\epsilon\sum_{n=1}^{N}(a_n+\hat{a}_n)+\sum_{n=1}^{N}(a_n-\hat{a}_n)t_n\end{aligned}\tag{7.22}
$$

其中核 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})$ 。

从而盒限制

$$
0\le a_n \le C\\
0\le \hat{a}_n \le C
$$

对于新的输⼊变量，可以使⽤下式进⾏预测

$$
y(\boldsymbol{x})=\sum_{n=1}^{N}(a_n-\hat{a}_n)k(\boldsymbol{x},\boldsymbol{x}_n)+b\tag{7.23}
$$

对应的`Karush-Kuhn-Tucker`（`KKT`）条件说明了在解的位置，对偶变量与限制的乘积必须等于零，形式为

$$
a_n(\epsilon+\xi_n+y_n-t_n)=0\\
\hat{a}_n(\epsilon+\hat{\xi}_n-y_n+t_n)=0\\
(C-a_n)\xi_n=0\\
(C-\hat{a}_n)\hat{\xi}_n=0
$$

考虑⼀个数据点，满⾜ $0 < a_n < C$ 。根据公式，⼀定有$\xi_n=0$ ，$\epsilon + y_n − t_n = 0$ 。使⽤公式(7.1)，然后求解 $b$ ，有

$$
\begin{aligned}b&=t_n-\epsilon-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)\\&=t_n-\epsilon-\sum_{m=1}^{N}(a_m-\hat{a}_m)k(\boldsymbol{x}_n,\boldsymbol{x}_m)\end{aligned}\tag{7.24}
$$

与分类问题的情形相同，有另⼀种⽤于回归的 **`SVM`** 的形式。这种形式的 **`SVM`** 中，控制复杂度的参数有⼀个更加直观的意义（`Schölkopf et al.`, 2000）。特别地，我们不固定不敏感区域 $\epsilon$ 的宽度，⽽是固定位于管道外部的数据点的⽐例 $\nu$ ，涉及到最⼤化

$$
\begin{aligned}\tilde{L}(\boldsymbol{a},\hat{\boldsymbol{a}})&=-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(a_n-\hat{a}_n)(a_m-\hat{a}_m)k(\boldsymbol{x}_n,\boldsymbol{x}_m)\\&+\sum_{n=1}^{N}(a_n-\hat{a}_n)t_n\end{aligned}\tag{7.25}
$$

限制条件为

$$
0\le a_n \le \frac{C}{N}\\
0\le \hat{a}_n \le \frac{C}{N}\\
\sum_{n=1}^{N}(a_n-\hat{a}_n)=0\\
\sum_{n=1}^{N}(a_n+\hat{a}_n)\le\nu C
$$

如图7.9，$\nu-SVM$ 回归应⽤到⼈⼯⽣成的正弦数据集上的说明，**`SVM`** 使⽤了⾼斯核。预测分布曲线为红⾊曲线，$\epsilon$ -不敏感管道对应于阴影区域。此外，数据点⽤绿⾊表⽰，⽀持向量⽤蓝⾊圆圈标记。

![SVM回归应⽤到⼈⼯⽣成的正弦数据集上的说明](/images/prml_20191023201238.png)

# 六，计算学习理论

历史上， ⽀持向量机⼤量地使⽤⼀个被称为**计算学习理论**（`computational learning theory`）的理论框架进⾏分析。这个框架有时候也被称为**统计学习理论**（`statistical learning theory`）（`Anthony and Biggs`, 1992; `Kearns and Vazirani`, 1994; `Vapnik`, 1995; `Vapnik`, 1998）。 这个框架起源于`Valiant`（1984），他建⽴了概率近似正确（`probably approximately correct`）或者称为 **`PAC`** 的学习框架。**`PAC`** 学习框架的⽬标是理解为两个给出较好的泛化能⼒，需要多⼤的数据集。

假设从联合概率分布 $p(\boldsymbol{x},\boldsymbol{t})$ 中抽取⼀个⼤⼩为 $N$ 的数据集 $\mathcal{D}$ ，其中 $\boldsymbol{x}$ 是输⼊变量，$\boldsymbol{t}$ 表⽰类别标签。我们把注意⼒集中于“⽆噪声”的情况，即类别标签由某个（未知的）判别函数 $\boldsymbol{t} = \boldsymbol{g}(\boldsymbol{x})$ 确定。在`PAC`学习中，空间 $\mathcal{F}$ 是⼀个以训练集 $\mathcal{D}$ 为基础的函数组成的空间，从空间 $\mathcal{F}$ 中抽取⼀个函数 $\boldsymbol{f}(\boldsymbol{x};\mathcal{D})$，如果它的期望错误率⼩于某个预先设定的阈值 $\epsilon$ ，即


$$
\mathbb{E}_{\boldsymbol{x},\boldsymbol{t}}[I(\boldsymbol{f}(\boldsymbol{x};\mathcal{D})\ne\boldsymbol{t})]<\epsilon\tag{7.26}
$$

那么就说函数 $\boldsymbol{f}(\boldsymbol{x};\mathcal{D})$ 具有较好的泛化能⼒。 其中 $I(·)$ 是⽰性函数，期望是关于概率分布 $p(\boldsymbol{x},\boldsymbol{t})$ 的期望。 式⼦左侧的项是⼀个随机变量， 因为它依赖于训练数据集 $\mathcal{D}$ 。**`PAC`** 框架要求，对于从概率分布 $p(\boldsymbol{x},\boldsymbol{t})$ 中随机抽取的数据集 $\mathcal{D}$ ，公式(7.26)成⽴的概率要⼤于 $1−\delta$ 。 这 ⾥ $\delta$ 是另⼀个预先设定的参数，术语“**概率近似正确**”来⾃于下⾯的要求：以⼀个较⾼的概 率（⼤于 $1−\delta$ ）， 使得错误率较⼩（⼩于 $\epsilon$ ）。 对于⼀个给定的模型空间 $\mathcal{F}$ ， 以及给定的参数 $\epsilon$ 和 $\delta$ ， **`PAC`** 学习的⽬标是提供满⾜这个准则所需的最⼩数据集规模 $N$ 的界限。在 **`PAC`** 学习中， ⼀个**关键的量**是 **`Vapnik-Chervonenkis`维度** （`Vapnik-Chervonenkis dimension`），或者被称为 **`VC`维度** ，它提供了函数空间复杂度的⼀个度量，使得 **`PAC`** 框架能够扩展到包含⽆穷多个函数的空间。

⼀种提升 **`PAC`** 界限的紧致程度的⽅法是  **`PAC`** -贝叶斯框架（`PAC-Bayesian framework`）（`McAllester`, 2003）， 它考虑了空间 $\mathcal{F}$ 上的函数的概率分布情况， 有些类似于贝叶斯⽅法中的先验概率。
