---
title: 【论文笔记】《基于深度学习的中文命名实体识别研究》阅读笔记
type: categories
copyright: false
date: 2020-03-01 12:29:30
tags: paper
categories: 科研笔记
title_en: paper_01
mathjax: true
---


> 作者及其单位：北京邮电大学，张俊遥，2019年6月，硕士论文

# 摘要

实验数据：来源于网络公开的新闻文本数据；用随机欠采样和过采样的方法解决分类不均衡问题；使用`BIO`格式的标签识别5类命名实体，标注11种标签。

学习模型：基于`RNN-CRF`框架，提出`Bi-GRU-Attention`模型；基于改进的`ELMo`可移植模型。

# 一，绪论

## 1，研究背景及意义

> 研究背景主要介绍的是时代背景及`NER`的应用领域。

## 2，研究现状

> 1）基于规则和词典的方法；
>
> 2）基于统计的方法：语言的`N`元模型，隐马尔科夫模型，最大熵模型，条件随机场，支持向量机，决策树，基于转换的学习方法；
>
> 3）基于深度学习的方法：基于双向循环神经网络与条件随机场结合的框架；基于标签转移与窗口滑动的方法；注意力机制(`Attention`)；
>
> 4）基于迁移学习的方法。


面临挑战：

> 1）中文命名实体界限难划分；
>
> 2）中文命名实体结构更多样复杂；
>
> 3）中文命名实体分类标准不同，划分标注结果不同。

## 3，研究内容

> 1）数据集收集与预处理；
>
> 2）基于双向循环神经网络与条件随机场模型的研究；
>
> 3）基于`ELMo`的可移植模型研究。

# 二，相关技术

## 1，基于循环神经网络方法的技术

> 1）神经单元结构：循环是指一个神经单元的计算是按照时间顺序展开依次进行的过程。具有记忆特征，常用来处理与序列相关的问题。
>
> 2）循环神经网络的发展：`LSTM`取代`CNN`，主要是解决`CNN`单元的反向传播的计算问题。
>
> 3）深层网络搭建：`Dropout`常被用作防止模型过拟合，减少网络冗余度，增加模型鲁棒性；批量归一化策略是批量梯度下降算法过程的一项操作；`clip`是一种有效控制梯度爆炸的算法。
>
> 4）目标函数，即损失函数，衡量经过模型计算的预测结果和事实上的结果之间的差距。如：平方差，交叉熵，`softmax`。
>
> 5）注意力机制：论文研究了在`LSTM`中引入注意力机制。
>
> 6）`Adam`优化算法：适合解决梯度稀疏或噪音较高的优化问题。

## 2，基于迁移学习方法的技术

> 1）基本思想：
>
> > （1）预训练的两种基本思路：
> >
> > > a）基于共同表示形式的思路：电子文本大多以某种向量形式（词，句，段，文本）表示输入到网络中，如`ELMo`模型。
> > >
> > > b）基于网络微调的思想：借鉴机器视觉领域的模型思想，在预训练好的模型上加入针对任务的功能层，在对后几层进行结构和参数设置的精调。
>
> 2）语言模型：双向语言模型
>
> 3）词向量技术：`One-hot`向量，稀疏向量和稠密向量。
>
> > （1）基于统计的方法
> >
> > > a）基于共现矩阵的方法：在设定的窗口大小内，统计了一个句子中词语前后相邻出现的次数，使用这个次数构成的向量当作词向量，这个向量比较稀疏。
> > >
> > > b）奇异值分解的方法：可以看作一种降维过程，把稀疏矩阵压缩为稠密矩阵的过程。
> >
> > （2）基于语言模型的方法：
> >
> > > a）跳字模型（`skip-gram`）：使用一个词来预测上下文词语；
> > >
> > > b）连续词袋模型（`CBOW`）：使用周围词语预测中心词；
> > >
> > > c）`ELMo`模型：词向量表达过程是动态的，即一词多义下的词向量完全不同。
>
> 4）混淆矩阵：数据科学，数据分析和机器学习中统计分类的实际结果和预测结果的表格表示。

# 三，命名实体识别任务与数据集

## 1，命名实体识别任务

> 1）定义：命名实体识别属于序列标注类问题，分为三大类（实体类，数量类，时间类），七小类（人名，地名，组织名，日期，时间，货币或者百分比）。
>
> 2）任务过程：准确划分出命名实体的边界，并将命名实体进行正确的分类。
>
> 3）判别标准：（1）准确划分出命名实体的边界；（2）命名实体的标注分类正确；（3）命名实体内部位置标注有序。
> $$
> 准确率=\frac{标注结果正确的数量}{标注结果的数量}\times{100\%} \\召回率=\frac{标注命名实体正确的数量}{标注命名实体的数量}\times{100\%}\\F_1=\frac{(\beta^{2}+1)\times 准确率\times 召回率}{(\beta^{2}\times 准确率) + 召回率}\times{100\%}
> $$

## 2，数据集收集与处理

> 1）数据源：本论文数据来源于搜狗实验室公开的2012年6月到7月期间的国内外国际、体育、社会、娱乐等18类新闻文本。
>
> 2）数据处理：`jieba`+盘古工具，本文研究`NER`分为五类：人名（58136），地名（87412），机构名（5142），时间（75491），数量（148392）。数据集（句子个数）分：训练集（197828），验证集（8994），测试集（3485）。

# 四，基于改进的神经网络与注意力机制结合的研究

## 1，RNN-CRF框架

> 1）框架结构：以`Bi-LSTM-CRF`模型为例，包括字嵌入层（字量化表示，输入到神经网络），`Bi-LSTM`神经网络层（双向网络记录了上下文信息，据此共同训练计算当前的字的新向量表示，其输出字或词的向量维度与神经单元数量有关），`CRF`层（进行进一步标签顺序的规则学习）。
>
> 2）模型原理：将输入的语句转换为词向量，然后输入到`LSTM`网络计算，接着在`CRF`层中计算输出标签，根据定义的目标函数计算损失，使用梯度下降等算法更新模型中的参数。

## 2，改进与设计

> 1）改进的思想与结构设计：改进思路就是简化神经单元结构，本文使用双向的`GRU`结构代替`LSTM`单元结构，使用神经网络与注意力机制结合。
>
> 2）改进的模型设计

## 3，实验与分析

> 1）实验思路是以`Bi-LSTM-CRF`为基础，并进行网络优化，对比本文提出的`Bi-GRU-Attention`模型。

实验一：`Bi-LSTM`网络参数

| 参数名称      | 数值  |
| ------------- | ----- |
| batch_size    | 20    |
| max_num_steps | 20    |
| 优化器        | Admin |
| 初始学习率    | 0.001 |
| 衰减率        | 0.8   |
| clip          | 5     |
| one-hot_dim   | 11    |

实验二：`GRU-Attention`模型实验参数

| 参数          | 数值  |
| ------------- | ----- |
| batch_size    | 20    |
| char_dim      | 100   |
| max_num_steps | 20    |
| 神经单元数    | 128   |
| 优化器        | Adam  |
| 初始学习率    | 0.001 |
| 衰减率        | 0.8   |
| one-hot_dim   | 11    |
| epoch         | 100   |

实验结果如下：

| 分类/F1/模型 | Bi-LSTM-CRF | Bi-LSTM-Attention | Bi-GRU-CRF | Bi-GRU-Attention |
| ------------ | ----------- | ----------------- | ---------- | ---------------- |
| 人名         | 82.32%      | 82.45%            | 82.22%     | 82.42%           |
| 地名         | 89.97%      | 90.19%            | 89.93%     | 91.06%           |
| 机构名       | 91.94%      | 91.96%            | 91.94%     | 91.95%           |
| 数量         | 94.98%      | 95.06%            | 95.01%     | 95.26%           |
| 时间         | 96.05%      | 96.14%            | 96.06%     | 96.14%           |

# 五，基于ELMo的可移植模型研究

## 1，改进的`ELMo`模型设计

> `ELMo`模型在2018年由`Peter`提出，`Peter`团队使用双层的循环神经网络实现模型的预先训练。本章基于`Peter`的`ELMo`模型设计，提出了直通结构，实现词向量的提前训练模型。
>
> 1）模型原理：`Peters`使用`CNN-BIG-LSTM`网络实现模型，使用卷积神经网络实现字符编码，使用两层双向循环神经网络实现词向量的训练模型。
>
> 2）改进与设计：本文使用改进的`ELMo`预先训练模型包含输入层，卷积神经网络7层，双向神经网络2层，输出层结构。

## 2，基于`ELMo`的嵌入式模型设计

> 1）连接结构：在模型嵌入的衔接层中，本文使用维度映射的方法，将不同维度的输入输出维度进行统一。
>
> 2）模型设计：本文的嵌入`ELMo`模型，包含`ELMo`层，衔接层，神经网络层，注意力层和输出调整层结构。

## 3，实验

实验参数配置如下：

1）`ELMo`模型实验参数

| 参数名称         | 数值  |
| ---------------- | ----- |
| word_dim         | 100   |
| char_dim         | 50    |
| activation       | ReLU  |
| 每层神经单元数目 | 512   |
| 优化器           | Adam  |
| 初始学习率       | 0.001 |
| lr_decay         | 0.8   |
| clip             | 3     |

2）卷积神经网络参数

| 卷积层 | 输出词向量维度 | 过滤器个数 |
| ------ | -------------- | ---------- |
| conv1  | 32             | 32         |
| conv2  | 32             | 32         |
| conv3  | 64             | 64         |
| conv4  | 128            | 128        |
| conv5  | 256            | 256        |
| conv6  | 512            | 512        |
| conv7  | 1024           | 1024       |

3）移植模型实验参数

| 参数名称      | 数值  |
| ------------- | ----- |
| batch_size    | 20    |
| char_dim      | 100   |
| max_num_steps | 20    |
| 神经单元数    | 128   |
| 优化器        | Adam  |
| 初始化学习率  | 0.001 |
| clip          | 5     |
| dropout       | 0.1   |
| one-hot_dim   | 11    |
| epoch         | 100   |

实验结果对比：

| 分类/F1/模型 | Bi-LSTM-CRF | Bi-GRU-Attention | 改进的ELMo嵌入模型 |
| ------------ | ----------- | ---------------- | ------------------ |
| 人名         | 82.32%      | 82.42%           | 83.14%             |
| 地名         | 89.97%      | 91.06%           | 92.36%             |
| 机构名       | 91.94%      | 91.95%           | 93.02%             |
| 数量         | 94.98%      | 95.26%           | 96.13%             |
| 时间         | 96.05%      | 96.14%           | 96.55%             |

# 六，总结与展望

## 1，总结

> 本文主要研究了基于深度学习的中文命名实体识别任务，提出了`Bi-GRU-Attention`模型减少训练时间，提升模型准确率；提出了基于改进的`ELMo`可移植模型，用于应对少量数据集和快速移植不同场景的问题。

## 2，不足与发展趋势

> 1）公开的权威的中文文本数据集不足；
>
> 2）可以划分更细的领域或分类，分别涉及分类器；
>
> 3）基于迁移学习的多任务模型研究是热点。

**阅读心得**：绪论内容相对详细，结构中规中矩，美中不足缺乏对研究对象现状的介绍，国内外研究现状，要解决的问题以及达到的预期效果未尽阐述。技术要点论述详尽，本文设计实验充分且多角度论证，扩展实验与改进设计也具有一定创新性。通过本篇论文研究学习，在`NER`领域收获颇多，很多知识有待弥补，如`ELMo`模型，迁移学习方面需要加强学习。