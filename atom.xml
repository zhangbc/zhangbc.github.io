<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>天堂的鸽子</title>
  
  <subtitle>天道酬勤</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangbc.github.io/"/>
  <updated>2019-10-18T15:53:22.804Z</updated>
  <id>https://zhangbc.github.io/</id>
  
  <author>
    <name>Bocheng Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【机器学习基础】贝叶斯神经网络</title>
    <link href="https://zhangbc.github.io/2019/10/18/prml_05_04/"/>
    <id>https://zhangbc.github.io/2019/10/18/prml_05_04/</id>
    <published>2019-10-18T10:21:17.000Z</published>
    <updated>2019-10-18T15:53:22.804Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，混合密度网络"><a href="#一，混合密度网络" class="headerlink" title="一，混合密度网络"></a>一，混合密度网络</h1><p>作为逆问题，考虑机械臂的运动学问题。<strong>正向问题</strong>（<code>forward problem</code>）是在给定连接角的情况下求解机械臂末端的位置，这个问题有唯⼀解。然⽽，在实际应⽤中，我们想把机械臂末端移动到⼀个具体的位置，为了完成移动，必须设定合适的连接角。正向问题通常对应于物理系统的因果关系，通常有唯⼀解。</p><p>图5.29～5.30，图5.29给展⽰了⼀个具有两个连接的机械臂，其中，末端的笛卡尔坐标 $(x_1, x_2)$ 由两个连接角 $\theta_1$ 和 $\theta_2$ 以及机械臂的（固定）长度 $L_1$ 和 $L_2$ 唯⼀确定。这被称为<strong>机械臂的正向运动学</strong> （<code>forward kinematics</code>）。在实际应⽤中，必须寻找给出所需的末端位置的连接角，如图5.30所⽰，这个<strong>逆向运动学</strong>（<code>inverse kinematics</code>）有两个对应的解，即“<strong>肘部向上</strong>”和“<strong>肘部向下</strong>”。</p><p><img src="/images/prml_20191016115033.png" alt="正向运动学"></p><p><img src="/images/prml_20191016115043.png" alt="逆向运动学"></p><p>考虑⼀个具有多峰性质的问题，数据的⽣成⽅式为：对服从区间 $(0, 1)$ 的均匀分布的变量 $x$ 进⾏取样，得到⼀组值 $\{x_n\}$ ，对应的⽬标值 $t_n$ 通过下⾯的⽅式得到：计算函数 $x_n + 0.3 \sin(2\pi{x_n})$ ，然后添加⼀个服从 $(−0.1, 0.1)$ 上的均匀分布的噪声。这样，逆问题就可以这样得到：使⽤相同的数据点，但是交换 $x$ 和 $t$ 的⾓⾊。</p><p>图5.31～5.32，图5.31是⼀个简单的“正向问题”的数据集，其中红⾊曲线给出了通过最⼩化平⽅和误差函数调节⼀个两层神经⽹络的结果。对应的逆问题，如图5.32所⽰，通过交换 $x$ 和 $t$ 的顺序的⽅式得到。这⾥，通过最⼩化平⽅和误差函数的⽅式训练的神经⽹络给出了对数据的⾮常差的拟合，因为数据集是多峰的。</p><p><img src="/images/prml_20191016115547.png" alt="正向问题数据集"></p><p><img src="/images/prml_20191016115556.png" alt="逆向问题数据集"></p><p>寻找⼀个对条件概率密度建模的⼀般的框架：为 $p(\boldsymbol{t}|\boldsymbol{x})$ 使⽤⼀个混合模型，模型的混合系数和每个分量的概率分布都是输⼊向量 $\boldsymbol{x}$ 的⼀个⽐较灵活的函数，这就构成了<strong>混合密度⽹络</strong>（<code>mixture density network</code>）。对于任意给定的 $\boldsymbol{x}$ 值，混合模型提供了⼀个通⽤的形式，⽤来对任意条件概率密度函数 $p(\boldsymbol{t}|\boldsymbol{x})$ 进⾏建模。</p><p>图5.33，<strong>混合密度⽹络</strong>（<code>mixture density network</code>）可以表⽰⼀般的条件概率密度 $p(\boldsymbol{t}|\boldsymbol{x})$ ， ⽅法为：考虑 $\boldsymbol{t}$ 的⼀个参数化的混合模型，参数由以 $\boldsymbol{x}$ 为输⼊的神经⽹络的输出确定。</p><p><img src="/images/prml_20191016115604.png" alt="混合密度⽹络"></p><p>显式地令模型的分量为⾼斯分布，即</p><script type="math/tex; mode=display">p(\boldsymbol{t}|\boldsymbol{x})=\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})\mathcal{N}(\boldsymbol{t}|\boldsymbol{\mu}(\boldsymbol{x}),\sigma_{k}^{2}(\boldsymbol{x})\boldsymbol{I})\tag{5.99}</script><p>混合系数必须满⾜下⾯的限制</p><script type="math/tex; mode=display">\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})=1,    0 \le \pi_{k}(\boldsymbol{x}) \le 1</script><p>可以通过使⽤⼀组<code>softmax</code>输出来实现</p><script type="math/tex; mode=display">\pi_{k}(\boldsymbol{x})=\frac{\exp(a_{k}^{\pi})}{\sum_{l=1}^{K}\exp(a_{l}^{\pi})}\tag{5.100}</script><p>⽅差必须满⾜ $\sigma_{k}^{2}(\boldsymbol{x})\ge0$ ，因此可以使⽤对应的⽹络激活的指数形式表⽰，即</p><script type="math/tex; mode=display">\sigma_{k}(\boldsymbol{x})=\exp(a_{k}^{\sigma})\tag{5.101}</script><p>由于均值 $\mu_{k}(\boldsymbol{x})$ 有实数分量，因此可以直接⽤⽹络的输出激活表⽰</p><script type="math/tex; mode=display">\mu_{kj}(\boldsymbol{x})=a_{kj}^{\mu}\tag{5.102}</script><p>混合密度⽹络的可调节参数由权向量 $\boldsymbol{w}$ 和偏置组成。这些参数可以通过最⼤似然法确定，或者等价地，使⽤最⼩化误差函数（负对数似然函数）的⽅法确定。对于独⽴的数据，误差函数的形式为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\ln\left\{\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x}_n,\boldsymbol{w})\mathcal{N}(\boldsymbol{t}_n|\boldsymbol{\mu}_k(\boldsymbol{x}_n,\boldsymbol{w}),\sigma_{k}^{2}(\boldsymbol{x}_n,\boldsymbol{w})\boldsymbol{I})\right\}\tag{5.103}</script><p>把混合系数 $\pi_k(\boldsymbol{x})$ 看成与 $\boldsymbol{x}$ 相关的先验概率分布，从⽽就引⼊了对应的后验概率，形式为</p><script type="math/tex; mode=display">\gamma_{nk}=\gamma_{k}(\boldsymbol{t}_n|\boldsymbol{x}_n)=\frac{\pi_{k}\mathcal{N}_{nk}}{\sum_{l=1}^{K}\pi_{l}\mathcal{N}_{nl}}\tag{5.104}</script><p>其中 $\mathcal{N}_{nk}$ 表⽰ $\mathcal{N}(\boldsymbol{t}_n | \boldsymbol{\mu}_k(\boldsymbol{x}_n),\sigma_{k}^{2}(\boldsymbol{x}_n))$ 。 </p><p>关于控制混合系数的⽹络输出激活的导数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{a_{k}^{\pi}}}=\pi_{k}-\gamma_{nk}\tag{5.105}</script><p>关于控制分量均值的⽹络输出激活的导数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{a_{kl}^{\mu}}}=\gamma_{k}\left\{\frac{\mu_{kl}-t_{nl}}{\sigma_{k}^{2}}\right\}\tag{5.106}</script><p>关于控制分量⽅差的⽹络激活函数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{a_{k}^{\sigma}}}=\gamma_{nk}\left\{L-\frac{\|\boldsymbol{t}_n-\boldsymbol{\mu}_{k}\|^{2}}{\sigma_{k}^{2}}\right\}\tag{5.107}</script><p>如图5.34～5.37，(<code>a</code>)对于给出的数据训练的混合密度⽹络的三个核函数，混合系数 $\pi_k(\boldsymbol{x})$ 与 $\boldsymbol{x}$ 的函数关系图像。模型有三个⾼斯分量，使⽤了⼀个多层感知器，在隐含层有五个“ $tanh$ ”单元，同时有9个输出单元 （对应于⾼斯分量的3个均值、3个⽅差以及3个混合系数）。在较⼩的 $x$ 值和较⼤的 $x$ 值处，⽬标数据的条件概率密度是单峰的，对于它的先验概率分布，只有⼀个核具有最⼤的值。⽽在中间的 $x$ 值处，条件概率分布具有3个峰，3个混合系数具有可⽐的值。(<code>b</code>)使⽤与混合系数相同的颜⾊表⽰⽅法来表⽰均值 $\mu_k (x)$ 。 (<code>c</code>)对于同样的混合密度⽹络，⽬标数据的条件概率密度的图像。 (d)条件概率密度的近似条件峰值的图像，⽤红⾊点表⽰。</p><p><img src="/images/prml_20191016151145.png" alt="a"></p><p><img src="/images/prml_20191016151155.png" alt="b"></p><p><img src="/images/prml_20191016151204.png" alt="c"></p><p><img src="/images/prml_20191016151212.png" alt="d"></p><p>⼀旦混合密度⽹络训练结束，可以预测对于任意给定的输⼊向量的⽬标数据的条件密度函数。只要我们关注的是预测输出向量的值的问题，那么这个条件概率密度就能完整地描述⽤于⽣成数据的概率分布。根据这个概率密度函数，可以计算不同应⽤中我们感兴趣的更加具体的量。⼀个最简单的量就是⽬标数据的条件均值，即</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{t}|\boldsymbol{x}]=\int\boldsymbol{t}p(\boldsymbol{t}|\boldsymbol{x})\mathrm{d}\boldsymbol{t}=\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})\boldsymbol{\mu}_{k}(\boldsymbol{x})\tag{5.108}</script><p>密度函数的⽅差，结果为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{s}^{2}(\boldsymbol{x})&=\mathbb{E}[\|\boldsymbol{t}-\mathbb{E}[\boldsymbol{t}|\boldsymbol{x}]\|^{2}|\boldsymbol{x}]\\&=\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})\left\{\boldsymbol{\sigma}_{k}^{2}+\left\|\boldsymbol{\mu}_k(\boldsymbol{x})-\sum_{l=1}^{K}\pi_{l}(\boldsymbol{x})\boldsymbol{\mu}_{l}(\boldsymbol{x})\right\|^{2}\right\}\end{aligned}\tag{5.109}</script><h1 id="二，贝叶斯神经网络"><a href="#二，贝叶斯神经网络" class="headerlink" title="二，贝叶斯神经网络"></a>二，贝叶斯神经网络</h1><h2 id="1，后验参数分布"><a href="#1，后验参数分布" class="headerlink" title="1，后验参数分布"></a>1，后验参数分布</h2><p>考虑从输⼊向量 $\boldsymbol{x}$ 预测单⼀连续⽬标变量 $t$ 的问题。假设条件概率分布 $p(t|\boldsymbol{x})$ 是⼀个⾼斯分布，均值与 $\boldsymbol{x}$ 有关，由神经⽹络模型的输出 $y(\boldsymbol{x}, \boldsymbol{x})$ 确定，精度（⽅差的倒数）$\beta$ 为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w},\beta)=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1})\tag{5.110}</script><p>将权值 $\boldsymbol{w}$ 的先验概率分布选为⾼斯分布，形式为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\alpha)=\mathcal{N}(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\boldsymbol{I})\tag{5.111}</script><p>对于 $N$ 次独⽴同分布的观测 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ ，对应的⽬标值集合 $\mathcal{D}=\{t_1,\dots, t_N\}$ ，似然函数为</p><script type="math/tex; mode=display">p(\mathcal{D}|\boldsymbol{w},\beta)=\prod_{n=1}^{N}\mathcal{N}(t_n|y(\boldsymbol{x}_n,\boldsymbol{w}),\beta^{-1})\tag{5.112}</script><p>最终的后验概率为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\mathcal{D},\alpha,\beta)\propto p(\boldsymbol{w}|\alpha)p(\mathcal{D}|\boldsymbol{w},\beta)\tag{5.113}</script><p>由于 $y(\boldsymbol{x},\boldsymbol{w})$ 与 $\boldsymbol{w}$ 的关系是⾮线性的，因此后验概率不是⾼斯分布。</p><p>使⽤拉普拉斯近似，可以找到对于后验概率分布的⼀个⾼斯近似。⾸先找到后验概率分布的⼀个（局部）最⼤值，这必须使⽤迭代的数值最优化算法才能找到，⽐较⽅便的做法是最⼤化后验概率分布的对数，可以写成</p><script type="math/tex; mode=display">\ln p(\boldsymbol{w}|\mathcal{D})=-\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}-\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w})-t_n\}^{2}+常数\tag{5.114}</script><p>负对数后验概率的⼆阶导数为</p><script type="math/tex; mode=display">\boldsymbol{A}=-\nabla\nabla\ln p(\boldsymbol{w}|\mathcal{D},\alpha,\beta)=\alpha\boldsymbol{I}+\beta\boldsymbol{H}\tag{5.115}</script><p>其中，$\boldsymbol{H}$ 是⼀个<code>Hessian</code>矩阵，由平⽅和误差函数关于 $\boldsymbol{w}$ 的分量组成，后验概率对应的⾼斯近似形式为</p><script type="math/tex; mode=display">q(\boldsymbol{w}|\mathcal{D})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{w}_{MAP},\boldsymbol{A}^{-1})\tag{5.116}</script><p>预测分布可以通过将后验概率分布求积分的⽅式获得</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D})=\int p(t|\boldsymbol{x},\boldsymbol{w})q(\boldsymbol{w}|\mathcal{D})\mathrm{d}\boldsymbol{w}\tag{5.117}</script><p>现在假设与 $y(\boldsymbol{x}, \boldsymbol{w})$ 发⽣变化造成的 $\boldsymbol{w}$ 的变化幅度相⽐，后验概率分布的⽅差较⼩。这使得可以在 $\boldsymbol{w}_{MAP}$ 附近对⽹络函数进⾏泰勒展开，只保留展开式的现⾏项，可得</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})\simeq y(\boldsymbol{x},\boldsymbol{w}_{WAP})+\boldsymbol{g}^{T}(\boldsymbol{w}-\boldsymbol{w}_{MAP})\tag{5.118}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{g}=\nabla_{\boldsymbol{w}}y(\boldsymbol{x},\boldsymbol{w})|_{\boldsymbol{w}=\boldsymbol{w}_{WAP}}</script><p>使⽤这个近似，现在得到了⼀个线性⾼斯模型，$p(\boldsymbol{w})$ 为⾼斯分布，并且，$p(t|\boldsymbol{w})$ 也是⾼斯分布，均值是 $\boldsymbol{w}$ 的线性函数，分布的形式为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w},\beta)\simeq \mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}_{MAP})+\boldsymbol{g}^{T}(\boldsymbol{w}-\boldsymbol{w}_{MAP}),\beta^{-1})\tag{5.119}</script><p>边缘分布 $p(t)$ 的⼀般结果，得到</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D},\alpha,\beta)=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}_{MAP}),\sigma^{2}(\boldsymbol{x}))\tag{5.120}</script><p>其中，与输⼊相关的⽅差为</p><script type="math/tex; mode=display">\sigma^{2}(\boldsymbol{x})=\beta^{-1}+\boldsymbol{g}^{T}\boldsymbol{A}^{-1}\boldsymbol{g}</script><p>由此可见，预测分布 $p(t|\boldsymbol{x},\mathcal{D})$ 是⼀个⾼斯分布， 它的均值由⽹络函数 $y(\boldsymbol{x},w_{MAP})$ 给出， 参数设置为 $MAP$ 值。⽅差由两项组成，第⼀项来⾃⽬标变量的固有噪声，第⼆项是⼀个与 $\boldsymbol{x}$ 相关的项，表⽰由于模型参数 $\boldsymbol{w}$ 的不确定性造成的内插的不确定性。</p><h2 id="2，超参数最优化"><a href="#2，超参数最优化" class="headerlink" title="2，超参数最优化"></a>2，超参数最优化</h2><p>超参数的边缘似然函数，或者模型证据，可以通过对⽹络权值进⾏积分的⽅法得到，即</p><script type="math/tex; mode=display">p(\mathcal{D}|\alpha,\beta)=\int p(\mathcal{D}|\boldsymbol{w},\beta)p(\boldsymbol{w}|\alpha)\mathrm{d}\boldsymbol{w}\tag{5.121}</script><p>取对数，可得</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\alpha,\beta)\simeq -E(\boldsymbol{w}_{MAP})-\frac{1}{2}\ln|\boldsymbol{A}|+\frac{W}{2}\ln\alpha+\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)\tag{5.122}</script><p>其中 $W$ 是 $\boldsymbol{w}$ 中参数的总数。正则化误差函数的定义为</p><script type="math/tex; mode=display">E(\boldsymbol{w}_{MAP})=\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})-t_n\}^{2}+\frac{\alpha}{2}\boldsymbol{w}_{MAP}^{T}\boldsymbol{w}_{MAP}\tag{5.123}</script><p>在模型证据框架中，我们通过最⼤化 $\ln p(\mathcal{D}|\alpha,\beta)$ 对 $\alpha$ 和 $\beta$ 进⾏点估计。⾸先考虑关于 $\alpha$ 进⾏最⼤化，定义特征值⽅程</p><script type="math/tex; mode=display">\beta\boldsymbol{H}\boldsymbol{\mu}_i=\lambda_{i}\boldsymbol{\mu}_i\tag{5.124}</script><p>其中 $\boldsymbol{H}$ 是在 $\boldsymbol{w}=\boldsymbol{w}_{MAP}$ 处计算的<code>Hessian</code>矩阵，由平⽅和误差函数的⼆阶导数组成。</p><p>有</p><script type="math/tex; mode=display">\alpha=\frac{\gamma}{\boldsymbol{w}_{MAP}^{T}\boldsymbol{w}_{MAP}}\tag{5.125}</script><p>其中 $\gamma$ 表⽰参数的有效数量，定义为</p><script type="math/tex; mode=display">\gamma=\sum_{i=1}^{W}\frac{\lambda_i}{\alpha+\lambda_i}</script><p>关于 $\beta$ 最⼤化模型证据，可以得到下⾯的重估计公式</p><script type="math/tex; mode=display">\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})-t_n\}^{2}\tag{5.126}</script><h2 id="3，⽤于分类的贝叶斯神经⽹络"><a href="#3，⽤于分类的贝叶斯神经⽹络" class="headerlink" title="3，⽤于分类的贝叶斯神经⽹络"></a>3，⽤于分类的贝叶斯神经⽹络</h2><p>考虑的⽹络有⼀个<code>logistic sigmoid</code>输出，对应于⼀个⼆分类问题。</p><p>模型的对数似然函数为</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\boldsymbol{w})=\sum_{n=1}^{N}\{t_n\ln{y_n}+(1-t_n)\ln(1-y_n)\}\tag{5.127}</script><p>其中 $t_n \in\{0,1\}$ 是⽬标值，且 $y_n \equiv y(\boldsymbol{x}_n,\boldsymbol{w})$ 。</p><p>将拉普拉斯框架⽤在这个模型中的第⼀个阶段是初始化超参数 $\alpha$，然后通过最⼤化对数后验概率分布的⽅法确定参数向量 $\boldsymbol{w}$ ，这等价于最⼩化正则化误差函数</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\ln{p(\mathcal{D}|\boldsymbol{w})}+\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}\tag{5.128}</script><p>找到权向量的解 $\boldsymbol{w}_{MAP}$ 之后， 下⼀步是计算由负对数似然函数的⼆阶导数组成的<code>Hessian</code>矩阵 $\boldsymbol{H}$ 。</p><p>为了最优化超参数 $\alpha$ ，再次最⼤化边缘似然函数，边缘似然函数的形式为</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\alpha)\simeq -E(\boldsymbol{w}_{MAP})-\frac{1}{2}\ln|\boldsymbol{A}|+\frac{W}{2}\ln\alpha\tag{5.129}</script><p>其中，正则化的误差函数为</p><script type="math/tex; mode=display">E(\boldsymbol{w}_{MAP})=-\sum_{n=1}^{N}\{t_n\ln{y_n}+(1-t_n)\ln(1-y_n)\}+\frac{\alpha}{2}\boldsymbol{w}_{MAP}^{T}\boldsymbol{w}_{MAP}\tag{5.130}</script><p>其中 $y_n \equiv y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})$ 。</p><p>图5.38，模型证据框架应⽤于⼈⼯⽣成的⼆分类数据集的说明。绿⾊曲线表⽰最优的决策边界，⿊⾊曲线表⽰通过最⼤化似然函数调节⼀个具有8个隐含结点的两层神经⽹络的结果，红⾊曲线表⽰包含⼀个正则化项的结果，其中 $\alpha$ 使⽤模型证据的步骤进⾏了最优化，初始值为 $\alpha = 0$ 。注意，模型证据步骤极⼤地缓 解了模型的过拟合现象。</p><p><img src="/images/prml_20191016161835.png" alt="模型证据框架应⽤于⼈⼯⽣成的⼆分类数据集"></p><p>最后，需要找到的预测分布，由于⽹络函数的⾮线性的性质，积分是⽆法直接计算的。最简单的近似⽅法是假设后验概率⾮常窄，因此可以进⾏下⾯的近似</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D})\simeq p(t|\boldsymbol{x},\boldsymbol{w}_{MAP})\tag{5.131}</script><p>对输出激活函数进⾏线性近似，形式为</p><script type="math/tex; mode=display">a(\boldsymbol{x},\boldsymbol{w})\simeq a_{MAP}(\boldsymbol{x})+\boldsymbol{b}^{T}(\boldsymbol{w}-\boldsymbol{w}_{MAP})\tag{5.132}</script><p>其中，$a_{MAP}(\boldsymbol{x})=a(\boldsymbol{x},\boldsymbol{w}_{MAP})$ 及向量 $\boldsymbol{b} \equiv \nabla a(\boldsymbol{x},\boldsymbol{w}_{MAP})$ 都可以通过反向传播⽅法求出。</p><p>由神经⽹络的权值的分布引出的输出单元激活的值的分布为</p><script type="math/tex; mode=display">p(a|\boldsymbol{x},\mathcal{D})=\int\delta(a-a_{MAP}(\boldsymbol{x})-\boldsymbol{b}^{T}(\boldsymbol{x})(\boldsymbol{w}-\boldsymbol{w}_{MAP}))q(\boldsymbol{w}|\mathcal{D})\mathrm{d}\boldsymbol{w}\tag{5.133}</script><p>其中 $q(\boldsymbol{w}|\mathcal{D})$ 是对后验概率分布的⾼斯近似，这个分布是⼀个⾼斯分布，均值为 $a_{MAP}(\boldsymbol{x})\equiv a(\boldsymbol{x},\boldsymbol{x}_{MAP})$ ，⽅差为</p><script type="math/tex; mode=display">\sigma_{a}^{2}(\boldsymbol{x})=\boldsymbol{b}^{T}(\boldsymbol{x})\boldsymbol{A}^{-1}\boldsymbol{b}(\boldsymbol{x})\tag{5.134}</script><p>为了得到预测分布，必须对 $a$ 进⾏积分</p><script type="math/tex; mode=display">p(t=1|\boldsymbol{x},\mathcal{D})=\int \delta(a)p(a|\boldsymbol{w},\mathcal{D})\mathrm{d}a\tag{5.135}</script><p>⾼斯分布与<code>logistic sigmoid</code>函数的卷积是⽆法计算的。于是</p><script type="math/tex; mode=display">p(t=1|\boldsymbol{x},\mathcal{D})=\sigma\left(\kappa(\sigma_{a}^{2})a_{MAP}\right)\tag{5.136}</script><p>图 5.39～5.40，对于⼀个具有8个隐含结点带有 $tanh$ 激活函数和⼀个<code>logistic sigmoid</code>输出结点的贝叶斯⽹络应⽤拉 普拉斯近似的说明。权参数使⽤缩放的共轭梯度⽅法得到，超参数 $\alpha$ 使⽤模型证据框架确定。图5.39是使⽤基于参数的 $\boldsymbol{w}_{MAP}$ 的点估计的简单近似得到的结果，其中绿⾊曲线表⽰ $y = 0.5$ 的决策边界，其他的轮廓线对应于 $y = 0.1, 0.3, 0.7$ 和 $0.9$ 的输出概率。图5.40是使⽤公式(5.136)得到的对应的结果。注意，求边缘概率分布的效果是扩散了轮廓线，使得预测的置信度变低，从⽽在每个输⼊点 $\boldsymbol{x}$ 处，后验概 率分布向着 $0.5$ 的⽅向偏移，⽽ $y = 0.5$ 的边界本⾝不受影响。</p><p><img src="/images/prml_20191016163025.png" alt="点估计"></p><p><img src="/images/prml_20191016163032.png" alt="卷积"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，混合密度网络&quot;&gt;&lt;a href=&quot;#一，混合密度网络&quot; class=&quot;headerlink&quot; title=&quot;一，混合密度网络&quot;&gt;&lt;/a&gt;一，混合密度
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】神经网络正则化</title>
    <link href="https://zhangbc.github.io/2019/10/18/prml_05_03/"/>
    <id>https://zhangbc.github.io/2019/10/18/prml_05_03/</id>
    <published>2019-10-18T10:02:19.000Z</published>
    <updated>2019-10-18T15:46:45.543Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，相容的⾼斯先验"><a href="#一，相容的⾼斯先验" class="headerlink" title="一，相容的⾼斯先验"></a>一，相容的⾼斯先验</h1><p>神经⽹络的输⼊单元和输出单元的数量通常由数据集的维度确定，⽽隐含单元的数量 $M$ 是⼀个⾃由的参数，可以通过调节来给出最好的预测性能。</p><p>控制神经⽹络的模型复杂度来避免过拟合，根据对多项式曲线拟合问题的讨论，⼀种⽅法是选择⼀个相对⼤的 $M$ 值，然后通过给误差函数增加⼀个正则化项，来控制模型的复杂度。最简单的正则化项是⼆次的，给出了正则化的误差函数，形式为</p><script type="math/tex; mode=display">\tilde{E}(\boldsymbol{w})=E(\boldsymbol{w})+\frac{\lambda}{2}\boldsymbol{w}^{T}\boldsymbol{w}\tag{5.73}</script><p>这个正则化项也被称为<strong>权值衰减</strong>（<code>weight decay</code>）。模型复杂度可以通过选择正则化系数 $\lambda$ 来确定，正则化项可以表⽰为权值 $\boldsymbol{w}$ 上的零均值⾼斯先验分布的负对数。</p><p>公式(5.73)给出的简单权值衰减的⼀个<strong>局限性</strong>是：它与⽹络映射的确定缩放性质不相容。考虑⼀个多层感知器⽹络，这个⽹络有两层权值和线性输出单元，给出了从输⼊变量集合 $\{x_i\}$ 到输出变量集合 $\{y_k\}$ 的映射。第⼀个隐含层的隐含单元的激活的形式为</p><script type="math/tex; mode=display">z_j=h\left(\sum_{i}w_{ji}x_{i}+w_{j0}\right)\tag{5.74}</script><p>输出单元的激活为</p><script type="math/tex; mode=display">y_k=\sum_{j}w_{kj}z_j+w_{k0}\tag{5.75}</script><p>假设对输⼊变量进⾏⼀个线性变换，形式为</p><script type="math/tex; mode=display">x_i\to\tilde{x}_{i}=ax_i+b\tag{5.76}</script><p>然后根据这个映射对⽹络进⾏调整，使得⽹络给出的映射不变。调整的⽅法为，对从输⼊单元到隐含层单元的权值和偏置也进⾏⼀个对应的线性变换，形式为</p><script type="math/tex; mode=display">w_{ji}\to\tilde{w}_{ji}=\frac{1}{a}w_{ji}</script><script type="math/tex; mode=display">w_{j0}\to\tilde{w}_{j0}=w_{j0}-\frac{b}{a}\sum_{i}w_{ji}</script><p>⽹络的输出变量的线性变换</p><script type="math/tex; mode=display">y_{k}\to\tilde{y}_{k}=cy_k+d\tag{5.77}</script><p>可以通过对第⼆层的权值和偏置进⾏线性变换的⽅式实现。变换的形式为</p><script type="math/tex; mode=display">w_{kj}\to\tilde{w}_{kj}=cw_{kj}</script><script type="math/tex; mode=display">w_{k0}\to\tilde{w}_{k0}=cw_{k0}+d</script><p>于是要寻找⼀个正则化项，它在上述线性变换和下具有不变性，这需要正则化项应该对于权值的重新缩放不变，对于偏置的平移不变。这样的正则化项为</p><script type="math/tex; mode=display">\frac{\lambda_1}{2}\sum_{w\in\mathcal{W_1}}w^2+\frac{\lambda_2}{2}\sum_{w\in\mathcal{W_2}}w^2</script><p>其中 $\mathcal{W}_1$ 表⽰第⼀层的权值集合，$\mathcal{W}_2$ 表⽰第⼆层的权值集合， 偏置未出现在求和式中。这个正则化项在权值的变换下不会发⽣变化，只要正则化参数进⾏下⾯的重新放缩即可：$\lambda_1 \to a^{\frac{1}{2}}\lambda_1$ 和 $\lambda_2 \to a^{-\frac{1}{2}}\lambda_2$  ， 正则化项对应于下⾯形式的先验概率分布。</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\alpha_1,\alpha_2)\propto\exp\left(-\frac{\alpha_1}{2}\sum_{w\in\mathcal{W_1}}w^2-\frac{\alpha_2}{2}\sum_{w\in\mathcal{W_2}}w^2\right)\tag{5.78}</script><p>注意， 这种形式的先验是反常的（<code>improper</code>）（不能够被归⼀化），因为偏置参数没有限制。</p><p>图5.15～5.18，控制两层神经⽹络的权值和偏置的先验概率分布的超参数的效果说明。其中，神经⽹络有⼀个输⼊，⼀个线性输出，以及12个隐含结点，隐含结点的激活函数为 $tanh$。先验概率分布通过四个超参数 $\alpha_{1}^{b}$， $\alpha_{1}^{w}$， $\alpha_{2}^{b}$ ， $\alpha_{2}^{w}$ 控制，它们分别表⽰第⼀层的偏置、第⼀层的权值、第⼆层的偏置、第⼆层的权值。</p><p><img src="/images/prml_20191016102909.png" alt="a=1,b=1"></p><p><img src="/images/prml_20191016102918.png" alt="a=1,b=10"></p><p><img src="/images/prml_20191016102927.png" alt="a=1000,b=100"></p><p><img src="/images/prml_20191016102936.png" alt="a=1000,b=1000"></p><p>⼀般地，可以考虑权值被分为任意数量的组 $\mathcal{W}_k$ 的情况下的先验，即</p><script type="math/tex; mode=display">p(\boldsymbol{w})\propto\exp\left(-\frac{1}{2}\sum_{k}\alpha_{k}\|\boldsymbol{w}\|_{k}^2\right)\tag{5.79}</script><p>其中，</p><script type="math/tex; mode=display">\|\boldsymbol{w}\|_{k}^2=\sum_{j\in\mathcal{W}_k}w_{j}^{2}</script><h1 id="二，早停止"><a href="#二，早停止" class="headerlink" title="二，早停止"></a>二，早停止</h1><p>另⼀种控制⽹络的复杂度的正则化⽅法是<strong>早停⽌</strong>（<code>early stopping</code>）。⾮线性⽹络模型的训练对应于误差函数的迭代减⼩，其中误差函数是关于训练数据集定义的。对于许多⽤于⽹络训练的最优化算法（例如共轭梯度法），误差函数是⼀个关于迭代次数的不增函数。然⽽，在独⽴数据（通常被称为<strong>验证集</strong>）上测量的误差，通常⾸先减⼩，接下来由于模型开始过拟合⽽逐渐增⼤。于是，训练过程可以在关于验证集误差最⼩的点停⽌，这样可以得到⼀个有着较好泛化性能的⽹络。</p><p>图5.19～5.20，训练集误差和验证集误差在典型的训练阶段的⾏为说明。图像给出了误差与迭代次数的函数，数据集为正弦数据集。得到最好的泛化表现的⽬标表明，训练应该在垂直虚线表⽰的点处停⽌，对应于验证集误差的最⼩值。</p><p><img src="/images/prml_20191016102950.png" alt="训练集误差"></p><p><img src="/images/prml_20191016102959.png" alt="验证集误差"></p><p>图5.21，在⼆次误差函数的情况下，关于早停⽌可以给出与权值衰减类似的结果的原因说明。椭圆给出了常数误差函数的轮廓线，$\boldsymbol{w}_{ML}$ 表⽰误差函数的最⼩值。如果权向量的起始点为原点，按照局部负梯度的⽅向移动，那么它会沿着曲线给出的路径移动。通过对训练过程早停⽌，我们找到了⼀个权值向量 $\tilde{\boldsymbol{w}}$。 定性地说，它类似于使⽤简单的权值衰减正则化项，然后最⼩化正则化误差函数的⽅法得到的权值。</p><p><img src="/images/prml_20191016103345.png" alt="⼆次误差函数的早停⽌"></p><h1 id="三，不变性"><a href="#三，不变性" class="headerlink" title="三，不变性"></a>三，不变性</h1><p>寻找让可调节的模型能够表述所需的不变性，⼤致可以分为四类：</p><blockquote><p>1）通过复制训练模式，同时根据要求的不变性进⾏变换，对训练集进⾏扩展。例如，在⼿写数字识别的例⼦中，我们可以将每个样本复制多次，每个复制后的样本中，图像被平移到 了不同的位置。</p><p>2）为误差函数加上⼀个正则化项，⽤来惩罚当输⼊进⾏变换时，输出发⽣的改变。</p><p>3）通过抽取在要求的变换下不发⽣改变的特征，不变性被整合到预处理过程中。任何后续的使⽤这些特征作为输⼊的回归或者分类系统就会具有这些不变性。</p><p>4）把不变性的性质整合到神经⽹络的构建过程中，或者对于相关向量机的⽅法，整合到核函数中。</p></blockquote><p>图5.22，对⼿写数字进⾏⼈⼯形变的说明。 原始图像见左图。 在右图中， 上⾯⼀⾏给出了三个经过了形变的数字，对应的位移场在下⾯⼀⾏给出。这些位移场按照下⾯的⽅法⽣成：在每个像素处， 对唯 ⼀ $\Delta{x}$ , $\Delta{y}\in(0,1)$ 进⾏随机取样，然后分别与宽度为0.01,30,60的⾼斯分布做卷积，进⾏平滑。</p><p><img src="/images/prml_20191016103406.png" alt="⼿写数字进⾏⼈⼯形变"></p><p>图5.23，⼆维输⼊空间的例⼦，展⽰了在⼀个特定的输⼊向量 $\boldsymbol{x}_n$ 上的连续变换的效果。⼀个参数为连续变量 $\xi$ 的⼀维变换作⽤于 $\boldsymbol{x}_n$ 上会使它扫过⼀个⼀维流形 $\mathcal{M}$ 。局部来看，变换的效果可以⽤切向量 $\boldsymbol{\tau}_n$ 来近似。</p><p><img src="/images/prml_20191016103416.png" alt="⼆维输⼊空间"></p><h1 id="四，切线传播"><a href="#四，切线传播" class="headerlink" title="四，切线传播"></a>四，切线传播</h1><p>通过<strong>切线传播</strong>（<code>tangent propagation</code>）的⽅法，可以使⽤正则化来让模型对于输⼊的变换具有不变性（<code>Simard et al.</code>, 1992）。对于⼀个特定的输⼊向量 $\boldsymbol{x}_n$ ，考虑变换产⽣的效果。 假设变换是连续的（例如平移或者旋转，⽽不是镜像翻转），那么变换的模式会扫过 $D$ 维输⼊空间的⼀个流形 $\mathcal{M}$ 。假设变换由单⼀参数 $\xi$ 控制（例如，$\xi$ 可能是旋转的角度）。那么被 $\boldsymbol{x}_n$ 扫过的⼦空间 $\mathcal{M}$ 是⼀维的，并且以 $\xi$ 为参数。令这个变换作⽤于 $\boldsymbol{x}_n$ 上产⽣的向量为 $\boldsymbol{s}(\boldsymbol{x}_n,\xi)$ ， 且  $\boldsymbol{s}(\boldsymbol{x}_n,0)=\boldsymbol{x}$ 。 这样曲线 $\mathcal{M}$ 的切线就由⽅向导数 $\boldsymbol{\tau} =\frac{\partial\boldsymbol{x}}{\partial\xi}$ 给出，且点 $\boldsymbol{x}_n$ 处的切线向量为</p><script type="math/tex; mode=display">\boldsymbol{\tau}_n=\frac{\partial{\boldsymbol{s}}(\boldsymbol{x}_n,\xi)}{\partial{\xi}}\Bigg{|}_{\xi=0}\tag{5.80}</script><p>对于输⼊向量进⾏变换之后，⽹络的输出通常会发⽣变化。输出 $k$ 关于 $\xi$ 的导数为</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{\xi}}\Bigg{|}_{\xi=0}=\sum_{i=1}^{D}\frac{\partial{y_k}}{\partial{x_i}}\frac{\partial{x_i}}{\partial{\xi}}\Bigg{|}_{\xi=0}=\sum_{i=1}^{D}J_{ki}\tau_{i}</script><p>其中 $J_{ki}$ 为<code>Jacobian</code>矩阵 $\boldsymbol{J}$ 的第 $(k, i)$ 个元素。这个结果可以⽤于修改标准的误差函数，使得在数据点的邻域之内具有不变性。修改的⽅法为：给原始的误差函数 $E$ 增加⼀个正则化函数 $\Omega$ ，得到下⾯形式的误差函数</p><script type="math/tex; mode=display">\tilde{E}=E+\lambda\Omega\tag{5.81}</script><p>其中 $\lambda$ 是正则化系数，且</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\sum_{n}\sum_{k}\left(\frac{\partial{y_{nk}}}{\partial{\xi}}\bigg{|}_{\xi=0}\right)^{2}=\frac{1}{2}\sum_{n}\sum_{k}\left(\sum_{i=1}^{D}J_{nki}\tau_{ni}\right)^{2}</script><p>当⽹络映射函数在每个模式向量的邻域内具有变换不变性时，正则化函数等于零。$\lambda$ 的值确定了训练数据和学习不变性之间的平衡。在实际执⾏过程中，切线向量 $\boldsymbol{\tau}_n$ 可以使⽤有限差近似，即将原始向量 $\boldsymbol{x}$ 从使⽤了⼩的 $\xi$ 进⾏变换后的对应的向量中减去，再除以 $\xi$ 。</p><p>图5.24～5.27，(<code>a</code>)原始的⼿写数字 $\boldsymbol{x}$ ，(<code>b</code>)对应于⽆穷⼩顺时针旋转的切向量 $\boldsymbol{\tau}$ ，其中蓝⾊和黄⾊分别对应于正值和负值，(<code>c</code>)将来⾃这个切向量的微⼩贡献作⽤于原始图像的结果，得到了 $\boldsymbol{x}+\epsilon\boldsymbol{\tau}$  ，其中 $\epsilon=15$ 度。(<code>d</code>)真实的图像旋转，⽤作对⽐。</p><p><img src="/images/prml_20191016103843.png" alt="a"></p><p><img src="/images/prml_20191016103853.png" alt="b"></p><p><img src="/images/prml_20191016103906.png" alt="c"></p><p><img src="/images/prml_20191016103919.png" alt="d"></p><p>如果变换由 $L$ 个参数控制（例如，对于⼆维图像的平移变换与⾯内旋转变换项结合），那么流形 $\mathcal{M}$ 的维度为 $L$ ，对应的正则化项由形如公式 $\Omega$ 的项求和得到，每个变换都对应求和式中的⼀项。如果同时考虑若⼲个变换，并且让⽹络映射对于每个变换分别具有不变性，那么对于变换的组合来说就会具有（局部）不变性（<code>Simard et al.</code>, 1992）。⼀个相关的技术，被称为<strong>切线距离（<code>tangent distance</code>）</strong>，可以⽤来构造基于距离的⽅法（例如最近邻分类器）的不变性（<code>Simard et al.</code>, 1993）。</p><h1 id="五，⽤变换后的数据训练"><a href="#五，⽤变换后的数据训练" class="headerlink" title="五，⽤变换后的数据训练"></a>五，⽤变换后的数据训练</h1><p>考虑由单⼀参数 $\xi$ 控制的变换，且这个变换由函数 $\boldsymbol{s}(\boldsymbol{x},\xi)$ 描述， 其中 $\boldsymbol{s}(\boldsymbol{x}_n,0)=\boldsymbol{x}$ ，也会考虑平⽅和误差函数。对于未经过变换的输⼊，误差函数可以写成 （在⽆限数据集的极限情况下）</p><script type="math/tex; mode=display">E=\frac{1}{2}\int\int\{y(\boldsymbol{x})-t\}^{2}p(t|\boldsymbol{x})p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\mathcal{d}t\tag{5.82}</script><p>为了保持记号的简洁，考虑有⼀个输出单元的⽹络。如果现在考虑每个数据点的⽆穷多个副本，每个副本都由⼀个变换施加了扰动，这个变换的参数为 $\xi$ ，且 $\xi$ 服从概率分布 $p(\xi)$ ，那么在这个扩展的误差函数上定义的误差函数可以写成</p><script type="math/tex; mode=display">\tilde{E}=\frac{1}{2}\int\int\int\{y(\boldsymbol{s}(\boldsymbol{x},\xi))-t\}^{2}p(t|\boldsymbol{x})p(\boldsymbol{x})p(\xi)\mathrm{d}\boldsymbol{x}\mathcal{d}t\mathcal{d}\xi\tag{5.83}</script><p>现在假设分布 $p(\xi)$ 的均值为零，⽅差很⼩，即只考虑对原始输⼊向量的⼩的变换。可以对变换函数进⾏关于 $\xi$ 的展开，可得</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{s}(\boldsymbol{x},\xi)&=\boldsymbol{s}(\boldsymbol{x},0)+\xi\frac{\partial}{\partial{\xi}}\boldsymbol{s}(\boldsymbol{x},\xi)\bigg{|}_{\xi=0}+\frac{\xi^{2}}{2}\frac{\partial^{2}}{\partial{\xi^{2}}}\bigg{|}_{\xi=0}+O(\xi^{3})\\&=\boldsymbol{x}+\xi\boldsymbol{\tau}+\frac{1}{2}\xi^{2}\boldsymbol{\tau}^{\prime}+O(\xi^{3})\end{aligned}\tag{5.84}</script><p>其中 $\boldsymbol{\tau}^{\prime}$ 表⽰ $\boldsymbol{s}(\boldsymbol{x},\xi)$ 关于 $\xi$ 的⼆阶导数在 $\xi = 0$ 处的值。这使得可以展开模型函数，可得</p><script type="math/tex; mode=display">y(\boldsymbol{s}(\boldsymbol{x},\xi))=y(\boldsymbol{x})+\xi\boldsymbol{\tau}^{T}\nabla{y(\boldsymbol{x})}+\frac{\xi^{2}}{2}[(\boldsymbol{\tau}^{\prime})^{T}\nabla{y(\boldsymbol{x})}+\boldsymbol{\tau}^{T}\nabla\nabla{y(\boldsymbol{x})}\boldsymbol{\tau}]+O(\xi^{3})</script><p>代⼊平均误差函数，有</p><script type="math/tex; mode=display">\begin{aligned} \tilde{E} &=\frac{1}{2} \iint\{y(x)-t\}^{2} p(t | \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \mathrm{d} t \\ &+\mathbb{E}[\xi] \iint\{y(\boldsymbol{x})-t\} \boldsymbol{\tau}^{T} \nabla y(\boldsymbol{x}) p(t | \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \mathrm{d} t \\ &+\mathbb{E}\left[\xi^{2}\right] \frac{1}{2} \iint\left[\{y(\boldsymbol{x})-t\}\left\{\left(\boldsymbol{\tau}^{\prime}\right)^{T} \nabla y(\boldsymbol{x})+\boldsymbol{\tau}^{T} \nabla \nabla y(\boldsymbol{x}) \boldsymbol{\tau}\right\}\right.\\ &\left.+\left(\boldsymbol{\tau}^{T} \nabla y(\boldsymbol{x})\right)^{2}\right] p(t | \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \mathrm{d} t+O\left(\xi^{3}\right) \end{aligned}\tag{5.85}</script><p>由于变换的分布的均值为零， 因此有 $\mathbb{E}[\xi] = 0$ ，并且把 $\mathbb{E}[\xi^{2}]$ 记作 $\lambda$ ，省略 $O(\xi^{3})$ 项，这样平均误差函数就变成了</p><script type="math/tex; mode=display">\tilde{E}=E+\lambda\Omega\tag{5.86}</script><p>其中 $E$ 是原始的平⽅和误差，正则化项 $\Omega$ 的形式为</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\int\bigg{[}\{y(\boldsymbol{x})-\mathbb{E}[t|\boldsymbol{x}]\}\left\{(\boldsymbol{\tau}^{\prime})^{T}\nabla{y(\boldsymbol{x})}+\boldsymbol{\tau}^{T}\nabla\nabla{y(\boldsymbol{x})\boldsymbol{\tau}}\right\}+(\boldsymbol{\tau}^{T}\nabla{y(\boldsymbol{x})})^{2}p(\boldsymbol{x})\bigg{]}\mathrm{d}\boldsymbol{x}</script><p>进⼀步简化这个正则化项，发现正则化的误差函数等于⾮正则化的误差函数加上⼀个 $O(\xi^{2})$ 的项，因此最⼩化总误差函数的⽹络函数的形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\mathbb{E}[t|\boldsymbol{x}]+O(\xi^{3})\tag{5.87}</script><p>从⽽，正则化项中的第⼀项消失，剩下的项为</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\int(\boldsymbol{\tau}^{T}\nabla{y(\boldsymbol{x})})^{2}p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}</script><p>这等价于<strong>切线传播</strong>的正则化项。<br>如果考虑⼀个特殊情况，即输⼊变量的变换只是简单地添加随机噪声，从⽽ $\boldsymbol{x}\to\boldsymbol{x}+\boldsymbol{\xi}$ ，那么正则化项的形式为</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\int\|\nabla{y(\boldsymbol{x})}\|^{2}p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\tag{5.88}</script><p>这被称为 <strong><code>Tikhonov</code>正则化</strong> （<code>Tikohonov and Arsenin</code>, 1977; <code>Bishop</code>, 1995<code>b</code>）。这个正则化项关于⽹络权值的导数可以使⽤扩展的反向传播算法求出（<code>Bishop</code>, 1993）。对于⼩的噪 声，<code>Tikhonov</code>正则化与对输⼊添加随机噪声有关系，在恰当的情况下，这种做法会提升模型的泛化能⼒。</p><h1 id="六，卷积神经⽹络"><a href="#六，卷积神经⽹络" class="headerlink" title="六，卷积神经⽹络"></a>六，卷积神经⽹络</h1><p>另⼀种构造对输⼊变量的变换具有不变性的模型的⽅法是将不变性的性质融⼊到神经⽹络结构的构建中。这是<strong>卷积神经⽹络</strong>（<code>convolutional neural network</code>）（<code>LeCun et al.</code>, 1989; <code>LeCun et al.</code>, 1998）的基础，它被⼴泛地应⽤于图像处理领域。</p><p>考虑⼿写数字识别这个具体的任务。每个输⼊图像由⼀组像素的灰度值组成，输出为10个数字类别的后验概率分布，数字的种类对于平移、缩放以及（微⼩的）旋转具有不变性。⼀种简单的⽅法是把图像作为⼀个完全链接的神经⽹络的输⼊，假设数据集充分⼤，那么这样的⽹络原则上可以产⽣这个问题的⼀个较好的解，从⽽可以从样本中学习到恰当的不变性。然⽽，这种⽅法忽略了图像的⼀个<strong>关键性质</strong>，即距离较近的像素的相关性要远⼤于距离较远的像素的相关性。这些想法被整合到了<strong>卷积神经⽹络</strong>中，通过下⾯三种⽅式：</p><blockquote><p>（1）局部接收场；<br>（2）权值共享；<br>（3）下采样。</p></blockquote><p>在卷积层， 各个单元被组织在⼀系列平⾯中，每个平⾯被称为⼀个<strong>特征地图</strong>（<code>feature map</code>）。⼀个特征地图中的每个单元只从图像的⼀个⼩的⼦区域接收输⼊，且⼀个特征地图中的所有单元被限制为共享相同的权值。</p><p>例如，⼀个 特征地图可能由100个单元组成， 这些单元被放在了10×10的⽹格中， 每个单元从图像的⼀ 个5×5的像素块接收输⼊。于是，整个特征地图就有25个可调节的参数，加上⼀个可调节的偏置参数。 来⾃⼀个像素块的输⼊值被权值和偏置进⾏线性组合， 线性组合的结果通过公式给出的 $S$ 形⾮线性函数进⾏变换。如果我们把每个单元想象成特征检测器，那么特征地图中的所有单元都检测了输⼊图像中的相同的模式，但是位置不同。由于权值共享，这些单元的激活的计算等价于使⽤⼀个由权向量组成和“核”对图像像素的灰度值进⾏卷积。如果输⼊图像发⽣平移，那么特征地图的激活也会发⽣等量的平移，否则就不发⽣改变。这提供了神经⽹络输出对于输⼊图像的平移和变形的（近似）不变性的基础。由于通常需要检测多个特征来构造⼀个有效的模型，因此通常在卷积层会有多个特征地图，每个都有⾃⼰的权值和偏置参数。</p><p>图5.28，卷积神经⽹络的⼀个例⼦，给出了⼀层卷积单元层跟着⼀个下采样单元层，可能连续使⽤这种层对。</p><p><img src="/images/prml_20191016112651.png" alt="卷积神经⽹络"></p><p>卷积单元的输出构成了⽹络的下采样层的输⼊。对于卷积层的每个特征地图，有⼀个下采样层的单元组成的平⾯，并且下采样层的每个单元从对应的卷积层的特征地图中的⼀个⼩的接收场接收输⼊，这些单元完成了下采样。 </p><p>例如，每个下采样单元可能从对应的特征地图中的⼀个2×2单元的区域中接收输⼊，然后计算这些输⼊的平均值，乘以⼀个可调节的权值和可调节的偏置参数，然后使⽤ $S$ 形⾮线性激活函数进⾏变换。选择的接收场是连续的、⾮重叠的，从⽽ 下采样层的⾏数和列数都是卷积层的⼀半。使⽤这种⽅式，下采样层的单元的响应对于对应的输⼊空间区域中的图⽚的微⼩平移相对不敏感。</p><h1 id="七，软权值共享"><a href="#七，软权值共享" class="headerlink" title="七，软权值共享"></a>七，软权值共享</h1><p>降低具有⼤量权值参数的⽹络复杂度的⼀种⽅法是将权值分组，然后令分组内的权值相等。 然⽽，它只适⽤于限制的形式可以事先确定的问题中。</p><p>考虑<strong>软权值共享</strong> （<code>soft weight sharing</code>）（<code>Nowlan and Hinton</code>, 1992）。这种⽅法中，权值相等的硬限制被替换为 ⼀种形式的正则化，其中权值的分组倾向于取近似的值。<br>可以将权值分为若⼲组，⽽不是将所有权值分为⼀个组。分组的⽅法是使⽤⾼斯混合概率分布。混合分布中，每个⾼斯分量的均值、⽅差，以及混合系数，都会作为可调节的参数在学习过程中被确定。于是，有下⾯形式的概率密度</p><script type="math/tex; mode=display">p(\boldsymbol{w})=\prod_{i}p(w_i)\tag{5.89}</script><p>其中，</p><script type="math/tex; mode=display">p(w_i)=\sum_{j=1}^{M}\pi_{j}\mathcal{N}(w_i|\mu_j,\sigma_{j}^{2})</script><p>$\pi_j$ 为混合系数。取负对数，即可得到正则化函数，形式为</p><script type="math/tex; mode=display">\Omega(\boldsymbol{w})=-\sum_{i}\ln\left(\sum_{j=1}^{M}\pi_{j}\mathcal{N}(w_i|\mu_j,\sigma_{j}^{2})\right)\tag{5.90}</script><p>从⽽，总的误差函数为</p><script type="math/tex; mode=display">\tilde{E}(\boldsymbol{w})=E(\boldsymbol{w})+\lambda\Omega(\boldsymbol{w})\tag{5.91}</script><p>其中，$\lambda$ 是正则化系数。这个误差函数同时关于权值 $w_i$ 和混合模型参数 $\{\pi_j,\mu_j,\sigma_j\}$ 进⾏最⼩化。<br>为了最⼩化总的误差函数，把 $\{\pi_j\}$ 当成先验概率，引⼊对应的后验概率，根据相关公式，后验概率由贝叶斯定理给出，形式为</p><script type="math/tex; mode=display">\gamma_{j}(w)=\frac{\pi_{j}\mathcal{N}(w|\mu_j,\sigma_{j}^{2})}{\sum_{k}\pi_{k}\mathcal{N}(w|\mu_k,\sigma_{k}^{2})}\tag{5.92}</script><p>总的误差函数关于权值的导数为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{w_i}}=\frac{\partial{E}}{\partial{w_i}}+\lambda\sum_{j}\gamma_{j}(w_i)\frac{(w_i-\mu_j)}{\sigma_{j}^{2}}\tag{5.93}</script><p>于是，正则化项的效果是把每个权值拉向第 $j$ 个⾼斯分布的中⼼，拉⼒正⽐于对于给定权值的⾼斯分布的后验概率。<br>误差函数关于⾼斯分布的中⼼的导数为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{\mu_j}}=\lambda\sum_{i}\gamma_{j}(w_i)\frac{(\mu_j-w_i)}{\sigma_{j}^{2}}\tag{5.94}</script><p>具有简单的直观含义：把 $\mu_j$ 拉向了权值的平均值，拉⼒为第 $j$ 个⾼斯分量产⽣的权值参数的后验概率。<br>关于⽅差的导数为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{\sigma_j}}=\lambda\sum_{i}\gamma_{j}(w_i)\left(\frac{1}{\sigma_j}-\frac{(w_i-\mu_j)^{2}}{\sigma_{j}^{3}}\right)\tag{5.95}</script><p>将 $\sigma_j$ 拉向权值在对应的中⼼ $\mu_j$ 附近的偏差的平⽅的加权平均，加权平均的权系数等于由第 $j$ 个⾼斯分量产⽣的权值参数的后验概率。注意，在实际执⾏过程中，引⼊⼀个新的变量 $\xi_j$ ，它由下式定义</p><script type="math/tex; mode=display">\sigma_{j}^{2}=\exp(\xi_j)\tag{5.96}</script><p>并且，最⼩化的过程是关于 $\xi_j$ 进⾏的，这确保了参数 $\sigma_j$ 是正数。</p><p>对于关于混合系数 $\pi_j$ 的导数，需要考虑下⾯的限制条件</p><script type="math/tex; mode=display">\sum_{j}\pi_{j}=1,    0 \le \pi_j \le 1</script><p>将混合系数通过⼀组辅助变量 $\{\eta_j\}$ ⽤<code>softmax</code>函数表⽰，即</p><script type="math/tex; mode=display">\pi_j=\frac{\exp(\eta_j)}{\sum_{k=1}^{M}\exp(\eta_k)}\tag{5.97}</script><p>正则化的误差函数关于 $\{\eta_j\}$ 的导数的形式为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{\eta_{j}}}=\sum_{i}\{\pi_j-\gamma_j(w_j)\}\tag{5.98}</script><p>由此可见，$\pi_j$ 被拉向第 $j$ 个⾼斯分量的平均后验概率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，相容的⾼斯先验&quot;&gt;&lt;a href=&quot;#一，相容的⾼斯先验&quot; class=&quot;headerlink&quot; title=&quot;一，相容的⾼斯先验&quot;&gt;&lt;/a&gt;一，相
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】Hessian矩阵</title>
    <link href="https://zhangbc.github.io/2019/10/16/prml_05_02/"/>
    <id>https://zhangbc.github.io/2019/10/16/prml_05_02/</id>
    <published>2019-10-16T01:05:46.000Z</published>
    <updated>2019-10-18T10:04:24.166Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，Hessian-矩阵"><a href="#一，Hessian-矩阵" class="headerlink" title="一，Hessian 矩阵"></a>一，<strong><code>Hessian</code></strong> 矩阵</h1><p>反向传播也可以⽤来计算误差函数的⼆阶导数，形式为</p><script type="math/tex; mode=display">\frac{\partial^{2}{E}}{\partial{w_{ji}}\partial{w_{kl}}}</script><p>注意，有时将所有的权值和偏置参数看成⼀个向量（记作 $\boldsymbol{w}$ ）的元素 $w_i$ 更⽅便，此时⼆阶导数组成了<code>Hessian</code>矩阵 $\boldsymbol{H}$ 的元素 $H_{ij}$ ，其中 $i, j \in \{1,\dots, W\}$ ，且 $W$ 是权值和偏置的总数。<code>Hessian</code>矩阵在神经⽹络计算的重要的作⽤，包括：</p><blockquote><p>1）⼀些⽤来训练神经⽹络的⾮线性最优化算法是基于误差曲⾯的⼆阶性质的， 这些性质由<code>Hessian</code>矩阵控制（<code>Bishop and Nabney</code>, 2008）；<br>2）对于训练数据的微⼩改变，<code>Hessian</code>矩阵构成了快速重新训练前馈⽹络的算法的基础 （<code>Bishop</code>, 1991）；<br>3）<code>Hessian</code>矩阵的逆矩阵⽤来鉴别神经⽹络中最不重要的权值，这是⽹络“剪枝”算法的⼀部分 （<code>LeCun et al.</code>, 1990）；<br>4）<code>Hessian</code>矩阵是贝叶斯神经⽹络的拉普拉斯近似的核⼼。它的逆矩阵⽤来确定训练过的神经⽹络的预测分布，它的特征值确定了超参数的值，它的⾏列式⽤来计算模型证据。</p></blockquote><h1 id="二，对角近似"><a href="#二，对角近似" class="headerlink" title="二，对角近似"></a>二，对角近似</h1><p>对于模式 $n$ ，<code>Hessian</code>矩阵的对角线元素可以写成</p><script type="math/tex; mode=display">\frac{\partial^{2}{E_n}}{\partial{w_{ji}^{2}}}=\frac{\partial^{2}{E_n}}{\partial{a_{j}^{2}}}z_{i}^{2}\tag{5.56}</script><p>从而，反向传播⽅程的形式为</p><script type="math/tex; mode=display">\frac{\partial^{2}{E_n}}{\partial{a_{j}^{2}}}=h^{\prime}(a_j)^2\sum_{k}\sum_{k^{\prime}}w_{kj}w_{k^{\prime}j}\frac{\partial^{2}{E_n}}{\partial{a_k}\partial{a_{k^{\prime}}}}+h^{\prime\prime}(a_j)\sum_{k}w_{kj}\frac{\partial{E_n}}{\partial{a_k}}\tag{5.57}</script><p>如果忽略⼆阶导数中⾮对角线元素， 那么有（<code>Becker and LeCun</code>, 1989; <code>LeCun et al.</code>, 1990）</p><script type="math/tex; mode=display">\frac{\partial^{2}{E_n}}{\partial{a_{j}^{2}}}=h^{\prime}(a_j)^2\sum_{k}w_{kj}^{2}\frac{\partial^{2}{E_n}}{\partial{a_k^{2}}}+h^{\prime\prime}(a_j)\sum_{k}w_{kj}\frac{\partial{E_n}}{\partial{a_k}}\tag{5.58}</script><h1 id="三，外积近似"><a href="#三，外积近似" class="headerlink" title="三，外积近似"></a>三，外积近似</h1><p>当神经⽹络应⽤于回归问题时，通常使⽤下⾯形式的平⽅和误差函数</p><script type="math/tex; mode=display">E=\frac{1}{2}\sum_{n=1}^{N}(y_n-t_n)^2\tag{5.59}</script><p>考虑单⼀输出的情形（推⼴到多个输出是很直接的），可以把<code>Hessian</code>矩阵写成下⾯的形式</p><script type="math/tex; mode=display">\boldsymbol{H}=\nabla\nabla{E}=\sum_{n=1}^{N}\nabla{y_n}(\nabla{y_n})^{T}+\sum_{n=1}^{N}(y_n-t_n)\nabla\nabla{y_n}\tag{5.60}</script><p>通过忽略公式(5.60)的第⼆项，我们就得到了 <strong><code>Levenberg-Marquardt</code>近似</strong>，或者称为<strong>外积近似</strong>（<code>outer product approximation</code>）（因为此时<code>Hessian</code>矩阵由向量外积的求和构造出来），形式为</p><script type="math/tex; mode=display">\boldsymbol{H}\simeq\sum_{n=1}^{N}\boldsymbol{b}_n\boldsymbol{b}_n^{T}\tag{5.61}</script><p>其中 $\boldsymbol{b}_n\equiv\nabla{a_n}=\nabla{y_n}$ ， 因为输出单元的激活函数就是恒等函数。这种近似只在⽹络被恰当地训练时才成⽴， 对于⼀个⼀般的⽹络映 射，公式(5.60)的右侧的⼆阶导数项通常不能忽略。</p><p>在误差函数为<strong>交叉熵误差函数</strong>，输出单元激活函数为<code>logistic sigmoid</code>函数的神经⽹络中，对应的近似为</p><script type="math/tex; mode=display">\boldsymbol{H}\simeq\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{b}_n\boldsymbol{b}_n^{T}\tag{5.62}</script><p>对于输出函数为<code>softmax</code>函数的多类神经⽹络，可以得到类似的结果。</p><h1 id="四，Hessian-矩阵的逆矩阵"><a href="#四，Hessian-矩阵的逆矩阵" class="headerlink" title="四，Hessian 矩阵的逆矩阵"></a>四，<strong><code>Hessian</code></strong> 矩阵的逆矩阵</h1><p>使⽤外积近似，可以提出⼀个计算<code>Hessian</code>矩阵的逆矩阵的⾼效⽅法（<code>Hassibi and Stork</code>, 1993）。⾸先，⽤矩阵的记号写出外积近似，即</p><script type="math/tex; mode=display">\boldsymbol{H}_{N}=\sum_{n=1}^{N}\boldsymbol{b}_n\boldsymbol{b}_n^{T}\tag{5.63}</script><p>其中， $\boldsymbol{b}_n\equiv\nabla_{\boldsymbol{w}}{a_n}$  是数据点 $n$ 产⽣的输出单元激活对梯度的贡献。 </p><p>现在推导⼀个建⽴<code>Hessian</code>矩阵的顺序步骤，每次处理⼀个数据点。假设已经使⽤前 $L$ 个数据点得到了<code>Hessian</code>矩阵的逆矩阵。通过将第 $L+1$ 个数据点的贡献单独写出来，有</p><script type="math/tex; mode=display">\boldsymbol{H}_{L+1}=\boldsymbol{H}_L+\boldsymbol{b}_{L+1}\boldsymbol{b}_{L+1}^{T}\tag{5.64}</script><p>考虑下⾯的矩阵恒等式</p><script type="math/tex; mode=display">(\boldsymbol{M}+\boldsymbol{v}\boldsymbol{v}^{T})^{-1}=\boldsymbol{M}^{-1}-\frac{(\boldsymbol{M}^{-1}\boldsymbol{v})(\boldsymbol{v^{T}\boldsymbol{M}^{-1}})}{1+\boldsymbol{v}^{T}\boldsymbol{M}^{-1}\boldsymbol{v}}\tag{5.65}</script><p>如果令 $\boldsymbol{H}_L=\boldsymbol{M}$ ，且 $\boldsymbol{b}_{L+1}=\boldsymbol{v}$ ，有</p><script type="math/tex; mode=display">\boldsymbol{H}_{L+1}^{-1}=\boldsymbol{H}_{L}^{-1}-\frac{\boldsymbol{H}_{L}^{-1}\boldsymbol{b}_{L+1}\boldsymbol{b}_{L+1}^{T}\boldsymbol{H}_{L}^{-1}}{1+\boldsymbol{b}_{L+1}^{T}\boldsymbol{H}_{L}^{-1}\boldsymbol{b}_{L+1}}\tag{5.66}</script><p>使⽤这种⽅式，数据点可以依次使⽤，直到 $L+1=N$ ，整个数据集被处理完毕。于是，这个结果表⽰⼀个计算<code>Hessian</code>矩阵的逆矩阵的算法， 这个算法只需对数据集扫描⼀次。最开始的矩阵 $\boldsymbol{H}_0$ 被选为 $\alpha\boldsymbol{I}$ ，其中 $\alpha$ 是⼀个较⼩的量，从⽽算法实际找的是 $\boldsymbol{H}+\alpha\boldsymbol{I}$ 的逆矩阵。</p><h1 id="五，有限差"><a href="#五，有限差" class="headerlink" title="五，有限差"></a>五，有限差</h1><p>如果对每对可能的权值施加⼀个扰动，那么有</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial^{2}E}{\partial{w_{ji}}\partial_{w_{lk}}}&=\frac{1}{4\epsilon^{2}}\{E(w_{ji}+\epsilon,w_{lk}+\epsilon)-E(w_{ji}+\epsilon,w_{lk}-\epsilon)\\&-E(w_{ji}-\epsilon,w_{lk}+\epsilon)+-E(w_{ji}-\epsilon,w_{lk}-\epsilon)+O(\epsilon^{2})\}\end{aligned}\tag{5.67}</script><p>通过使⽤对称的中⼼差，确保了残留的误差项是 $O(\epsilon^{2})$ ⽽不是 $O(\epsilon)$ 。 由于在<code>Hessian</code>矩阵中有 $W^2$ 个元素，且每个元素的计算需要四次正向传播过程，每个传播过程需要 $O(W)$ 次操作（每个模式），因此看到这种⽅法计算完整的<code>Hessian</code>矩阵需要 $O(W^3)$ 次操作。所以，这个⽅法的计算性质很差，虽然在实际应⽤中它对于检查反向传播算法的执⾏的正确性很有⽤。 </p><p>⼀个更加⾼效的数值导数的⽅法是将中⼼差应⽤于⼀阶导数，⽽⼀阶导数可以通过反向传播⽅法计算。即</p><script type="math/tex; mode=display">\frac{\partial^{2}E}{\partial{w_{ji}}\partial_{w_{lk}}}=\frac{1}{2\epsilon}\left\{\frac{\partial{E}}{\partial{w_{ji}}}(w_{kl}+\epsilon)-\frac{\partial{E}}{\partial{w_{ji}}}(w_{kl}-\epsilon)\right\}+O(\epsilon^{2})\tag{5.68}</script><p>由于只需要对 $W$ 个权值施加扰动，且梯度可以通过 $O(W)$ 次计算得到，因此看到这种⽅法可以在 $O(W^2)$ 次操作内得到<code>Hessian</code>矩阵。</p><h1 id="六，Hessian-矩阵的精确计算"><a href="#六，Hessian-矩阵的精确计算" class="headerlink" title="六，Hessian 矩阵的精确计算"></a>六，<strong><code>Hessian</code></strong> 矩阵的精确计算</h1><p>考虑⼀个具有两层权值的⽹络，这种⽹络中待求的⽅程很容易推导，将使⽤下标 $i$ 和 $i^{\prime}$ 表⽰输⼊，⽤下标 $j$ 和 $j^{\prime}$ 表⽰隐含单元，⽤下标 $k$ 和 $k^{\prime}$ 表⽰输出。⾸先定义</p><script type="math/tex; mode=display">\delta_k=\frac{\partial{E_n}}{\partial{a_k}},M_{kk^{\prime}}=\frac{\partial^{2}{E_n}}{\partial{a_k}\partial{a_{k^{\prime}}}}</script><p>其中 $E_n$ 是数据点 $n$ 对误差函数的贡献。于是，这个⽹络的<code>Hessian</code>矩阵可以被看成三个独⽴的模块，即</p><p>1）两个权值都在第⼆层</p><script type="math/tex; mode=display">\frac{\partial^{2}E_n}{\partial{w_{kj}^{(2)}}\partial{w_{k^{\prime}j^{\prime}}^{(2)}}}=z_jz_{j^{\prime}}M_{kk^{\prime}}\tag{5.69}</script><p>2）两个权值都在第⼀层</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial^{2}E_n}{\partial{w_{ji}^{(1)}}\partial{w_{j^{\prime}i^{\prime}}^{(1)}}}&=x_ix_{i^{\prime}}h^{\prime\prime}(a_{j^{\prime}})I_{jj^{\prime}}\sum_{k}w_{kj^{\prime}}^{(2)}\delta_{k}\\&+x_ix_{i^{\prime}}h^{\prime}(a_{j^{\prime}})h^{\prime}(a_j)\sum_{k}\sum_{k^{\prime}}w_{k^{\prime}j^{\prime}}^{(2)}w_{kj}^{(2)}M_{kk^{\prime}}\end{aligned}\tag{5.70}</script><p>3）每⼀层有⼀个权值</p><script type="math/tex; mode=display">\frac{\partial^{2}E_n}{\partial{w_{ji}^{(1)}}\partial{w_{kj^{\prime}}^{(2)}}}=x_{i}h^{\prime}(a_{j})\left\{\delta_{k}I_{jj^{\prime}}+z_{j^{\prime}}\sum_{k^{\prime}}w_{k^{\prime}j}^{(2)}M_{kk^{\prime}}\right\}\tag{5.71}</script><p>其中，$I_{jj^{\prime}}$ 是单位矩阵的第 $j$, $j^{\prime}$ 个元素。</p><h1 id="七，Hessian-矩阵的快速乘法"><a href="#七，Hessian-矩阵的快速乘法" class="headerlink" title="七，Hessian 矩阵的快速乘法"></a>七，<strong><code>Hessian</code></strong> 矩阵的快速乘法</h1><p>尝试寻找⼀种只需 $O(W)$ 次操作的直接计算 $\boldsymbol{v}^{T}\boldsymbol{H}$ 的⾼效⽅法。⾸先注意到</p><script type="math/tex; mode=display">\boldsymbol{v}^{T}\boldsymbol{H}=\boldsymbol{v}^{T}\nabla(\nabla{E})\tag{5.72}</script><p>其中 $\nabla$ 表⽰权空间的梯度算符。然后，可以写下计算 $\nabla{E}$ 的标准正向传播和反向传播的⽅程， 继而得到⼀组计算 $\boldsymbol{v}^{T}\boldsymbol{H}$ 的正向传播和反向传播的⽅程（<code>Møller</code>, 1993; <code>Pearlmutter</code>, 1994）。这对应于将微分算符 $\boldsymbol{v}^{T}\nabla$ 作⽤于原始的正向传播和反向传播的⽅程。<code>Pearlmutter</code>（1994）使⽤记号 $\mathcal{R}\{·\}$ 表⽰算符 $\boldsymbol{v}^{T}\nabla$ 。下⾯的分析过程很直接，会使⽤通常的微积分规则，以及下⾯的结果</p><script type="math/tex; mode=display">\mathcal{R}\{\boldsymbol{w}\}=\boldsymbol{v}</script><p>使⽤⼀个简单的例⼦来说明这个⽅法，使⽤两层⽹络，以及线性的输出单元和平⽅和误差函数。考虑数据集⾥的⼀个模式对于误差函数的贡献。这样，所要求解的向量可以通过求出每个模式各⾃的贡献然后求和的⽅式得到。对于两层神经⽹络，正向传播⽅程为</p><script type="math/tex; mode=display">a_j=\sum_{i}w_{ji}x_{i}\\z_j=h(a_j)\\y_k=\sum_{j}w_{kj}z_j</script><p>现在使⽤ $\mathcal{R}\{·\}$ 算符作⽤于这些⽅程上，得到⼀组正向传播⽅程，形式为</p><script type="math/tex; mode=display">\mathcal{R}\{a_j\}=\sum_{i}v_{ji}x_{i}\\\mathcal{R}\{z_j\}=h^{\prime}(a_j)\mathcal{R}\{a_j\}\\\mathcal{R}\{y_k\}=\sum_{j}w_{kj}\mathcal{R}\{z_j\}+\sum_{j}v_{kj}z_j</script><p>其中，$v_{ji}$ 是向量 $\boldsymbol{v}$ 中对应于权值 $w_{ji}$ 的元素。</p><p>由于考虑的是平⽅和误差函数，因此有下⾯的标准的反向传播表达式</p><script type="math/tex; mode=display">\delta_{k}=y_k-t_k\\\delta_j=h^{\prime}(a_j)\sum_{k}w_{kj}\delta_{k}</script><p>将 $\mathcal{R}\{·\}$ 算符作⽤于这些⽅程上，得到⼀组反向传播⽅程，形式为</p><script type="math/tex; mode=display">\mathcal{R}\{\delta_k\}=\mathcal{R}\{y_k\}\\\begin{aligned}\mathcal{R}\{\delta_j\}&=h^{\prime\prime}(a_j)\mathcal{R}\{a_j\}\sum_{k}w_{kj}\delta_{k}\\&+h^{\prime}(a_j)\sum_{k}v_{kj}\delta_{k}+h^{\prime}(a_j)\sum_{k}w_{kj}\mathcal{R}\{\delta_{k}\}\end{aligned}</script><p>最后，有误差函数的⼀阶导数的⽅程</p><script type="math/tex; mode=display">\frac{\partial{E}}{\partial{w_{kj}}}=\delta_{k}z_j\\\frac{\partial{E}}{\partial{w_{ji}}}=\delta_{j}x_i</script><p>使⽤ $\mathcal{R}\{·\}$ 算符作⽤在这些⽅程上，我们得到了下⾯的关于 $\boldsymbol{v}^{T}\boldsymbol{H}$ 的表达式</p><script type="math/tex; mode=display">\mathcal{R}\left\{\frac{\partial{E}}{\partial{w_{kj}}}\right\}=\mathcal{R}\{\delta_{k}\}z_j+\delta_{k}\mathcal{R}\{z_j\}\\\mathcal{R}\left\{\frac{\partial{E}}{\partial{w_{ji}}}\right\}=x_i\mathcal{R}\left\{\delta_j\right\}</script><p>如图5.11～13，使⽤从正弦数据集中抽取的10个数据点训练的两层神经⽹络的例⼦。 各图分别给出了使⽤ $M = 1, 3, 10$ 个隐含单元调节⽹络的结果，调节的⽅法是使⽤放缩的共轭梯度算法来最⼩化平⽅和误差函数。</p><p><img src="/images/prml_20191015235858.png" alt="M=1"></p><p><img src="/images/prml_20191015235909.png" alt="M=3"></p><p><img src="/images/prml_20191015235919.png" alt="M=10"></p><p>如图5.14，对于多项式数据集，测试集的平⽅和误差与⽹络的隐含单元的数量的图像。对于每个⽹络规模，都随机选择了30个初始点，这展⽰了局部最⼩值的效果。对于每个新的初始点，权向量通过从⼀个各向 同性的⾼斯分布中取样，这个⾼斯分布的均值为零，⽅差为10。</p><p><img src="/images/prml_20191015235404.png" alt="多项式数据集"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，Hessian-矩阵&quot;&gt;&lt;a href=&quot;#一，Hessian-矩阵&quot; class=&quot;headerlink&quot; title=&quot;一，Hessian 矩阵
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】前馈神经网络</title>
    <link href="https://zhangbc.github.io/2019/10/16/prml_05_01/"/>
    <id>https://zhangbc.github.io/2019/10/16/prml_05_01/</id>
    <published>2019-10-16T00:29:43.000Z</published>
    <updated>2019-10-16T01:42:01.327Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，前馈神经网络"><a href="#一，前馈神经网络" class="headerlink" title="一，前馈神经网络"></a>一，前馈神经网络</h1><h2 id="1，前馈神经网络"><a href="#1，前馈神经网络" class="headerlink" title="1，前馈神经网络"></a>1，前馈神经网络</h2><p>基于固定⾮线性基函数 $\phi_{j}(\boldsymbol{x})$ 的线性组合，形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})=f\left(\sum_{j=1}^{M}w_{j}\phi_{j}(\boldsymbol{x})\right)\tag{5.1}</script><p>其中 $f(·)$ 在分类问题中是⼀个⾮线性激活函数， 在回归问题中为恒等函数。现在的⽬标是推⼴这个模型，使得基函数 $\phi_{j}(\boldsymbol{x})$ 依赖于参数，从⽽能够让这些参数以及系数 $\{w_j\}$ 能够在训练阶段调节。</p><p><strong>神经⽹络</strong>使⽤与公式(5.1)形式相同的基函数，即每个基函数本⾝是输⼊的线性组合的⾮线性函数，其中线性组合的系数是可调节参数。</p><p>⾸先，构造输⼊变量 $x_1, \dots , x_D$ 的 $M$ 个线性组合，形式为</p><script type="math/tex; mode=display">a_{j}=\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\tag{5.2}</script><p>其中 $j = 1,\dots, M$ ， 且上标<code>(1)</code>表⽰对应的参数是神经⽹络的<strong>第⼀“层”</strong>。参数 $w_{ji}^{(1)}$ 称为 <strong>权</strong>（<code>weight</code>）， 参数 $w_{j0}^{(1)}$ 称为<strong>偏置</strong>（<code>bias</code>），$a_{j}$ 被称为<strong>激活</strong>（<code>activation</code>）。每个激活都使⽤⼀个可微的⾮线性激活函数（<code>activation function</code>）$h(·)$ 进⾏变换，可得</p><script type="math/tex; mode=display">z_{j}=h(a_{j})\tag{5.3}</script><p>这些量对应于公式(5.1)中的基函数的输出， 这些基函数在神经⽹络中被称为<strong>隐含单元</strong> （<code>hidden unit</code>）。⾮线性函数 $h(·)$ 通常被选为 <strong><code>S</code>形的函数</strong>，例如<code>logistic sigmoid</code>函数或者双曲正切函数。根据公式(5.1)，这些值再次线性组合，得到<strong>输出单元激活</strong>（<code>output unit activation</code>）</p><script type="math/tex; mode=display">a_{k}=\sum_{j=1}^{M}w_{kj}^{(2)}z_{j}+w_{k0}^{(2)}\tag{5.4}</script><p>其中 $k = 1, \dots, K$ ， 且 $K$ 是输出的总数量。 这个变换对应于神经⽹络的第⼆层， 并且 $w_{k0}^{(2)}$ 是偏置参数。使⽤⼀个恰当的激活函数对输出单元激活进⾏变换，得到神经⽹络的⼀组输出 $y_k$ ，激活函数的选择由数据本⾝以及⽬标变量的假定的分布确定。因此对于标准的回归问题， 激活函数是恒等函数， 从⽽ $y_k = a_k$ 。 类似地， 对于多个⼆元分类问题， 每个输出单元激活使⽤<code>logistic sigmoid</code>函数进⾏变换，即</p><script type="math/tex; mode=display">y_k=\sigma(a_k)\tag{5.5}</script><p>其中，</p><script type="math/tex; mode=display">\sigma(a)=\frac{1}{1+\exp(-a)}</script><p>综上可知，对于<code>sigmoid</code>输出单元激活函数，整体的⽹络函数为</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})=\sigma\left(\sum_{j=1}^{M}w_{kj}^{(2)}h\left(\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\right)+w_{k0}^{(2)}\right)\tag{5.6}</script><p>如图5.1，对应于公式(5.6)的两层神经⽹络的⽹络图。输⼊变量、隐含变量、输出变量都表⽰为结点，权参数被表⽰为结点之间的链接，其中偏置参数被表⽰为来⾃额外的输⼊变量 $x_0$ 和隐含变量 $z_0$ 的链接。箭头表⽰信息流在⽹络中进⾏前向传播的⽅向。</p><p><img src="/images/prml_20191008084859.png" alt="两层神经⽹络"></p><p>可以通过定义额外的输⼊变量 $x_0$ 的⽅式将公式(5.2)中的偏置参数整合到权参数集合中，其中额外的输⼊变量 $x_0$ 的值被限制为 $x_0 = 1$，因此公式(5.2)的形式为</p><script type="math/tex; mode=display">a_{j}=\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}\tag{5.7}</script><p>类似地，把第⼆层的偏置整合到第⼆层的权参数中，从⽽整体的⽹络函数为</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})=\sigma\left(\sum_{j=1}^{M}w_{kj}^{(2)}h\left(\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}\right)\right)\tag{5.8}</script><p>神经⽹络模型由两个处理阶段组成，每个阶段都类似于感知器模型，因此神经⽹络也被称为<strong>多层感知器</strong>（<code>multilayer perceptron</code>），或者 <strong><code>MLP</code></strong>。与感知器模型相⽐，⼀个重要的<strong>区别</strong>是神经⽹络在隐含单元中使⽤连续的<code>sigmoid</code>⾮线性函数，⽽感知器使⽤阶梯函数⾮线性函数。这意味着神经⽹络函数关于神经⽹络参数是可微的，这个性质在神经⽹络的训练过程中起着重要的作⽤。</p><p>神经⽹络结构的⼀个<strong>扩展</strong>是引⼊<strong>跨层</strong>（<code>skip-layer</code>）<strong>链接</strong>，每个跨层链接都关联着⼀个对应的可调节参数。</p><p><strong>前馈</strong>（<code>feed-forward</code>）<strong>结构</strong>：⽹络中不能存在有向圈，从⽽确保了输出是输⼊的确定函数。</p><p>举例：⽹络中每个（隐含或者输出）单元都计算了⼀个下⾯的函数</p><script type="math/tex; mode=display">z_k=h\left(\sum_{j}w_{kj}z_{i}\right)</script><p>其中，求和的对象是所有向单元 $k$ 发送链接的单元（偏置参数也包含在了求和式当中）。</p><p>如图5.2，具有⼀般的前馈拓扑结构的神经⽹络，注意，每个隐含电源和输出单元都与⼀个偏置参数关联。</p><p><img src="/images/prml_20191008091241.png" alt="前馈拓扑结构"></p><p>如图5.3～5.6，多层感知器的能⼒说明，它⽤来近似四个不同的函数。 (<code>a</code>) $f(x) = x^2$ ，(<code>b</code>) $f(x) = \sin(x)$， (<code>c</code>) $f(x) = |x|$，(<code>d</code>) $f(x) = H(x)$，其中 $H(x)$ 是⼀个硬阶梯函数。在每种情况下，$N = 50$ 个数据点（⽤蓝点 表⽰）从区间 $(−1, 1)$ 中均匀分布的 $x$ 中进⾏取样，然后计算出对应的 $f(x)$ 值。这些数据点之后⽤来训练⼀个具有3个隐含单元的两层神经⽹络，隐含单元的激活函数为<code>tanh</code>函数，输出为线性输出单元。⽣成的⽹络函数使⽤红⾊曲线表⽰，三个隐含单元的输出⽤三条虚线表⽰。</p><p><img src="/images/prml_20191008092026.png" alt="a"></p><p><img src="/images/prml_ 20191008092041.png" alt="b"></p><p><img src="/images/prml_20191008092055.png" alt="c"></p><p><img src="/images/prml_20191008092105.png" alt="d"></p><h2 id="2，权空间对称性"><a href="#2，权空间对称性" class="headerlink" title="2，权空间对称性"></a>2，权空间对称性</h2><p>前馈神经⽹络的⼀个<strong>性质</strong>是，对于多个不同的权向量 $\boldsymbol{w}$ 的选择，⽹络可能产⽣同样的从输⼊到输出的映射函数（<code>Chen et al.</code>, 1993）。</p><p>考虑两层⽹络，⽹络有 $M$ 个隐含结点，激活函数是双曲正切函数，且两层之间完全链接。如果把作⽤于某个特定的隐含单元的所有的权值以及偏置全部变号，那么对于给定的输⼊模式， 隐含单元的激活的符号也会改变。 这是因为双曲正切函数是⼀个奇函数， 即 $\tan h(−a) = −\tan h(a)$。这种变换可以通过改变所有从这个隐含单元到输出单元的权值的符号的⽅式进⾏精确补偿。因此，通过改变特定⼀组权值（以及偏置）的符号，⽹络表⽰的输⼊-输出映射函数不会改变，因此我们已经找到了两个不同的权向量产⽣同样的映射函数。对于 $M$ 个 隐含单元，会有 $M$ 个这样的“符号改变”对称性，因此任何给定的权向量都是 $2^M$ 个等价的权向量中的⼀个。</p><p>类似地，假设将与某个特定的隐含结点相关联的所有输⼊和输出的权值（和偏置）都变为与不同的隐含结点相关联的对应的权值（和偏置）。与之前⼀样，这显然使得⽹络的输⼊-输出映射不变，但是对应了⼀个不同的权向量。对于 $M$ 个隐含结点，任何给定的权向量都属于这种交换对称性产⽣的 $M!$ 个等价的权向量中的⼀个，它对应于 $M!$ 个不同的隐含单元的顺序。于是，⽹络有⼀个整体的权空间对称性因⼦ $M!2^M$ 。</p><h1 id="二，网络训练"><a href="#二，网络训练" class="headerlink" title="二，网络训练"></a>二，网络训练</h1><h2 id="1，回归问题"><a href="#1，回归问题" class="headerlink" title="1，回归问题"></a>1，回归问题</h2><p>给定⼀个由输⼊向量 $\{\boldsymbol{x_n}\}(n = 1, \dots , N)$ 组成的训练集，以及⼀ 个对应的⽬标向量 $\boldsymbol{t}_n$ 组成的集合，要最⼩化误差函数</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}||\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w})-\boldsymbol{t}_n||^{2}\tag{5.9}</script><p>⾸先， 讨论回归问题。考虑⼀元⽬标变量 $t$ 的情形，假定 $t$ 服从⾼斯分布，均值与 $\boldsymbol{x}$ 相关，由神经⽹络的输出确定，即</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w})=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1})\tag{5.10}</script><p>其中 $\beta$ 是⾼斯噪声的精度（⽅差的倒数）。</p><p>给定⼀个由 $N$ 个独⽴同分布的观测组成的数据集 $\mathbf{X} = \{\boldsymbol{x}_1, \dots, \boldsymbol{x}_N\}$， 以及对应的⽬标值 $\mathbf{t} = \{t_1, \dots, t_N\}$</p><p>，构造对应的似然函数</p><script type="math/tex; mode=display">p(\mathbf{t}|\boldsymbol{X},\boldsymbol{w},\beta)=\prod_{n=1}^{N}p(t_n|\boldsymbol{x}_n,\boldsymbol{w},\beta)\tag{5.11}</script><p>取负对数，可得到误差函数</p><script type="math/tex; mode=display">\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x},\boldsymbol{w})-t_n\}^{2}-\frac{N}{2}\ln\beta+\frac{N}{2}\ln(2\pi)</script><p>这可以⽤来学习参数 $\boldsymbol{w}$ 和 $\beta$ 。</p><p>⾸先考虑 $\boldsymbol{w}$ 的确定。最⼤化似然函数等价于最⼩化平⽅和误差函数：</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}\{\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w})-\boldsymbol{t}_n\}^{2}\tag{5.12}</script><p>其中去掉了相加的和相乘的常数。通过最⼩化 $E(\boldsymbol{w})$ 的⽅式得到 $\boldsymbol{w}$ 值被记作  $\boldsymbol{w}_{ML}$ ， 因为它对应于最⼤化似然函数。</p><p>$\beta$ 的值可以通过最⼩化似然函数的负对数的⽅式求得，为</p><script type="math/tex; mode=display">\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}\{\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w}_{ML})-\boldsymbol{t}_n\}^{2}\tag{5.13}</script><p>如果有多个⽬标变量，并且假设给定 $\boldsymbol{x}$ 和 $\boldsymbol{w}$ 的条件下，⽬标变量之间相互独⽴，且噪声精度均为 $\beta$ ，那么⽬标变量的条件分布为</p><script type="math/tex; mode=display">p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w})=\mathcal{N}(\boldsymbol{t}|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1}\boldsymbol{I})\tag{5.14}</script><p>噪声的精度为</p><script type="math/tex; mode=display">\frac{1}{\beta_{ML}}=\frac{1}{NK}\sum_{n=1}^{N}\|\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w}_{ML})-\boldsymbol{t}_n\|^{2}\tag{5.15}</script><p>其中 $K$ 是⽬标变量的数量。</p><p>在回归问题中，我们可以把神经⽹络看成具有⼀个恒等输出激活函数的模型，即 $y_k = a_k$ 。对应的平⽅和误差函数有下⾯的性质：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial a_k}=y_k-t_k\tag{5.16}</script><p>现在考虑⼆分类的情形。⼆分类问题中，有⼀个单⼀⽬标变量 $t$，且 $t = 1$ 表⽰类别 $\mathcal{C}_1$ ，$t = 0$ 表⽰类别 $\mathcal{C}_2$ 。遵循对于标准链接函数的讨论，考虑⼀个具有单⼀输出的⽹络，它的激活函数是<code>logistic sigmoid</code>函数</p><script type="math/tex; mode=display">y=\sigma(a)\equiv\frac{1}{1+\exp(-a)}\tag{5.17}</script><p>从⽽ $0\le y(\boldsymbol{x}, \boldsymbol{w})\le1$ 。可以把 $y(\boldsymbol{x},\boldsymbol{w})$ 表⽰为条件概率 $p(\mathcal{C}_1|\boldsymbol{x})$ ， 此时 $p(\mathcal{C}_2|\boldsymbol{x})$ 为 $1 − y(\boldsymbol{x},\boldsymbol{w})$。如果给定了输⼊，那么⽬标变量的条件概率分布是⼀个伯努利分布，形式为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w})=y(\boldsymbol{x},\boldsymbol{w})^{t}\{1-y(\boldsymbol{x},\boldsymbol{w})\}^{1-t}\tag{5.18}</script><p>如果考虑⼀个由独⽴的观测组成的训练集，那么由负对数似然函数给出的误差函数就是⼀个交叉熵（<code>cross-entropy</code>）误差函数，形式为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\{t_n\ln y_n+(1-t_n)\ln(1-y_n)\}\tag{5.19}</script><p>其中 $y_n$ 表⽰ $y(\boldsymbol{x}_n,\boldsymbol{w})$ 。</p><p>如果有 $K$ 个相互独⽴的⼆元分类问题， 那么可以使⽤具有 $K$ 个输出的神经⽹络， 每个输出都有⼀个<code>logistic sigmoid</code>激活函数，与每个输出相关联的是⼀个⼆元类别标签 $t_k\in\{0,1\}$ ，其中 $k=1, \dots, K$ 。如果假定类别标签是独⽴的，那么给定输⼊向量，⽬标向量的条件概率分布为</p><script type="math/tex; mode=display">p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w})=\prod_{k=1}^{K}y_k(\boldsymbol{x},\boldsymbol{w})^{t_k}[1-y_k(\boldsymbol{x},\boldsymbol{w})]^{1-t_k}\tag{5.20}</script><p>取似然函数的负对数，可以得误差函数</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\sum_{k=1}^{K}\{t_{nk}\ln y_{nk}+(1-t_{nk})\ln(1-y_{nk})\}\tag{5.21}</script><p>其中 $y_{nk}$ 表⽰ $y_k(\boldsymbol{x}_n,\boldsymbol{w})$ 。</p><p>如图5.7，误差函数 $E(\boldsymbol{w})$ 的⼏何表⽰，其中，误差函数被表⽰为权空间上的⼀个曲⾯。点 $\boldsymbol{w}_A$ 是⼀个局部最⼩值，点 $\boldsymbol{w}_B$ 是全局最⼩值。在任意点 $\boldsymbol{w}_C$ 处，误差函数的局部梯度由向量 $\nabla{E}$ 给出。</p><p><img src="/images/prml_20191012111823.png" alt="误差函数E的⼏何表⽰"></p><p>最后，我们考虑标准的多分类问题，其中每个输⼊被分到 $K$ 个互斥的类别中。⼆元⽬标变量 $t_k\in\ {0,1}$ 使 ⽤“<code>1-of-K</code>”表达⽅式来表⽰类别，从⽽⽹络的输出可以表⽰为 $y_k(\boldsymbol{x},\boldsymbol{w}) = p(t_k=1|\boldsymbol{x})$ ，因此误差函数为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln y_{k}(\boldsymbol{x}_n,\boldsymbol{w})\tag{5.22}</script><p>输出单元激活函数（对应于标准链接函数）是<code>softmax</code>函数</p><script type="math/tex; mode=display">y_k(\boldsymbol{x},\boldsymbol{w})=\frac{\exp(a_k(\boldsymbol{x},\boldsymbol{w}))}{\sum_{j}\exp(a_j(\boldsymbol{x},\boldsymbol{w}))}\tag{5.23}</script><p>其中，$0 \le y_k \le 1$ ，且 $\sum_{k} y_k=1$ 。</p><p>总之，根据解决的问题类型，关于输出单元激活函数和对应的误差函数，都存在⼀个⾃然的选择。对于<strong>回归问题</strong>，使⽤线性输出和平⽅和误差函数，对于（多类独⽴的）<strong>⼆元分类问题</strong>， 使⽤<code>logistic sigmoid</code>输出以及交叉熵误差函数，对于<strong>多类分类问题</strong>， 使⽤<code>softmax</code>输出以及对应的多分类交叉熵错误函数。对于涉及到<strong>两类分类问题</strong>，可以使⽤单⼀的<code>logistic sigmoid</code>输出，也可以使⽤神经⽹络，这个神经⽹络有两个输出，且输出激活函数为<code>softmax</code>函数。</p><h2 id="2，参数最优化"><a href="#2，参数最优化" class="headerlink" title="2，参数最优化"></a>2，参数最优化</h2><p>考虑寻找能够使得选定的误差函数 $E(\boldsymbol{w})$ 达到最⼩值的权向量 $\boldsymbol{w}$ 。现在，考虑误差函数的⼏何表⽰是很有⽤的，可以把误差函数看成位于权空间的⼀个曲⾯。⾸先注意到，如果在权空间中⾛⼀⼩步， 从 $\boldsymbol{w}$ ⾛到 $\boldsymbol{w}+\delta\boldsymbol{w}$ ， 那么误差函数的改变为 $\delta E \simeq \delta \boldsymbol{w}^{T}\nabla E(\boldsymbol{w})$ ，其中向量 $\delta E(\boldsymbol{w})$ 在误差函数增加速度最⼤的⽅向上。 由于误差 $E(\boldsymbol{w})$ 是 $\boldsymbol{w}$ 的光滑连续函数，因此它的最⼩值出现在权空间中误差函数梯度等于零的位置上， 即</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=0</script><p>如果最⼩值不在这个位置上，我们就可以沿着⽅向 $−\nabla E(\boldsymbol{w})$ ⾛⼀⼩步，进⼀步减⼩误差。梯度为零的点被称为驻点，它可以进⼀步地被分为极⼩值点、极⼤值点和鞍点。<br>对于所有的权向量，误差函数的最⼩值被称为<strong>全局最⼩值</strong>（<code>golobal minimum</code>）。任何其他的使误差函数的值较⼤的极⼩值被称为<strong>局部极⼩值</strong>（<code>local minima</code>）。</p><p>由于显然⽆法找到⽅程 $\nabla E(\boldsymbol{w})=0$ 的解析解，因此使⽤迭代的数值⽅法。⼤多数⽅法涉及到为权向量选择某个初始值 $\boldsymbol{w}_0$ ，然后在权空间中进⾏⼀系列移动，形式为</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{\tau}+\nabla w^{(\tau)}\tag{5.24}</script><p>其中 $\tau$ 表⽰迭代次数。</p><h2 id="3，局部⼆次近似"><a href="#3，局部⼆次近似" class="headerlink" title="3，局部⼆次近似"></a>3，局部⼆次近似</h2><p>考虑 $E(\boldsymbol{w})$ 在权空间某点 $\hat{\boldsymbol{w}}$ 处的泰勒展开</p><script type="math/tex; mode=display">E(\boldsymbol{w})\simeq E(\hat{\boldsymbol{w}})+(\boldsymbol{w}-\hat{\boldsymbol{w}})^{T}\boldsymbol{b}+\frac{1}{2}(\boldsymbol{w}-\hat{\boldsymbol{w}})^{T}\boldsymbol{H}(\boldsymbol{w}-\hat{\boldsymbol{w}})\tag{5.25}</script><p>其中，已省略⽴⽅项和更⾼阶的项， $\boldsymbol{b}$ 定义为 $E$ 的梯度在 $\hat{\boldsymbol{w}}$ 处的值。 </p><script type="math/tex; mode=display">\boldsymbol{b}\equiv\nabla E|_{\boldsymbol{w}=\hat{\boldsymbol{w}}}</script><p><code>Hessian</code>矩阵 $H=\nabla\nabla E$ 的元素为</p><script type="math/tex; mode=display">(\boldsymbol{H})_{ij}\equiv \frac{\partial E}{\partial w_i \partial w_j}\bigg{|}_{\boldsymbol{w}=\hat{\boldsymbol{w}}}</script><p>根据公式，梯度的局部近似为</p><script type="math/tex; mode=display">\nabla E\simeq \boldsymbol{b}+\boldsymbol{H}(\boldsymbol{w}-\hat{\boldsymbol{w}})\tag{5.26}</script><p>考虑⼀个特殊情况：在误差函数最⼩值点 $\boldsymbol{w}^{*}$ 附近的局部⼆次近似。在这种情况下，没有线性项，因为在 $\boldsymbol{w}^{*}$ 处 $\nabla E=0$ ，公式(5.25)变成了</p><script type="math/tex; mode=display">E(\boldsymbol{w})\simeq E(\boldsymbol{w}^{*})+\frac{1}{2}(\boldsymbol{w}-\boldsymbol{w}^{*})^{T}\boldsymbol{H}(\boldsymbol{w}-\boldsymbol{w}^{*})\tag{5.27}</script><p>这⾥<code>Hessian</code>矩阵在点 $\boldsymbol{w}^{*}$ 处计算。 为了⽤⼏何的形式表⽰这个结果， 考虑<code>Hessian</code>矩阵的特征值⽅程</p><script type="math/tex; mode=display">\boldsymbol{H}\boldsymbol{\mu}_i=\lambda_i\boldsymbol{\mu}_i\tag{5.28}</script><p>其中特征向量 $\boldsymbol{\mu}_i$ 构成了完备的单位正交集合，即</p><script type="math/tex; mode=display">\boldsymbol{\mu}_{i}^{T}\boldsymbol{\mu}_{j}=\delta_{ij}</script><p>现在把 $(\boldsymbol{w}-\boldsymbol{w}^{*})$ 展开成特征值的线性组合的形式</p><script type="math/tex; mode=display">\boldsymbol{w}-\boldsymbol{w}^{*}=\sum_{i}\alpha_{i}\boldsymbol{\mu}_i\tag{5.29}</script><p>这可以被看成坐标系的变换，坐标系的原点变为了 $\boldsymbol{w}^{*}$ ，坐标轴旋转，与特征向量对齐（通过列为 $\boldsymbol{\mu}_i$ 的正交矩阵），误差函数可以写成下⾯的形式</p><script type="math/tex; mode=display">E(\boldsymbol{w})=E(\boldsymbol{w}^{*})+\frac{1}{2}\sum_{i}\alpha_{i}\boldsymbol{\mu}_{i}\tag{5.30}</script><p>矩阵 $\boldsymbol{H}$ 是正定的（<code>positive definite</code>）当且仅当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}&gt;0$ 对所有的 $\boldsymbol{v}\ne 0$ 都成立。</p><p>如图5.8，在最⼩值 $\boldsymbol{w}^{*}$ 的邻域中，误差函数可以⽤⼆次函数近似。这样，常数误差函数的轮廓线为椭圆，它的轴与<code>Hessian</code>矩阵的特征向量 $\boldsymbol{\mu}_i$ 给出，长度与对应的特征值 $\lambda_i$ 的平⽅根成反⽐。</p><p><img src="/images/prml_20191013230336.png" alt="⼆次函数近似"></p><p>由于特征向量 $\{\boldsymbol{\mu}_i\}$ 组成了⼀个完备集，因此任意的向量 $\boldsymbol{v}$ 都可以写成下⾯的形式</p><script type="math/tex; mode=display">\boldsymbol{v}=\sum_{i}c_i\boldsymbol{\mu}_i</script><p>有，</p><script type="math/tex; mode=display">\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}=\sum_{i}c_{i}^{2}\lambda_{i}\tag{5.31}</script><p>因此 $\boldsymbol{H}$ 是正定的，当且仅当它的所有的特征值均严格为正。在新的坐标系中，基向量是特征向量 $\{\boldsymbol{\mu}_i\}$ ，$E$ 为常数的轮廓线是以原点为中⼼的椭圆。对于⼀维权空间，驻点 $\boldsymbol{w}^{∗}$ 满⾜下⾯条件时取得最⼩值</p><script type="math/tex; mode=display">\frac{\partial^{2}E}{\partial w^{2}}\Bigg{|}_{\boldsymbol{w}^{*}}>0</script><p>对应的 $\boldsymbol{D}$ 维的结论是，在 $\boldsymbol{w}^{*}$ 处的<code>Hessian</code>矩阵是正定矩阵。</p><h2 id="4，使⽤梯度信息"><a href="#4，使⽤梯度信息" class="headerlink" title="4，使⽤梯度信息"></a>4，使⽤梯度信息</h2><p>可以使⽤误差反向传播的⽅法⾼效地计算误差函数的梯度，这个梯度信息的使⽤可以⼤幅度加快找到极⼩值点的速度。<br>在公式(5.25)给出的误差函数的⼆次近似中，误差曲⾯由 $\boldsymbol{b}$ 和 $\boldsymbol{H}$ 确定， 它包含了总共 $\frac{W(W+3)}{2}$ 个独⽴的元素（因为矩阵 $\boldsymbol{H}$ 是对称的），其中 $W$ 是 $\boldsymbol{w}$ 的维度（即⽹络中可调节参数的总数）。这个⼆次近似的极⼩值点的位置因此依赖于 $O(W^2)$ 个参数，并且不应该奢求能够在收集到 $O(W^2)$ 条独⽴的信息之前就能够找到最⼩值。如果不使⽤梯度信息，不得不进⾏ $O(W^2)$ 次函数求值，每次求值都需要 $O(W)$ 个步骤。 因此，使⽤这种⽅法求最⼩值需要的计算复杂度为 $O(W^3)$ 。现在将这种⽅法与使⽤梯度信息的⽅法进⾏对⽐，由于每次计算 $\nabla{E}$ 都会带来 $W$ 条信息，因此可能预计找到函数的极⼩值需要计算 $O(W)$ 次梯度。通过使⽤误差反向传播算法，每个这样的计算只需要 $O(W)$ 步， 因此使⽤这种⽅法可以在 $O(W^2)$ 个步骤内找到极⼩值。</p><h2 id="5，梯度下降最优化"><a href="#5，梯度下降最优化" class="headerlink" title="5，梯度下降最优化"></a>5，梯度下降最优化</h2><p>最简单的使⽤梯度信息的⽅法是：每次权值更新都是在负梯度⽅向上的⼀次⼩的移动，即</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{(\tau)}-\eta\nabla E\left(\boldsymbol{w}^{(\tau)}\right)\tag{5.32}</script><p>其中参数 $\eta&gt;0$ 被称为<strong>学习率</strong>（<code>learning rate</code>）。</p><p>误差函数是关于训练集定义的，因此为了计算 $\nabla{E}$ ，每⼀步都需要处理整个数据集。在每⼀步，权值向量都会沿着误差函数下降速度最快的⽅向移动， 因此这种⽅法被称为<strong>梯度下降法</strong>（<code>gradient descent</code>）或者<strong>最陡峭下降法</strong>（<code>steepest descent</code>）。</p><p>对于批量最优化⽅法，存在更⾼效的⽅法，例如<strong>共轭梯度法</strong>（<code>conjugate gradient</code>）或者<strong>拟⽜顿法</strong>（<code>quasi-Newton</code>）。与简单的梯度下降⽅法相⽐，这些⽅法更鲁棒，更快（<code>Gill et al.</code>, 1981; <code>Fletcher</code>, 1987; <code>Nocedal and Wright</code>, 1999）。 与梯度下降⽅法不同， 这些算法具有这样的<strong>性质</strong>： 误差函数在每次迭代时总是减⼩的，除⾮权向量到达了局部的或者全局的最⼩值。</p><p>基于⼀组独⽴观测的最⼤似然函数的误差函数由⼀个求和式构成，求和式的每⼀项都对应着⼀个数据点</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\sum_{n=1}^{N}E_n(\boldsymbol{w})\tag{5.33}</script><p>在线梯度下降，也被称为<strong>顺序梯度下降</strong>（<code>sequential gradient descent</code>） 或者<strong>随机梯度下降</strong>（<code>stochastic gradient descent</code>），使得权向量的更新每次只依赖于⼀个数据点，即</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{(\tau)}-\eta\nabla E_n\left(\boldsymbol{w}^{(\tau)}\right)\tag{5.34}</script><p>这个更新在数据集上循环重复进⾏，并且既可以顺序地处理数据，也可以随机地有重复地选择数据点。</p><h1 id="三，误差反向传播"><a href="#三，误差反向传播" class="headerlink" title="三，误差反向传播"></a>三，误差反向传播</h1><p>在局部信息传递的思想中，信息在神经⽹络中交替地向前、向后传播， 这种⽅法被称为<strong>误差反向传播</strong>（<code>error backpropagation</code>），有时简称“<strong>反传</strong>”（<code>backprop</code>）。</p><h2 id="1，误差函数导数的计算"><a href="#1，误差函数导数的计算" class="headerlink" title="1，误差函数导数的计算"></a>1，误差函数导数的计算</h2><p>针对⼀组独⽴同分布的数据的最⼤似然⽅法定义的误差函数，由若⼲项的求和式组成，每⼀项对应于训练集的⼀个数据点，即</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\sum_{n=1}^{N}E_n(\boldsymbol{w})</script><p>考虑⼀个简单的线性模型，其中输出 $y_k$ 是输⼊变量 $x_i$ 的线性组合，即</p><script type="math/tex; mode=display">y_k=\sum_{i}w_{ki}x_{i}\tag{5.35}</script><p>对于⼀个特定的输⼊模式 $n$，误差函数的形式为</p><script type="math/tex; mode=display">E_n=\frac{1}{2}\sum_{k}(y_{nk}-t_{nk})^2\tag{5.36}</script><p>其中 $y_{nk} = y_k(\boldsymbol{x}_n,\boldsymbol{w})$ 。这个误差函数关于⼀个权值 $w_{ji}$ 的梯度为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=(y_{nj}-t_{nj})x_{ni}\tag{5.37}</script><p>它可以表⽰为与链接 $w_{ji}$ 的输出端相关联的“误差信号” $y_{nj}−t_{nj}$ 和与链接的输⼊端相关联的变量 $x_{ni}$ 的乘积。</p><p>在⼀个⼀般的前馈⽹络中，每个单元都会计算输⼊的⼀个加权和，形式为</p><script type="math/tex; mode=display">a_j=\sum_{i}w_{ji}z_{i}\tag{5.38}</script><p>其中 $z_i$ 是⼀个单元的激活，或者是输⼊，它向单元 $j$ 发送⼀个链接，$w_{ji}$ 是与这个链接关联的权值。偏置可以被整合到这个求和式中，整合的⽅法是引⼊⼀个额外的单元或输⼊，然后令激活恒为 $+1$。求和式通过⼀个⾮线性激活函数 $h(·)$ 进⾏变换，得到单元 $j$ 的激活 $z_j$ ，形式为</p><script type="math/tex; mode=display">z_{i}=h(a_j)\tag{5.39}</script><p>对于训练集⾥的每个模式，假定给神经⽹络提供了对应的输⼊向量，然后通过反复应⽤公式(5.38)和公式(5.39)，计算神经⽹络中所有隐含单元和输出单元的激活。这个过程通常被称为<strong>正向传播</strong>（<code>forward propagation</code>），因为它可以被看做⽹络中的⼀个向前流动的信息流。</p><p>现在考虑计算 $E_n$ 关于权值 $w_{ji}$ 的导数，各个单元的输出会依赖于某个特定的输⼊模式 $n$ 。⾸先，注意到 $E_n$ 只通过 单元 $j$ 的经过求和之后的输⼊ $a_j$ 对权值 $w_{ji}$ 产⽣依赖。因此，可以应⽤偏导数的<strong>链式法则</strong>， 得到</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\frac{\partial{E_n}}{\partial{a_{j}}}\frac{\partial{a_j}}{\partial{w_{ji}}}</script><p>如图5.9，对于隐含单元 $j$ ，计算 $\delta_{j}$ 的说明。计算时使⽤了向单元 $j$ 发送信息的那些单元 $k$ 的 $\delta$ ，使⽤反向误差传播⽅法进⾏计算。蓝⾊箭头表⽰在正向传播阶段信息流的⽅向，红⾊箭头表⽰误差信息的反向传播。</p><p><img src="/images/prml_20191014091853.png" alt="正反向传播"></p><p>现在引⼊⼀个有⽤的记号</p><script type="math/tex; mode=display">\delta_{j}\equiv\frac{\partial{E_n}}{\partial{a_{j}}}\tag{5.40}</script><p>其中 $\delta$ 通常被称为<strong>误差</strong>（<code>error</code>），使⽤公式(5.38)，有</p><script type="math/tex; mode=display">\frac{\partial{a_j}}{\partial{w_{ji}}}=z_i</script><p>继而有</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\delta_{j}z_{i}\tag{5.41}</script><p>从而可知，要找的导数可以通过简单地将权值输出单元的 $\delta$ 值与权值输⼊端的 $z$ 值相乘的⽅式得到（对于偏置的情形，$z = 1$ ）。</p><script type="math/tex; mode=display">\delta_{k}=y_k-t_k\tag{5.42}</script><p>为了计算隐含单元的 $\delta$ 值，使⽤偏导数的链式法则</p><script type="math/tex; mode=display">\delta_j\equiv\frac{\partial{E_n}}{\partial{a_{j}}}=\sum_{k}\frac{\partial{E_n}}{\partial{a_{k}}}\frac{\partial{a_k}}{\partial{a_{j}}}</script><p>其中求和式的作⽤对象是所有向单元 $j$ 发送链接的单元 $k$ 。注 意，单元 $k$ 可以包含其他的隐含单元和（或）输出单元。$a_j$ 的改变所造成的误差函数的改变的唯⼀来源是变量 $a_k$ 的改变。经计算可得，<strong>反向传播</strong>（<code>backpropagation</code>）公式</p><script type="math/tex; mode=display">\delta_{j}=h^{\prime}(a_j)\sum_{k}w_{kj}\delta_{k}\tag{5.43}</script><p>这表明，⼀个特定的隐含单元的 $\delta$ 值可以通过将⽹络中更⾼层单元的 $\delta$ 进⾏反向传播来实现。</p><p>反向传播算法可以总结如下：</p><blockquote><p>1）对于⽹络的⼀个输⼊向量 $\boldsymbol{x}_n$ ，使⽤公式(5.38)和公式(5.39)进⾏正向传播，找到所有隐含单元和输出单元的激活；<br>2） 使⽤公式(5.42)计算所有输出单元的 $\delta_k$ ；<br>3）使⽤公式(5.43)反向传播 $\delta$ ，获得⽹络中所有隐含单元的 $\delta_j$ ；<br>4）使⽤公式(5.41)计算导数。</p></blockquote><p>对于批处理⽅法， 总误差函数 $E$ 的导数可以通过下⾯的⽅式得到：对于训练集⾥的每个模式，重复上⾯的步骤，然后对所有的模式求和，即</p><script type="math/tex; mode=display">\frac{\partial{E}}{\partial{w_{ji}}}=\sum_{n}\frac{\partial{E_n}}{\partial{w_{ji}}}\tag{5.44}</script><h2 id="2，⼀个简单的例⼦"><a href="#2，⼀个简单的例⼦" class="headerlink" title="2，⼀个简单的例⼦"></a>2，⼀个简单的例⼦</h2><p>考虑简单的两层神经⽹络，误差函数为平⽅和误差函数，输出单元的激活函数为线性激活函数，即 $y_k=a_k$ ，⽽隐含单元的激活函数为 $S$ 形函数，形式为</p><script type="math/tex; mode=display">h(a)\equiv\tanh(a)\tag{5.45}</script><p>其中，</p><script type="math/tex; mode=display">\tanh(a)=\frac{e^a-e^{-a}}{e^a+e^{-a}}</script><p>此函数的⼀个有⽤的<strong>特征</strong>是：其导数可以表⽰成⼀个相当简单形式</p><script type="math/tex; mode=display">h^{\prime}(a)=1-h(a)^{2}</script><p>考虑⼀个标准的平⽅和误差函数，即对于模式 $n$ ，误差为</p><script type="math/tex; mode=display">E_n=\frac{1}{2}\sum_{k=1}^{K}(y_{k}-t_{k})^{2}\tag{5.46}</script><p>其中，对于⼀个特定的输⼊模式 $\boldsymbol{x}_n$ ，$y_k$ 是输出单元 $k$ 的激活，$t_k$ 是对应的⽬标值。<br>对于训练集⾥的每个模式，⾸先使⽤下⾯的公式组进⾏前向传播。</p><script type="math/tex; mode=display">a_j=\sum_{i=0}^{D}w_{ji}^{(1)}x_{i}\\z_j=\tanh(a_j)\\y_k=\sum_{j=0}^{M}w_{kj}^{(2)}z_j</script><p>再使⽤下⾯的公式计算每个输出单元的 $\delta$ 值。</p><script type="math/tex; mode=display">\delta_k=y_k-t_k\tag{5.47}</script><p>然后，使⽤下⾯的公式将这些值反向传播，得到隐含单元的 $\delta$ 值。 </p><script type="math/tex; mode=display">\delta_j=(1-z_{j}^{2})\sum_{k=1}^{K}w_{kj}\delta_{k}</script><p>最后，关于第⼀层权值和第⼆层权值的导数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}^{(1)}}}=\delta_j x_i\\\frac{\partial{E_n}}{\partial{w_{kj}^{(2)}}}=\delta_k z_j</script><h2 id="3，反向传播的效率"><a href="#3，反向传播的效率" class="headerlink" title="3，反向传播的效率"></a>3，反向传播的效率</h2><p>计算误差函数导数的反向传播⽅法是使⽤有限差。⾸先让每个权值有⼀个扰动，然后使⽤下⾯的表达式来近似导数</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\frac{E_n(w_{ji}+\epsilon)-E_n(w_{ji})}{\epsilon}+O(\epsilon)\tag{5.48}</script><p>其中 $\epsilon\ll1$ 。在软件仿真中，通过让 $\epsilon$ 变⼩，对于导数的近似的精度可以提升，直到 $\epsilon$ 过⼩，造成下溢问题。通过使⽤对称的<strong>中⼼差</strong>（<code>central difference</code>），有限差⽅法的精度可以极⼤地提⾼。 <strong>中⼼差</strong>的形式为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\frac{E_n(w_{ji}+\epsilon)-E_n(w_{ji}-\epsilon)}{2\epsilon}+O(\epsilon^{2})\tag{5.49}</script><p>计算数值导数的⽅法的主要问题是，计算复杂度为 $O(W)$ 这⼀性质不再成⽴。每次正向传播需要 $O(W)$ 步，⽽⽹络中有 $W$ 个权值，每个权值必须被单独地施加扰动， 因此整体的时间复杂度为 $O(W^2)$ 。</p><h2 id="4，Jacobian-矩阵"><a href="#4，Jacobian-矩阵" class="headerlink" title="4，Jacobian 矩阵"></a>4，<strong><code>Jacobian</code></strong> 矩阵</h2><p>考虑<code>Jacobian</code>矩阵的计算，它的元素的值是⽹络的输出关于输⼊的导数</p><script type="math/tex; mode=display">J_{ki}\equiv\frac{\partial{y_{k}}}{\partial{x_{i}}}\tag{5.50}</script><p>其中，计算每个这样的导数时，其他的输⼊都固定。</p><p>如图5.10，模块化模式识别系统，其中<code>Jacobian</code>矩阵可以⽤来将误差信号从输出模块在系统中反向传播 到更早的模块。</p><p><img src="/images/prml_20191014230852.png" alt="模块化模式识别系统"></p><p>假设我们想关于图5.10中的参数 $w$ ，最⼩化误差函数 $E$ 。误差函数的导数为</p><script type="math/tex; mode=display">\frac{\partial{E}}{\partial{w}}=\sum_{k,j}\frac{\partial{E}}{\partial{y_k}}\frac{\partial{y_k}}{\partial{z_{j}}}\frac{\partial{z_j}}{\partial{w}}</script><p>其中，图5.10中的红⾊模块的<code>Jacobian</code>矩阵出现在中间项。</p><p>由于<code>Jacobian</code>矩阵度量了输出对于每个输⼊变量的改变的敏感性，因此它也允许与输⼊关联的任意已知的误差 $\Delta{x_i}$ 在训练过的⽹络中传播，从⽽估计他们对于输出误差 $\Delta{y_k}$ 的贡献。⼆者的关系为</p><script type="math/tex; mode=display">\Delta{y_k}\simeq\sum_{i}\frac{\partial{y_k}}{\partial{x_i}}\Delta{x_i}\tag{5.51}</script><p><code>Jacobian</code>矩阵可以使⽤反向传播的⽅法计算，计算⽅法类似于之前推导误差函数关于权值的导数的⽅法。⾸先，把元素 $J_{ki}$ 写成下⾯的形式</p><script type="math/tex; mode=display">\begin{aligned}J_{ki}=\frac{\partial{y_k}}{\partial{x_i}}&=\sum_{j}\frac{\partial{y_k}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{x_i}}\\&=\sum_{j}w_{ji}\frac{\partial{y_k}}{\partial{a_j}}\end{aligned}\tag{5.52}</script><p>其中求和式作⽤于所有单元 $i$ 发送链接的单元 $j$ 上 。现在⼀个递归的反向传播公式来确定导数 $\frac{\partial{y_k}}{\partial{a_j}}$ 。</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial{y_k}}{\partial{a_j}}&=\sum_{l}\frac{\partial{y_k}}{\partial{a_l}}\frac{\partial{a_l}}{\partial{a_j}}\\&=h^{\prime}(a_j)\sum_{j}w_{lj}\frac{\partial{y_k}}{\partial{a_l}}\end{aligned}</script><p>其中求和的对象为所有单元 $j$ 发送链接的单元 $l$（对应于 $w_{lj}$ 的第⼀个下标）。如果对于每个输出单元，都有各⾃的<code>sigmoid</code>函数，那么</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{a_l}}=\delta_{kl}\sigma^{\prime}(a_l)\tag{5.53}</script><p>对于<code>softmax</code>输出，有</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{a_l}}=\delta_{kl}y_{k}-y_{k}y_{l}\tag{5.54}</script><p>计算<code>Jacobian</code>矩阵的⽅法总结：将输⼊空间中要寻找<code>Jacobian</code>矩阵的点映射成⼀个输⼊向量，将这个输⼊向量作为⽹络的输⼊，使⽤通常的正向传播⽅法，得到⽹络的所有隐含单元和输出单元的激活。然后，对于<code>Jacobian</code>矩阵的每⼀⾏ $k$ （对应于输出单元 $k$ ），使⽤递归关系进⾏反向传播。对于⽹络中所有的隐含结点，反向传播开始于公式(5.53)和公式(5.54)。 最后， 使⽤公式(5.52)进⾏对输⼊单元的反向传播。</p><p><code>Jacobian</code>矩阵的另⼀种计算⽅法是正向传播算法，它可以使⽤与这⾥给出的反向传播算法相类似的⽅式推导出来。这个算法的执⾏可以通过下⾯的数值导数的⽅法检验正确性。</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{x_i}}=\frac{y_{k}(x_i+\epsilon)-y_k(x_i-\epsilon)}{2\epsilon}+O(\epsilon^{2})\tag{5.55}</script><p>对于⼀个有着 $D$ 个输⼊的⽹络来说，这种⽅法需要 $2D$ 次正向传播。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，前馈神经网络&quot;&gt;&lt;a href=&quot;#一，前馈神经网络&quot; class=&quot;headerlink&quot; title=&quot;一，前馈神经网络&quot;&gt;&lt;/a&gt;一，前馈神经
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】概率判别式模型</title>
    <link href="https://zhangbc.github.io/2019/10/10/prml_04_03/"/>
    <id>https://zhangbc.github.io/2019/10/10/prml_04_03/</id>
    <published>2019-10-10T15:29:58.000Z</published>
    <updated>2019-10-11T00:57:50.071Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，概率判别式模型"><a href="#一，概率判别式模型" class="headerlink" title="一，概率判别式模型"></a>一，概率判别式模型</h1><p>考察⼆分类问题，对于⼀⼤类的类条件概率密度 $p(\boldsymbol{x}|\mathcal{C}_k)$ 的选择， 类别 $\mathcal{C}_1$ 后验概率分布可以写成作⽤于 $\boldsymbol{x}$ 的线性函数上的<code>logistic sigmoid</code>函数的形式。类似地，对于多分类的情形，类别 $\mathcal{C}_k$ 的后验概率由 $\boldsymbol{x}$ 的线性函数的<code>softmax</code>变换给出。对于类条件概率密度 $p(\boldsymbol{x}|\mathcal{C}_k)$ 的具体的选择， 我们已经使⽤了最⼤似然⽅法估计了概率密度的参数以及类别先验 $p(\mathcal{C}_k)$ ，然后使⽤贝叶斯定理就可以求出后验类概率。<br>寻找⼀般的线性模型参数的间接⽅法是，分别寻找类条件概率密度和类别先验，然后使⽤贝叶斯定理。</p><h2 id="1，固定基函数"><a href="#1，固定基函数" class="headerlink" title="1，固定基函数"></a>1，固定基函数</h2><p>考虑直接对输⼊向量 $(x)$ 进⾏分类的分类模型，然⽽，如果⾸先使⽤⼀个基函数向量 $\boldsymbol{\phi}(\boldsymbol{x})$ 对输⼊变量进⾏⼀个固定的⾮线性变换，所有的这些算法仍然同样适⽤，最终的决策边界在特征空间 $\boldsymbol{\phi}$ 中是线性的，因此对应于原始 $\boldsymbol{x}$ 空间中的⾮线性决策边界。在特征空间 $\boldsymbol{\phi}(\boldsymbol{x})$ 线性可分的类别未必在原始的观测空间 $\boldsymbol{x}$ 中线性可分，基函数中的某⼀个通常设置为常数，例如 $\phi_{0}(\boldsymbol{x})=1$ ，使得对应的参数 $w_0$ 扮演偏置的作⽤。</p><h2 id="2，logistic回归"><a href="#2，logistic回归" class="headerlink" title="2，logistic回归"></a>2，<code>logistic</code>回归</h2><p>考虑⼆分类问题在⼀般的假设条件下，类别 $\mathcal{C}_1$ 的后验概率可以写成作⽤在特征向量 $\boldsymbol{\phi}$ 的线性函数上的<code>logistic sigmoid</code>函数的形式，即</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{\phi})=y(\boldsymbol{\phi})=\sigma(\boldsymbol{w}^T\boldsymbol{\phi})\tag{4.55}</script><p>且 $p(\mathcal{C}_2|\boldsymbol{\phi})=1-p(\mathcal{C}_1|\boldsymbol{\phi})$ ， $\sigma(·)$ 是<code>logistic sigmoid</code>函数。使⽤统计学的术语，这个模型被称为 <strong><code>logistic</code>回归</strong> ，特别注意，这是⼀个分类模型⽽不是回归模型。对于⼀个 $M$ 维特征空间 $\boldsymbol{\phi}$ ，这个模型有 $M$ 个可调节参数。</p><p>现在使⽤最⼤似然⽅法来确定<code>logistic</code>回归模型的参数。使⽤<code>logistic sigmoid</code>函数的导数</p><script type="math/tex; mode=display">\frac{\mathrm{d}\sigma}{\mathrm{d}a}=\sigma(1-\sigma)\tag{4.56}</script><p>对于⼀个数据集 $\boldsymbol{\phi}_n$ ,  $t_n$ ，其中 $t_n\in\{0,1\}$ 且  $\boldsymbol{\phi}_n=\boldsymbol{\phi}(\boldsymbol{x}_n)$ ，并且 $n=1,\dots,N$，似然函数可以写成</p><script type="math/tex; mode=display">p(\mathbf{t}|\boldsymbol{w})=\prod_{n=1}^{N}y_{n}^{t_n}\{1-y_n\}^{1-t_n}\tag{4.57}</script><p>其中 $\mathbf{t} = (t_1,\dots,t_N)^T$ 且 $y_n=p(\mathcal{C}_1|\boldsymbol{\phi}_n)$ 。通过取似然函数的负对数的⽅式，定义⼀个误差函数，这种⽅式产⽣了<strong>交叉熵</strong>（<code>cross-entropy</code>）误差函数，形式为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\ln p(\mathbf{t}|\boldsymbol{w}) = -\sum_{n=1}^{N}\{t_n\ln y_{n}+(1-t_n)\ln(1-y_n)\}\tag{4.58}</script><p>其中 $y_n=\sigma(a_n)$ 且 $a_n=\boldsymbol{w}^{T}\boldsymbol{\phi}_n$ 。两侧关于 $\boldsymbol{w}$ 取误差函数的梯度，有</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})= -\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n\tag{4.59}</script><h2 id="3，迭代重加权最⼩平⽅"><a href="#3，迭代重加权最⼩平⽅" class="headerlink" title="3，迭代重加权最⼩平⽅"></a>3，迭代重加权最⼩平⽅</h2><p>误差函数可以通过⼀种⾼效的迭代⽅法求出最⼩值，这种迭代⽅法基于<code>Newton-Raphson</code>迭代最优化框架， 使⽤了对数似然函数的局部⼆次近似。为了最⼩化函数 $E(\boldsymbol{w})$ ，<code>Newton-Raphson</code>对权值的更新形式为（<code>Fletcher</code>, 1987; <code>Bishop and Nabney</code>, 2008）</p><script type="math/tex; mode=display">\boldsymbol{w}^{新}=\boldsymbol{w}^{旧}-\boldsymbol{H}^{-1}\nabla E(\boldsymbol{w})\tag{4.60}</script><p>其中 $\boldsymbol{H}$ 是⼀个 <strong><code>Hessian</code>矩阵</strong>，它的元素由 $E(\boldsymbol{w})$ 关于 $\boldsymbol{w}$ 的⼆阶导数组成。</p><p>⾸先，把<code>Newton-Raphson</code>⽅法应⽤到线性回归模型上，误差函数为平⽅和误差函数。这个误差函数的梯度和<code>Hessian</code>矩阵为</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}(\boldsymbol{w}^{T}\boldsymbol{\phi}_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\boldsymbol{w}-\boldsymbol{\Phi}^{T}\mathbf{t}\tag{4.61}</script><script type="math/tex; mode=display">\boldsymbol{H}=\nabla\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}=\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\tag{4.62}</script><p>其中 $\boldsymbol{\Phi}$ 是 $N \times M$ 矩阵，第 $n$ ⾏为 $\boldsymbol{\phi}_{n}^{T}$ 。于是，<code>Newton-Raphson</code>更新形式为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w}^{新}&=\boldsymbol{w}^{旧}-(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\{\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\boldsymbol{w}^{旧}-\boldsymbol{\Phi}^{T}\mathbf{t}\}\\&=(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\mathbf{t}\end{aligned}\tag{4.63}</script><p>这是标准的最⼩平⽅解。</p><p>现在，把<code>Newton-Raphson</code>更新应⽤到<code>logistic</code>回归模型的交叉熵误差函数上。这个误差函数的梯度和<code>Hessian</code>矩阵为</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}^{T}(\mathbf{y}-\mathbf{t})\tag{4.64}</script><script type="math/tex; mode=display">\boldsymbol{H}=\nabla\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}=\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi}\tag{4.65}</script><p>其中，引⼊了⼀个 $N \times N$ 的对⾓矩阵 $\boldsymbol{R}$ ，元素为</p><script type="math/tex; mode=display">R_{nn}=y_n(1-y_n)\tag{4.66}</script><p>由此可见，<code>Hessian</code>矩阵不再是常量，⽽是通过权矩阵 $\boldsymbol{R}$ 依赖于 $\boldsymbol{w}$ 。对于任意向量 $\boldsymbol{\mu}$ 都有 $\boldsymbol{\mu}^{T}\boldsymbol{H}\boldsymbol{\mu}&gt;0$ ， 因此<code>Hessian</code>矩阵 $\boldsymbol{H}$ 是正定的，误差函数是 $\boldsymbol{w}$ 的⼀个凸函数， 从 ⽽有唯⼀的最⼩值。</p><p><code>logistic</code>回归模型的<code>Newton-Raphson</code>更新公式就变成了</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w}^{新}&=\boldsymbol{w}^{旧}-(\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}(\mathbf{y}-\mathbf{t})\\&=(\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi})^{-1}\{\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi}\boldsymbol{w}^{旧}-\boldsymbol{\Phi}^{T}(\mathbf{y}-\mathbf{t})\}\\&=(\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\boldsymbol{R}\mathbf{z}\end{aligned}\tag{4.67}</script><p>其中 $\mathbf{z}$ 是⼀个 $N$ 维向量，元素为</p><script type="math/tex; mode=display">\mathbf{z}=\boldsymbol{\Phi}\boldsymbol{w}^{旧}-\boldsymbol{R}^{-1}(\mathbf{y}-\mathbf{t})</script><p>更新公式(4.67)的形式为⼀组加权最⼩平⽅问题的规范⽅程，由于权矩阵 $\boldsymbol{R}$ 不是常量，⽽是依赖于参数向量 $\boldsymbol{w}$ ， 因此必须迭代地应⽤规范⽅程， 每次使⽤新的权向量 $\boldsymbol{w}$ 计算⼀个修正的权矩阵 $\boldsymbol{R}$ ，这个算法被称为<strong>迭代重加权最⼩平⽅</strong>（<code>iterative reweighted least squares</code>）， 或者简称为 <strong><code>IRLS</code></strong>（<code>Rubin</code>, 1983）。</p><p>对角矩阵 $\boldsymbol{R}$ 可以看成⽅差，因为<code>logistic</code>回归模型的 $t$ 的均值和⽅差为</p><script type="math/tex; mode=display">\mathbb{E}[t]=\sigma(\boldsymbol{x})=y\tag{4.68}</script><script type="math/tex; mode=display">\text{var}[t]=\mathbb{E}[t^2]-\mathbb{E}[t]^2=\sigma(\boldsymbol{x})-\sigma(\boldsymbol{x})^2=y(1-y)\tag{4.69}</script><p>事实上， 可以把 <strong><code>IRLS</code></strong> 看成变量空间 $a=\boldsymbol{w}^{T}\boldsymbol{\phi}$ 的线性问题的解。这样，$\mathbf{z}$ 的第 $n$ 个元素 $z_n$ 就可以简单地看成这个空间中的有效的⽬标值。$z_n$ 可以通过对当前操作点 $\boldsymbol{w}^{旧}$ 附近的<code>logistic sigmoid</code>函数的局部线性近似的⽅式得到。</p><script type="math/tex; mode=display">\begin{aligned}a_n(\boldsymbol{w}) &\simeq a_n(\boldsymbol{w}^{旧})+\frac{\mathrm{d}a_n}{\mathrm{d}y_n}\Bigg{|}_{\boldsymbol{w}^{旧}}(t_n-y_n)\\&=\boldsymbol{\phi}_{n}^{T}\boldsymbol{w}^{旧}-\frac{y_n-t_n}{y_n(1-y_n)}=z_n\end{aligned}\tag{4.70}</script><h2 id="4，多类logistic回归"><a href="#4，多类logistic回归" class="headerlink" title="4，多类logistic回归"></a>4，多类<code>logistic</code>回归</h2><p>对于⼀⼤类概率分布来说，后验概率由特征变量的线性函数的<code>softmax</code>变换给出，即</p><script type="math/tex; mode=display">p(\mathcal{C}_k|\boldsymbol{\phi})=y_k(\boldsymbol{\phi})=\frac{\exp(a_k)}{\sum_{j}\exp(a_j)}\tag{4.71}</script><p>其中，“激活” $a_k$ 为</p><script type="math/tex; mode=display">a_k=\boldsymbol{w}_{k}^{T}\boldsymbol{\phi}</script><p>$y_k$ 关于所有激活 $a_j$ 的导数为</p><script type="math/tex; mode=display">\frac{\partial y_k}{\partial a_j}=y_k(\boldsymbol{I}_{kj}-y_j)\tag{4.72}</script><p>其中 $\boldsymbol{I}_{kj}$ 为单位矩阵的元素。</p><p>使⽤“<code>1-of-K</code>”表达⽅式计算似然函数。这种表达⽅式中，属于类别 $\mathcal{C}_k$ 的特征向量 $\boldsymbol{\phi}_k$ 的⽬标向量 $\mathbf{t}_n$ 是⼀个⼆元向量，这个向量的第 $k$ 个元素等于1，其余元素都等于0。从⽽，似然函数为</p><script type="math/tex; mode=display">p(\boldsymbol{T}|\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=\prod_{n=1}^{N}\prod_{k=1}^{K}p(\mathcal{C}_k|\boldsymbol{\phi}_n)^{t_{nk}}=\prod_{n=1}^{N}\prod_{k=1}^{K}y_{nk}^{t_{nk}}\tag{4.73}</script><p>其中 $y_{nk}=y_k(\boldsymbol{\phi}_n)$ ，$\boldsymbol{T}$ 是⽬标变量的⼀个 $N \times K$ 的矩阵，元素为 $t_nk$ 。取负对数，可得</p><script type="math/tex; mode=display">E(\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=-\ln p(\boldsymbol{T}|\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln y_{nk}\tag{4.74}</script><p>被称为<strong>多分类问题的交叉熵</strong>（<code>cross-entropy</code>）<strong>误差函数</strong>。</p><p>现在取误差函数关于参数向量 $\boldsymbol{w}_j$ 的梯度。利⽤<code>softmax</code>函数的导数，有</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}_j} E(\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=\sum_{n=1}^{N}(y_{nj}-t_{nj})\boldsymbol{\phi}_n\tag{4.75}</script><p>其中, $\sum_{k}t_{nk}=1$ 。</p><p>为了找到⼀个批处理算法，再次使⽤<code>Newton-Raphson</code>更新来获得多类问题的对应的<code>IRLS</code>算法。这需要求出由⼤⼩为 $M \times M$ 的块组成的<code>Hessian</code>矩阵，其中块 $i, j$ 为</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}_k}\nabla_{\boldsymbol{w}_j} E(\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=\sum_{n=1}^{N}y_{nk}(\boldsymbol{I}_{kj}-y_{nj})\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}\tag{4.76}</script><p>多类<code>logistic</code>回归模型的<code>Hessian</code>矩阵是正定的，因此误差函数有唯⼀的最⼩值。</p><h2 id="5，probit回归"><a href="#5，probit回归" class="headerlink" title="5，probit回归"></a>5，<code>probit</code>回归</h2><p>考察⼆分类的情形，使⽤⼀般的线性模型的框架，即</p><script type="math/tex; mode=display">p(t=1|a)=f(a)\tag{4.77}</script><p>其中 $a=\boldsymbol{w}^{T}\boldsymbol{\phi}$ ，且 $f(·)$ 为激活函数。<br>对于每个输 ⼊ $\boldsymbol{\phi}_n$ ，我们计算 $a_n=\boldsymbol{w}^{T}\boldsymbol{\phi}_n$ ，然后按照下⾯的⽅式设置⽬标值</p><script type="math/tex; mode=display">\begin{cases}t_n=1, & 如果 a_n\ge \theta \\ t_n=0, & 其他情况\end{cases}</script><p>如果 $\theta$ 的值从概率密度 $p(\theta)$ 中抽取，那么对应的激活函数由累积分布函数给出</p><script type="math/tex; mode=display">f(a)=\int_{-\infty}^{a}p(\theta)\mathrm{d}\theta\tag{4.78}</script><p>假设概率密度 $p(\theta)$ 是零均值、单位⽅差的⾼斯概率密度，对应的累积分布函数为</p><script type="math/tex; mode=display">\boldsymbol{\Phi}(a)=\int_{-\infty}^{a}\mathcal{N}(\theta|0,1)\mathrm{d}\theta\tag{4.79}</script><p>这被称为 <strong>逆<code>probit</code>（<code>inverse probit</code>）函数</strong>。</p><p>如图4.17，概率分布 $p(\theta)$ 的图形表⽰，这个概率分布⽤蓝⾊曲线标记出。这个分布由两个⾼斯分布混合⽽成，同时给出的还有它的累积密度函数 $f(a)$ ，⽤红⾊曲线表⽰。注意，蓝⾊曲线上的任意⼀点，例如垂直绿⾊直线标记出的点，对应于红⾊曲线在相同⼀点处的斜率。相反，红⾊曲线在这点上的值对应于蓝⾊曲线下⽅的绿⾊阴影的⾯积。</p><p><img src="/images/prml_20191010130423.png" alt="概率分布"></p><p><strong><code>erf</code>函数</strong>，或者被称为 <strong><code>error</code>函数</strong></p><script type="math/tex; mode=display">\text{erf}(a)=\frac{2}{\sqrt{\pi}}\int_{0}^{a}\exp(-\theta^{2})\mathrm{d}\theta\tag{4.80}</script><p>与逆<code>probit</code>函数的关系为</p><script type="math/tex; mode=display">\boldsymbol{\Phi}(a)=\frac{1}{2}\left\{1+\text{erf}\left(\frac{a}{\sqrt{2}}\right)\right\}\tag{4.81}</script><p>基于<code>probit</code>激活函数的⼀般的线性模型被称为 <strong><code>probit</code>回归</strong> 。<br>在实际应⽤中经常出现的⼀个问题是<strong>离群点</strong>，它可能由输⼊向量 $\boldsymbol{x}$ 的测量误差产⽣，或者由⽬标值 $t$ 的错误标记产⽣。 由于这些点可以位于错误的⼀侧中距离理想决策边界相当远的位置上，因此他们会严重地⼲扰分类器。注意，在这⼀点上，<code>logistic</code>回归模型与<code>probit</code>回归模型的表现不同， 因为对于 $x \to \infty$ ，<code>logistic sigmoid</code>函数像 $\exp(−x)$ 那样渐进地衰减， ⽽<code>probit</code>激活函数像 $\exp(−x^2)$ 那样衰减，因此<code>probit</code>模型对于离群点会更加敏感。</p><p>引⼊⼀个概率 $\epsilon$ ，它是⽬标值 $t$ 被翻转到错误值的概率（<code>Opper and Winther</code>, 2000a）。这时，数据点 $\boldsymbol{x}$ 的⽬标值的分布为</p><script type="math/tex; mode=display">\begin{aligned}p(t|\boldsymbol{x})&=(1-\epsilon)\sigma(\boldsymbol{x})+\epsilon(1-\sigma(\boldsymbol{x}))\\&=\epsilon+(1-2\epsilon)\sigma(\boldsymbol{x})\end{aligned}\tag{4.82}</script><p>其中 $\sigma(\boldsymbol{x})$ 是输⼊向量 $\boldsymbol{x}$ 的激活函数。这⾥， $\epsilon$ 可以事先设定，也可以被当成超参数，然后从数据中推断它的值。</p><h2 id="6，标准链接函数"><a href="#6，标准链接函数" class="headerlink" title="6，标准链接函数"></a>6，标准链接函数</h2><p>把指数族分布的假设应⽤于⽬标变量 $t$ ，⽽不是应⽤于输⼊向量 $\boldsymbol{x}$ 。考虑⽬标变量的条件分布</p><script type="math/tex; mode=display">p(t|\eta,s)=\frac{1}{s}h(\frac{t}{s})g(\eta)\exp\left\{\frac{\eta t}{s}\right\}\tag{4.83}</script><p>$t$ 的条件均值（记作 $y$ ）为</p><script type="math/tex; mode=display">y\equiv \mathbb{E}[t|\eta]=-s\frac{\mathrm{d}}{\mathrm{d}\eta}\ln g(\eta)\tag{4.84}</script><p>因此 $y$ 和 $\eta$ ⼀定相关，记作 $\eta=\psi(y)$ 。</p><p>按照<code>Nelder and Wedderburn</code>（1972）的⽅法，我们将⼀般线性模型（<code>generalised linear model</code>）定义为这样的模型：$y$ 是输⼊变量（或者特征变量）的线性组合的⾮线性函数，即</p><script type="math/tex; mode=display">y=f(\boldsymbol{w}^{T}\boldsymbol{\phi})\tag{4.85}</script><p>其中 $f(·)$ 在机器学习的⽂献中被称为<strong>激活函数</strong>（<code>activation function</code>），$f^{-1}(·)$ 在统计学中被称为<strong>链接函数</strong>（<code>link function</code>）。</p><p>现在考虑这个模型的对数似然函数。它是 $\eta$ 的⼀个函数，形式为</p><script type="math/tex; mode=display">\ln p(\mathbf{t}|\eta,s)=\sum_{n=1}^{N}\ln p(t_n|\eta,s)=\sum_{n=1}^{N}\left\{\ln g(\eta_n)+\frac{\eta_n t_n}{s}\right\}+常数\tag{4.86}</script><p>其中假定所有的观测有⼀个相同的缩放参数（它对应着例如服从⾼斯分布的噪声的⽅差），因此 $s$ 与 $n$ ⽆关。对数似然函数关于模型参数 $\boldsymbol{w}$ 的导数为</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{w}}&=\sum_{n=1}^{N}\left\{\frac{\mathrm{d}}{\mathrm{d}\eta}\ln g(\eta_n)+\frac{t_n}{s}\right\}\frac{\mathrm{d}\eta_n}{\mathrm{d}y_n}\frac{\mathrm{d}y_n}{\mathrm{d}a_n}\nabla a_n\\&=\sum_{n=1}^{N}\frac{1}{s}\left\{t_n-y_n\right\}\psi^{\prime}(y_n)f^{\prime}(a_n)\boldsymbol{\phi}_n\end{aligned}\tag{4.87}</script><p>其中 $a_n=\boldsymbol{w}^{T}\boldsymbol{\phi}_n$ 。</p><p>令</p><script type="math/tex; mode=display">f^{-1}(y)=\psi(y)</script><p>则误差函数的梯度可以化简为</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=\frac{1}{s}\sum_{n=1}^{N}\left\{t_n-y_n\right\}\boldsymbol{\phi}_n\tag{4.88}</script><p>对于⾼斯分布，$s = \beta^ {-1}$ ，⽽对于<code>logistic</code>模型，$s=1$ 。</p><h1 id="二，拉普拉斯近似"><a href="#二，拉普拉斯近似" class="headerlink" title="二，拉普拉斯近似"></a>二，拉普拉斯近似</h1><h2 id="1，拉普拉斯近似"><a href="#1，拉普拉斯近似" class="headerlink" title="1，拉普拉斯近似"></a>1，拉普拉斯近似</h2><p><strong>拉普拉斯近似</strong>的<strong>⽬标</strong>是找到定义在⼀组连续变量上的概率密度的⾼斯近似。⾸先考虑单⼀连续变量 $z$ 的情形，假设分布 $p(z)$ 的定义为</p><script type="math/tex; mode=display">p(z)=\frac{1}{Z}f(z)\tag{4.89}</script><p>其中 $Z =\int f(z)\mathrm{d}z$ 是归⼀化系数。假定 $Z$ 的值是未知的，在拉普拉斯⽅法中，⽬标是寻找⼀个⾼斯近似 $q(z)$ ， 它的中⼼位于 $p(z)$ 的众数的位置。<br>第⼀步是寻找 $p(z)$ 的众数， 即寻找⼀个点 $z_0$ 使得 $p^{\prime}(z_0)=0$ ，或者等价地</p><script type="math/tex; mode=display">\frac{\mathrm{d}f(z)}{\mathrm{d}z}\bigg{|}_{z=z_0}=0</script><p>⾼斯分布有⼀个<strong>性质</strong>：它的对数是变量的⼆次函数。于是考虑 $\ln f(z)$ 以众数 $z_0$ 为中⼼的泰勒展开，即</p><script type="math/tex; mode=display">\ln f(z)\simeq \ln f(z_0)-\frac{1}{2}A(z-z_0)^2\tag{4.90}</script><p>其中，</p><script type="math/tex; mode=display">A=-\frac{\mathrm{d}^2}{\mathrm{d}z^2}\ln f(z)\bigg{|}_{z=z_0}</script><p>注意，泰勒展开式中的⼀阶项没有出现，因为 $z_0$ 是概率分布的局部最⼤值。两侧同时取指数， 有</p><script type="math/tex; mode=display">f(z)\simeq f(z_0)\exp\left\{-\frac{A}{2}(z-z_0)^2\right\}</script><p>这样，使⽤归⼀化的⾼斯分布的标准形式，就可以得到归⼀化的概率分布 $q(z)$ ，即</p><script type="math/tex; mode=display">q(z)=\left(\frac{A}{2\pi}\right)^{\frac{1}{2}}\exp\left\{-\frac{A}{2}(z-z_0)^2\right\}\tag{4.91}</script><p><strong>举例</strong>：应⽤于概率分布 $p(z) \propto \exp(−\frac{z^2}{2})\sigma(20z+4)$ 的拉普拉斯近似，其中 $\sigma(z)$ 是<code>logistic sigmoid</code>函数，定义为 $\sigma(z) = (1 + \exp^{−z})^{-1}$ 。<br>如图4.18，归⼀化的概率分布 $p(z)$，⽤黄⾊表⽰。同时给出了以 $p(z)$ 的众数 $z_0$ 为中⼼的拉普拉斯近似，⽤红⾊表⽰。</p><p><img src="/images/prml_20191010135922.png" alt="拉普拉斯近似"></p><p>如图4.19，图4.18中对应的曲线的负对数。</p><p><img src="/images/prml_20191010135947.png" alt="曲线的负对数"></p><p>将拉普拉斯⽅法推⼴，去近似定义在 $M$ 维空间 $\boldsymbol{z}$ 上的概率分布 $p(\boldsymbol{z}) =\frac{f(\boldsymbol{z})}{Z}$ 。在驻点 $\boldsymbol{z}_0$ 处，梯度 $\nabla f(\boldsymbol{z})$ 将会消失。在驻点处展开，有</p><script type="math/tex; mode=display">\ln f(\boldsymbol{z})\simeq \ln f(\boldsymbol{z}_0)-\frac{1}{2}(\boldsymbol{z}-\boldsymbol{z}_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\tag{4.92}</script><p>其中 $M \times M$ 的<code>Hessian</code>矩阵 $\boldsymbol{A}$ 的定义为</p><script type="math/tex; mode=display">\boldsymbol{A}=-\nabla\nabla\ln f(\boldsymbol{z})|_{\boldsymbol{z}=\boldsymbol{z}_0}</script><p>其中 $\nabla$ 为梯度算⼦。两边同时取指数，有</p><script type="math/tex; mode=display">f(\boldsymbol{z})\simeq f(\boldsymbol{z}_0)\exp\left\{-\frac{1}{2}(z-z_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\right\}</script><p>分布 $q(\boldsymbol{z})$ 正⽐于 $f(\boldsymbol{z})$ ，归⼀化系数可以通过观察归⼀化的多元⾼斯分布的标准形式得到。因此</p><script type="math/tex; mode=display">q(\boldsymbol{z})=\frac{|\boldsymbol{A}|^{\frac{1}{2}}}{(2\pi)^{\frac{M}{2}}}\exp\left\{-\frac{1}{2}(z-z_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\right\}=\mathcal{N(\boldsymbol{z}|\boldsymbol{z}_0,\boldsymbol{A}^{-1})}\tag{4.93}</script><p>其中 $|\boldsymbol{A}|$ 是 $\boldsymbol{A}$ 的⾏列式。这个⾼斯分布有良好定义的前提是，精度矩阵 $\boldsymbol{A}$ 是正定的， 表明驻点 $\boldsymbol{z}_0$ ⼀定是⼀个局部最⼤值，⽽不是⼀个最⼩值或者鞍点。</p><p>拉普拉斯近似的⼀个<strong>主要缺点</strong>是，由于它是以⾼斯分布为基础的，因此它只能直接应⽤于实值变量。<br>拉普拉斯框架的最严重的<strong>局限性</strong>是，它完全依赖于真实概率分布在变量的某个具体值位置上的性质，因此会⽆法描述⼀些重要的全局属性。</p><h2 id="2，模型⽐较和-BIC"><a href="#2，模型⽐较和-BIC" class="headerlink" title="2，模型⽐较和 BIC"></a>2，模型⽐较和 <strong><code>BIC</code></strong></h2><p>除了近似概率分布 $p(\boldsymbol{z})$ ，也可以获得对归⼀化常数 $Z$ 的⼀个近似，有</p><script type="math/tex; mode=display">\begin{aligned}Z&=\int f(\boldsymbol{z})\mathrm{d}\boldsymbol{z}\\&\simeq f(\boldsymbol{z}_0)\int\exp\left\{-\frac{1}{2}(z-z_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\right\}\mathrm{d}\boldsymbol{z}\\&=f(\boldsymbol{z}_0)\frac{(2\pi)^{\frac{M}{2}}}{|\boldsymbol{A}|^{\frac{1}{2}}}\end{aligned}\tag{4.94}</script><p>考虑⼀个数据集 $\mathcal{D}$ 以及⼀组模型 $\{\mathcal{M}_i\}$ ， 模型参数为 $\{\boldsymbol{\theta}_i\}$ 。 对于每个模型， 定义⼀个似然函数 $p(\mathcal{D}|\boldsymbol{\theta}_i,\mathcal{M}_i)$ 。如果引⼊⼀个参数的先验概率 $p(\boldsymbol{\theta}_i|\mathcal{M}_i)$ ，那么感兴趣的是计算不同模型的模型证据 $p(\mathcal{D}|\mathcal{M}_i)$ 。为了简化记号，省略对于 $\{\mathcal{M}_i\}$ 的条件依赖。 根据贝叶斯定理，模型证据为</p><script type="math/tex; mode=display">p(\mathcal{D})=\int p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})\mathrm{d}\boldsymbol{\theta}\tag{4.95}</script><p>令 $f(\boldsymbol{\theta})=p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})$ 以及 $Z=p(\mathcal{D})$，然后使⽤公式(4.94)，有</p><script type="math/tex; mode=display">\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} | \boldsymbol{\theta}_{MAP}\right)+\underbrace{\ln p\left(\boldsymbol{\theta}_{MAP}\right)+\frac{M}{2} \ln (2 \pi)-\frac{1}{2} \ln |\boldsymbol{A}|}_{\text {Occam因子}}\tag{4.96}</script><p>其中 $\boldsymbol{\theta}_{MAP}$ 是在后验概率分布众数位置的 $\boldsymbol{\theta}$ 值，$\boldsymbol{A}$ 是负对数后验概率的⼆阶导数组成的<code>Hessian</code>矩阵。</p><script type="math/tex; mode=display"> \boldsymbol{A}=-\nabla\nabla\ln p(\mathcal{D}|\boldsymbol{\theta}_{MAP})p(\boldsymbol{\theta}_{MAP})=-\nabla\nabla\ln p(\boldsymbol{\theta}_{MAP}|\mathcal{D})</script><p>公式(4.96)表⽰使⽤最优参数计算的对数似然值，⽽余下的三项由“ <strong><code>Occam</code>因⼦</strong> ”组成， 它对模型的复杂度进⾏惩罚。</p><p>如果假设参数的⾼斯先验分布⽐较宽，且<code>Hessian</code>矩阵是<strong>满秩</strong>的， 那么可以使⽤下式来⾮常粗略地近似公式(4.96)</p><script type="math/tex; mode=display">\ln p(\mathcal{D}) \simeq \ln p(\mathcal{D} | \boldsymbol{\theta}_{MAP})-\frac{1}{2}M\ln N\tag{4.97}</script><p>其中 $N$ 是数据点的总数，$M$ 是 $\boldsymbol{\theta}$ 中参数的数量， 并且省略了⼀些额外的常数。 这被称为<strong>贝叶斯信息准则</strong>（<code>Bayesian Information Criterion</code>）（<strong><code>BIC</code></strong>）， 或者称为 <strong><code>Schwarz</code>准则</strong>（<code>Schwarz</code>, 1978）。</p><p>历史上各种各样的“信息准则”被提出来，这些“信息准则”尝试修正最⼤似然的偏差。修正的⽅法是增加⼀个惩罚项来补偿过于复杂的模型造成的过拟合。 例如，<strong>⾚池信息准则</strong>（<code>Akaike information criterion</code>），或者简称为 <strong><code>AIC</code></strong>（<code>Akaike</code>, 1974），选择下⾯使这个量最⼤的模型：</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\boldsymbol{w}_{ML})-M</script><p>其中，$p(\mathcal{D}|\boldsymbol{w}_{ML})$ 是最合适的对数似然函数，$M$ 是模型中可调节参数的数量。</p><p>与 <strong><code>AIC</code></strong> 相比，<strong><code>BIC</code></strong> 对模型复杂度的惩罚更严重。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，概率判别式模型&quot;&gt;&lt;a href=&quot;#一，概率判别式模型&quot; class=&quot;headerlink&quot; title=&quot;一，概率判别式模型&quot;&gt;&lt;/a&gt;一，概
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】概率生成式模型</title>
    <link href="https://zhangbc.github.io/2019/10/10/prml_04_02/"/>
    <id>https://zhangbc.github.io/2019/10/10/prml_04_02/</id>
    <published>2019-10-10T15:03:30.000Z</published>
    <updated>2019-10-11T01:08:23.340Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，概率生成式模型"><a href="#一，概率生成式模型" class="headerlink" title="一，概率生成式模型"></a>一，概率生成式模型</h1><p>⾸先考虑⼆分类的情形。类别 $\mathcal{C}_1$ 的后验概率可以写成</p><script type="math/tex; mode=display">\begin{aligned}p(\mathcal{C}_1|\boldsymbol{x})&=\frac{p(\boldsymbol{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\boldsymbol{x}|\mathcal{C}_1)p(\mathcal{C}_1)+p(\boldsymbol{x}|\mathcal{C}_2)p(\mathcal{C}_2)}\\&=\frac{1}{1+\exp(-a)}=\sigma(a)\end{aligned}\tag{4.36}</script><p>其中，</p><script type="math/tex; mode=display">a=\ln\frac{p(\boldsymbol{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\boldsymbol{x}|\mathcal{C}_2)p(\mathcal{C}_2)}</script><p>$\sigma(a)$ 称之为 <strong><code>logistic sigmoid</code>函数</strong> 。</p><p>如图4.12，<code>logistic sigmoid</code>函数 $\sigma(a)$ 的图像， ⽤红⾊表⽰，同时给出的是放缩后的逆<code>probit</code>函数 $\Phi(\lambda a)$ 的图像， 其中 $\lambda^2=\frac{\pi}{8}$ ， ⽤蓝⾊曲线表⽰。</p><p><img src="/images/prml_20191010091804.png" alt="logistic sigmoid函数"></p><p><strong><code>logistic sigmoid</code>函数</strong> 在许多分类算法中都有着重要的作⽤，满⾜下⾯的对称性</p><script type="math/tex; mode=display">\sigma(-a)=1-\sigma(a)\tag{4.37}</script><p><code>logistic sigmoid</code>的反函数为</p><script type="math/tex; mode=display">a=\ln\left(\frac{\sigma}{1-\sigma}\right)\tag{4.38}</script><p>被称为 <strong><code>logit</code>函数</strong>。它表⽰两类的概率⽐值的对数 $\ln[\frac{p(\mathcal{C}_1|\boldsymbol{x})}{p(\mathcal{C}_2|\boldsymbol{x})}]$ ，也被称为 <strong><code>log odds</code>函数</strong> 。</p><p>对于 $K &gt; 2$ 个类别的情形，有</p><script type="math/tex; mode=display">\begin{aligned}p(\mathcal{C}_k|\boldsymbol{x})&=\frac{p(\boldsymbol{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{\sum_{j}p(\boldsymbol{x}|\mathcal{C}_j)p(\mathcal{C}_j)}\\&=\frac{\exp(a_k)}{\sum_{j}\exp(a_j)}\end{aligned}\tag{4.39}</script><p>被称为<strong>归⼀化指数</strong>（<code>normalized exponential</code>），也叫 <strong><code>softmax</code>函数</strong> ，可以被当做<code>logistic sigmoid</code>函数对于多类情况的推⼴。其中， $a_k$ 被定义为</p><script type="math/tex; mode=display">a_k=\ln p(\boldsymbol{x}|\mathcal{C}_k)p(\mathcal{C}_k)</script><p>如果对于所有的 $j\ne k$ 都有 $a_k \gg a_j$ ，那么 $p(\mathcal{C}_k|\boldsymbol{x})\simeq 1$ 且 $p(\mathcal{C}_j|\boldsymbol{x}) \simeq 0$。</p><h2 id="1，连续输⼊"><a href="#1，连续输⼊" class="headerlink" title="1，连续输⼊"></a>1，连续输⼊</h2><p>假设类条件概率密度是⾼斯分布，然后求解后验概率的形式。假定所有的类别的协⽅差矩阵相同，这样类别 $\mathcal{C}_k$ 的类条件概率为</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\mathcal{C}_k)=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}}\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_k)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_k)\right\}\tag{4.40}</script><p>⾸先考虑两类的情形。根据公式(4.36)，有</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{x})=\sigma(\boldsymbol{w}^{T}\boldsymbol{x}+w_0)\tag{4.41}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{w}=\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)\\w_0=-\frac{1}{2}\boldsymbol{\mu}_{1}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{1}+\frac{1}{2}\boldsymbol{\mu}_{2}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{2}+\ln\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}</script><p>如图4.13，左图给出了两个类别的类条件概率密度，分别⽤红⾊和蓝⾊表⽰。 右图给出了对应的后验概率分布 $p(\mathcal{C}_1|\boldsymbol{x})$ ， 它由 $\boldsymbol{x}$ 的线性函数的 <code>logistic sigmoid</code> 函数给出。 右图的曲⾯的颜⾊中， 红⾊所占的⽐例由 $p(\mathcal{C}_1|\boldsymbol{x})$ 给出，蓝⾊所占的⽐例由 $p(\mathcal{C}_2|\boldsymbol{x})=1-p(\mathcal{C}_1|\boldsymbol{x})$ 给出。</p><p><img src="/images/prml_20191010092734.png" alt="⼆维输⼊空间x的情况"></p><p>对于 $K$ 个类别的⼀般情形，根据公式(4.39)，有</p><script type="math/tex; mode=display">a_k(\boldsymbol{x})=\boldsymbol{w}_{k}^{T}\boldsymbol{x}+w_{k0}\tag{4.42}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{w}_k=\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{k}\\w_{k0}=-\frac{1}{2}\boldsymbol{\mu}_{k}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{k}+\ln p(\mathcal{C}_k)</script><p>如图4.14，左图给出了三个类别的类条件概率密度，每个都是⾼斯分布，分别⽤红⾊、绿⾊、蓝⾊表⽰，其中红⾊和绿⾊的类别有相同的协⽅差矩阵。右图给出了对应的后验概率分布，其中 $RGB$ 的颜⾊向量表⽰三个类别各⾃的后验概率。决策边界也被画出。注意，具有相同协⽅差矩阵的红⾊类别和绿⾊类别的决策边界是线性的，⽽其他类别之间的类别的决策边界是⼆次的。</p><p><img src="/images/prml_20191010100117.png" alt="线性决策边界和⼆次决策边界"></p><h2 id="2，最⼤似然解"><a href="#2，最⼤似然解" class="headerlink" title="2，最⼤似然解"></a>2，最⼤似然解</h2><p>⾸先考虑两类的情形，每个类别都有⼀个⾼斯类条件概率密度，且协⽅差矩阵相同。假设有⼀个数据集 $\{\boldsymbol{x}_n, t_n\}$ ，其中 $n = 1,\dots,N$ 。这⾥ $t_n = 1$ 表⽰类别 $\mathcal{C}_1$ ，$t_n=0$ 表⽰类别 $\mathcal{C}_2$ 。 把先验概率记作 $p(\mathcal{C}_1)=\pi$ ， 从⽽ $p(\mathcal{C}_2)=1-\pi$ 。 对于⼀个来⾃类别 $\mathcal{C}_1$ 的数据点 $\boldsymbol{x}_n$ ， 有 $t_n = 1$ ，因此</p><script type="math/tex; mode=display">p(\boldsymbol{x}_n,\mathcal{C}_1)=p(\mathcal{C}_1)p(\boldsymbol{x}_n|\mathcal{C}_1)=\pi\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})</script><p>类似地，对于类别 $\mathcal{C}_2$ ，有 $t_n = 0$ ，因此</p><script type="math/tex; mode=display">p(\boldsymbol{x}_n,\mathcal{C}_2)=p(\mathcal{C}_2)p(\boldsymbol{x}_n|\mathcal{C}_2)=(1-\pi)\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_2,\boldsymbol{\Sigma})</script><p>于是似然函数为</p><script type="math/tex; mode=display">p(\mathbf{t},\mathbf{X}|\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=\prod_{n=1}^{N}[\pi\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})]^{t_n}[(1-\pi)\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_2,\boldsymbol{\Sigma})]^{1-t_n}\tag{4.43}</script><p>其中，$\mathbf{t}=(t_1,\dots,t_n)^{T}$ 。</p><p>⾸先考虑关于 $\pi$ 的最⼤化，对数似然函数中与 $\pi$ 相关的项为</p><script type="math/tex; mode=display">\sum_{n=1}^{N}\{t_n\ln\pi+(1-\pi)\ln(1-\pi)\}</script><p>令其关于 $\pi$ 的导数等于零，整理，可得</p><script type="math/tex; mode=display">\pi=\frac{1}{N}\sum_{n=1}^{N}t_n=\frac{N_1}{N}=\frac{N_1}{N_1+N_2}\tag{4.44}</script><p>其中 $N_1$ 表⽰类别 $\mathcal{C}_1$  的数据点的总数，⽽ $N_2$ 表⽰类别 $\mathcal{C}_2$ 的数据点总数。</p><p>现在考虑关于 $\mu_1$ 的最⼤化，把对数似然函数中与 $\mu_1$ 相关的量挑出来，即</p><script type="math/tex; mode=display">\sum_{n=1}^{N}t_n\ln\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})=-\frac{1}{2}\sum_{n=1}^{N}t_n(\boldsymbol{x}_n-\boldsymbol{\mu}_1)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_n-\boldsymbol{\mu}_1)+常数\tag{4.45}</script><p>令它关于 $\mu_1$ 的导数等于零，整理可得</p><script type="math/tex; mode=display">\boldsymbol{\mu}_1=\frac{1}{N_1}\sum_{n=1}^{N}t_n\boldsymbol{x}_n\tag{4.46}</script><p>通过类似的推导，对应的 $\mu_2$ 的结果为</p><script type="math/tex; mode=display">\boldsymbol{\mu}_2=\frac{1}{N_2}\sum_{n=1}^{N}(1-t_n)\boldsymbol{x}_n\tag{4.47}</script><p>最后，考虑协⽅差矩阵 $\boldsymbol{\Sigma}$ 的最⼤似然解。选出与 $\boldsymbol{\Sigma}$ 相关的项，有</p><script type="math/tex; mode=display">\begin{array}{lcl}-\frac{1}{2}\sum_{n=1}^{N}t_n\ln|\boldsymbol{\Sigma}|-\frac{1}{2}\sum_{n=1}^{N}t_n(\boldsymbol{x}_n-\boldsymbol{\mu}_1)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_n-\boldsymbol{\mu}_1) \\ -\frac{1}{2}\sum_{n=1}^{N}(1-t_n)\ln|\boldsymbol{\Sigma}|-\frac{1}{2}\sum_{n=1}^{N}(1-t_n)(\boldsymbol{x}_n-\boldsymbol{\mu}_2)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_n-\boldsymbol{\mu}_2) \\ =-\frac{1}{2}\ln|\boldsymbol{\Sigma}|-\frac{N}{2}\text{Tr}\{\boldsymbol{\Sigma}^{-1}\boldsymbol{S}\}\end{array}\tag{4.48}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}=\frac{N_1}{N}\boldsymbol{S}_1+\frac{N_2}{N}\boldsymbol{S}_2 \\ \boldsymbol{S}_1=\frac{1}{N_1}\sum_{n\in\mathcal{C}_1}(\boldsymbol{x}_n-\boldsymbol{\mu}_1)(\boldsymbol{x}_n-\boldsymbol{\mu}_1)^{T}\\\boldsymbol{S}_2=\frac{1}{N_2}\sum_{n\in\mathcal{C}_2}(\boldsymbol{x}_n-\boldsymbol{\mu}_2)(\boldsymbol{x}_n-\boldsymbol{\mu}_2)^{T}</script><p>使⽤⾼斯分布的最⼤似然解的标准结果，我们看到 $\boldsymbol{\Sigma} = \boldsymbol{S}$ ，它表⽰对⼀个与两类都有关系的协⽅差矩阵求加权平均。</p><h2 id="3，离散特征"><a href="#3，离散特征" class="headerlink" title="3，离散特征"></a>3，离散特征</h2><p>⾸先考察⼆元特征值 $x_i\in\{0, 1\}$ ，如果有 $D$ 个输⼊，那么⼀般的概率分布会对应于⼀个⼤⼩为 $2^D$ 的表格，包含 $2^D−1$ 个独⽴变量（由于要满⾜加和限制）。由于这会 随着特征的数量指数增长，因此我们想寻找⼀个更加严格的表⽰⽅法。这⾥，我们做出<strong>朴素贝叶斯</strong>（<code>naive Bayes</code>）的假设，这个假设中，特征值被看成相互独⽴的，以类别 $\mathcal{C}_k$ 为条件。因此得到类条件分布，形式为</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\mathcal{C}_k)=\prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i}\tag{4.49}</script><p>其中对于每个类别，都有 $D$ 个独⽴的参数，有</p><script type="math/tex; mode=display">a_k(\boldsymbol{x})=\sum_{i=1}^{D}\{x_i\ln\mu_{ki}+(1-x_i)\ln(1-\mu_{ki})\}+\ln p(\mathcal{C}_k)\tag{4.50}</script><p>这是输⼊变量 $x_i$ 的线性函数。</p><h2 id="4，指数族分布"><a href="#4，指数族分布" class="headerlink" title="4，指数族分布"></a>4，指数族分布</h2><p>使⽤指数族分布的形式，可以看到 $\boldsymbol{x}$ 的分布可以写成</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\boldsymbol{\lambda}_k)=h(\boldsymbol{x})g(\boldsymbol{\lambda}_k)\exp\{\boldsymbol{\lambda}_{k}^{T}\boldsymbol{\mu}(\boldsymbol{x})\}\tag{4.51}</script><p>现在把注意⼒集中在 $\boldsymbol{\mu}(\boldsymbol{x})=\boldsymbol{x}$ 这种分布上，引⼊⼀个缩放参数 $s$，这样就得到了指数族类条件概率分布的⼀个⼦集</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\boldsymbol{\lambda}_k,s)=\frac{1}{s}h(\frac{1}{s}\boldsymbol{x})g(\boldsymbol{\lambda}_k)\exp\left\{\frac{1}{s}\boldsymbol{\lambda}_{k}^{T}\boldsymbol{x}\right\}\tag{4.52}</script><p>注意让每个类别有⾃⼰的参数向量 $\boldsymbol{\lambda}_k$ ，并且假定各个类别有同样的缩放参数 $s$ 。</p><p>对于⼆分类问题，把这个类条件概率密度的表达式代⼊相关公式，计算后可得后验概率是⼀个作⽤在线性函数 $a(\boldsymbol{x})$ 上的<code>logistic sigmoid</code>函数。$a(\boldsymbol{x})$ 的形式为</p><script type="math/tex; mode=display">a(\boldsymbol{x})=\frac{1}{s}(\boldsymbol{\lambda}_1-\boldsymbol{\lambda}_2)^{T}\boldsymbol{x}+\ln g(\boldsymbol{\lambda}_1)-\ln g(\boldsymbol{\lambda}_2)+\ln p(\mathcal{C}_1)-\ln p(\mathcal{C}_2)\tag{4.53}</script><p>类似地，对于 $K$ 类问题，有</p><script type="math/tex; mode=display">a_k(\boldsymbol{x})=\frac{1}{s}(\boldsymbol{\lambda}_k)^{T}\boldsymbol{x}+\ln g(\boldsymbol{\lambda}_k)+\ln p(\mathcal{C}_k)\tag{4.54}</script><p>这又是⼀个 $\boldsymbol{x}$ 的线性函数。</p><p>如图4.15，线性分类模型的⾮线性基函数的作⽤的说明。下图给出了原始的输⼊空间 $(x_1,x_2)$ 以及标记为红⾊和蓝⾊的数据点。这个空间中定义了两个“⾼斯”基函数 $\phi_1(\boldsymbol{x})$ 和 $\phi_2(\boldsymbol{x})$ ，中⼼⽤绿⾊⼗字表⽰，轮廓线⽤绿⾊圆形表⽰。</p><p><img src="/images/prml_20191010105434.png" alt="特征空间"></p><p>如图4.16，对应图4.15中的特征空间 $(\phi_1,\phi_2)$ 以及线性决策边界。</p><p><img src="/images/prml_20191010105443.png" alt="线性决策边界"></p><h1 id="二，贝叶斯logistics回归"><a href="#二，贝叶斯logistics回归" class="headerlink" title="二，贝叶斯logistics回归"></a>二，贝叶斯<code>logistics</code>回归</h1><p>对于<code>logistic</code>回归，精确的贝叶斯推断是⽆法处理的。特别地，计算后验概率分布需要对先验概率分布于似然函数的乘积进⾏归⼀化，⽽似然函数本⾝由⼀系列<code>logistic sigmoid</code>函数的乘积组成，每个数据点都有⼀个<code>logistic sigmoid</code>函数，对于预测分布的计算类似地也是⽆法处理的。</p><h2 id="1，-拉普拉斯近似"><a href="#1，-拉普拉斯近似" class="headerlink" title="1， 拉普拉斯近似"></a>1， 拉普拉斯近似</h2><p>由于寻找后验概率分布的⼀个⾼斯表⽰，因此在开始的时候选择⾼斯先验是很⾃然的。故把⾼斯先验写成⼀般的形式</p><script type="math/tex; mode=display">p(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_0,\boldsymbol{S}_0)\tag{4.98}</script><p>其中 $\boldsymbol{m}_0$ 和  $\boldsymbol{S}_0$ 是固定的超参数。 $\boldsymbol{w}$  的后验概率分布为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\mathbf{t})\propto p(\boldsymbol{w})p(\mathbf{t}|\boldsymbol{w})</script><p>其中 $\mathbf{t} = (t_1,\dots, t_N)^{T}$ 。两侧取对数，然后代⼊先验分布，对于似然函数，有</p><script type="math/tex; mode=display">\begin{aligned}\ln p(\boldsymbol{w}|\mathbf{t}) &= -\frac{1}{2}(\boldsymbol{w}-\boldsymbol{m}_0)^{T}\boldsymbol{S}_{0}^{-1}(\boldsymbol{w}-\boldsymbol{m}_0)\\&+\sum_{n=1}^{N}\{t_n\ln y_{n}+(1-t_n)\ln(1-y_n)\}+常数\end{aligned}\tag{4.99}</script><p>其中 $y_n=\sigma(\boldsymbol{w}^{T}\boldsymbol{\phi}_n)$ 。为了获得后验概率的⾼斯近似，⾸先最⼤化后验概率分布，得到<code>MAP</code>（最⼤后验）解 $\boldsymbol{w}_{WAP}$ ，定义了⾼斯分布的均值，这样协⽅差就是负对数似然函数的⼆阶导数矩阵的逆矩阵，形式为</p><script type="math/tex; mode=display">\boldsymbol{S}_{N}^{-1}=-\nabla\nabla \ln p(\boldsymbol{w}|\mathbf{t})=\boldsymbol{S}_{0}^{-1}+\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}\tag{4.100}</script><p>后验概率分布的⾼斯近似的形式为</p><script type="math/tex; mode=display">q(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{w}_{MAP},\boldsymbol{S}_{N})\tag{4.101}</script><h2 id="2，-预测分布"><a href="#2，-预测分布" class="headerlink" title="2， 预测分布"></a>2， 预测分布</h2><p>给定⼀个新的特征向量 $\boldsymbol{\phi}(\boldsymbol{x})$ ，类别 $\mathcal{C}_1$ 的预测分布可以通过对后验概率 $p(\boldsymbol{w} | \mathbf{t})$ 积分，后验概率本⾝由⾼斯分布 $q(\boldsymbol{w})$ 近似，即</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{\phi},\mathbf{t})=\int p(\mathcal{C}_1|\boldsymbol{\phi},\boldsymbol{w})p(\boldsymbol{w}|\mathbf{t})\mathrm{d}\boldsymbol{w}\simeq \int \sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})q(\boldsymbol{w})\mathrm{d}\boldsymbol{w}\tag{4.102}</script><p>且类别 $\mathcal{C}_2$ 的对应的概率为 $p(\mathcal{C}_2|\boldsymbol{\phi},\mathbf{t})=1-p(\mathcal{C}_1|\boldsymbol{\phi},\mathbf{t})$ 。为了计算预测分布，⾸先注意到函数 $\sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})$ 对于 $\boldsymbol{w}$ 的依赖只通过它在 $\boldsymbol{\phi}$ 上的投影⽽实现。记 $a=\boldsymbol{w}^{T}\boldsymbol{\phi}$，有</p><script type="math/tex; mode=display">\sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})=\int \delta(a-\boldsymbol{w}^{T}\boldsymbol{\phi})\sigma(a)\mathrm{d}a</script><p>其中 $\delta(·)$ 是 <strong>狄拉克<code>Delta</code>函数</strong> 。由此有</p><script type="math/tex; mode=display">\int \sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})q(\boldsymbol{w})\mathrm{d}\boldsymbol{w}=\int\sigma(a)p(a)\mathrm{d}a\tag{4.103}</script><p>其中，</p><script type="math/tex; mode=display">p(a)=\int \delta(a-\boldsymbol{w}^{T}\boldsymbol{\phi})q(\boldsymbol{w})\mathrm{d}\boldsymbol{w}</script><p>计算 $p(a)$ ：注意到<code>Delta</code>函数给 $\boldsymbol{w}$ 施加了⼀个线性限制，因此在所有与 $\boldsymbol{\phi}$ 正交的⽅向上积分，就得到了联合概率分布 $q(\boldsymbol{w})$ 的边缘分布。<br>通过计算各阶矩然后交换 $a$ 和 $\boldsymbol{w}$ 的积分顺序的⽅式计算均值和协⽅差，即</p><script type="math/tex; mode=display">\mu_a=\mathbb{E}[a]=\int p(a)a\mathrm{d}a=\int q(\boldsymbol{w})\boldsymbol{w}^{T}\boldsymbol{\phi}\mathrm{d}\boldsymbol{w}=\boldsymbol{w}_{WAP}^{T}\boldsymbol{\phi}\tag{4.104}</script><script type="math/tex; mode=display">\begin{aligned}\sigma_{a}^{2}&=\text{var}[a]=\int p(a)\{a^2-\mathbb{E}[a]^{2}\}\mathrm{d}a\\&=\int q(\boldsymbol{w})\{(\boldsymbol{w}^{T}\boldsymbol{\phi})^{2}-(\boldsymbol{m}_{N}^{T}\boldsymbol{\phi})^{2}\}\mathrm{d}\boldsymbol{w}=\boldsymbol{\phi}^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}\end{aligned}\tag{4.105}</script><p>注意，$a$ 的分布的函数形式与线性回归模型的预测分布相同， 其中噪声⽅差被设置为零。因此对于预测分布的近似变成了</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\mathbf{t})=\int \sigma(a)p(a)\mathrm{d}a=\int \sigma(a)\mathcal{N}(a|\mu_{a},\sigma_{a}^{2})\mathrm{d}a\tag{4.106}</script><p>使⽤ <strong>逆<code>probit</code>函数</strong> 的⼀个<strong>优势</strong>是：它与⾼斯的卷积可以⽤另⼀个逆<code>probit</code>函数解析地表⽰出来。 特别地，可以证明</p><script type="math/tex; mode=display">\int \Phi(\lambda a)\mathcal{N}(a|\mu,\sigma^{2})\mathrm{d}a=\Phi\left(\frac{\mu}{(\lambda^{-2}+\sigma^{2})^{\frac{1}{2}}}\right)\tag{4.107}</script><p>现在将逆<code>probit</code>函数的近似 $\sigma(a)\simeq\Phi(\lambda a)$ 应⽤于这个⽅程的两侧， 得到下⾯的对于<code>logistic sigmoid</code>函数与⾼斯的卷积近似</p><script type="math/tex; mode=display">\int \sigma(a)\mathcal{N}(a|\mu,\sigma^{2})\mathrm{d}a\simeq\sigma\left(\kappa(\sigma^{2})\mu\right)\tag{4.108}</script><p>其中，</p><script type="math/tex; mode=display">\kappa(\sigma^{2})=\left(1+\frac{\pi\sigma^{2}}{8}\right)^{-\frac{1}{2}}</script><p>得到了近似的预测分布，形式为</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{\phi},\mathbf{t})=\sigma\left(\kappa(\sigma_{a}^{2})\mu_{a}\right)\tag{4.109}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，概率生成式模型&quot;&gt;&lt;a href=&quot;#一，概率生成式模型&quot; class=&quot;headerlink&quot; title=&quot;一，概率生成式模型&quot;&gt;&lt;/a&gt;一，概
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】判别函数</title>
    <link href="https://zhangbc.github.io/2019/10/09/prml_04_01/"/>
    <id>https://zhangbc.github.io/2019/10/09/prml_04_01/</id>
    <published>2019-10-09T14:36:47.000Z</published>
    <updated>2019-10-09T15:08:23.208Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，分类线性模型概述"><a href="#一，分类线性模型概述" class="headerlink" title="一，分类线性模型概述"></a>一，分类线性模型概述</h1><p>分类的⽬标是将输⼊变量 $\boldsymbol{x}$ 分到 $K$ 个离散的类别 $\mathcal{C}_k$ 中的某⼀类。 最常见的情况是， 类别互相不相交， 因此每个输⼊被分到唯⼀的⼀个类别中。因此输⼊空间被划分为不同的<strong>决策区域</strong>（<code>decision region</code>），它的边界被称为<strong>决策边界</strong>（<code>decision boundary</code>）或者<strong>决策⾯</strong>（<code>decision surface</code>）。</p><p><strong>分类线性模型</strong>是指决策⾯是输⼊向量 $\boldsymbol{x}$ 的线性函数，因此被定义为 $D$ 维输⼊空间中的 $(D − 1)$ 维超平⾯。如果数据集可以被线性决策⾯精确地分类，那么我们说这个数据集是<strong>线性可分</strong>的（<code>linearly separable</code>）。</p><p>在线性回归模型中，使⽤⾮线性函数 $f(·)$ 对 $\boldsymbol{w}$ 的线性函数进⾏变换，即</p><script type="math/tex; mode=display">y(\boldsymbol{x})=f(\boldsymbol{w}^{T}\boldsymbol{x}+w_0)\tag{4.1}</script><p>在机器学习的⽂献中，$f(·)$ 被称为<strong>激活函数</strong>（<code>activation function</code>），⽽它的反函数在统计学的⽂献中被称为<strong>链接函数</strong>（<code>link function</code>）。决策⾯对应于 $y(\boldsymbol{x}) = 常数$，即 $\boldsymbol{w}^{T}\boldsymbol{x} + w_0 = 常数$，因此决策⾯是 $\boldsymbol{x}$ 的线性函数，即使函数 $f(·)$ 是⾮线性函数也是如此。因此，由公式(4.1)描述的⼀类模型被称为<strong>推⼴的线性模型</strong>（<code>generalized linear model</code>）（<code>McCullagh and Nelder</code>, 1989）。</p><p>如图4.1，⼆维线性判别函数的⼏何表⽰。决策⾯（红⾊）垂直于 $\boldsymbol{w}$ ，它距离原点的偏移量由偏置参数 $w_0$ 控制。</p><p><img src="/images/prml_20191008233456.png" alt="⼆维线性判别函数的⼏何表⽰"></p><h1 id="二，判别函数"><a href="#二，判别函数" class="headerlink" title="二，判别函数"></a>二，判别函数</h1><p>判别函数是⼀个以向量 $\boldsymbol{x}$ 为输⼊，把它分配到 $K$ 个类别中的某⼀个类别（记作 $\mathcal{C}_k$ ）的函数。</p><h2 id="1，⼆分类"><a href="#1，⼆分类" class="headerlink" title="1，⼆分类"></a>1，⼆分类</h2><p>线性判别函数的最简单的形式是输⼊向量的线性函数，即</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+w_0\tag{4.2}</script><p>其中 $\boldsymbol{w}$ 被称为<strong>权向量</strong>（<code>weight vector</code>），$w_0$ 被称为<strong>偏置</strong>（<code>bias</code>）。偏置的相反数有时被称为<strong>阈值</strong>（<code>threshold</code>）。</p><p>考虑两个点 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ ，两个点都位于决策⾯上。 由于 $y(\boldsymbol{x}_A)=y(\boldsymbol{x}_B)=0$，我们有 $\boldsymbol{w}^{T}(\boldsymbol{x}_A-\boldsymbol{x}_B) = 0$，因此向量 $\boldsymbol{w}$ 与决策⾯内的任何向量都正交，从⽽ $\boldsymbol{w}$ 确定了决策⾯的⽅向。类似地，如果 $\boldsymbol{x}$ 是决策⾯内的⼀个点，那么 $y(\boldsymbol{x}) = 0$ ，因此从原点到决策⾯的垂直距离为</p><script type="math/tex; mode=display">\frac{\boldsymbol{w}^{T}\boldsymbol{x}}{\|\boldsymbol{w}\|}=-\frac{w_0}{\|\boldsymbol{x}\|}\tag{4.3}</script><p>其中，偏置参数 $\boldsymbol{w}_0$ 确定了决策⾯的位置。</p><p>记任意⼀点 $\boldsymbol{x}$ 到决策⾯的垂直距离 $r$ ，在决策⾯上的投影 $\boldsymbol{x}_{\perp}$ ，则有</p><script type="math/tex; mode=display">\boldsymbol{x}=\boldsymbol{x}_{\perp}+r \frac{\boldsymbol{w}}{\|\boldsymbol{w}\|}\tag{4.4}</script><p>利用已知公式和 $y(\boldsymbol{x}_{\perp})=0$ 可得</p><script type="math/tex; mode=display">r=\frac{y(\boldsymbol{x})}{\|w\|}\tag{4.5}</script><p>为方便简洁，引⼊“虚”输⼊ $x_0=1$ ，并且定义 $\tilde{\boldsymbol{w}} = (w_0,\boldsymbol{w})$ 以及 $\tilde{\boldsymbol{x}} = (x_0,\boldsymbol{x})$ ，从⽽</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\tilde{\boldsymbol{w}}^{T}\tilde{\boldsymbol{x}}\tag{4.6}</script><p>在这种情况下， 决策⾯是⼀个 $D$ 维超平⾯， 并且这个超平⾯会穿过 $D+1$ 维扩展输⼊空间的原点。</p><h2 id="2，多分类"><a href="#2，多分类" class="headerlink" title="2，多分类"></a>2，多分类</h2><p>考虑把线性判别函数推⼴到 $K&gt;2$ 个类别。</p><p>方法一，使⽤ $K − 1$ 个分类器，每个分类器⽤来解决⼀个⼆分类问题，把属于类别 $\mathcal{C}_k$ 和不属于那个类别的点分开。这被称为“<strong>1对其他</strong>”（<code>one-versus-the-rest</code>）<strong>分类器</strong>。此方法的缺点在于产⽣了输⼊空间中⽆法分类的区域。</p><p>方法二，引⼊ $\frac{K(K−1)}{2}$ 个⼆元判别函数， 对每⼀对类别都设置⼀个判别函数。 这被称为“<strong>1对1</strong>”（<code>one-versus-one</code>）<strong>分类器</strong>。每个点的类别根据这些判别函数中的⼤多数输出类别确定，但是，这也会造成输⼊空间中的⽆法分类的区域。</p><p>如图4.2，尝试从⼀组两类的判别准则中构建出⼀个 $K$ 类的判别准则会导致具有奇异性的区域， ⽤绿⾊表⽰。</p><p><img src="/images/prml_20191009093808.png" alt="判别准则"></p><p>方法三，通过引⼊⼀个 $K$ 类判别函数，可以避免上述问题。这个 $K$ 类判别函数由 $K$ 个线性函数组成，形式为</p><script type="math/tex; mode=display">y_{k}(\boldsymbol{x})=\boldsymbol{w}_{k}^{T}\boldsymbol{x}+w_{k0}\tag{4.7}</script><p>对于点 $\boldsymbol{x}$ ，如果对于所有的 $j \ne k$ 都 有 $y_{k}(\boldsymbol{x})\gt y_{j}(\boldsymbol{x})$ ，那么就把它分到 $\mathcal{C}_k$ 。 于是类别 $\mathcal{C}_k$ 和 $\mathcal{C}_j$ 之间的决策⾯为 $y_{k}(\boldsymbol{x})=y_{j}(\boldsymbol{x})$，并且对应于⼀个 $(D − 1)$ 维超平⾯，形式为</p><script type="math/tex; mode=display">(\boldsymbol{w}_{k}-\boldsymbol{w}_{j})^{T}\boldsymbol{x}+(w_{k0}-w_{j0})=0\tag{4.8}</script><p>考虑两个点 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ ，两个点都位于决策区域 $\mathcal{R}_k$ 中， 任何位于连接 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ 的线段上的点都可以表⽰成下⾯的形式</p><script type="math/tex; mode=display">\hat{\boldsymbol{x}}=\lambda \boldsymbol{x}_{A}+(1-\lambda)\boldsymbol{x}_{B}\tag{4.9}</script><p>其中，$0\le\lambda\le1$ 。根据判别函数的线性性质，有</p><script type="math/tex; mode=display">y_{k}(\hat{\boldsymbol{x}})=\lambda y_{k}(\boldsymbol{x}_{A})+(1-\lambda)y_{k}(\boldsymbol{x}_{B})\tag{4.10}</script><p>由于 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ 位于 $\mathcal{R}_k$ 内部，因此对于所有 $j \ne k$ ， 都有 $y_{k}(\boldsymbol{x}_{A})\gt y_{j}(\boldsymbol{x}_{A})$ 以及 $y_{k}(\boldsymbol{x}_{B})\gt y_{j}(\boldsymbol{x}_{B})$ ，因此 $y_{k}(\hat{\boldsymbol{x}})\gt y_{j}(\hat{\boldsymbol{x}})$ ，从⽽ $\hat{\boldsymbol{x}}$ 也位于 $\mathcal{R}_k$ 内部，即 $\mathcal{R}_k$ 是单连通的并且是凸的。</p><p>如图4.3，多类判别函数的决策区域的说明， 决策边界⽤红⾊表⽰。</p><p><img src="/images/prml_20191009101103.png" alt="多类判别函数的决策区域"></p><h2 id="3，⽤于分类的最⼩平⽅⽅法"><a href="#3，⽤于分类的最⼩平⽅⽅法" class="headerlink" title="3，⽤于分类的最⼩平⽅⽅法"></a>3，⽤于分类的最⼩平⽅⽅法</h2><p>每个类别 $\mathcal{C}_k$ 由⾃⼰的线性模型描述，即公式(4.7)，其中 $k = 1, \dots , K$ 。使⽤向量记号表⽰，即</p><script type="math/tex; mode=display">\boldsymbol{y}(\boldsymbol{x})=\tilde{\boldsymbol{W}}^{T}\tilde{\boldsymbol{x}}\tag{4.11}</script><p>其中 $\tilde{\boldsymbol{W}}$ 是⼀个矩阵，第 $k$ 列由 $D + 1$ 维向量 $\tilde{\boldsymbol{w}}_k=(w_{k0},w_{k}^{T})^{T}$ 组成，$\tilde{\boldsymbol{x}}$ 是对应的增⼴输⼊向量 $(1, \boldsymbol{x}^{T})^{T}$， 它带有⼀个虚输⼊ $x_0 = 1$ 。</p><p>现在通过最⼩化平⽅和误差函数来确定参数矩阵 $\tilde{\boldsymbol{W}}$  ，考虑⼀个训练数据集 $\{\boldsymbol{x}_n, \boldsymbol{t}_n\}$，其中 $n = 1,\dots , N $，然后定义⼀个矩阵 $\boldsymbol{T}$ ，它的第 $n$ ⾏是向量 $\boldsymbol{t}_{n}^{T}$ ，定义⼀个矩阵 $\tilde{\boldsymbol{X}}$ ，它的第 $n$ ⾏是 $\tilde{\boldsymbol{x}}_{n}^{T}$ 。这样，平⽅和误差函数可以写成</p><script type="math/tex; mode=display">E_{D}(\tilde{\boldsymbol{W}})=\frac{1}{2}\text{Tr}\{(\tilde{\boldsymbol{X}}\tilde{\boldsymbol{W}}-\boldsymbol{T})^{T}(\tilde{\boldsymbol{X}}\tilde{\boldsymbol{W}}-\boldsymbol{T})\}\tag{4.12}</script><p>令关于 $\tilde{\boldsymbol{W}}$ 的导数等于零，整理，可以得到 $\tilde{\boldsymbol{W}}$ 的解，形式为</p><script type="math/tex; mode=display">\tilde{\boldsymbol{W}}=(\tilde{\boldsymbol{X}}^{T}\tilde{\boldsymbol{W}})^{-1}\tilde{\boldsymbol{X}}^{T}\boldsymbol{T}=\tilde{\boldsymbol{X}}^{\dagger}\boldsymbol{T}\tag{4.13}</script><p>其中 $\tilde{\boldsymbol{X}}^{\dagger}$ 是矩阵 $\tilde{\boldsymbol{X}}$ 的伪逆矩阵。即得判别函数，形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\tilde{\boldsymbol{W}}^{T}\tilde{\boldsymbol{x}}=\boldsymbol{T}^{T}(\tilde{\boldsymbol{X}}^{\dagger})^{T}\tilde{\boldsymbol{x}}\tag{4.14}</script><p>如图4.4，左图给出了来⾃两个类别的数据，⽤红⾊叉形和蓝⾊圆圈表⽰。同时给出的还有通过最⼩平⽅⽅法找到的决策边界（洋红⾊曲线）以及<code>logistic</code>回归模型给出的决策边界（绿⾊曲线）；右图给出了当额外的数据点被添加到左图的底部之后得到的结果，这表明最⼩平⽅⽅法对于异常点很敏感，这与<code>logistic</code>回归不同。</p><p><img src="/images/prml_20191009104504.png" alt="⽤于分类的最⼩平⽅⽅法"></p><p>多⽬标变量的最⼩平⽅解的⼀个重要的<strong>性质</strong>是：如果训练集⾥的每个⽬标向量都满⾜某个线性限制</p><script type="math/tex; mode=display">\boldsymbol{a}^{T}\boldsymbol{t}_{n}+b=0\tag{4.15}</script><p>其中 $\boldsymbol{a}$ 和 $b$ 为常量，那么对于任何 $\boldsymbol{x}$ 值，模型的预测也满⾜同样的限制，即</p><script type="math/tex; mode=display">\boldsymbol{a}^{T}\boldsymbol{y}(\boldsymbol{x})+b=0\tag{4.16}</script><p>因此如果使⽤ $K$ 分类的“<strong>1-of-K</strong> ”表达⽅式，那么这个模型做出的预测会具有下⾯的<strong>性质</strong>：对于任意的 $\boldsymbol{x}$ 的值， $\boldsymbol{y}(\boldsymbol{x})$ 的元素的和等于1。</p><p>举例，由三个类别组成的⼈⼯数据集，训练数据点分别⽤红⾊（×）、绿⾊（+）、蓝⾊（◦）标出。 直线表⽰决策边界， 背景颜⾊表⽰决策区域代表的类别。<br>如图4.5，使⽤最⼩平⽅判别函数，分配到绿⾊类别的输⼊空间的区域过⼩，⼤部分来⾃这个类别的点都被错误分类。</p><p><img src="/images/prml_20191009110353.png" alt="最⼩平⽅判别函数训练"></p><p>如图4.6，使⽤<code>logistic</code>回归的结果，给出了训练数据的正确分类情况。</p><p><img src="/images/prml_20191009110405.png" alt="logistic回归训练"></p><h2 id="4，Fisher线性判别函数"><a href="#4，Fisher线性判别函数" class="headerlink" title="4，Fisher线性判别函数"></a>4，<code>Fisher</code>线性判别函数</h2><p>假设有⼀个 $D$ 维输⼊向量 $\boldsymbol{x}$ ，然后使⽤下式投影到⼀维</p><script type="math/tex; mode=display">y=\boldsymbol{w}^{T}\boldsymbol{x}\tag{4.17}</script><p>如果在 $y$ 上设置⼀个阈值，然后把 $y\ge -w_0$ 的样本分为 $\mathcal{C}_1$ 类，把其余的样本分为 $\mathcal{C}_2$ 类，那么就得到了一个标准的线性分类器。</p><p>考虑⼀个⼆分类问题，这个问题中有 $\mathcal{C}_1$ 类的 $N_1$ 个点以及 $\mathcal{C}_2$ 类的 $N_2$ 个点。因此两类的均值向量为</p><script type="math/tex; mode=display">\boldsymbol{m}_{1}=\frac{1}{N_1}\sum_{n\in\mathcal{C_1}}\boldsymbol{x}_{n}\\\boldsymbol{m}_{2}=\frac{1}{N_2}\sum_{n\in\mathcal{C_2}}\boldsymbol{x}_{n}</script><p>如果投影到 $\boldsymbol{w}$ 上，那么最简单的度量类别之间分开程度的⽅式就是类别均值投影之后的距离。这说明可以选择 $\boldsymbol{w}$ 使得下式取得最⼤值</p><script type="math/tex; mode=display">m_2-m_1=\boldsymbol{w}^{T}(\boldsymbol{m}_2-\boldsymbol{m}_1)\tag{4.18}</script><p>其中，</p><script type="math/tex; mode=display">m_k=\boldsymbol{w}^{T}\boldsymbol{m}_{k}</script><p>是来⾃类别 $\mathcal{C}_k$ 的投影数据的均值。</p><p>如图4.7，左图给出了来⾃两个类别（表⽰为红⾊和蓝⾊）的样本，以及在连接两个类别的均值的直线上的投影的直⽅图。注意，在投影空间中，存在⼀个⽐较严重的类别重叠。右图给出的基于<code>Fisher</code>线性判别准则的对应投影，表明了类别切分的效果得到了极⼤的提升。</p><p><img src="/images/prml_20191009110828.png" alt="Fisher线性判别准则"></p><p><code>Fisher</code>提出的<strong>思想</strong>是最⼤化⼀个函数，这个函数能够让类均值的投影分开得较⼤，同时让每个类别内部的⽅差较⼩，从⽽最⼩化了类别的重叠。</p><p>投影公式(4.17)将 $\boldsymbol{x}$ 的⼀组有标记的数据点变换为⼀位空间 $y$ 的⼀组有标记数据点。来⾃类别 $\mathcal{C}_k$ 的数据经过变换后的类内⽅差为</p><script type="math/tex; mode=display">s_{k}^{2}=\sum_{n\in \mathcal{C}_k}(y_n-m_k)^{2}\tag{4.19}</script><p>其中，$y_n=\boldsymbol{w}^{T}\boldsymbol{x}_{n}$ 。把整个数据集的总的类内⽅差定义为 $s_1^2+s_2^2$ ，<strong><code>Fisher</code>准则</strong> 根据类间⽅差和类内⽅差的⽐值定义，即</p><script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{(m_2-m_1)^{2}}{s_1^2+s_2^2}\tag{4.20}</script><p>不难推导， $J(\boldsymbol{w})$ 对 $\boldsymbol{w}$ 的依赖</p><script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{\boldsymbol{w}^{T}\boldsymbol{S}_B\boldsymbol{w}}{\boldsymbol{w}^{T}\boldsymbol{S}_W\boldsymbol{w}}\tag{4.21}</script><p>其中 $\boldsymbol{S}_B$ 是<strong>类间（<code>between-class</code>）协⽅差矩阵</strong>，形式为</p><script type="math/tex; mode=display">\boldsymbol{S}_B=(\boldsymbol{m}_2-\boldsymbol{m}_1)(\boldsymbol{m}_2-\boldsymbol{m}_1)^{T}</script><p>$\boldsymbol{S}_W$ 被称为<strong>类内（<code>within-class</code>）协⽅差矩阵</strong>，形式为</p><script type="math/tex; mode=display">\boldsymbol{S}_W=\sum_{n\in \mathcal{C}_1}(\boldsymbol{x}_n-\boldsymbol{m}_1)(\boldsymbol{x}_n-\boldsymbol{m}_1)^{T}+\sum_{n\in \mathcal{C}_2}(\boldsymbol{x}_n-\boldsymbol{m}_2)(\boldsymbol{x}_n-\boldsymbol{m}_2)^{T}</script><p>对公式(4.21)关于 $\boldsymbol{w}$ 求导，发现 $J(\boldsymbol{w})$ 取得最⼤值的条件为</p><script type="math/tex; mode=display">(\boldsymbol{w}^{T}\boldsymbol{S}_B\boldsymbol{w})\boldsymbol{S}_W\boldsymbol{w}=(\boldsymbol{w}^{T}\boldsymbol{S}_W\boldsymbol{w})\boldsymbol{S}_B\boldsymbol{w}\tag{4.22}</script><p>可以发现， $\boldsymbol{S}_B\boldsymbol{w}$ 总是在 $(\boldsymbol{m}_2−\boldsymbol{m}_1)$ 的⽅向上。 更重要的是， 若不关⼼ $\boldsymbol{w}$ 的⼤⼩， 只关⼼它的⽅向， 因此可以忽略标量因⼦ $(\boldsymbol{w}^{T}\boldsymbol{S}_B\boldsymbol{w})$ 和 $(\boldsymbol{w}^{T}\boldsymbol{S}_W\boldsymbol{w})$ 。 将公式(4.22)的两侧乘以 $\boldsymbol{S}_{W}^{-1}$ ，即得 <strong><code>Fisher</code>线性判别函数</strong>（<code>Fisher linear discriminant</code>）</p><script type="math/tex; mode=display">\boldsymbol{w}\propto \boldsymbol{S}_{W}^{-1}(\boldsymbol{m}_2-\boldsymbol{m}_1)\tag{4.23}</script><p>如果类内协⽅差矩阵是各向同性的，从⽽ $\boldsymbol{S}_W$ 正⽐于单位矩阵，那么我们看到 $\boldsymbol{w}$ 正⽐于类均值的差。</p><p>构建 <strong><code>Fisher</code>线性判别函数</strong> ，其⽅法为：选择⼀个阈值 $y_0$ ，使得当 $y(\boldsymbol{x})\ge y_0$ 时，把数据点分到 $\mathcal{C}_1$ ，否则把数据点分到 $\mathcal{C}_2$ 。</p><h2 id="5，与最⼩平⽅的关系"><a href="#5，与最⼩平⽅的关系" class="headerlink" title="5，与最⼩平⽅的关系"></a>5，与最⼩平⽅的关系</h2><p><strong>最⼩平⽅⽅法</strong>确定线性判别函数的⽬标是使模型的预测尽可能地与⽬标值接近。相反， <strong><code>Fisher</code>判别准则</strong> 的⽬标是使输出空间的类别有最⼤的区分度。</p><p>对于⼆分类问题，<code>Fisher</code>准则可以看成最⼩平⽅的⼀个特例。作如下假设：让属于 $\mathcal{C}_1$ 的⽬标值等于 $\frac{N}{N_1}$ ，其中 $N_1$ 是类别 $\mathcal{C}_1$ 的模式的数量，$N$ 是总的模式数量。这个⽬标值近似于类别 $\mathcal{C}_1$ 的先验概率的导数。 对于类别 $\mathcal{C}_2$ ， 令⽬标值等于 $−\frac{N}{N_2}$ ， 其中 $N_2$ 是类别 $\mathcal{C}_2$ 的模式的数量。平⽅和误差函数可以写成</p><script type="math/tex; mode=display">E=\frac{1}{2}\sum_{n=1}^{N}(\boldsymbol{w}^{T}\boldsymbol{x}_{n}+w_0-t_n)^{2}\tag{4.24}</script><p>令 $E$ 关于 $w_0$ 和 $\boldsymbol{w}$ 的导数等于零，使⽤对于⽬标值 $t_n$ 的表⽰⽅法，可以得到偏置的表达式</p><script type="math/tex; mode=display">w_0=-\boldsymbol{w}^{T}\boldsymbol{m}\tag{4.25}</script><p>其中，</p><script type="math/tex; mode=display">\sum_{n=1}^{N}t_n=N_1\frac{N}{N_1}-N_2\frac{N}{N_2}=0\\\boldsymbol{m}=\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{x}_n=\frac{1}{N}(N_1\boldsymbol{m}_1+N_2\boldsymbol{m}_2)</script><p>使⽤对于 $t_n$ 的新的表⽰⽅法可得</p><script type="math/tex; mode=display">\left(\boldsymbol{S}_W+\frac{N_1N_2}{N}\boldsymbol{S}_B\right)\boldsymbol{w}=N(\boldsymbol{m_1}-\boldsymbol{m}_2)\tag{4.26}</script><p>由此可见，可以推导出公式(4.23)，即权向量恰好与根据<code>Fisher</code>判别准则得到的结果相同。</p><h2 id="6，多分类的Fisher判别函数"><a href="#6，多分类的Fisher判别函数" class="headerlink" title="6，多分类的Fisher判别函数"></a>6，多分类的<code>Fisher</code>判别函数</h2><p>现在考虑<code>Fisher</code>判别函数对于 $K&gt;2$ 个类别的推⼴。 假设输⼊空间的维度 $D$ ⼤于类别数量 $K$ ，引⼊ $D^{\prime} &gt; 1$ 个线性“特征” $y_k = \boldsymbol{w}_k^{T}\boldsymbol{x}$ ，其中 $k=1,\dots,D^{\prime}$ 。 为了⽅便， 这些特征值可以聚集起来组成向量 $\boldsymbol{y}$ ，类似地，权向量 $\{\boldsymbol{w}_k\}$ 可以被看成矩阵 $\boldsymbol{W}$ 的列。因此</p><script type="math/tex; mode=display">\boldsymbol{y}=\boldsymbol{W}^{T}\boldsymbol{x}\tag{4.27}</script><p>类内协⽅差矩阵推⼴到 $K$ 类，有</p><script type="math/tex; mode=display">\boldsymbol{S}_{W}=\sum_{k=1}^{K}\boldsymbol{S}_{k}\tag{4.28}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}_{k}=\sum_{n\in \mathcal{C}_k}(\boldsymbol{x}_n-\boldsymbol{m}_k)(\boldsymbol{x}_n-\boldsymbol{m}_k)^{T}\\\boldsymbol{m}_{k}=\frac{1}{N_k}\sum_{n\in\mathcal{C_k}}\boldsymbol{x}_{n}</script><p>其中 $N_k$ 是类别 $\mathcal{C}_k$ 中模式的数量。</p><p>为了找到类间协⽅差矩阵的推⼴，使⽤<code>Duda and Hart</code>（1973）的⽅法，⾸先考虑整体的协⽅差矩阵</p><script type="math/tex; mode=display">\boldsymbol{S}_{T}=\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{m})(\boldsymbol{x}_n-\boldsymbol{m})^{T}\tag{4.29}</script><p>其中 $\boldsymbol{m}$ 是全体数据的均值</p><script type="math/tex; mode=display">\boldsymbol{m}=\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{x}_{n}=\frac{1}{N}\sum_{k=1}^{K}N_k\boldsymbol{m}_{k}</script><p>其中 $N = \sum_{k} N_k$ 是数据点的总数。</p><p>整体的协⽅差矩阵可以分解为公式(4.28)给出的类内协⽅差矩阵，加上另⼀个矩阵 $\boldsymbol{S}_B$ ，它可以看做类间协⽅差矩阵。</p><script type="math/tex; mode=display">\boldsymbol{S}_{T}=\boldsymbol{S}_{W}+\boldsymbol{S}_{B}\tag{4.30}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}_B=\sum_{k=1}^{K}N_k(\boldsymbol{m}_k-\boldsymbol{m})(\boldsymbol{m}_k-\boldsymbol{m})^{T}</script><p>协⽅差矩阵被定义在原始的 $\boldsymbol{x}$ 空间中。现在在投影的 $D^{\prime}$ 维 $\boldsymbol{y}$ 空间中定义类似的矩阵</p><script type="math/tex; mode=display">\boldsymbol{S}_{W}=\sum_{k=1}^{K}\sum_{n\in \mathcal{C}_k}(\boldsymbol{y}_n-\boldsymbol{\mu}_k)(\boldsymbol{y}_n-\boldsymbol{\mu}_k)^{T}\\\boldsymbol{S}_B=\sum_{k=1}^{K}N_k(\boldsymbol{\mu}_k-\boldsymbol{\mu})(\boldsymbol{\mu}_k-\boldsymbol{\mu})^{T}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{\mu}_k=\frac{1}{N_k}\sum_{n\in \mathcal{C}_k}\boldsymbol{y}_n\\\boldsymbol{\mu}=\frac{1}{N}\sum_{k=1}^{K}N_k\boldsymbol{\mu}_k</script><p>我们想构造⼀个标量，当类间协⽅差较⼤且类内协⽅差较⼩时，这个标量会较⼤。有许多可能的准则选择⽅式（<code>Fukunaga</code>, 1990）。其中⼀种选择是</p><script type="math/tex; mode=display">J(\boldsymbol{W})=\text{Tr}\{\boldsymbol{s}_{W}^{-1}\boldsymbol{s}_{B}\}\tag{4.31}</script><p>这个判别准则可以显式地写成投影矩阵 $\boldsymbol{W}$ 的函数，形式为</p><script type="math/tex; mode=display">J(\boldsymbol{W})=\text{Tr}\{(\boldsymbol{W}^{T}\boldsymbol{S}_{W}\boldsymbol{W})^{-1}(\boldsymbol{W}^{T}\boldsymbol{S}_{B}\boldsymbol{W})\}\tag{4.32}</script><h2 id="7，感知器算法"><a href="#7，感知器算法" class="headerlink" title="7，感知器算法"></a>7，感知器算法</h2><p>线性判别模型的另⼀个例⼦是<code>Rosenblatt</code>（1962）提出的<strong>感知器算法</strong>。对应于⼀个⼆分类的模型，输⼊向量 $\boldsymbol{x}$ ⾸先使⽤⼀个固定的⾮线性变换得到⼀个特征向量 $\boldsymbol{\phi}(\boldsymbol{x})$ ， 这个特征向量然后被⽤于构造⼀个⼀般的线性模型，形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=f(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}))\tag{4.33}</script><p>其中⾮线性激活函数 $f(·)$ 是⼀个阶梯函数，形式为</p><script type="math/tex; mode=display">f(a)=\begin{cases}+1,&a\ge 0\\ -1,&a<0\end{cases}</script><p>向量 $\boldsymbol{\phi}(\boldsymbol{x})$ 通常包含⼀个偏置分量 $\phi_{0}(\boldsymbol{x})=0$ 。对于感知器，使⽤ $t=+1$ 表⽰ $\mathcal{C}_1$ ，使⽤ $t=−1$ 表⽰ $\mathcal{C}_2$ ，这与激活函数的选择相匹配。<br>为了推导误差函数，即<strong>感知器准则</strong>（<code>perceptron criterion</code>）， 注意到我们正在做的是寻找⼀个权向量 $\boldsymbol{w}$ 使得对于类别 $\mathcal{C}_1$ 中的模式 $\boldsymbol{x}_n$ 都有 $\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)&gt;0$ ， ⽽对于类别 $\mathcal{C}_2$ 中的模式 $\boldsymbol{x}_n$ 都有 $\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)<0$ 。 使⽤ $t \in\{−1, +1\}$ 这种⽬标变量的表⽰⽅法，要做的就是使得所有的模式都满⾜ $\boldsymbol{w}^{t}\boldsymbol{\phi}(\boldsymbol{x}_n)t_{n}>0$ 。 对于正确分类的模式，感知器准则赋予零误差，⽽对于误分类的模式 $\boldsymbol{x}_n$ ，它试着最⼩化 $-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)t_{n}$ 。因此，<strong>感知器准则</strong>为</0$></p><script type="math/tex; mode=display">E_{P}(\boldsymbol{w})=-\sum_{n\in\mathcal{M}}\boldsymbol{w}^{T}\boldsymbol{\phi}_nt_{n}\tag{4.34}</script><p>其中 $\boldsymbol{\phi}_n=\boldsymbol{\phi}(\boldsymbol{x}_n)$ 和 $\mathcal{M}$ 表⽰所有误分类模式的集合。某个特定的误分类模式对于误差函数的贡献是 $\boldsymbol{w}$ 空间中模式被误分类的区域中 $\boldsymbol{w}$ 的线性函数， ⽽在正确分类的区域，误差函数等于零。 总的误差函数因此是分段线性的。</p><p>现在对这个误差函数使⽤随机梯度下降算法。这样，权向量 $\boldsymbol{w}$ 的变化为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w}^{(\tau+1)}&=\boldsymbol{w}^{(\tau)}-\eta\nabla E_{P}(\boldsymbol{w})\\&=\boldsymbol{w}^{(\tau)}+\eta\boldsymbol{\phi}_{n}t_n\end{aligned}\tag{4.35}</script><p>其中 $\eta$ 是学习率参数，$\tau$ 是⼀个整数，是算法运⾏次数的索引。</p><p>感知器学习算法可以简单地表⽰如下：我们反复对于训练模式进⾏循环处理，对于每个模式 $\boldsymbol{x}_n$ 计算感知器函数(4.33)。如果模式正确分类，那么权向量保持不变，⽽如果模式被错误分类，那么对于类别 $\mathcal{C}_1$ ， 我们把向量 $\boldsymbol{\phi}(\boldsymbol{x}_n)$ 加到当前对于权向量 $\boldsymbol{w}$ 的估计值上，⽽对于类别 $\mathcal{C}_2$ ，我们从 $\boldsymbol{w}$ 中减掉向量 $\boldsymbol{\phi}(\boldsymbol{x}_n)$。</p><p>如图4.8～4.11，感知器算法收敛性的说明， 给出了⼆维特征空间 $(\phi_1,\phi_2)$ 中的来⾃两个类别的数据点（红⾊和蓝 ⾊）。图4.8给出了初始参数向量 $\boldsymbol{w}$ ，表⽰为⿊⾊箭头，以及对应的决策边界（⿊⾊直线），其中箭头指向被分类为红⾊类别的决策区域。⽤绿⾊圆圈标出的数据点被误分类，因此它的特征向量被加到当前的权向量中，给出了新的决策边界，如图4.9所⽰。 图4.10给出了下⼀个误分类的点，⽤绿⾊圆圈标出，它的特征向量再次被加到权向量上，给出了图4.11的决策边界。这个边界中所有的数据点都被正确分类。</p><p><img src="/images/prml_20191009110918.png" alt="初始参数向量"></p><p><img src="/images/prml_20191009110936.png" alt="新的决策边界"></p><p><img src="/images/prml_20191009110946.png" alt="下⼀个误分类的点"></p><p><img src="/images/prml_20191009111003.png" alt="正确分类"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，分类线性模型概述&quot;&gt;&lt;a href=&quot;#一，分类线性模型概述&quot; class=&quot;headerlink&quot; title=&quot;一，分类线性模型概述&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】贝叶斯线性模型</title>
    <link href="https://zhangbc.github.io/2019/10/07/prml_03_02/"/>
    <id>https://zhangbc.github.io/2019/10/07/prml_03_02/</id>
    <published>2019-10-07T08:42:29.000Z</published>
    <updated>2019-10-10T01:12:49.406Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，贝叶斯线性回归"><a href="#一，贝叶斯线性回归" class="headerlink" title="一，贝叶斯线性回归"></a>一，贝叶斯线性回归</h1><h2 id="1，参数分布"><a href="#1，参数分布" class="headerlink" title="1，参数分布"></a>1，参数分布</h2><p>关于线性拟合的贝叶斯⽅法的讨论，⾸先引⼊模型参数 $\boldsymbol{w}$ 的先验概率分布。现在这个阶段，把噪声精度参数 $\beta$ 当做已知常数。⾸先，由公式(3.8)定义的似然函数 $p(t|\boldsymbol{w})$ 是 $\boldsymbol{w}$ 的⼆次函数的指数形式，于是对应的共轭先验是⾼斯分布，形式为：</p><script type="math/tex; mode=display">p(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_{0},\boldsymbol{S}_{0})\tag{3.30}</script><p>均值为 $\boldsymbol{m}_{0}$ ，协⽅差为 $\boldsymbol{S}_{0}$ 。</p><p>由于共轭⾼斯先验分布的选择，后验分布也将是⾼斯分布。 我们可以对指数项进⾏配平⽅， 然后使⽤归⼀化的⾼斯分布的标准结果找到归⼀化系数，这样就计算出了后验分布的形式：</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\boldsymbol{t})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_{N},\boldsymbol{S}_{N})\tag{3.31}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{m}_{N}=\boldsymbol{S}_{N}(\boldsymbol{S}_{0}^{-1}\boldsymbol{m}_{0}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{t}) \\\boldsymbol{S}_{N}^{-1}=\boldsymbol{S}_{0}^{-1}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{\Phi}</script><p>为了简化起见，考虑⾼斯先验的⼀个特定的形式，即考虑零均值各向同性⾼斯分布，这个分布由⼀个精度参数 $\alpha$ 控制，即：</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\alpha)=\mathcal{N}(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\boldsymbol{I})\tag{3.32}</script><p>对应的 $\boldsymbol{w}$ 后验概率分布由公式(3.31)给出，其中，</p><script type="math/tex; mode=display">\boldsymbol{m}_{N}=\beta \boldsymbol{S}_{N}\boldsymbol{\Phi}^{T}\boldsymbol{t}\\\boldsymbol{S}_{N}^{-1}=\alpha \boldsymbol{I}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{\Phi}</script><p>后验概率分布的对数由对数似然函数与先验的对数求和的⽅式得到。它是 $\boldsymbol{w}$ 的函数，形式为：</p><script type="math/tex; mode=display">\ln p(\boldsymbol{w}|\boldsymbol{t})=-\frac{\beta}{2}\sum_{n=1}^{N}\{t_n-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}-\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}+常数\tag{3.33}</script><h2 id="2，预测分布"><a href="#2，预测分布" class="headerlink" title="2，预测分布"></a>2，预测分布</h2><p>在实际应⽤中，我们通常感兴趣的不是 $\boldsymbol{w}$ 本⾝的值，⽽是对于新的 $\boldsymbol{x}$ 值预测出 $t$ 的值。这需要我们计算出<strong>预测分布</strong>（<code>predictive distribution</code>），定义为：</p><script type="math/tex; mode=display">p(t|\mathbf{t},\alpha,\beta)=\int p(t|\boldsymbol{w},\beta)p(\boldsymbol{w}|\mathbf{t},\alpha,\beta)\mathrm{d}\boldsymbol{w}\tag{3.34}</script><p>其中 $\mathbf{t}$ 是训练数据⽬标变量的值组成的向量。经综合分析，预测分布的形式可以进一步具体化为：</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathbf{t},\alpha,\beta)=\mathcal{N}(t|\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x}),\sigma_{N}^{2}(\boldsymbol{x}))\tag{3.35}</script><p>其中，</p><script type="math/tex; mode=display">\sigma_{N}^{2}(\boldsymbol{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x})</script><p>其中，式中第⼀项表⽰数据中的噪声，第⼆项反映了与参数 $\boldsymbol{w}$ 关联的不确定性。当额外的数据点被观测到的时候，后验概率分布会变窄。从⽽可以证明出 $\sigma_{N+1}^{2}(\boldsymbol{x})\le \sigma_{N}^{2}(\boldsymbol{x})$（<code>Qazaz et al.</code>, 1997）。 在极限 $N \to \infty$ 的情况下， 式中第⼆项趋于零， 从⽽预测分布的⽅差只与参数 $\beta$ 控制的具有可加性的噪声有关。</p><p>在下图3.15～3.18中，我们调整⼀个由⾼斯基函数线性组合的模型，使其适应于不同规模的数据集，然后观察对应的后验概率分布。其中，绿⾊曲线对应着产⽣数据点的函数 $\sin(2\pi x)$（带有附加的⾼斯噪声），⼤⼩为 $N = 1, N = 2, N = 4$ 和 $N = 25$ 的数据集在四幅图中⽤蓝⾊圆圈表⽰。对于每幅图，红⾊曲线是对应的⾼斯预测分布的均值，红⾊阴影区域是均值两侧的⼀个标准差范围的区域。注意，预测的不确定性依赖于 $x$，并且在数据点的邻域内最⼩。</p><p><img src="/images/prml_20191005233709.png" alt="N=1"></p><p><img src="/images/prml_20191005233718.png" alt="N=2"></p><p><img src="/images/prml_20191005233730.png" alt="N=4"></p><p><img src="/images/prml_20191005233748.png" alt="N=25"></p><p>为了更加深刻地认识对于不同的 $x$ 值的预测之间的协⽅差，我们可以从 $\boldsymbol{w}$ 的后验概率分布中抽取样本，然后画出对应的函数 $y(x, \boldsymbol{w})$ ，如图3.19～3.22所⽰。</p><p><img src="/images/prml_20191005233947.png" alt="N=1"></p><p><img src="/images/prml_20191005233957.png" alt="N=2"></p><p><img src="/images/prml_20191005234011.png" alt="N=4"></p><p><img src="/images/prml_20191005234022.png" alt="N=25"></p><h2 id="3，等价核"><a href="#3，等价核" class="headerlink" title="3，等价核"></a>3，等价核</h2><p>考虑以下<strong>预测均值</strong>形式：</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{m}_{N})=\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x})=\beta \boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\Phi}^{T}\mathbf{t}=\sum_{n=1}^{N}\beta \boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x}_{n})t_{n}\tag{3.36}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}_{N}^{-1}=\boldsymbol{S}_{0}^{-1}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{\Phi}</script><p>因此在点 $\boldsymbol{x}$ 处的预测均值由训练集⽬标变量 $t_n$ 的线性组合给出，即：</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{m}_{N})=\sum_{n=1}^{N}k(\boldsymbol{x},\boldsymbol{x}_{n})t_{n}\tag{3.37}</script><p>其中，    </p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\beta \boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})\tag{3.38}</script><p>被称为<strong>平滑矩阵</strong>（<code>smoother matrix</code>）或者<strong>等价核</strong>（<code>equivalent kernel</code>）。像这样的回归函数，通过对训练集⾥⽬标值进⾏线性组合做预测，被称为<strong>线性平滑</strong>（<code>linear smoother</code>）。</p><p>如图3.23，⾼斯基函数的等价核 $k(x, x^{\prime})$ ， 图中给出了 $x$ 关于 $x^{\prime}$ 的图像， 以及通过这个矩阵的三个切⽚， 对应于三个不同的 $x$ 值，⽤来⽣成这个核的数据集由 $x$ 的200个值组成，$x$ 均匀地分布在区 间 $(−1, 1)$ 中。</p><p><img src="/images/prml_20191006132738.png" alt="⾼斯基函数的等价核"></p><p>如图3.24，多项式基函数在 $x=0$ 的等价核 $k(x, x^{\prime})$ 。 </p><p><img src="/images/prml_20191006132829.png" alt="多项式基函数的等价核"></p><p>如图3.25，<code>Sigmoid</code>基函数在 $x=0$ 的等价核 $k(x, x^{\prime})$ 。 </p><p><img src="/images/prml_20191006132838.png" alt="Sigmoid基函数的等价核"></p><p>考虑 $y(\boldsymbol{x})$ 和 $y(\boldsymbol{x}^{\prime})$ 的协⽅差</p><script type="math/tex; mode=display">\begin{aligned}\text{cov}[y(\boldsymbol{x}),\boldsymbol{x}^{\prime}]&=\text{cov}[\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{w},\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})]\\&=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})\\&=\beta^{-1}k(\boldsymbol{x},\boldsymbol{x}^{\prime})\end{aligned}\tag{3.39}</script><p>⽤核函数表⽰线性回归给出了解决回归问题的另⼀种⽅法。通过不引⼊⼀组基函数（它隐式地定义了⼀个等价的核），⽽是直接定义⼀个局部的核函数，然后在给定观测数据集的条件下， 使⽤这个核函数对新的输⼊变量  $\boldsymbol{x}$ 做预测。 这就引出了⽤于回归问题（以及分类问题）的⼀个很实⽤的框架，被称为<strong>⾼斯过程</strong>（<code>Gaussian process</code>）。</p><p>⼀个等价核定义了模型的权值，通过这个权值，训练数据集⾥的⽬标值被组合，然后对新的 $\boldsymbol{x}$ 值做预测，可以证明这些权值的和等于1，即</p><script type="math/tex; mode=display">\sum_{n=1}^{N}k(\boldsymbol{x},\boldsymbol{x}_{n})=1\tag{3.40}</script><p>对于所有的 $\boldsymbol{x}$ 值都成⽴。</p><p>公式(3.38)给出的等价核满⾜⼀般的核函数共有的⼀个<strong>重要性质</strong>：可以表⽰为⾮线性函数的向量 $\boldsymbol{\psi}(\boldsymbol{x})$ 的内积的形式，即</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{z})=\boldsymbol{\psi}(\boldsymbol{x})^{T}\boldsymbol{\psi}(\boldsymbol{z})\tag{3.41}</script><p>其中，$\boldsymbol{\psi}(\boldsymbol{x})=\beta^{\frac{1}{2}}\boldsymbol{S}_{N}^{\frac{1}{2}}\boldsymbol{\phi}(\boldsymbol{x})$ 。</p><h1 id="二，贝叶斯模型比较"><a href="#二，贝叶斯模型比较" class="headerlink" title="二，贝叶斯模型比较"></a>二，贝叶斯模型比较</h1><p>假设我们想⽐较 $L$ 个模型 ${\mathcal{M}_i}$ ，其中 $i=1, \dots,L$ ，这⾥，⼀个模型指的是观测数据 $\mathcal{D}$ 上的概率分布。在多项式曲线拟合的问题中，概率分布被定义在⽬标值 $\mathbf{t}$ 上， ⽽输⼊值 $\mathbf{X}$ 被假定为已知的。其他类型的模型定义了 $\mathbf{X}$ 和 $\mathbf{t}$ 上的联合分布。我们会假设数据是由这些模型中的⼀个⽣成的， 但是我们不知道究竟是哪⼀个，其不确定性通过先验概率分布 $p(\mathcal{M}_i)$ 表⽰。给定⼀个训练数据集 $\mathcal{D}$ ，估计后验分布</p><script type="math/tex; mode=display">p(\mathcal{M}_{i}|\mathcal{D})\propto p(\mathcal{M}_{i})p(\mathcal{D}|\mathcal{M}_{i})</script><p>其中，<strong>模型证据</strong>（<code>model evidence</code>） $p(\mathcal{D}|\mathcal{M}_{i})$ ，也叫<strong>边缘似然</strong>（<code>marginal likelihood</code>），它表达了数据展现出的不同模型的优先级，也可以被看做在模型空间中的似然函数，在这个空间中参数已经被求和或者积分。两个模型的模型证据的⽐值 $\frac{p(\mathcal{D}|\mathcal{M}_{i})}{p(\mathcal{D}|\mathcal{M}_{j})}$ 被称为<strong>贝叶斯因⼦</strong>（<code>Bayes factor</code>）（<code>Kass and Raftery</code>, 1995）。</p><p>⼀旦知道了模型上的后验概率分布，那么根据概率的加和规则与乘积规则，预测分布为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D})=\sum_{i=1}^{L}p(t|\boldsymbol{x},\mathcal{M}_{i},\mathcal{D})p(\mathcal{M}_{i}|\mathcal{D})\tag{3.42}</script><p>对于模型求平均的⼀个简单的近似是使⽤最可能的⼀个模型⾃⼰做预测，这被称为<strong>模型选择</strong> （<code>model selection</code>）。</p><p>对于⼀个由参数 $\boldsymbol{w}$ 控制的模型，根据概率的加和规则和乘积规则，模型证据为</p><script type="math/tex; mode=display">p(\mathcal{D}|\mathcal{M}_{i})=\int p(\mathcal{D}|\boldsymbol{w},\mathcal{M}_{i})p(\boldsymbol{w}|\mathcal{M}_{i})\mathrm{d}\boldsymbol {w}\tag{3.43}</script><p>⾸先考虑模型有⼀个参数 $w$ 的情形。这个参数的后验概率正⽐于 $p(\mathcal{D}|w)p(w)$ ，其中为了简化记号，我们省略 了它对于模型  ${\mathcal{M}_i}$ 的依赖。 假设后验分布在最⼤似然值 $w_{MAP}$ 附近是⼀个尖峰，宽度为 $\Delta w_{后验}$ ，那么可以⽤被积函数的值乘以尖峰的宽度来近似这个积分。进⼀步假设先验分布是平的，宽度为 $\Delta w_{先验}$ ，即 $p(w)=\frac{1}{\Delta w_{先验}}$ ，那么有</p><script type="math/tex; mode=display">\begin{aligned}p(\mathcal{D})&=\int p(\mathcal{D}|w)p(w)\mathrm{d}w \\ &\simeq p(\mathcal{D}|w_{MAP})\frac{\Delta w_{后验}}{\Delta w_{先验}}\end{aligned}\tag{3.44}</script><p>取对数，可得</p><script type="math/tex; mode=display">\ln p(\mathcal{D})\simeq \ln p(\mathcal{D}|w_{MAP}) + \ln\left(\frac{\Delta w_{后验}}{\Delta w_{先验}}\right)\tag{3.45}</script><p>其中，式中第⼀项表⽰拟合由最可能参数给出的数据，对于平的先验分布来说，这对应于对数似然；第⼆项⽤于根据模型的复杂度来惩罚模型。</p><p>如图3.26，近似模型证据，如果我们假设参数上的后验概率分布在众数 $w_{MAP}$ 附近有⼀个尖峰。</p><p><img src="/images/prml_20191006201043.png" alt="近似模型证据"></p><p>对于⼀个有 $M$ 个参数的模型，可以对每个参数进⾏类似的近似。假设所有的参数 $\frac{\Delta w_{后验}}{\Delta w_{先验}}$ 都相同，则有</p><script type="math/tex; mode=display">\ln p(\mathcal{D})\simeq \ln p(\mathcal{D}|w_{MAP}) + M\ln\left(\frac{\Delta w_{后验}}{\Delta w_{先验}}\right)\tag{3.46}</script><p>如图3.27，对于三个具有不同复杂度的模型，数据集的概率分布的图形表⽰，其中 $\mathcal{M}_{1}$ 是最简单的，$\mathcal{M}_{3}$ 是 最复杂的。</p><p><img src="/images/prml_20191006201647.png" alt="M个参数的模型"></p><p><strong>贝叶斯模型⽐较框架</strong>中隐含的⼀个假设是，⽣成数据的真实的概率分布包含在考虑的模型集合当中。如果这个假设确实成⽴，那么可以证明，平均来看，贝叶斯模型⽐较会倾向于选择出正确的模型。</p><p>考虑两个模型 $\mathcal{M}_{1}$ 和 $\mathcal{M}_{2}$ ，其中真实的概率分布对应于模型 $\mathcal{M}_{1}$ 。对于给定的有限数据集，确实有可能出现错误的模型反⽽使贝叶斯因⼦较⼤的事情。 但是，如果把贝叶斯因⼦在数据集分布上进⾏平均，那么可以得到期望贝叶斯因⼦，即关于数据的真实分布求的平均值：</p><script type="math/tex; mode=display">\int p(\mathcal{D}|\mathcal{M}_{1})\ln \frac{p(\mathcal{D}|\mathcal{M}_{1})}{p(\mathcal{D}|\mathcal{M}_{2})}\mathrm{d}\mathcal{D}</script><h1 id="三，证据近似"><a href="#三，证据近似" class="headerlink" title="三，证据近似"></a>三，证据近似</h1><p>⾸先对参数 $\boldsymbol{w}$ 求积分， 得到边缘似然函数（<code>marginal likelihood function</code>），然后通过最⼤化边缘似然函数，确定超参数的值。 这个框架在统计学的⽂献中被称为<strong>经验贝叶斯</strong>（<code>empirical Bayes</code>）（<code>Bernardo and Smith</code>, 1994; <code>Gelman et al.</code>, 2004），或者被称为<strong>第⼆类最⼤似然</strong>（<code>type 2 maximum likelihood</code>）（<code>Berger</code>, 1985），或者被称为<strong>推⼴的最⼤似然</strong>（<code>generalized maximum likelihood</code>）。在机器学习的⽂献中， 这种⽅法也被称为<strong>证据近似</strong>（<code>evidence approximation</code>）（<code>Gull</code>, 1989; <code>MacKay</code>, 1992<code>a</code>）。</p><p>引⼊ $\alpha$ 和 $\beta$ 上的超先验分布，那么预测分布可以通过对 $\boldsymbol{w}$ ,  $\alpha$ 和 $\beta$ 求积分的⽅法得到， 即</p><script type="math/tex; mode=display">p(t|\mathbf{t})=\iiint p(t|\boldsymbol{w},\beta)p(\boldsymbol{w}|\mathbf{t},\alpha,\beta)p(\alpha,\beta|\mathbf{t})\mathrm{d}\boldsymbol{w} \mathrm{d}\alpha \mathrm{d}\beta\tag{3.47}</script><p>如果后验分布 $p(\alpha,\beta|\mathbf{t})$ 在 $\hat{\alpha}$ 和 $\hat{\beta}$ 附近有尖峰，那么预测分布可以通过对 $\boldsymbol{w}$ 积分的⽅式简单地得到，其中 $\alpha$ 和 $\beta$ 被固定为 $\hat{\alpha}$ 和 $\hat{\beta}$  ，即</p><script type="math/tex; mode=display">\begin{aligned}p(t|\mathbf{t}) &\simeq p(t|\mathbf{t},\hat{\alpha},\hat{\beta})\\&=\int p(t|\boldsymbol{w},\hat{\beta})p(\boldsymbol{w}|\mathbf{t},\hat{\alpha},\hat{\beta})\mathrm{d}\boldsymbol{w}\end{aligned}\tag{3.48}</script><h2 id="1，计算证据函数"><a href="#1，计算证据函数" class="headerlink" title="1，计算证据函数"></a>1，计算证据函数</h2><p>边缘似然函数 $p(\mathbf{t}|\alpha,\beta)$ 是通过对权值参数 $\boldsymbol{w}$ 进⾏积分得到的，即</p><script type="math/tex; mode=display">p(\mathbf{t}|\alpha,\beta)=\int p(\mathbf{t}|\boldsymbol{w},\beta)p(\boldsymbol{w}|\alpha)\mathrm{d}\boldsymbol{w}\tag{3.49}</script><p>根据以前的相关公式，也可以写成</p><script type="math/tex; mode=display">p(\mathbf{t}|\alpha,\beta)=\left(\frac{\beta}{2\pi}\right)^{\frac{N}{2}}\left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2}}\int \exp\{-E(\boldsymbol{w})\}\mathrm{d}\boldsymbol{w}\tag{3.50}</script><p>其中 $M$ 是 $\boldsymbol{w}$ 的维数，并且，</p><script type="math/tex; mode=display">\begin{aligned}E(\boldsymbol{w})&=\beta E_{D}(\boldsymbol{w})+\alpha E_{W}(\boldsymbol{w})\\&=\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\boldsymbol{w}||^{2}+\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}\end{aligned}\tag{3.51}</script><p>现在对 $\boldsymbol{w}$ 配平⽅，可得</p><script type="math/tex; mode=display">E(\boldsymbol{w})=E(\boldsymbol{m}_{N})+\frac{1}{2}(\boldsymbol{w}-\boldsymbol{m}_{N})^{T}\boldsymbol{A}(\boldsymbol{w}-\boldsymbol{m}_{N})\tag{3.52}</script><p>令</p><script type="math/tex; mode=display">\boldsymbol{A}=\alpha\boldsymbol{I}+\beta\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\\\boldsymbol{m}_{N}=\beta \boldsymbol{A}^{-1}\boldsymbol{\Phi}\mathbf{t}\\E(\boldsymbol{m}_{N})=\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\boldsymbol{m}_{N}||^{2}+\frac{\alpha}{2}\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}</script><p><strong>注意</strong> $\boldsymbol{A}$ 对应于误差函数的⼆阶导数 $\boldsymbol{A} = \nabla\nabla E(\boldsymbol{w})$ 被称为 <strong><code>Hessian</code>矩阵</strong>。</p><p>经计算，可得边缘似然函数的对数，即证据函数的表达式：</p><script type="math/tex; mode=display">\ln p(\mathbf{t}|\alpha,\beta)=\frac{M}{2}\ln\alpha+\frac{N}{2}\ln\beta-E(\boldsymbol{m}_{N})-\frac{1}{2}\ln |\boldsymbol{A}|-\frac{N}{2}\ln(2\pi)\tag{3.53}</script><p>如图3.28，模型证据与多项式阶数之间的关系。</p><p><img src="/images/prml_20191007134944.png" alt="模型证据与多项式阶数之间的关系"></p><h2 id="2，最⼤化证据函数"><a href="#2，最⼤化证据函数" class="headerlink" title="2，最⼤化证据函数"></a>2，最⼤化证据函数</h2><p>⾸先考虑 $p(\mathbf{t}|\alpha,\beta)$ 关于 $\alpha$ 的最⼤化，定义下⾯的特征向量⽅程</p><script type="math/tex; mode=display">(\beta\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})\boldsymbol{\mu}_{i}=\lambda_{i}\boldsymbol{\mu}_{i}\tag{3.54}</script><p>根据公式，可知 $\boldsymbol{A}$ 的特征值为 $\alpha + \lambda_{i}$ 。 现在考虑公式(3.53)中涉及到 $\ln|\boldsymbol{A}|$ 的项关于 $\alpha$ 的导数</p><script type="math/tex; mode=display">\frac{\mathrm{d}}{\mathrm{d} \alpha} \ln |\boldsymbol{A}|=\frac{\mathrm{d}}{\mathrm{d} \alpha} \ln \prod_{i}\left(\lambda_{i}+\alpha\right)=\frac{\mathrm{d}}{\mathrm{d} \alpha} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)=\sum_{i} \frac{1}{\lambda_{i}+\alpha}\tag{3.55}</script><p>因此函数公式(3.53)关于 $\alpha$ 的驻点满⾜</p><script type="math/tex; mode=display">0=\frac{M}{2\alpha}-\frac{1}{2}\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}-\frac{1}{2}\sum_{i}\frac{1}{\lambda_{i}+\alpha}</script><p>整理，有</p><script type="math/tex; mode=display">\alpha\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}=M-\alpha\sum_{i}\frac{1}{\lambda_{i}+\alpha}=\gamma</script><p>由于 $i$ 的求和式中⼀共有 $M$ 项，因此 $\gamma$ 可以写成</p><script type="math/tex; mode=display">\gamma=\sum_{i}\frac{\lambda_{i}}{\alpha+\lambda_{i}}\tag{3.56}</script><p>因而，最⼤化边缘似然函数的 $\alpha$ 满⾜</p><script type="math/tex; mode=display">\alpha=\frac{\gamma}{\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}}\tag{3.57}</script><p><strong>注意</strong>： $\alpha$ 的值是纯粹通过观察训练集确定的。</p><p>类似地，关于 $\beta$ 最⼤化对数边缘似然函数，注意到公式(3.54)定义的特征值 $\lambda_{i}$ 正⽐于 $\beta$ ，因此  $\frac{\mathrm{d}}{\mathrm{d}\beta}=\frac{\lambda}{\beta_{i}}$ 。于是</p><script type="math/tex; mode=display">\frac{\mathrm{d}}{\mathrm{d} \beta} \ln |\boldsymbol{A}|=\frac{\mathrm{d}}{\mathrm{d} \beta} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)=\frac{1}{\beta}\sum_{i} \frac{\lambda_{i}}{\lambda_{i}+\alpha}=\frac{\gamma}{\beta}\tag{3.58}</script><p>边缘似然函数的驻点因此满⾜</p><script type="math/tex; mode=display">0=\frac{N}{2\beta}-\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}-\frac{\gamma}{2\beta}</script><p>整理，可以得到最⼤化边缘似然函数的 $\beta$ 满⾜</p><script type="math/tex; mode=display">\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}\tag{3.59}</script><h2 id="3，参数的有效数量"><a href="#3，参数的有效数量" class="headerlink" title="3，参数的有效数量"></a>3，参数的有效数量</h2><p>如图3.29，似然函数的轮廓线（红⾊）和先验概率分布（绿⾊），其中参数空间中的坐标轴被旋转，与<code>Hessian</code>矩阵的特征向量 $\boldsymbol{\mu}_i$ 对齐。</p><p><img src="/images/prml_20191007143628.png" alt="似然函数的轮廓线"></p><p>考察单⼀变量 $x$ 的⾼斯分布的⽅差的最⼤似然估计为</p><script type="math/tex; mode=display">\sigma_{ML}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}\tag{3.60}</script><p>这个估计是有偏的，因为均值的最⼤似然解 $\mu_{ML}$ 拟合了数据中的⼀些噪声。从效果上来看，这占⽤了模型的⼀个⾃由度。对应的⽆偏的估计形式为</p><script type="math/tex; mode=display">\sigma_{MAP}^{2}=\frac{1}{N-1}\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}\tag{3.61}</script><p>分母中的因⼦ $N−1$ 反映了模型中的⼀个⾃由度被⽤于拟合均值的事实，它抵消了最⼤似然解的偏差。</p><p>如图3.30，$\gamma$ 与 $\ln\alpha$ 的关系（红⾊曲线）以及 $2\alpha E_{W}(\boldsymbol{m}_{N})$ 与 $\ln\alpha$ 的关系（蓝⾊曲线）， 数据集为正弦数据集。这两条曲线的交点定义了 $\alpha$ 的最优解，由模型证据的步骤给出。</p><p><img src="/images/prml_20191007145425.png" alt="证据框架来确定α"></p><p>如图3.31，对应的对数证据 $\ln p(\mathbf{t}|\alpha,\beta)$ 关于 $\ln\alpha$ 的图像（红⾊曲线），说明了峰值与图3.30中曲线的交点恰好重合。同样给出的时测试集误差（蓝⾊曲线），说明模型证据最⼤值的位置接近于具有最好泛化能⼒的点。</p><p><img src="/images/prml_20191007145440.png" alt="对数证据"></p><p>如图3.32，独⽴的参数关于有效参数数量 $\gamma$ 的函数图像。⾼斯基函数模型中的10个参数 $w_i$ 与参数有效数量 $\gamma$ 的关系，其中超参数的变化范围为 $0\le\alpha\le\infty$，使得 $\gamma$ 的变化范围为 $0\le\gamma\le M$ 。</p><p><img src="/images/prml_20191007155836.png" alt="独⽴的参数关于有效参数数量"></p><p>如果考虑极限情况 $N\gg M$ ， 数据点的数量⼤于参数的数量，那么根据公式，所有的参数都可以根据数据良好确定。因为 $\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}$ 涉及到数据点的隐式求和，因此特征值 $\lambda_{i}$ 随着数据集规模的增加⽽增⼤。在这种情况下，$\gamma = M$ ，并且 $\alpha$ 和 $\beta$ 的重新估计⽅程变为</p><script type="math/tex; mode=display">\alpha=\frac{M}{2E_{W}(\boldsymbol{m}_{N})}\\\beta=\frac{N}{2E_{D}(\boldsymbol{m}_{N})}</script><h1 id="四，固定基函数的局限性"><a href="#四，固定基函数的局限性" class="headerlink" title="四，固定基函数的局限性"></a>四，固定基函数的局限性</h1><p>基函数的数量随着输⼊空间的维度 $D$ 迅速增长，通常是指数⽅式的增长。</p><p>真实数据集有两个<strong>性质</strong>：第⼀， 数据向量 $\{\boldsymbol{x}_n\}$ 通常位于⼀个⾮线性流形内部。由于输⼊变量之间的相关性，这个流形本⾝的维度⼩于输⼊空间的维度。如果我们使⽤局部基函数，那么可以让基函数只分布在输⼊空间中包含数据的区域。这种⽅法被⽤在径向基函数⽹络中，也被⽤在⽀持向量机和相关向量机当中。神经⽹络模型使⽤可调节的基函数，这些基函数有着<code>sigmoid</code>⾮线性的性质。神经⽹络可以通过调节参数，使得在输⼊空间的区域中基函数会按照数据流形发⽣变化。第⼆，⽬标变量可能只依赖于数据流形中的少量可能的⽅向。利⽤这个性质，神经⽹络可以通过选择输⼊空间中基函数产⽣响应的⽅向。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，贝叶斯线性回归&quot;&gt;&lt;a href=&quot;#一，贝叶斯线性回归&quot; class=&quot;headerlink&quot; title=&quot;一，贝叶斯线性回归&quot;&gt;&lt;/a&gt;一，贝
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】线性基函数模型</title>
    <link href="https://zhangbc.github.io/2019/10/07/prml_03_01/"/>
    <id>https://zhangbc.github.io/2019/10/07/prml_03_01/</id>
    <published>2019-10-07T08:16:48.000Z</published>
    <updated>2019-10-07T15:22:18.873Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，线性基函数模型"><a href="#一，线性基函数模型" class="headerlink" title="一，线性基函数模型"></a>一，线性基函数模型</h1><h2 id="1，线性基函数"><a href="#1，线性基函数" class="headerlink" title="1，线性基函数"></a>1，线性基函数</h2><p>回归问题的<strong>⽬标</strong>是在给定 $D$ 维输⼊（<code>input</code>） 变量 $\boldsymbol{x}$ 的情况下， 预测⼀个或者多个连续⽬标（<code>target</code>）变量 $t$ 的值。</p><p>通过将⼀组输⼊变量的⾮线性函数进⾏线性组合， 我们可以获得⼀类更加有⽤的函数， 被称为<strong>基函数</strong>（<code>basis function</code>）。</p><p>回归问题的最简单模型是输⼊变量的<strong>线性组合</strong>：</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w}) = w_0+w_1x_1+\dots+w_Dx_D\tag{3.1}</script><p>其中，$\boldsymbol{x}=(x_1,x_2,\dots,x_D)^T$ ，通常称为<strong>线性回归</strong>（<code>linear regression</code>），这个模型的<strong>关键性质</strong>在于它是参数 $w_0 ,\dots ,w_D$ 的⼀个线性函数。 但是， 它也是输⼊变量 $x_i$ 的⼀个线性函数， 这给模型带来了极⼤的局限性。因此扩展模型的类别：将输⼊变量的固定的⾮线性函数进⾏线性组合：</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w}) = w_0+\sum_{j=1}^{M-1}w_{j}\phi_{j}(\boldsymbol{x})\tag{3.2}</script><p>其中， $\phi_{j}(\boldsymbol{x})$ 被称为<strong>基函数</strong>（<code>basis function</code>），参 数 $w_0$ 使得数据中可以存在任意固定的偏 置，这个值通常被称为<strong>偏置参数</strong>（<code>bias parameter</code>）。此模型称为<strong>线性模型</strong>。</p><p>通常，定义⼀个额外的<strong>虚“基函数”</strong> $\phi_{0}(\boldsymbol{x}) = 1$ 是很⽅便的，这时，</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w}) = \sum_{j=0}^{M-1}w_{j}\phi_{j}(\boldsymbol{x}) = \boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x})\tag{3.3}</script><p>其中，$\boldsymbol{w}=(w_0,x_1,\dots,w_{M-1})^T$ ，$\boldsymbol{\phi}=(\phi_0,\phi_2,\dots,\phi_{M-1})^T$  。</p><p>在许多模式识别的实际应⽤中， 我们会对 原始的数据变量进⾏某种固定形式的预处理或者特征抽取。如果原始变量由向量 $\boldsymbol{x}$ 组成，那么特征可以⽤基函数 $\{\phi_{j}(\boldsymbol{x})\}$ 来表⽰。</p><p><strong>多项式基函数</strong>的⼀个<strong>局限性</strong>在于它们是输⼊变量的全局函数，因此对于输⼊空间⼀个区域的改变将会影响所有其他的区域。这个问题的解决方案：把输⼊空间切分成若⼲个区域，然后对于每个区域⽤不同的多项式函数拟合，这样的函数叫做<strong>样条函数</strong>（<code>spline function</code>）（<code>Hastie et al.</code>, 2001）。</p><p><strong>⾼斯基函数</strong>：</p><script type="math/tex; mode=display">\phi_{j}(x)=\exp\left\{-\frac{(x-\mu_{j})^2}{2s^{2}}\right\}\tag{3.4}</script><p>其中，$\mu_{j}$ 控制了基函数在输⼊空间中的位置，参数 $s$ 控制了基函数的空间⼤⼩。</p><p><strong><code>sigmoid</code>基函数</strong>：</p><script type="math/tex; mode=display">\phi_{j}(x)=\sigma\left(\frac{x-\mu_{j}}{s}\right)\tag{3.5}</script><p>其中 $\sigma(a)$ 是 <strong><code>logistic sigmoid</code>函数</strong>，定义为：</p><script type="math/tex; mode=display">\sigma_{a}=\frac{1}{1+\exp(-a)}\tag{3.6}</script><p>除此之外，基函数还可以选择<strong>傅⾥叶基函数</strong>，<strong><code>tanh</code>函数</strong>等等。其中，<strong><code>tanh</code>函数</strong> 和 <strong><code>logistic sigmoid</code>函数</strong> 的关系如下：$\tanh(a)=2\sigma(2a)-1$。</p><p>如图3.1～3.3，分别为是多项式基函数，⾼斯基函数，<code>sigmoid</code>基函数。</p><p><img src="/images/prml_20190929233008.png" alt="多项式基函数"></p><p><img src="/images/prml_20190929 233018.png" alt="⾼斯基函数"></p><p><img src="/images/prml_20190929233035.png" alt="sigmoid基函数"></p><h2 id="2，最⼤似然与最⼩平⽅"><a href="#2，最⼤似然与最⼩平⽅" class="headerlink" title="2，最⼤似然与最⼩平⽅"></a>2，最⼤似然与最⼩平⽅</h2><p>假设⽬标变量 $t$ 由确定的函数 $y(\boldsymbol{x},\boldsymbol{w})$ 给出，这个函数被附加了<strong>⾼斯噪声</strong>，即</p><script type="math/tex; mode=display">t=y(\boldsymbol{x},\boldsymbol{w})+\epsilon</script><p>其中，$\epsilon$ 是⼀个零均值的⾼斯随机变量，精度（⽅差的倒数）为 $\beta$，则有：</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w},\beta)=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1})\tag{3.7}</script><p>均值为：</p><script type="math/tex; mode=display">\mathbb{E}[t|\boldsymbol{x}]=\int tp(t|\boldsymbol{x})\mathrm{d}t=y(\boldsymbol{x},\boldsymbol{w})</script><p>考虑⼀个输⼊数据集 $\mathbf{X}=\{\boldsymbol{x}_1,\dots, \boldsymbol{x}_N\}$， 对应的⽬标值为 $t_1,\dots , t_N$ 。 我们把⽬标向量 $\{t_n\}$ 组成⼀个列向量， 记作 $\mathbf{t}$。 假设这些数据点是独⽴地从分布公式(3.7)中抽取的，那么可以得到下⾯的似然函数的表达式， 它是可调节参数 $\boldsymbol{w}$ 和 $\beta$ 的函数，形式为：</p><script type="math/tex; mode=display">p(\mathbf{t}|\mathbf{X},\boldsymbol{w},\beta)=\prod_{n=1}^{N}\mathcal{N}(t_{n}|\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n}),\beta^{-1})\tag{3.8}</script><p>取似然函数的对数，使⽤⼀元⾼斯分布的标准形式，可得：</p><script type="math/tex; mode=display">\begin{aligned}\ln p(\mathbf{t}|\boldsymbol{w},\beta)&=\sum_{n=1}^{N}\ln \mathcal{N}(t_{n}|\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n}),\beta^{-1}) \\ &= \frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)-\beta E_{D}(\boldsymbol{w}) \end{aligned}\tag{3.9}</script><p>其中，<strong>平⽅和误差函数</strong>的定义为：</p><script type="math/tex; mode=display">E_{D}(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}\tag{3.10}</script><p>对数似然函数的梯度为：</p><script type="math/tex; mode=display">\nabla\ln p(\mathbf{t}|\boldsymbol{w},\beta)=\beta\sum_{n=1}^{N}\{t_{n}-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}\boldsymbol{\phi}(\boldsymbol{x}_{n})^{T}\tag{3.11}</script><p>令梯度等于零，求解 $\boldsymbol{w}$ 可得：</p><script type="math/tex; mode=display">\boldsymbol{w}_{ML}=(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\mathbf{t}\tag{3.12}</script><p>这被称为<strong>最⼩平⽅问题的规范⽅程</strong>（<code>normal equation</code>）。这⾥ $\boldsymbol{\Phi}$ 是⼀个 $N \times M$ 的矩阵，被称为<strong>设计矩阵</strong>（<code>design matrix</code>），它的元素为 $\Phi_{nj}=\phi_{j}(\boldsymbol{x}_{n})$ ，即</p><script type="math/tex; mode=display">\mathbf{\Phi}=\left(\begin{array}{cccc}{\phi_{0}\left(\boldsymbol{x}_{1}\right)} & {\phi_{1}\left(\boldsymbol{x}_{1}\right)} & {\cdots} & {\phi_{M-1}\left(\boldsymbol{x}_{1}\right)} \\ {\phi_{0}\left(\boldsymbol{x}_{2}\right)} & {\phi_{1}\left(\boldsymbol{x}_{2}\right)} & {\cdots} & {\phi_{M-1}\left(\boldsymbol{x}_{2}\right)} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\phi_{0}\left(\boldsymbol{x}_{N}\right)} & {\phi_{1}\left(\boldsymbol{x}_{N}\right)} & {\cdots} & {\phi_{M-1}\left(\boldsymbol{x}_{N}\right)}\end{array}\right)</script><p>其中，量</p><script type="math/tex; mode=display">\mathbf{\Phi}^{\dagger} \equiv\left(\mathbf{\Phi}^{T} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{T}</script><p>被称为矩阵 $\mathbf{\Phi}$ 的 <strong><code>Moore-Penrose</code>伪逆矩阵</strong>（<code>pseudo-inverse matrix</code>）（<code>Rao and Mitra</code>, 1971; <code>Golub and Van Loan</code>, 1996）。</p><p>图3.4，最⼩平⽅解的⼏何表⽰，在⼀个 $N$ 维空间中，坐标轴是 $t_1,\dots , t_N$ 的值。<strong>最⼩平⽅回归函数</strong>可以通过下⾯的⽅式得到：寻找数据向量 $\mathbf{t}$ 在由基函数 $\phi_{j}(\boldsymbol{x})$ 张成的⼦空间上的正交投影，其中每个基函数都可以看成⼀个长度为 $N$ 的向量 $\varphi_j$ ，它的元素为 $\phi_{j}(\boldsymbol{x}_{n})$ 。注意， $\varphi_j$ 对应于 $\mathbf{\Phi}$ 的第 $j$ 列， ⽽ $\boldsymbol{\phi}(\boldsymbol{x}_{n})$ 对应于 $\mathbf{\Phi}$ 的第 $i$ ⾏。</p><p><img src="/images/prml_20190930113538.png" alt="最⼩平⽅解的⼏何表⽰"></p><p>如果显式地写出偏置参数，那么误差函数公式(3.10)变为：</p><script type="math/tex; mode=display">E_{D}(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-w_{0}-\sum_{j=1}^{M-1}w_{j}\phi_{j}(\boldsymbol{x}_{n})\}^{2}\tag{3.13}</script><p>令关于 $w_0$ 的导数等于零，解出 $w_0$ ，可得</p><script type="math/tex; mode=display">w_0=\bar{t}-\sum_{j=1}^{M-1}w_{j}\bar\phi_{j}</script><p>其中，</p><script type="math/tex; mode=display">\bar{t}=\frac{1}{N}\sum_{n=1}^{N}t_{n} \\ \bar{\phi}_{j}=\frac{1}{N}\sum_{n=1}^{N}\phi_{j}(\boldsymbol{x}_n)</script><p>因此，偏置 $w_0$ 补偿了⽬标值的平均值（在训练集上的）与基函数的值的平均值的加权求和之间的差。 </p><p>关于噪声精度参数 $\beta$ 最⼤化似然函数公式(3.9)，结果为：</p><script type="math/tex; mode=display">\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{w}_{ML}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}\tag{3.14}</script><p>因此，噪声精度的倒数由⽬标值在回归函数周围的<strong>残留⽅差</strong>（<code>residual variance</code>）给出。</p><h2 id="3，顺序学习"><a href="#3，顺序学习" class="headerlink" title="3，顺序学习"></a>3，顺序学习</h2><p>顺序算法中，每次只考虑⼀个数据点，模型的参数在每观测到⼀个数据点之后进⾏更新。顺序学习也适⽤于实时的应⽤，在实时应⽤中，数据观测以⼀个连续的流的⽅式持续到达，我们必须在观测到所有数据之前就做出预测。</p><p>我们可以获得⼀个顺序学习的算法通过考虑随机梯度下降（<code>stochastic gradient descent</code>）也 被称为<strong>顺序梯度下降</strong>（<code>sequential gradient descent</code>）的⽅法。 如果误差函数由数据点的和组成 $E = \sum_{n} E_n$ ，那么在观测到模式 $n$ 之后，随机梯度下降算法使⽤下式更新参数向量 $\boldsymbol{w}$ ：</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{(\tau)}-\eta\nabla E_{n}\tag{3.15}</script><p>其中 $\tau$ 表⽰迭代次数，$\eta$  是学习率参数。 $\boldsymbol{w}$ 被初始化为某个起始向 量  $\boldsymbol{w}^{(0)}$ 。对于平⽅和误差函数公式(3.10)的情形，我们有：</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{(\tau)}+\eta(t_{n}-\boldsymbol{w}^{(\tau)T}\boldsymbol{\phi}_{n})\boldsymbol{\phi}_{n}\tag{3.16}</script><p>其中 $\boldsymbol{\phi}_{n}=\boldsymbol{\phi}(\boldsymbol{x}_{n})$。 这被称为<strong>最⼩均⽅</strong>（<code>least-mean-squares</code>）或者 <strong><code>LMS</code>算法</strong>。$\eta$ 的值需要仔细选择，确保<strong>算法收敛</strong>（<code>Bishop and Nabney</code>, 2008）。</p><h2 id="4，正则化最⼩平⽅"><a href="#4，正则化最⼩平⽅" class="headerlink" title="4，正则化最⼩平⽅"></a>4，正则化最⼩平⽅</h2><p>为误差函数添加正则化项的思想来控制过拟合，因此需要最⼩化的总的误差函数的形式为</p><script type="math/tex; mode=display">E_{D}(\boldsymbol{w})+\lambda E_{W}(\boldsymbol{w})</script><p>其中 $\lambda$ 是正则化系数，正则化项的⼀个最简单的形式为权向量的各个元素的平⽅和</p><script type="math/tex; mode=display">E_{W}(\boldsymbol{w})=\frac{1}{2}\boldsymbol{w}^{T}\boldsymbol{w}</script><p>考虑平⽅和误差函数</p><script type="math/tex; mode=display">E_{D}(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}\tag{3.17}</script><p>那么总误差函数就变成了</p><script type="math/tex; mode=display">\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}+\frac{\lambda}{2}\boldsymbol{w}^{T}\boldsymbol{w}</script><p>这种对于正则化项的选择⽅法在机器学习的⽂献中被称为<strong>权值衰减</strong>（<code>weight decay</code>）。</p><p>令总误差函数关于 $\boldsymbol{w}$ 的梯度等于零，解出 $\boldsymbol{w}$ ，有：</p><script type="math/tex; mode=display">\boldsymbol{w}=(\lambda \boldsymbol{I}+\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\boldsymbol{t}\tag{3.18}</script><p>有时使⽤⼀个更加⼀般的正则化项，这时正则化的误差函数的形式为：</p><script type="math/tex; mode=display">\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}+\frac{\lambda}{2}\sum_{j=1}^{M}|w_{j}|^{q}\tag{3.19}</script><p>如图3.5～3.8，对于不同的参数 $q$，公式(3.19)中的正则化项的轮廓线。</p><p><img src="/images/prml_20191003214249.png" alt="q=0.5"></p><p><img src="/images/prml_20191003214301.png" alt="q=1"></p><p><img src="/images/prml_20191003214316.png" alt="q=2"></p><p><img src="/images/prml_20191003214325.png" alt="q=4"></p><p>在统计学的⽂献中，$q=1$ 的情形被称为<strong>套索</strong>（<code>lasso</code>）（<code>Tibshirani</code>, 1996）。它的<strong>性质</strong>为：如果 $\lambda$ 充分⼤，那么某些系数 $w_j$ 会变为零，从⽽产⽣了⼀个<strong>稀疏</strong>（<code>sparse</code>）<strong>模型</strong>，这个模型中对应的基函数不起作⽤。</p><p>如图3.9，$q=2$ 的⼆次正则化项的限制区域。</p><p><img src="/images/prml_20191004154353.png" alt="q=2"></p><p>如图3.10，$q=1$ 的套索正则化项的限制区域。</p><p><img src="/images/prml_20191004154407.png" alt="q=1"></p><h2 id="5，多个输出"><a href="#5，多个输出" class="headerlink" title="5，多个输出"></a>5，多个输出</h2><p>对于预测 $K&gt;1$ 个⽬标变量，我们把这些⽬标变量聚集起来，记作⽬标向量 $\boldsymbol{t}$ ，其解决方案是：对于 $\boldsymbol{t}$ 的每个分量，引⼊⼀个不同的基函数集合，从⽽变成了多个独⽴的回归问题。但是，⼀个更有趣的并且更常⽤的⽅法是对⽬标向量的所有分量使⽤⼀组相同的基函数来建模，即：</p><script type="math/tex; mode=display">\boldsymbol{y}(\boldsymbol{x}, \boldsymbol{w})=\boldsymbol{W}^{T}\phi(\boldsymbol{x})\tag{3.20}</script><p>其中 $\boldsymbol{y}$ 是⼀个 $K$ 维列向量，$\boldsymbol{W}$ 是⼀个 $M\times K$ 的参数矩阵，$\phi(\boldsymbol{x})$ 是⼀个 $M$ 为列向量， 每个元素 为 $\phi_{j}(\boldsymbol{x})$ ，$\phi_{0}(\boldsymbol{x}) = 1$ 。 假设令⽬标向量的条件概率分布是⼀个各向同性的⾼斯分布，形式为：</p><script type="math/tex; mode=display">p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{W},\beta)=\mathcal{N}(\boldsymbol{t}|\boldsymbol{W}^{T}\boldsymbol{\phi}(\boldsymbol{x}),\beta^{-1}\boldsymbol{I})\tag{3.21}</script><p>如果有⼀组观测 $\boldsymbol{t}_1,\dots,\boldsymbol{t}_N$ ，可以把这些观测组合为⼀个 $N \times K$ 的矩阵 $\boldsymbol{T}$ ，使得矩阵的第 $n$ ⾏为 $\boldsymbol{t}_{n}^{T}$ 。类似地，把输⼊向量 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ 组合为矩阵 $\boldsymbol{X}$ 。这样，对数似然函数：</p><script type="math/tex; mode=display">\begin{aligned}\ln p(\mathbf{T}|\boldsymbol{X},\boldsymbol{W},\beta)&=\sum_{n=1}^{N}\ln \mathcal{N}(\boldsymbol{t}_{n}|\boldsymbol{W}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n}),\beta^{-1}\boldsymbol{I}) \\ &= \frac{NK}{2}\ln\left(\frac{\beta}{2\pi}\right)-\frac{\beta}{2}\sum_{n=1}^{N}||\boldsymbol{t}_{n}-\boldsymbol{W}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})||^{2} \end{aligned}\tag{3.22}</script><p>关于 $\boldsymbol{W}$ 最⼤化这个函数，可得：</p><script type="math/tex; mode=display">\boldsymbol{W}_{ML}=(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\boldsymbol{T}\tag{3.23}</script><p>对于每个⽬标变量 $t_k$ 考察这个结果，那么有</p><script type="math/tex; mode=display">\boldsymbol{w}_{k}=(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\boldsymbol{t}_{k}=\mathbf{\Phi}^{\dagger}\boldsymbol{t}_{k}\tag{3.24}</script><p>其中，$\boldsymbol{t}_{k}$ 是⼀个 $N$ 维列向量， 元素为 $t_{nk}$ 其中 $n=1,\dots,N$ 。 因此不同⽬标变量的回归问题在这⾥被分解开，并且我们只需要计算⼀个伪逆矩阵 $\mathbf{\Phi}^{\dagger}$ ，这个矩阵是被所有向量 $\boldsymbol{w}_k$ 所共享的。</p><h1 id="二，-偏置-方差分解"><a href="#二，-偏置-方差分解" class="headerlink" title="二， 偏置-方差分解"></a>二， 偏置-方差分解</h1><p> 假如已知条件概率分布 $p(t|\boldsymbol{x})$，每⼀种损失函数都能够给出对应的最优预测结果。使⽤最多的⼀个选择是平⽅损失函数，此时最优的预测由条件期望（记作 $h(\boldsymbol{x})$ ）给出，即</p><script type="math/tex; mode=display">h(\boldsymbol{x}) = \mathbb{E}[t|\boldsymbol{x}]=\int tp(t|\boldsymbol{x})\mathrm{d}t\tag{3.25}</script><p>考察平⽅损失函数的期望：</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{L}]=\int\{y(\boldsymbol{x})-h(\boldsymbol{x})\}^{2}p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}+\int\int\{h(\boldsymbol{x})-t\}^{2}p(\boldsymbol{x},t)\mathrm{d}\boldsymbol{x}\mathrm{d}t\tag{3.26}</script><p>其中，与 $y(\boldsymbol{x})$ ⽆关的第⼆项，是由数据本⾝的噪声造成的，表⽰期望损失能够达到的最⼩值。第⼀项与对函数 $y(\boldsymbol{x})$ 的选择有关，我们要找⼀个 $y(\boldsymbol{x})$ 的解，使得这⼀项最⼩。由于它是⾮负的，因此我们希望能够让这⼀项的最⼩值等于零。</p><p>考察公式(3.26)的第⼀项被积函数，对于⼀个特定的数据集 $\mathcal{D}$，它的形式为</p><script type="math/tex; mode=display">\{y(\boldsymbol{x};\mathcal{D})-h(\boldsymbol{x})\}^{2}</script><p>由于这个量与特定的数据集 $\mathcal{D}$ 相关，因此对所有的数据集取平均。如果我们在括号内减去然后加上 $\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]$ ，然后展开，有</p><script type="math/tex; mode=display">\begin{aligned}\{y(\boldsymbol{x};\mathcal{D})-\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]+\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]-h(\boldsymbol{x})\}^{2} \\=\{y(\boldsymbol{x};\mathcal{D})-\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]\}^{2}+\{\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]-h(\boldsymbol{x })\}^{2}\\+2\{y(\boldsymbol{x};\mathcal{D})-\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]\}\{\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]-h(\boldsymbol{x})\}\end{aligned}</script><p>现在关于 $\mathcal{D}$ 求期望，然后注意到最后⼀项等于零，可得</p><script type="math/tex; mode=display">\begin{array}{l}{\mathbb{E}_{\mathcal{D}}\left[\{y(\boldsymbol{x} ; \mathcal{D})-h(\boldsymbol{x})\}^{2}\right]} \\ {\quad=\underbrace{\left\{\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x}; \mathcal{D})]-h(\boldsymbol{x})\right\}^{2}}_{(偏置)^{2}}+\underbrace{\mathbb{E}_{\mathcal{D}}\left[\left\{y(\boldsymbol{x} ; \mathcal{D})-\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x} ; \mathcal{D})]\right\}^{2}\right]}_{方差}}\end{array}\tag{3.27}</script><p>其中，$y(\boldsymbol{x};\mathcal{D})$ 与回归函数 $h(\boldsymbol{x})$ 的差的平⽅的期望可以表⽰为两项的和。第⼀项，被称为<strong>平⽅偏置</strong>（<code>bias</code>），表⽰所有数据集的平均预测与预期的回归函数之间的差异。第⼆项，被称为<strong>⽅差</strong>（<code>variance</code>），度量了对于单独的数据集，模型所给出的解在平均值附近波动的情况，因此也就度量了函数 $y(\boldsymbol{x};\mathcal{D})$ 对于特定的数据集的选择的敏感程度。</p><p>综上，对于期望平⽅损失的分解：</p><script type="math/tex; mode=display">期望损失=偏置^{2}+方差+噪声\tag{3.28}</script><p>其中，</p><script type="math/tex; mode=display">偏置^{2}=\int \{\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x};\mathcal{D})]-h(\boldsymbol{x})\}^{2}p(\boldsymbol{x})\mathrm{d}\boldsymbol{x} \\方差=\int \mathbb{E}_{\mathcal{D}}[\{y(\boldsymbol{x} ; \mathcal{D})-\mathbb{E}_{\mathcal{D}}[y(\boldsymbol{x} ; \mathcal{D})]\}^{2}p(\boldsymbol{x} )\mathrm{d}\boldsymbol{x}  \\噪声=\int\int\{h(\boldsymbol{x})-t\}^{2}p(\boldsymbol{x},t)\mathrm{d}\boldsymbol{x}\mathrm{d}t</script><p>对于⾮常灵活的模型来说，偏置较⼩， ⽅差较⼤；对于相对固定的模型来说，偏置较⼤，⽅差较⼩。</p><p>图3.11~3.13，模型复杂度对于偏置和⽅差的依赖的说明。左侧⼀列给出了对于不同的 $\ln \lambda$ 值，根据数据集拟合模型的结果。 为了清晰起见， 只给出了100个拟合模型中的20个。 右侧⼀列给出了对应的100个拟合的均值 （红⾊）以及⽤于⽣成数据集的正弦函数（绿⾊）。</p><p><img src="/images/prml_20191005141816.png" alt="lambda=2.6"></p><p><img src="/images/prml_20191005141838.png" alt="lambda=-0.31"></p><p><img src="/images/prml_20191005141852.png" alt="lambda=-2.4"></p><p><strong>举例</strong>：讨论正弦数据集，我们产⽣了100个数据集合， 每个集合都包含 $N = 25$ 个数据点，都是独⽴地从正弦曲线 $h(x)=\sin(2\pi x)$ 抽取的。数据集的编号为 $l = 1, \dots , L$ ， 其中 $L=100$，并且对于每个数据 集 $\mathcal{D}^{(l)}$ ，我们通过最⼩化正则化的误差函数拟合了⼀个带有24个⾼斯基函数的模型，然 后给出了预测函数 $y^{(l)}(x)$ ，如图3.11~13所⽰。如图3.11，对应着较⼤的正则化系数 $\lambda$，这样的模型的⽅差很⼩（因为左侧图中的红⾊曲线看起来很相似），但是偏置很⼤（因为右侧图中的两条曲线看起来相当不同）。相反，如图3.13，正则化系数 $\lambda$ 很⼩，这样模型的⽅差较⼤（因为左侧图中 的红⾊曲线变化性相当⼤）， 但是偏置很⼩（因为平均拟合的结果与原始正弦曲线⼗分吻合）。注意，把 $M = 25$ 这种复杂模型的多个解进⾏平均，会产⽣对于回归函数⾮常好的拟合， 这表明求平均是⼀个很好的步骤。事实上，将多个解加权平均是贝叶斯⽅法的核⼼，虽然这种求平均针对的是参数的后验分布，⽽不是针对多个数据集。</p><p>对于这个例⼦，我们也可以定量地考察<strong>偏置-⽅差折中</strong>。平均预测由下式求出：</p><script type="math/tex; mode=display">\bar{y}(x)=\frac{1}{L}\sum_{l=1}^{L}y^{(l)}(x)\tag{3.29}</script><p>有，</p><script type="math/tex; mode=display">偏置^{2}=\frac{1}{N}\sum_{n=1}^{N}\{\bar{y}(x_{n})-h(x)\}^{2}</script><script type="math/tex; mode=display">方差=\frac{1}{N}\sum_{n=1}^{N}\frac{1}{L}\sum_{l=1}^{L}\{y^{(l)}(x_{n})-\bar{y}(x_{n})\}^{2}</script><p>图3.14，平⽅偏置和⽅差的图像，以及它们的加和。</p><p><img src="/images/prml_20191005144308.png" alt="平⽅偏置和⽅差"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，线性基函数模型&quot;&gt;&lt;a href=&quot;#一，线性基函数模型&quot; class=&quot;headerlink&quot; title=&quot;一，线性基函数模型&quot;&gt;&lt;/a&gt;一，线
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】概率分布之指数族分布</title>
    <link href="https://zhangbc.github.io/2019/09/29/prml_02_03/"/>
    <id>https://zhangbc.github.io/2019/09/29/prml_02_03/</id>
    <published>2019-09-29T11:34:14.000Z</published>
    <updated>2019-10-07T15:41:45.228Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，指数族分布"><a href="#一，指数族分布" class="headerlink" title="一，指数族分布"></a>一，指数族分布</h1><h2 id="1，指数族分布基本概念"><a href="#1，指数族分布基本概念" class="headerlink" title="1，指数族分布基本概念"></a>1，指数族分布基本概念</h2><p>参数为 $\boldsymbol{\eta}$ 的变量 $\boldsymbol{x}$ 的指数族分布定义为具有下⾯形式的概率分布的集合：</p><script type="math/tex; mode=display">p(\boldsymbol{x|\eta}) = h(\boldsymbol{x})g(\boldsymbol{\eta})\exp \{\boldsymbol{\eta}^{T}\boldsymbol{\mu}(\boldsymbol{x})\}\tag{2.106}</script><p>其中 $\boldsymbol{x}$ 可能是标量或者向量， 可能是离散的或者是连续的。 这⾥ $\boldsymbol{\eta}$ 被称为概率分布的 <strong>⾃然参数</strong> （<code>natural parameters</code>），$\boldsymbol{\mu}(\boldsymbol{x})$ 是 $\boldsymbol{x}$ 的某个函数。函数 $g(\boldsymbol{\eta})$ 可以被看成系数，它确保了概率分布是归⼀化的，因此满⾜：</p><script type="math/tex; mode=display">g(\boldsymbol{\eta})\int h(\boldsymbol{x})\exp \{\boldsymbol{\eta}^{T}\boldsymbol{\mu}(\boldsymbol{x})\}\mathrm{d}\boldsymbol{x}=1\tag{2.107}</script><p>如果 $\boldsymbol{x}$ 是离散变量，那么上式中的积分就要替换为求和。</p><p>考虑伯努利分布：</p><script type="math/tex; mode=display">p(x|\mu) = \text {Bern}(x|\mu) = \mu^{x}(1-\mu)^{1-x}\tag{2.108}</script><p>变形，有：</p><script type="math/tex; mode=display">\begin{aligned} p(x|\mu) &= \exp \{x\ln \mu +(1-x) \ln (1-\mu)\} \\ &= (1-\mu)\exp \left\{\ln \left(\frac{\mu}{1-\mu}\right)x\right\}\end{aligned}\tag{2.109}</script><p>对比公式(2.106)，可得：</p><script type="math/tex; mode=display">\eta = \ln \left(\frac{\mu}{1-\mu}\right)</script><p>从而，有：</p><script type="math/tex; mode=display">\begin{aligned}\mu &= \sigma(\eta) \\ &= \frac{1}{1+\exp(-\eta)}\end{aligned}\tag{2.110}</script><p>被称为 <strong><code>logistic sigmoid</code>函数</strong>。<br>因此，伯努利分布的指数族分布标准形式：</p><script type="math/tex; mode=display">p(x|\mu) = \sigma(-\eta)\exp(\eta x)\tag{2.111}</script><p>其中，</p><script type="math/tex; mode=display">\mu(x) = x \\ h(x) = 1 \\ g(\eta)=\sigma(-\eta)</script><p>考虑单⼀观测 $\boldsymbol{x}$ 的多项式分布，形式为：</p><script type="math/tex; mode=display">p(\boldsymbol{x|\mu}) = \prod_{k=1}^{K}\mu_{k}^{x_{k}} = \exp\left\{\sum_{k=1}^K x_{k}\ln \mu_{k}\right\}\tag{2.112}</script><p>其中 $\boldsymbol{x} = (\boldsymbol{x}_1,\dots ,\boldsymbol{x}_M)^T$ 。把它写成公式(2.106)的标准形式，即：</p><script type="math/tex; mode=display">p(\boldsymbol{x|\mu}) = \exp(\boldsymbol{\eta}^{T}\boldsymbol{x})\tag{2.113}</script><p>其中，$\eta_{k} = \ln \mu_{k}$ ，$\boldsymbol{\eta}=(\eta_1,\dots,\eta_{M})^T$，并且</p><script type="math/tex; mode=display">\boldsymbol{\mu}(\boldsymbol{x}) = \boldsymbol{x} \\ h(\boldsymbol{x}) = 1 \\ g(\boldsymbol{\eta}) = 1 \\ \sum_{k=1}^{K} \mu_{k}=1</script><p>考虑只⽤ $M−1$ 个参数来表⽰这个分布，把 $\mu_M$ ⽤剩余的 $\{\mu_k\}$ 表⽰，其中 $k = 1, \dots , M−1$，这样就只剩下了 $M−1$ 个参数，公式(2.112)变为：</p><script type="math/tex; mode=display">\begin{aligned}p(\boldsymbol{x|\mu}) &= \exp\left\{\sum_{k=1}^K x_{k}\ln \mu_{k}\right\} \\ &= \exp \left\{\sum_{k=1}^{M-1}x_{k}\ln\left(\frac{\mu_{k}}{1-\sum_{j=1}^{M-1}\mu_{j}}\right) + \ln \left(1-\sum_{k=1}^{M-1}\mu_{k}\right)\right\} \end{aligned}\tag{2.114}</script><p>令</p><script type="math/tex; mode=display">\eta_{k} = \ln\left(\frac{\mu_{k}}{1-\sum_{j=1}^{M-1}\mu_{j}}\right)</script><p>即得：</p><script type="math/tex; mode=display">\mu_{k} = \frac{\exp (\eta_{k})}{1+\sum_{j=1}^{M-1}\exp(\eta_{j})}\tag{2.115}</script><p>这被称为 <strong><code>softmax</code>函数</strong>，或者<strong>归⼀化指数</strong>（<code>normalized exponential</code>）。因此，单⼀观测 $\boldsymbol{x}$ 的多项式分布的指数族分布标准形式：</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\boldsymbol{\eta}) = \left(1+\sum_{k=1}^{M-1}\exp(\eta_{k})\right)^{-1}\exp(\boldsymbol{\mu}^T\boldsymbol{x})\tag{2.116}</script><p>其中 $\boldsymbol{\eta}=(\eta_1,\dots,\eta_{M-1},0)^T$，并且</p><script type="math/tex; mode=display">\boldsymbol{\mu}(\boldsymbol{x}) = \boldsymbol{x} \\ h(\boldsymbol{x}) = 1 \\ g(\boldsymbol{\eta}) =\left(1+\sum_{k=1}^{M-1}\exp(\eta_{k})\right)^{-1}</script><p>对于⼀元⾼斯分布，有：</p><script type="math/tex; mode=display">\begin{aligned} p\left(x | \mu, \sigma^{2}\right) &=\frac{1}{\left(2 \pi \sigma^{2}\right)^{\frac{1}{2}}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\} \\ &=\frac{1}{\left(2 \pi \sigma^{2}\right)^{\frac{1}{2}}} \exp \left\{-\frac{1}{2 \sigma^{2}}x^{2} + \frac{\mu}{\sigma^{2}}x -\frac{1}{2 \sigma^{2}}\mu^{2}\right\} \end{aligned}\tag{2.117}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{\eta}=\dbinom{\frac{\mu}{\sigma^{2}}}{\frac{-1}{2\sigma^2}} \\ \boldsymbol{\mu}({x})=\dbinom{x}{x^2} \\ h(x) = (2\pi)^{-\frac{1}{2}} \\ g(\boldsymbol{\eta}) = (-2\eta_2)^{\frac{1}{2}}\exp \left(\frac{\eta_{1}^{2}}{4\eta_{2}}\right)</script><h2 id="2，最⼤似然与充分统计量"><a href="#2，最⼤似然与充分统计量" class="headerlink" title="2，最⼤似然与充分统计量"></a>2，最⼤似然与充分统计量</h2><p>设二元函数 $z=f(x,y)$ 在平面区域 $D$上具有一阶连续偏导数，则对于每一个点 $P_0(x_0,y_0)\in D$ 都可定出一个向量 </p><script type="math/tex; mode=display">\left\{\frac{\partial f}{\partial x_0},\frac{\partial f}{\partial y_0} \right\} = f_x(x_0,y_0)\boldsymbol{i} + f_y(x_0,y_0)\boldsymbol{j}</script><p>，该向量称为函数 $z=f(x,y)$ 在点$P_0(x_0,y_0)$的<strong>梯度</strong>，记作 $\text{gradf}(x_0,y_0)$ 或 $\nabla f(x_0, y_0)$<br>即有：</p><script type="math/tex; mode=display">\text { gradf }(x_0, y_0)=\nabla f(x_0, y_0)=\left\{\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial y_0}\right\}=f_{x}(x_0, y_0) \boldsymbol{i}+f_{y}(x_0, y_0) \boldsymbol{j}\tag{2.118}</script><p>其中 $\nabla =\frac{\partial}{\partial x} \boldsymbol{i} + \frac{\partial}{\partial y}\boldsymbol{j}$ 称为（二维的）<strong>向量微分算子</strong>或 <strong><code>Nabla</code>算子</strong>，  $\nabla {f}=\frac{\partial {f}}{\partial x} \boldsymbol{i} + \frac{\partial{f}}{\partial y}\boldsymbol{j}$ 。</p><p>对公式(2.107)的两侧关于 $\boldsymbol{\mu}$ 取梯度，有：</p><script type="math/tex; mode=display">\begin{aligned} \nabla g(\boldsymbol{\eta})\int h(\boldsymbol{x})\exp \{\boldsymbol{\eta}^{T}\boldsymbol{\mu}(\boldsymbol{x})\}\mathrm{d}\boldsymbol{x} + g(\boldsymbol{\eta})\int h(\boldsymbol{x})\exp \{\boldsymbol{\eta}^{T}\boldsymbol{\mu}(\boldsymbol{x})\}\boldsymbol{\mu}(\boldsymbol{x})\mathrm{d}\boldsymbol{x} =0 \end{aligned}\tag{2.119}</script><p>从而可以推导出：</p><script type="math/tex; mode=display">-\nabla \ln g(\boldsymbol{\eta}) = \mathbb{E}[\boldsymbol{\mu}(\boldsymbol{x})]\tag{2.120}</script><p>现在考虑⼀组独⽴同分布的数据 $\boldsymbol{X} = \{\boldsymbol{x}_1, \dots, \boldsymbol{x}_N\}$。对于这个数据集，似然函数为：</p><script type="math/tex; mode=display">p(\boldsymbol{X|\eta}) = \left(\prod_{n=1}^{N}h(\boldsymbol{x}_{n})\right) g(\boldsymbol{\eta})^{N} \exp\left\{\sum_{n=1}^{N} \boldsymbol{\eta}^{T}\boldsymbol{\mu}(\boldsymbol{x}_n)\right\}\tag{2.121}</script><p>令 $\ln p(\boldsymbol{X|\eta})$ 关于 $\boldsymbol{\eta}$ 的导数等于零，我们可以得到最⼤似然估计 $\boldsymbol{\mu}_{ML}$ 满⾜的条件：</p><script type="math/tex; mode=display">-\nabla \ln g(\boldsymbol{\eta}_{ML})=\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{\mu}(\boldsymbol{x}_n)\tag{2.122}</script><p>原则上可以通过解这个⽅程来得到 $\boldsymbol{\mu}_{ML}$ 。我们看到最⼤似然估计的解只通过 $\boldsymbol{\mu}(\boldsymbol{x}_n)$ 对数据产⽣依赖，因此这个量被称为指数族分布的<strong>充分统计量</strong>（<code>sufficient statistic</code>）。</p><h2 id="3，共轭先验"><a href="#3，共轭先验" class="headerlink" title="3，共轭先验"></a>3，共轭先验</h2><p>对于指数族分布的任何成员，都存在⼀个共轭先验，可以写成下⾯的公式：</p><script type="math/tex; mode=display">p(\boldsymbol{\eta} | \boldsymbol{\chi}, \nu)=f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^{\nu} \exp \left\{\nu \boldsymbol{\eta}^{T} \boldsymbol{\chi}\right\}\tag{2.123}</script><h2 id="4，无信息先验"><a href="#4，无信息先验" class="headerlink" title="4，无信息先验"></a>4，无信息先验</h2><p>在许多情形下， 我们可能对分布应该具有的形式⼏乎完全不知道。 这时， 我们可以寻找⼀种形式的先验分布， 被称为<strong>⽆信息先验</strong>（<code>noninformative prior</code>）。 这种先验分布的⽬的是尽量对后验分布产⽣尽可能⼩的影响（<code>Jeffreys</code>, 1946; <code>Box and Tiao</code>, 1973; <code>Bernardo and Smith</code>, 1994）。这有时被称为“<strong>让数据⾃⼰说话</strong>”。</p><p>考虑⽆信息先验的两个简单的例⼦（<code>Berger</code>, 1985）。</p><p>例1，如果概率密度的形式为：</p><script type="math/tex; mode=display">p(x|\mu)=f(x-\mu)\tag{2.124}</script><p>那么参数 $\mu$ 被称为<strong>位置参数</strong>（<code>location parameter</code>）。这⼀类概率分布具有<strong>平移不变性</strong>（<code>translation invariance</code>），因为如果把 $x$ 平移⼀个常数，得到 $\hat{x}=x+c$，那么：</p><script type="math/tex; mode=display">p(\hat{x}|\hat{\mu})=f(\hat{x}-\hat{\mu})\tag{2.125}</script><p>其中，$\hat{\mu}=\mu+c$。新变量的概率密度的形式与原变量相同，因此概率密度与原点的选择⽆关。想要选择⼀个能够反映这种平移不变性的先验分布，因此我们选择的先验概率分布要对区间 $A \le \mu \le B$ 以及平移后的区间 $A−c \le \mu \le B−c$ 赋予相同的概率质量。这说明：</p><script type="math/tex; mode=display">\int_{A}^{B} p(\mu)\mathrm{d}\mu = \int_{A-c}^{B-c} p(\mu)\mathrm{d}\mu = \int_{A}^{B} p(\mu - c)\mathrm{d}\mu\tag{2.126}</script><p>并且由于这必须对于任意的 $A$ 和 $B$ 的选择都成⽴，因此有：</p><script type="math/tex; mode=display">p(\mu-c)=p(\mu)\tag{2.127}</script><p>这表明 $p(\mu)$ 是常数。</p><p>例2，考虑概率分布的形式为：</p><script type="math/tex; mode=display">p(x|\sigma)=\frac{1}{\sigma}f\left(\frac{x}{\sigma}\right)\tag{2.128}</script><p>其中，$\sigma \gt 0$。参数 $\sigma$ 被称为 <strong>缩放参数</strong>（<code>scale parameter</code>），概率密度具有<strong>缩放不变性</strong>（<code>scale invariance</code>）因为如果把 $x$ 缩放⼀个常数，得到 $\hat{x} = cx$，那么：</p><script type="math/tex; mode=display">p(\hat{x}|\hat{\sigma})=\frac{1}{\hat{\sigma}}f\left(\frac{\hat{x}}{\hat{\sigma}}\right)\tag{2.129}</script><p>其中，$\hat{\sigma}=c\sigma$。这个变换对应于单位的改变。想要选择⼀个能够反映这种缩放不变性的先验分布，因此我们选择的先验概率分布要对区间 $A \le \sigma \le B$ 以及平移后的区间 $\frac{A}{c} \le \sigma \le \frac{B}{c}$ 赋予相同的概率质量。这说明：</p><script type="math/tex; mode=display">\int_{A}^{B} p(\sigma)\mathrm{d}\sigma = \int_{\frac{A}{c}}^{\frac{B}{c}} p(\sigma)\mathrm{d}\sigma = \int_{A}^{B} p\left(\frac{1}{c}\sigma\right)\frac{1}{c}\mathrm{d}\sigma\tag{2.130}</script><p>并且由于这必须对于任意的 $A$ 和 $B$ 的选择都成⽴，因此有：</p><script type="math/tex; mode=display">p(\sigma) = p\left(\frac{1}{c}\sigma\right)\frac{1}{c}\tag{2.131}</script><h1 id="二，非参数化方法"><a href="#二，非参数化方法" class="headerlink" title="二，非参数化方法"></a>二，非参数化方法</h1><p>具有具体函数形式的概率分布，并且由少量的参数控制，这些参数的值可以由数据集确定。这被称为<strong>概率密度建模的参数化（<code>parametric</code>）⽅法</strong>。</p><h2 id="1，核密度估计"><a href="#1，核密度估计" class="headerlink" title="1，核密度估计"></a>1，核密度估计</h2><p>假设观测服从 $D$ 维空间的某个未知的概率密度分布 $p(\boldsymbol{x})$。把这个 $D$ 维空间选择成<strong>欧⼏⾥得空间</strong>， 并且我们想估计 $p(\boldsymbol{x})$ 的值。 根据之前对于局部性的讨论， 考虑包含 $\boldsymbol{x}$ 的某个⼩区域 $\mathcal{R}$。这个区域的概率质量为：</p><script type="math/tex; mode=display">P = \int_{\mathcal{R}} p(\boldsymbol{x}) \mathrm{d}\boldsymbol{x}\tag{2.132}</script><p>假设收集了服从 $p(\boldsymbol{x})$ 分布的 $N$ 次观测，由于每个数据点都有⼀个落在区域 $\mathcal{R}$ 中的概率 $P$ ，因此位于区域 $\mathcal{R}$ 内部的数据点的总数 $K$ 将服从<strong>⼆项分布</strong>：</p><script type="math/tex; mode=display">\text {Bin}(K|N, P) = \frac{N!}{K!(N-K)!} P^{K}(1-P)^{N-K}\tag{2.133}</script><p>对于⼤的 $N$ 值， 这 个分布将会在均值附近产⽣尖峰，并且 $K\simeq NP$。<br>假定区域 $\mathcal{R}$ ⾜够⼩，使得在这个区域内的概率密度 $p(\boldsymbol{x})$ ⼤致为常数，设 $V$ 是区域 $\mathcal{R}$ 的体积，那么 $P \simeq p(\boldsymbol{x})V$ 。</p><p>由以上分析，可以得到概率密度的估计：</p><script type="math/tex; mode=display">p(\boldsymbol{x}) = \frac{K}{NV}\tag{2.134}</script><p>我们有两种⽅式利⽤公式(2.232)的结果。 我们可以固定 $K$ 然后从数据中确定 $V$ 的值， 这就 是 $K$ <strong>近邻⽅法</strong>。我们还可以固定 $V$ 然后从数据中确定 $K$ ，这就是<strong>核⽅法</strong>。在极限 $N \to \infty$ 的情况下，如果 $V$ 随着 $N$ ⽽合适地收缩，并且 $K$ 随着 $N$ 增⼤，那么可以证明 $K$ <strong>近邻</strong>概率密度估计和<strong>核⽅法</strong>概率密度估计都会收敛到真实的概率密度（<code>Duda and Hart</code>, 1973）。</p><p>取区域 $\mathcal{R}$ 以 $\boldsymbol{x}$ 为中⼼的⼩超⽴⽅体，确定概率密度。为了统计落在这个区域内的数据点的数量 $K$ ，定义下⾯的函数：</p><script type="math/tex; mode=display">k(\boldsymbol{\mu})=\left\{\begin{array}{l}{1，|\mu_i| \le \frac{1}{2}, i=1,\dots,D} \\ {0，其他情况}\end{array}\right.\tag{2.135}</script><p>并且满足：</p><p>1）$k(\boldsymbol{\mu}) \ge 0$<br>2）$\int k(\boldsymbol{\mu}) \mathrm{d} \boldsymbol{\mu} = 1$</p><p>这表⽰⼀个以原点为中⼼的单位⽴⽅体。 函数 $k(\boldsymbol{\mu})$ 是 <strong>核函数</strong>（<code>kernel function</code>）的⼀个例⼦， 在这个问题中也被称为 <strong><code>Parzen</code>窗</strong>（<code>Parzen window</code>）。 不难发现，如果数据点 $\boldsymbol{x}_n$ 位于以 $\boldsymbol{x}$ 为中⼼的边长为 $h$ 的⽴⽅体中，则位于这个⽴⽅体内的数据点的总数为：</p><script type="math/tex; mode=display">K=\sum_{k=1}^{K}k\left(\frac{\boldsymbol{x}-\boldsymbol{x_n}}{h}\right)\tag{2.136}</script><p>由公式(2.134)可得点 $\boldsymbol{x}$ 处的概率密度估计，称为<strong>核密度估计</strong>，或者 <strong><code>Parzen</code>估计</strong>：</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\frac{1}{N}\sum_{n=1}^{N}\frac{1}{h^{D}} k \left(\frac{\boldsymbol{x}-\boldsymbol{x_n}}{h}\right)\tag{2.137}</script><p>其中，记 $D$ 维边长为 $h$ 的⽴⽅体的体积公式 $V = h^D$ 。</p><p>使⽤<strong>⾼斯核函数</strong>，可以得到下⾯的核概率密度模型：</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\frac{1}{N}\sum_{n=1}^{N}\frac{1}{\left(2 \pi h^{2}\right)^{\frac{D}{2}}} \exp \left\{-\frac{||\boldsymbol{x}-\boldsymbol{x}_n||^{2}}{2 h^{2}}\right\}\tag{2.138}</script><p>其中 $h$ 表⽰⾼斯分布的<strong>标准差</strong>。因此概率密度模型可以通过以下⽅式获得：令每个数据点都服从<strong>⾼斯分布</strong>，然后把数据集⾥的每个数据点的贡献相加，之后除以 $N$ ，使得概率密度正确地被归⼀化。</p><p>如图2.33，核密度模型。</p><p><img src="/images/prml_20190929000243.png" alt="核密度模型"></p><h2 id="2，近邻⽅法"><a href="#2，近邻⽅法" class="headerlink" title="2，近邻⽅法"></a>2，近邻⽅法</h2><p><strong>核⽅法</strong>进⾏概率密度估计的⼀个难点是控制核宽度的参数 $h$ 对于所有的核都是固定的。 在⾼数据密度的区域，⼤的 $h$ 值可能会造成过度平滑，并且破坏了本应从数据中提取出的结构； 但是，减⼩ $h$ 的值可能导致数据空间中低密度区域估计的噪声。因此，$h$ 的最优选择可能依赖于数据空间的位置。这个问题可以通过概率密度的<strong>近邻⽅法</strong>解决。</p><p>假设球体的半径可以⾃由增长，直到它精确地包含 $K$ 个数据点。 这样，概率密度 $p(\boldsymbol{x})$ 的估计就由公式(2.134)给出， 其中 $V$ 等于最终球体的体积。这种⽅法被称为 <strong><code>K</code>近邻⽅法</strong>。</p><p>如图2.34，$K$ 近邻⽅法。</p><p><img src="/images/prml_20190929000307.png" alt="K近邻⽅法"></p><p>现在讨论明概率密度估计的 $K$ <strong>近邻⽅法</strong>如何推⼴到分类问题。</p><p>假设有⼀个数据集，其中 $N_k$ 个数据点属于类别 $\mathcal{C}_k$ ，数据点的总数为 $N$ ，因此 $\sum_{k} N_k = N$ 。如果想对⼀个新的数据点 $\boldsymbol{x}$ 进⾏分类，那么可以画⼀个以 $\boldsymbol{x}$ 为中⼼的球体，这个球体精确地包含 $K$ 个数据点（⽆论属于哪个类别）。假设球体的体积为 $V$ ，并且包含来⾃类别 $\mathcal{C}_k$ 的 $K_k$ 个数据点，这样与每个类别关联的⼀个概率密度的估计：</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\mathcal{C}_k) = \frac{K_k}{N_kV}\tag{2.139}</script><p>⽆条件概率密度为：</p><script type="math/tex; mode=display">p(\boldsymbol{x}) = \frac{K}{NV}\tag{2.140}</script><p>类先验为：</p><script type="math/tex; mode=display">p(\mathcal{C}_k) = \frac{N_k}{N}\tag{2.141}</script><p>可以得到类别的后验概率公式：</p><script type="math/tex; mode=display">p(\mathcal{C}_k | \boldsymbol{x}) = \frac{p(\boldsymbol{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\boldsymbol{x})} = \frac{K_k}{K}\tag{2.142}</script><p><strong>最近邻</strong> ($K = 1$) <strong>分类器</strong>的⼀个重要的性质是：在极限 $N \to \infty$ 的情况下，错误率不会超过<strong>最优分类器</strong>（即使⽤真实概率分布的分类器）可以达到的最⼩错误率的⼆倍（<code>Cover and Hart</code>, 1967）。</p><p>如图2.35～2.36，$K$ 近邻分类器（$K=1$ 和 $K=3$）。</p><p><img src="/images/prml_20190929121905.png" alt="K=3"></p><p><img src="/images/prml_20190929121914.png" alt="K=1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，指数族分布&quot;&gt;&lt;a href=&quot;#一，指数族分布&quot; class=&quot;headerlink&quot; title=&quot;一，指数族分布&quot;&gt;&lt;/a&gt;一，指数族分布&lt;/
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】概率分布之高斯分布</title>
    <link href="https://zhangbc.github.io/2019/09/29/prml_02_02/"/>
    <id>https://zhangbc.github.io/2019/09/29/prml_02_02/</id>
    <published>2019-09-29T08:11:56.000Z</published>
    <updated>2019-10-07T15:04:47.210Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，多元高斯分布"><a href="#一，多元高斯分布" class="headerlink" title="一，多元高斯分布"></a>一，多元高斯分布</h1><p>考虑<strong>⾼斯分布</strong>的⼏何形式，⾼斯对于 $\boldsymbol{x}$ 的依赖是通过下⾯形式的⼆次型：</p><script type="math/tex; mode=display">\Delta^{2} = (\boldsymbol{x} - \boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\tag{2.30}</script><p>其中，$\Delta$ 被叫做 $\boldsymbol{\mu}$ 和 $\boldsymbol{x}$ 之间的<strong>马⽒距离</strong>（<code>Mahalanobis distance</code>）。 当 $\boldsymbol{\Sigma}$ 是单位矩阵时，就变成了<strong>欧式距离</strong>。对于 $\boldsymbol{x}$ 空间中这个⼆次型是常数的曲⾯，⾼斯分布也是常数。</p><p>现在考虑<strong>协⽅差矩阵</strong>的<strong>特征向量⽅程</strong>：</p><script type="math/tex; mode=display">\boldsymbol{\Sigma} \boldsymbol{\mu}_i = \lambda_{i} \boldsymbol{\mu}_{i}\tag{2.31}</script><p>其中 $i = 1,\dots , D$。由于 $\boldsymbol{\Sigma}$ 是<strong>实对称矩阵</strong>，因此它的特征值也是实数，并且特征向量可以被选成<strong>单位正交</strong>的，即：</p><script type="math/tex; mode=display">\boldsymbol{\mu}_{i}^{T} \boldsymbol{\mu}_{j} = I_{ij}\tag{2.32}</script><p>其中 $I_{ij}$ 是单位矩阵的第 $i, j$ 个元素，满⾜：</p><script type="math/tex; mode=display">I_{i j}=\left\{\begin{array}{l}{1，如果 i=j} \\ {0，其他情况}\end{array}\right. \tag{2.33}</script><p>协⽅差矩阵 $\boldsymbol{\Sigma}$ 可以表⽰成特征向量的展开的形式：</p><script type="math/tex; mode=display">\boldsymbol{\Sigma} = \sum_{i=1}^{D} \lambda_i \boldsymbol{\mu}_{i}\boldsymbol{\mu}_{i}^{T}\tag{2.34}</script><p>协⽅差矩阵的逆矩阵 $\boldsymbol{\Sigma}^{-1}$ 可以表⽰成特征向量的展开的形式：</p><script type="math/tex; mode=display">\boldsymbol{\Sigma}^{-1} = \sum_{i=1}^{D} \frac{1}{\lambda_i} \boldsymbol{\mu}_{i}\boldsymbol{\mu}_{i}^{T}\tag{2.35}</script><p>⼆次型公式(2.30)即可表示为：</p><script type="math/tex; mode=display">\Delta^{2} = \sum_{i=1}^{D} \frac{y_{i}^{2}}{\lambda_{i}}\tag{2.36}</script><p>其中，$y_{i}^{2} = \boldsymbol{u_i^T} (\boldsymbol{x} - \boldsymbol{\mu})$ 。</p><p>把 $\{y_i\}$ 表⽰成单位正交向量 $\boldsymbol{\mu_i}$ 关于原始的 $x_i$ 坐标经过平移和旋转后形成的新的坐标系。定义向量 $\boldsymbol{y} = (y_1,\dots, y_D)^T$ ，即有：</p><script type="math/tex; mode=display">\boldsymbol {y} = \boldsymbol{U} (\boldsymbol{x} - \boldsymbol{\mu})\tag{2.37}</script><p>其中 $\boldsymbol{U}$ 是⼀个矩阵，它的⾏是向量 $\boldsymbol{u}_{i}^{T}$ 。从公式(2.32)可以看出 $\boldsymbol{U}$ 是⼀个<strong>正交矩阵</strong>， 即它满⾜性质 $\boldsymbol{U}\boldsymbol{U}^T = \boldsymbol{I}$ ，因此也满⾜ $\boldsymbol{U}^T \boldsymbol{U} = \boldsymbol{I}$ ，其中 $\boldsymbol{I}$ 是单位矩阵。</p><p>⼀个特征值严格⼤于零的矩阵被称为<strong>正定（<code>positive definite</code>）矩阵</strong>。偶尔遇到⼀个或者多个特征值为零的⾼斯分布，那种情况下分布是奇异的，被限制在 了⼀个低维的⼦空间中。如果所有的特征值都是⾮负的，那么这个矩阵被称为<strong>半正定（<code>positive semidefine</code>）矩阵</strong>。</p><p>如图2.12，红⾊曲线表⽰⼆维空间 $\boldsymbol{x} = (x_1 , x_2)$ 的⾼斯分布的常数概率密度的椭圆⾯， 它表⽰的概率密度为 $\exp(−\frac{1}{2})$，值是在 $\boldsymbol{x} = \boldsymbol{\mu}$ 处计算的。椭圆的轴由协⽅差矩阵的特征向量 $\mu_i$ 定义，对应的特征值为 $\lambda_i$ 。</p><p><img src="/images/prml_20190920171000.png" alt="椭圆面"></p><p>现在考虑在由 $y_i$ 定义的新坐标系下⾼斯分布的形式。 从 $\boldsymbol{x}$ 坐标系到 $\boldsymbol{y}$ 坐标系， 我们有⼀个 <strong><code>Jacobian</code>矩阵</strong> $\boldsymbol{J}$ ，它的元素为：</p><script type="math/tex; mode=display">\boldsymbol{J}_{ij} = \frac{\partial {x_i}}{\partial {j_j}} = U_{ij}\tag{2.38}</script><p>其中 $U_{ji}$ 是矩阵 $\boldsymbol{U}^T$ 的元素。使⽤矩阵 $\boldsymbol{U}$ 的单位正交性质，我们看到 <strong><code>Jacobian</code>矩阵</strong> ⾏列式的平⽅为：</p><script type="math/tex; mode=display">| \boldsymbol{J}^{2} | = |\boldsymbol{U}^{T}|^{2} = |\boldsymbol{U}^{T}||\boldsymbol{U}| = |\boldsymbol{U}^{T}\boldsymbol{U}| = |\boldsymbol{I}| = 1\tag{2.39}</script><p>从而可知，$|\boldsymbol{J}|=1$ ，并且，⾏列式 $|\boldsymbol{\Sigma}|$ 的协⽅差矩阵可以写成特征值的乘积，因此：</p><script type="math/tex; mode=display">|\boldsymbol{\Sigma}|^{\frac{1}{2}} = \prod_{j=1}^{D} \lambda_{j}^{\frac{1}{2}}\tag{2.40}</script><p>因此在 $\boldsymbol{y}$ 坐标系中，⾼斯分布的形式为：</p><script type="math/tex; mode=display">p(\boldsymbol{y}) = p(\boldsymbol{x})|\boldsymbol{J}| = \prod_{j=1}^{D} \frac{1}{(2 \pi \lambda_{j})^{\frac{1}{2}}} \exp \left \{- \frac{y_{i}^2}{2\lambda_j} \right \}\tag{2.41}</script><p>这是 $D$ 个独⽴⼀元⾼斯分布的乘积。</p><p>在 $\boldsymbol{y}$ 坐标系中，概率分布的积分为：</p><script type="math/tex; mode=display">\int p(\boldsymbol{y}) \mathrm{d} \boldsymbol{y} = \prod_{j=1}^{D} \int_{-\infty}^{\infty} \frac{1}{(2 \pi \lambda_{j})^{\frac{1}{2}}} \exp \left \{- \frac{y_{i}^2}{2\lambda_j} \right \} \mathrm{d} y_j = 1\tag{2.42}</script><p><strong>⾼斯分布</strong>下 $\boldsymbol{x}$ 的期望为：</p><script type="math/tex; mode=display">\begin{aligned} \mathbb{E}[\boldsymbol{x}] &= \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}} \int \exp \left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\} \boldsymbol{x} \mathrm{d} \boldsymbol{x} \\ &= \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}} \int \exp \left\{-\frac{1}{2}\boldsymbol{z}^{T} \boldsymbol{\Sigma}^{-1} \boldsymbol{z}\right\} (\boldsymbol{z+\mu}) \mathrm{d} \boldsymbol{z} \end{aligned}\tag{2.43}</script><p>其中，$\boldsymbol{z = x - \mu}$ 。注意到指数位置是 $\boldsymbol{z}$ 的偶函数，并且由于积分区间为 $(−\infty, \infty)$，因此在因⼦ $(\boldsymbol{z + \mu})$ 中的 $\boldsymbol{z}$ 中的项会由于对称性变为零。因此 $\mathbb{E}[\boldsymbol{x}] = \boldsymbol{\mu}$ 。称 $\boldsymbol{\mu}$ 为⾼斯分布的<strong>均值</strong>。</p><p>现在考虑⾼斯分布的⼆阶矩。对于多元⾼斯分布，有 $D^2$ 个由 $\mathbb{E}[x_i x_j]$ 给出的⼆阶矩，可以聚集在⼀起组成矩阵 $\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^T ]$。</p><script type="math/tex; mode=display">\begin{aligned} \mathbb{E}[\boldsymbol{x}\boldsymbol{x}^{T}] &= \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}} \int \exp \left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\} \boldsymbol{x} \boldsymbol{x}^{T}\mathrm{d} \boldsymbol{x} \\ &= \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}} \int \exp \left\{-\frac{1}{2}\boldsymbol{z}^{T} \boldsymbol{\Sigma}^{-1} \boldsymbol{z}\right\} (\boldsymbol{z+\mu})(\boldsymbol{z+\mu})^{T} \mathrm{d} \boldsymbol{z} \end{aligned}\tag{2.44}</script><p>其中，$\boldsymbol{z = x - \mu}$ ，$\boldsymbol{z} = \sum_{j=1}^{D} y_i \boldsymbol{u_j}$ ，$y_i = \boldsymbol{u_j}^{T}\boldsymbol{z}$ 。</p><p>由此可以推导出：</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^{T}] = \boldsymbol{\mu}\boldsymbol{u}^{T} + \boldsymbol{\Sigma}\tag{2.45}</script><p>随机变量 $\boldsymbol{x}$ 的<strong>协⽅差</strong>（<code>covariance</code>），定义为：</p><script type="math/tex; mode=display">\text{var}[\boldsymbol{x}] = \mathbb{E}[(\boldsymbol{x} - \mathbb{E}[\boldsymbol{x}])(\boldsymbol{x} - \mathbb{E}[\boldsymbol{x}])^{T}]\tag{2.46}</script><p>对于⾼斯分布这⼀特例，我们可以使⽤ $\mathbb{E}[\boldsymbol{x}] = \boldsymbol{\mu}$ 以及公式(2.45)的结果，得到：</p><script type="math/tex; mode=display">\text{var}[\boldsymbol{x}] = \boldsymbol{\Sigma}\tag{2.47}</script><p>由于参数 $\boldsymbol{\Sigma}$ 公式了⾼斯分布下 $\boldsymbol{x}$ 的协⽅差，因此它被称为<strong>协⽅差矩阵</strong>。</p><h1 id="二，条件⾼斯分布"><a href="#二，条件⾼斯分布" class="headerlink" title="二，条件⾼斯分布"></a>二，条件⾼斯分布</h1><p>多元⾼斯分布的⼀个<strong>重要性质</strong>：如果两组变量是联合⾼斯分布，那么以⼀组变量为条件， 另⼀组变量同样是⾼斯分布。</p><p>假设 $\boldsymbol{x}$ 是⼀个服从⾼斯分布 $\mathcal{N}(\boldsymbol{x} | \boldsymbol{\mu}, \mathbf{\Sigma})$ 的 $D$ 维向量。我们把 $\boldsymbol{x}$ 划分成两个不相交的⼦集 $\boldsymbol{x}_a$ 和 $\boldsymbol{x}_b$ 。 不失⼀般性， 令 $\boldsymbol{x}_a$ 为 $\boldsymbol{x}$ 的前 $M$ 个分量， 令 $\boldsymbol{x}_b$ 为剩余的 $D − M$ 个分量，因此</p><script type="math/tex; mode=display">\boldsymbol{x} = \dbinom{\boldsymbol{x}_a}{\boldsymbol{x}_b}</script><p>同理，对应的对均值向量 $\boldsymbol{\mu}$ 的划分，即</p><script type="math/tex; mode=display">\boldsymbol{\mu} = \dbinom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}</script><p>协⽅差矩阵 $\boldsymbol{\Sigma}$ 为：</p><script type="math/tex; mode=display">\boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{aa} & \boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{ba} & \boldsymbol{\Sigma}_{bb}  \end{pmatrix}\tag{2.48}</script><p>注意，协⽅差矩阵的对称性 $\boldsymbol{\Sigma} ^T= \boldsymbol{\Sigma}$ 表明 $\boldsymbol{\Sigma}_{aa}$ 和 $\boldsymbol{\Sigma}_{bb}$ 也是对称的，⽽ $\boldsymbol{\Sigma}_{ba} = \boldsymbol{\Sigma}_{ab}^{T}$ 。</p><p>在许多情况下，使⽤<strong>协⽅差矩阵的逆矩阵</strong>⽐较⽅便，也叫<strong>精度矩阵（<code>precision matrix</code>）</strong>，即：</p><script type="math/tex; mode=display">\boldsymbol{\Lambda} \equiv \boldsymbol{\Sigma}^{-1}\tag{2.49}</script><p><strong>精度矩阵</strong>的划分形式</p><script type="math/tex; mode=display">\boldsymbol{\Lambda} = \begin{pmatrix} \boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab} \\ \boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb}  \end{pmatrix}</script><p>关于分块矩阵的逆矩阵的恒等式：</p><script type="math/tex; mode=display">\begin{pmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D}  \end{pmatrix}^{-1} = \begin{pmatrix} \boldsymbol{M} & \boldsymbol{-MBD^{-1}} \\ \boldsymbol{-D^{-1}CM} & \boldsymbol{D^{-1}+CMBD^{-1}}  \end{pmatrix}\tag{2.50}</script><p>其中， $\boldsymbol{M = (A-BD^{-1}C)^{-1}}$ ，$\boldsymbol{M}^{-1}$ 被称为公式(2.50)左侧矩阵关于⼦矩阵 $\boldsymbol{D}$ 的<strong>舒尔补</strong>（<code>Schur complement</code>）。</p><p>由以上公式和相关结论可以推导出条件概率分布 $p(\boldsymbol{x}_a | \boldsymbol{x}_b)$ 的<strong>均值</strong>和<strong>协⽅差</strong>的表达式：</p><script type="math/tex; mode=display">\boldsymbol{\mu}_{a|b} = \boldsymbol{\mu}_a + \boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}(\boldsymbol{x}_b-\boldsymbol{\mu}_b)\tag{2.51}</script><script type="math/tex; mode=display">\boldsymbol{\Sigma}_{a|b} = \boldsymbol{\Sigma}_{aa} - \boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\tag{2.52}</script><h1 id="三，边缘⾼斯分布"><a href="#三，边缘⾼斯分布" class="headerlink" title="三，边缘⾼斯分布"></a>三，边缘⾼斯分布</h1><p>对于<strong>边缘高斯分布</strong>：</p><script type="math/tex; mode=display">p(\boldsymbol{x}_a) = \int p(\boldsymbol{x}_a, \boldsymbol{x}_b) \mathrm{d} \boldsymbol{x}_b\tag{2.53}</script><p>同条件高斯分布一样，可以推导出边缘概率分布 $p(\boldsymbol{x}_a)$ 的<strong>均值</strong>和<strong>协⽅差</strong>的表达式：</p><script type="math/tex; mode=display">\boldsymbol{\Sigma}_{a} = (\boldsymbol{\Lambda}_{aa} - \boldsymbol{\Lambda}{ab}\boldsymbol{\Lambda}_{bb}^{-1}\boldsymbol{\Lambda}_{ba})^{-1}\tag{2.54}</script><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{x}_a] = \boldsymbol{\mu}_a\tag{2.55}</script><script type="math/tex; mode=display">\text{cov}[\boldsymbol{x}_a] = \boldsymbol{\Sigma}_{aa}\tag{2.56}</script><p>如图2.13，两个变量上的⾼斯概率分布 $p(x_a , x_b)$ 的轮廓线。</p><p><img src="/images/prml_20190920214738.png" alt="⾼斯概率分布轮廓线"></p><p>如图2.14，边缘概率分布 $p(x_a)$（蓝⾊曲线）和 $x_b = 0.7$ 的条件概率分布 $p(x_a|x_b)$（红⾊曲线）。</p><p><img src="/images/prml_20190920214747.png" alt="边缘概率分布和条件概率分布"></p><h1 id="四，⾼斯变量的贝叶斯定理"><a href="#四，⾼斯变量的贝叶斯定理" class="headerlink" title="四，⾼斯变量的贝叶斯定理"></a>四，⾼斯变量的贝叶斯定理</h1><p>令边缘概率分布和条件概率分布的形式：</p><script type="math/tex; mode=display">p(\boldsymbol{x}) = \mathcal{N}(\boldsymbol{x} |\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})\tag{2.57}</script><script type="math/tex; mode=display">p(\boldsymbol{y} | \boldsymbol{x}) = \mathcal{N}(\boldsymbol{y} |\boldsymbol{Ax+b}, \boldsymbol{L}^{-1})\tag{2.58}</script><p>其中，$\boldsymbol{\mu}$ ， $\boldsymbol{A}$ 和 $\boldsymbol{b}$ 是控制均值的参数，$\boldsymbol{\Lambda}$ 和 $\boldsymbol{L}$ 是精度矩阵。如果 $\boldsymbol{x}$ 的维度为 $M$ ，$\boldsymbol{y}$ 的维度为 $D$，那么矩阵 $A$ 的⼤⼩为 $D \times M$ 。</p><p>⾸先，我们寻找 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 的联合分布的表达式。令</p><script type="math/tex; mode=display">\boldsymbol{z} = \dbinom{\boldsymbol{x}}{\boldsymbol{y}}</script><p>然后考虑联合概率分布的对数：</p><script type="math/tex; mode=display">\begin{aligned}\ln p(\boldsymbol{z}) &= \ln p(\boldsymbol{x}) + \ln p(\boldsymbol{y} | \boldsymbol{x}) \\ &= -\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \Lambda (\boldsymbol{x} - \boldsymbol{\mu}) \\  &-\frac{1}{2}(\boldsymbol{y} - \boldsymbol{Ax} - \boldsymbol{b})^{T} \boldsymbol{L} (\boldsymbol{y}-\boldsymbol{Ax}-\boldsymbol{b}) + 常数 \end{aligned} \tag{2.59}</script><p>可以推导出，$\boldsymbol{z}$ 上的⾼斯分布的<strong>精度矩阵</strong>（协⽅差的逆矩阵）为：</p><script type="math/tex; mode=display">\boldsymbol{R} = \begin{pmatrix} \boldsymbol{\Lambda + A^{T}LA} & \boldsymbol{-A^{T}L} \\ \boldsymbol{-LA} & \boldsymbol{L}  \end{pmatrix}</script><p>从而，$\boldsymbol{z}$ 上的⾼斯分布的<strong>均值</strong>和<strong>协⽅差</strong>的表达式：</p><script type="math/tex; mode=display">\text{cov}[\boldsymbol{z}] = \boldsymbol{R}^{-1} = \begin{pmatrix} \boldsymbol{\Lambda^{-1} } & \boldsymbol{\Lambda^{-1}A^{T}} \\ \boldsymbol{A\Lambda^{-1}} & \boldsymbol{L^{-1}+A\Lambda^{-1}A^{T}} \end{pmatrix}\tag{2.60}</script><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{z}] = \boldsymbol{R}^{-1} \dbinom{\boldsymbol{\Lambda \mu - A^{T}Lb}}{\boldsymbol{Lb}}\tag{2.61}</script><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{z}] = \dbinom{\boldsymbol{\mu}}{\boldsymbol{A\mu+b}}\tag{2.62}</script><p>边缘分布 $p(\boldsymbol{y})$ 的<strong>均值</strong>和<strong>协⽅差</strong>为：</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{y}] = \boldsymbol{A\mu+b}\tag{2.63}</script><script type="math/tex; mode=display">\text{cov}[\boldsymbol{y}] = \boldsymbol{L^{-1}+A\Lambda^{-1}A^{T}}\tag{2.64}</script><p>条件分布 $p(\boldsymbol{x}|\boldsymbol{y})$ 的<strong>均值</strong>和<strong>协⽅差</strong>为：</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{x} | \boldsymbol{y}] = (\boldsymbol{\Lambda + A^{T}LA})^{-1}\{ \boldsymbol{A^{T}L(y-b) + \Lambda \mu} \}\tag{2.65}</script><script type="math/tex; mode=display">\text{cov}[\boldsymbol{x|y}] = (\boldsymbol{\Lambda + A^{T}LA})^{-1}\tag{2.66}</script><h1 id="五，⾼斯分布的最⼤似然估计"><a href="#五，⾼斯分布的最⼤似然估计" class="headerlink" title="五，⾼斯分布的最⼤似然估计"></a>五，⾼斯分布的最⼤似然估计</h1><p>给定⼀个数据集 $\boldsymbol{X} = (\boldsymbol{x}_1, \dots, \boldsymbol{x}_N)^T$ ， 其中观测 $\{\boldsymbol{x}_n\}$ 假定是独⽴地从多元⾼斯分布中抽取的。我们可以使⽤最⼤似然法估计分布的参数。对数似然函数为：</p><script type="math/tex; mode=display">\ln p(\boldsymbol{X|\mu, \Sigma}) = -\frac{ND}{2} \ln (2\pi) - \frac{N}{2}\ln \boldsymbol{|\Sigma|} - \frac{1}{2}\sum_{n=1}^{N}\boldsymbol{(x_n -\mu)^{T}\Sigma^{-1}(x_n-\mu)}\tag{2.67}</script><p>令对数似然函数关于 $\mu$ 的导数为零，可以求得均值的最大似然估计：</p><script type="math/tex; mode=display">\boldsymbol{\mu}_{ML} = \frac{1}{N}\sum_{n=1}^{N}\boldsymbol{x}_n\tag{2.68}</script><p>方差的最大似然估计：</p><script type="math/tex; mode=display">\boldsymbol{\Sigma}_{ML} = \frac{1}{N}\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu}_{ML})(\boldsymbol{x}_n-\boldsymbol{\mu}_{ML})^{T}\tag{2.69}</script><p>从而，</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{\mu}_{ML}] = \boldsymbol{\mu}\tag{2.70}</script><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{\Sigma}_{ML}] = \frac{N-1}{N}\boldsymbol{\Sigma}\tag{2.71}</script><script type="math/tex; mode=display">\tilde {\boldsymbol{\Sigma}}_{ML} = \frac{1}{N}\sum_{n=1}^{N-1}(\boldsymbol{x}_n-\boldsymbol{\mu}_{ML})(\boldsymbol{x}_n-\boldsymbol{\mu}_{ML})^{T}\tag{2.72}</script><h1 id="六，顺序估计"><a href="#六，顺序估计" class="headerlink" title="六，顺序估计"></a>六，顺序估计</h1><p>考虑公式(2.68)给出的均值的最⼤似然估计结果 $\boldsymbol{\mu}_{ML}$ 。 当它依赖于第 $N$ 次观察时， 将记作 $\boldsymbol{\mu}_{ML}^{(N)}$  。如果想分析最后⼀个数据点 $\boldsymbol{x}_N$ 的贡献，即有：</p><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{\mu}_{ML}^{(N)} &= \frac{1}{N}\sum_{n=1}^{N}\boldsymbol{x}_n \\ &= \frac{1}{N}\boldsymbol{x}_{N} + \frac{1}{N}\sum_{n=1}^{N-1}\boldsymbol{x}_n \\ &= \frac{1}{N}\boldsymbol{x}_{N} + \frac{N-1}{N} \boldsymbol{\mu}_{ML}^{(N-1)} \\ &= \boldsymbol{\mu}_{ML}^{(N-1)} + \frac{1}{N}(\boldsymbol{x}_{n} -\boldsymbol{\mu}_{ML}^{(N-1)}) \end{aligned}\tag{2.73}</script><p>考虑⼀对随机变量 $\theta$ 和 $z$ ， 它们由⼀个联合概率分布 $p(z, \theta)$ 所控制。已知 $\theta$ 的条件下， $z$ 的条件期望定义了⼀个确定的函数 $f(\theta)$ ，叫<strong>回归函数</strong>，形式如下：</p><script type="math/tex; mode=display">f(\theta) \equiv \mathbb{E}[z|\theta] = \int zp(z|\theta)\mathrm{d}z\tag{2.74}</script><p>如图2.15，回归函数 $f(\theta)$ 。</p><p><img src="/images/prml_20190924081124.png" alt="z的条件期望"></p><p>⽬标是寻找根 $\theta^{∗}$ 使得 $f(\theta^{∗}) = 0$。 如果有观测 $z$ 和 $\theta$ 的⼀个⼤数据集， 那么可以直接对回归函数建模， 得到根的⼀个估计。 但是假设每次观测到⼀个 $z$ 的值， 我们想找到⼀个对应的<strong>顺序估计⽅法</strong>来找到 $\theta^{∗}$ 。 下⾯的解决这种问题的通⽤步骤由 <strong><code>Robbins and Monro</code></strong>（1951）给出。假定 $z$ 的条件⽅差是有穷的，即：</p><script type="math/tex; mode=display">\mathbb{E}[(z-f)^2|\theta] \lt \infty</script><p>并且不失⼀般性， 我们也假设当 $\theta \gt \theta^{∗}$ 时 $f(\theta) \gt 0$， 当 $\theta \lt \theta^{∗}$ 时 $f(\theta) \lt 0$，<strong><code>Robbins-Monro</code></strong> 的⽅法定义了⼀个根 $\theta^{∗}$ 的顺序估计的序列，由公式(2.75)给出。</p><script type="math/tex; mode=display">\theta^{(N)} = \theta^{(N-1)} + \alpha_{N-1}z(\theta^{(N-1)})\tag{2.75}</script><p>其中 $z(\theta^{(N)})$ 是当 $\theta$ 的取值为 $\theta (N)$ 时 $z$ 的观测值。系数 $\{\alpha_N\}$ 表⽰⼀个满⾜下列条件的正数序列：</p><script type="math/tex; mode=display">\lim_{N \to \infty}\alpha_{N}=0</script><script type="math/tex; mode=display">\sum_{N=1}^{\infty} \alpha_{N} = \infty</script><script type="math/tex; mode=display">\sum_{N=1}^{\infty} \alpha_{N}^{2} \lt \infty</script><p>根据定义，最⼤似然解 $\theta_{ML}$ 是负对数似然函数的⼀个驻点，因此满⾜：</p><script type="math/tex; mode=display">\left . \frac{\partial}{\partial \theta} \left\{\frac{1}{N}\sum_{n=1}^{N}- \ln p(x_N|\theta) \right\} \right|_{\theta_{ML}} = 0\tag{2.76}</script><p>交换导数与求和，取极限 $N \to \infty$ ，可以寻找最⼤似然解对应于寻找回归函数的根。 于是可以应⽤ <strong><code>Robbins-Monro</code>⽅法</strong>，此时它的形式为：</p><script type="math/tex; mode=display">\theta^{(N)} = \theta^{(N-1)} + \alpha_{N-1} \frac{\partial}{\partial\theta^{(N-1)}} \left [-\ln p(x_N |\theta^{(N-1)}) \right ]\tag{2.77}</script><h1 id="七，⾼斯分布的贝叶斯推断"><a href="#七，⾼斯分布的贝叶斯推断" class="headerlink" title="七，⾼斯分布的贝叶斯推断"></a>七，⾼斯分布的贝叶斯推断</h1><p>考虑⼀个⼀元⾼斯随机变量 $\mathbf{x}$，我们假设⽅差 $\sigma^2$ 是已知的，其任务是从⼀组 $N$ 次观测 $\mathbf{x}=(x_1,\dots, x_N)^T$ 中推断均值 $\mu$。 似然函数，即给定 $\mu$ 的情况下，观测数据集出现的概率。它可以看成 $\mu$ 的函数，由公式(2.78)给出。</p><script type="math/tex; mode=display">p(\mathbf{x}|\mu) = \prod_{n=1}^{N}p(x_n|\mu) = \frac{1}{\left(2 \pi \sigma^{2}\right)^{\frac{N}{2}}} \exp \left\{-\frac{1}{2 \sigma^{2}}\sum_{n=1}^{N}(x_n-\mu)^{2}\right\}\tag{2.78}</script><p>注意：似然函数 $p(\mathbf{x}|\mu)$ 不是 $\mu$ 的概率密度，没有被归⼀化。</p><p>如图2.16，在⾼斯分布的情形中，回归函数的形式。</p><p><img src="/images/prml_20190924103238.png" alt="回归函数的形式"></p><p>令先验概率分布为：</p><script type="math/tex; mode=display">p(\mu) = \mathcal{N}\left(\mu | \mu_0, \sigma_{0}^{2}\right)\tag{2.79}</script><p>从⽽后验概率为：</p><script type="math/tex; mode=display">p(\mu | \mathbf{x}) = \mathcal{N}\left(\mu | \mu_N, \sigma_{N}^{2}\right)\tag{2.80}</script><p>其中，</p><script type="math/tex; mode=display">\mu_N = \frac{\sigma^2}{N\sigma_{0}^2 + \sigma^2}\mu_0 + \frac{N\sigma_{0}^2}{N\sigma_{0}^2 + \sigma^2}\mu_{ML}</script><script type="math/tex; mode=display">\frac{1}{\sigma_{N}^{2}} = \frac{1}{\sigma_{0}^{2}} + \frac{N}{\sigma^{2}}</script><script type="math/tex; mode=display">\mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_n</script><p>图2.17，⾼斯分布均值的贝叶斯推断。</p><p><img src="/images/prml_20190924105141.png" alt="⾼斯分布均值的贝叶斯推断"></p><p>现在假设均值是已知的，我们要推断⽅差。令 $\lambda \equiv \frac{1}{\sigma^{2}}$ ，$\lambda$ 的似然函数的形式为：</p><script type="math/tex; mode=display">p(\mathbf{x}|\lambda) = \prod_{n=1}^{N}\mathcal{N}(x_n|\mu, \lambda^{-1}) \propto \lambda^{\frac{N}{2}} \exp \left\{-\frac{\lambda}{2}\sum_{n=1}^{N}(x_n-\mu)^{2}\right\}\tag{2.81}</script><p>对应的<strong>共轭先验</strong>因此应该正⽐于 $\lambda$ 的幂指数，也正⽐于 $\lambda$ 的线性函数的指数。这对应于 <strong><code>Gamma</code>分布</strong>，定义为：</p><script type="math/tex; mode=display">\text{Gam}(\lambda|a,b) = \frac{1}{\Gamma(a)}b^{a}\lambda^{a-1}\exp (-b\lambda)\tag{2.82}</script><p><strong>均值</strong>和<strong>协⽅差</strong>分别为：</p><script type="math/tex; mode=display">\mathbb{E}[\lambda] = \frac{a}{b}\tag{2.83}</script><script type="math/tex; mode=display">\text{var}[\lambda] = \frac{a}{b^2}\tag{2.84}</script><p>如图2.18～2.20，不同的 $a$ 和 $b$ 的情况下 <code>Gamma</code>分布的图像。</p><p><img src="/images/prml_20190927194937.png" alt="a=b=0.1"></p><p><img src="/images/prml_20190927194947.png" alt="a=b=1"></p><p><img src="/images/prml_20190927195015.png" alt="a=4,b=6"></p><p>考虑⼀个先验分布 $\text{Gam}(\lambda|a_0,b_0)$。如果乘以公式(2.81)给出的似然函数，那么即可得到后验分布：</p><script type="math/tex; mode=display">p(\lambda | \mathbf{x}) \propto \lambda^{a_0-1} \lambda^{\frac{N}{2}} \exp \left\{-b_0 \lambda -\frac{\lambda}{2}\sum_{n=1}^{N}(x_n-\mu)^{2}\right\}\tag{2.85}</script><p>我们可以把它看成形式为 $\text{Gam}(\lambda|a_N,b_N)$ 的 <strong><code>Gamma</code>分布</strong>，其中</p><script type="math/tex; mode=display">a_N = a_0 + \frac{N}{2}</script><script type="math/tex; mode=display">b_N = b_0 \frac{1}{2}\sum_{n=1}^{N}(x_n-\mu)^2 = b_0 + \frac{N}{2}\sigma_{ML}^{2}</script><p>现在假设<strong>均值</strong>和<strong>精度</strong>都是未知的。为了找到<strong>共轭先验</strong>，考虑似然函数对于 $\mu$ 和 $\lambda$ 的依赖关系：</p><script type="math/tex; mode=display">\begin{aligned} p(\mathbf{x}|\mu,\lambda) &= \prod_{n=1}^{N} \left(\frac{\lambda}{2\pi} \right)^{\frac{1}{2}} \exp \left\{-\frac{\lambda}{2}(x_n-\mu)^{2}\right\} \\ &\propto \left[\lambda^{\frac{1}{2}} \exp\left(-\frac{\lambda \mu^{2}}{2}\right) \right]^{N} \exp \left\{\lambda \mu \sum_{n=1}^{N}x_n - \frac{\lambda}{2}\sum_{n=1}^{N}x_{n}^{2}\right\} \end{aligned}\tag{2.86}</script><p>假设先验分布的形式为：</p><script type="math/tex; mode=display">\begin{aligned} p(\mu,\lambda) &= \exp \left\{-\frac{\beta \lambda}{2}\left(\mu-\frac{c}{\beta}\right)^2 \right\} \lambda^{\frac{\beta}{2}} \exp \left\{-\left(d-\frac{c^2}{2\beta}\right)\lambda \right\} \\ &\propto \left[\lambda^{\frac{1}{2}} \exp\left(-\frac{\lambda \mu^{2}}{2}\right) \right]^{\beta} \exp \left\{c\lambda \mu - d\lambda\right\} \end{aligned}\tag{2.87}</script><p>其 中 $c, d$ 和 $\beta$ 都是常数。</p><p>归⼀化的先验概率的形式为：</p><script type="math/tex; mode=display">p(\mu,\lambda) = \mathcal{N}(\mu|\mu_0, (\beta \lambda)^{-1})\text{Gam}(\lambda|a,b)\tag{2.88}</script><p>这被称为<strong>正态-<code>Gamma</code>分布</strong>或者<strong>⾼斯-<code>Gamma</code>分布</strong>。如图2.21：</p><p><img src="/images/prml_20190927204821.png" alt="正态-Gamma分布"></p><p>对于 $D$ 维向量 $\boldsymbol{x}$ 的多元⾼斯分布 $\mathcal{N}(\boldsymbol{x|\mu, \Lambda}^{−1})$，假设精度已知，则均值 $\boldsymbol{\mu}$ 的共轭先验分布仍然是⾼斯分布。对于已知均值未知精度矩阵 $\boldsymbol{\Lambda}$ 的情形，共轭先验是<strong><code>Wishart</code>分布</strong>，定义为：</p><script type="math/tex; mode=display">\mathcal{W}(\mathbf{\Lambda} | \boldsymbol{W}, \nu)=B|\boldsymbol{\Lambda}|^{\frac{\nu-D-1}{2}} \exp \left(-\frac{1}{2} \operatorname{Tr}\left(\boldsymbol{W}^{-1} \boldsymbol{\Lambda}\right)\right)\tag{2.89}</script><p>其中 $\nu$ 被称为<strong>分布的⾃由度数量</strong>(<code>degrees of freedom</code>)，$\boldsymbol{W}$ 是⼀个 $D \times D$ 的标量矩阵，$\operatorname{Tr}(·)$ 表⽰矩阵的<strong>迹</strong>。归⼀化系数 $B$ 为：</p><script type="math/tex; mode=display">B(\boldsymbol{W}, \nu)=|\boldsymbol{W}|^{-\frac{\nu}{2}}\left(2^{\frac{\nu D}{2}} \pi^{\frac{D(D-1)}{4}} \prod_{i=1}^{D} \Gamma\left(\frac{\nu+1-i}{2}\right)\right)^{-1}\tag{2.90}</script><p>如果均值和精度都是未知的，那么类似于⼀元变量的推理⽅法，<strong>共轭先验</strong>为：</p><script type="math/tex; mode=display">p(\boldsymbol{\mu,\Lambda|\mu}_0,\beta,\boldsymbol{W}, \nu) = \mathcal{N}(\boldsymbol{\mu|\mu}_0, (\beta \boldsymbol{\Lambda})^{-1})\mathcal{W}(\mathbf{\Lambda} | \boldsymbol{W}, \nu)\tag{2.91}</script><p>这被称为<strong>正态-<code>Wishart</code>分布</strong>或者<strong>⾼斯-<code>Wishart</code>分布</strong>。</p><h1 id="八，学生-mathbf-t-分布"><a href="#八，学生-mathbf-t-分布" class="headerlink" title="八，学生 $\mathbf{t}$ 分布"></a>八，学生 $\mathbf{t}$ 分布</h1><p>如果有⼀个⼀元⾼斯分布 $\mathcal{N}\left(x | \mu, \tau^{-1}\right)$ 和⼀个 <code>Gamma</code>先验分布 $\text{Gam}(\tau|a, b)$，把精度积分出来，便可以得到 $x$ 的边缘分布，形式为：</p><script type="math/tex; mode=display">\begin{aligned} p(x | \mu, a, b) &=\int_{0}^{\infty} \mathcal{N}\left(x | \mu, \tau^{-1}\right) \operatorname{Gam}(\tau | a, b) \mathrm{d} \tau \\ &=\int_{0}^{\infty} \frac{b^{a} e^{(-b r)} \tau^{a-1}}{\Gamma(a)}\left(\frac{\tau}{2 \pi}\right)^{\frac{1}{2}} \exp \left\{-\frac{\tau}{2}(x-\mu)^{2}\right\} \mathrm{d} \tau \\ &=\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{\frac{1}{2}}\left[b+\frac{(x-\mu)^{2}}{2}\right]^{-a-\frac{1}{2}} \Gamma\left(a+\frac{1}{2}\right) \end{aligned}\tag{2.92}</script><p>形如 $p(x|\mu a,b)$ 如下：</p><script type="math/tex; mode=display">\text{St}(x|\mu,\lambda,\nu) = \frac{\Gamma(\frac{\nu}{2}+\frac{1}{2})}{\Gamma(\frac{\nu}{2})}\left(\frac{\lambda}{\pi \nu}\right)^{\frac{1}{2}}\left[1+\frac{\lambda(x-\mu)^2}{\nu}\right]^{-\frac{\nu}{2}-\frac{1}{2}}\tag{2.93}</script><p>称为<strong>学生 t 分布</strong>（<code>Student&#39;s t-distribution</code>）。 参数 $\lambda$ 有时被称为 $\mathbf{t}$ 分布的<strong>精度</strong>（<code>precision</code>）， 即使它通常不等于⽅差的倒数。参数 $\nu$ 被称为<strong>⾃由度</strong>（<code>degrees of freedom</code>）。如图2.22：</p><p><img src="/images/prml_20190927221621.png" alt="学生t分布"></p><p>学生 $\mathbf{t}$ 分布的⼀个<strong>重要性质</strong>：<strong>鲁棒性</strong>（robustness），即对于数据集⾥的⼏个离群点<code>outlier</code>的出现，分布不会像⾼斯分布那样敏感。</p><p>图 2.23，从⼀个<strong>⾼斯分布</strong>中抽取的30个数据点的直⽅图，以及得到的最⼤似然拟合。红⾊曲线表⽰使⽤ $\mathbf{t}$ 分布进⾏的拟合，绿⾊曲线（⼤部分隐藏在了红⾊曲 线后⾯）表⽰使⽤⾼斯分布进⾏的拟合。由于 $\mathbf{t}$  分布将⾼斯分布作为⼀种特例，因此它给出了与⾼斯分布⼏乎相同的解。</p><p><img src="/images/prml_20190927223450.png" alt="t分布与高斯分布a"></p><p>图 2.24，与图2.23同样的数据集，但是多了三个异常数据点。这幅图展⽰了⾼斯分布（绿⾊曲线）是如 何被异常点强烈地⼲扰的，⽽ $\mathbf{t}$ 分布（红⾊曲线）相对不受影响。</p><p><img src="/images/prml_20190927223502.png" alt="t分布与高斯分布b"></p><p>推⼴到多元⾼斯分布 $\mathcal{N}(\boldsymbol{x|\mu, \Lambda})$ 来得到对应的多元学生 $\mathbf{t}$ 分布，形式为：</p><script type="math/tex; mode=display">\operatorname{St}(\boldsymbol{x} | \boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)=\int_{0}^{\infty} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu},(\eta \boldsymbol{\Lambda})^{-1}\right) \operatorname{Gam}\left(\eta | \frac{\nu}{2}, \frac{\nu}{2}\right) \mathrm{d} \nu \tag{2.94}</script><p>求积分，可得：</p><script type="math/tex; mode=display">\text{St}(\boldsymbol{x} | \boldsymbol{\mu}, \boldsymbol{\Lambda},,\nu) = \frac{\Gamma(\frac{\nu}{2}+\frac{D}{2})}{\Gamma(\frac{\nu}{2})}\left(\frac{|\boldsymbol{\Lambda}|}{(\pi \nu)^D}\right)^{\frac{1}{2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-\frac{\nu}{2}-\frac{D}{2}}\tag{2.95}</script><p>其中 $D$ 是 $\boldsymbol{x}$ 的维度，$\Delta^2$ 是<strong>平⽅马⽒距离</strong>，定义为：</p><script type="math/tex; mode=display">\Delta^2 = (\boldsymbol{x-\mu})^T \boldsymbol{\Lambda} (\boldsymbol{x-\mu})\tag{2.96}</script><p>多元变量形式的学生 $\mathbf{t}$ 分布，满⾜下⾯的性质：</p><p>1）$\mathbb{E}[\boldsymbol{x}] = \boldsymbol{\mu}$   如果 $\nu \gt 1$</p><p>2）$\text{cov}[\boldsymbol{x}] = \frac{\nu}{\nu-2}\boldsymbol{\Lambda}^{-1}$   如果 $\nu \gt 2$</p><p>3）$\text{mode}[\boldsymbol{x}] = \boldsymbol{\mu}$   </p><h1 id="九，周期变量"><a href="#九，周期变量" class="headerlink" title="九，周期变量"></a>九，周期变量</h1><p>考察⼀个⼆维单位向量 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ ， 其中 $||\boldsymbol{x}_n|| = 1$ 且 $n = 1,\dots , N$ ， 如图2.25所⽰。</p><p><img src="/images/prml_20190927230755.png" alt="⼆维单位向量"></p><p>可以对向量 $\{\boldsymbol{x}_n\}$ 求平均，可得</p><script type="math/tex; mode=display">\bar{\boldsymbol{x}} = \frac{1}{N}\sum_{n=1}^{N}\boldsymbol{x}_n</script><p>注意，$\bar{\boldsymbol{x}}$ 通常位于单位圆的内部。</p><p>$\bar{\boldsymbol{x}}$ 对应的角度 $\bar{\theta}$ 为：</p><script type="math/tex; mode=display">\bar{\theta} = \tan^{-1} \left\{\frac{\sum_{n}\sin \theta_n}{\sum_{n}\cos \theta_n} \right\}\tag{2.97}</script><p>考虑的周期概率分布 $p(\theta)$ 的周期为 $2\pi$ 。$\theta$ 上的任何概率密度 $p(\theta)$ ⼀定⾮负， 积分等于1，并且⼀定是周期性的。因此， $p(\theta)$ ⼀定满⾜下⾯三个条件：</p><p>1） $p(\theta) \ge 0$ </p><p>2） $\int_{0}^{2\pi} p(\theta) \mathrm{d}\theta = 1$ </p><p>3） $p(\theta + 2\pi) = p(\theta)$ </p><p>考虑两个变量 $\boldsymbol{x} = (x_1 , x_2)$ 的⾼斯分布，均值为 $\boldsymbol{\mu} = (\mu_1, \mu_2)$，协⽅差矩阵为 $\boldsymbol{\Sigma} = \sigma^2 \boldsymbol{I}$ ，其中 $\boldsymbol{I}$ 是⼀个 $2\times2$ 的单位矩阵。因此有：</p><script type="math/tex; mode=display">p(x_1,x_2) = \frac{1}{2\pi \sigma^{2}} \exp \left\{-\frac{(x_1-\mu_1)^2+(x_2-\mu_2)^{2}}{2\sigma^{2}}\right\}\tag{2.98}</script><p><strong><code>von Mises</code>分布</strong>(<strong>环形正态分布</strong>（<code>circular normal</code>））：在单位圆 $r=1$上的概率分布 $p(\theta)$ 的最终表达式：</p><script type="math/tex; mode=display">p(\theta|\theta_0,m) = \frac{1}{2\pi I_0(m)} \exp \left\{m\cos(\theta-\theta_0)\right\}\tag{2.99}</script><p>其中，参数 $\theta_0$ 对应于分布的均值，$m$ 被称为 <strong><code>concentration</code>参数</strong>，类似于⾼斯分布的⽅差的倒数（<strong>精度</strong>）。归⼀化系数包含项 $I_0 (m)$，是<strong>零阶修正的第⼀类<code>Bessel</code>函数</strong>（<code>Abramowitz and Stegun</code>, 1965）， 定义为：</p><script type="math/tex; mode=display">I_0(m) = \frac{1}{2\pi} \int_{0}^{2\pi}\exp\{m\cos \theta\}\mathrm{d}\theta\tag{2.100}</script><p>如图2.26～2.27，<code>von Mises</code>分布的图像。</p><p><img src="/images/prml_20190927234905.png" alt="笛卡尔坐标系"></p><p><img src="/images/prml_20190927234914.png" alt="极坐标系"></p><p>如图2.28， <code>Bessel</code>函数 $I_0 (m)$ 的图像。</p><p><img src="/images/prml_20190928090610.png" alt="Bessel函数"></p><p>现在考虑 <strong><code>von Mises</code>分布</strong> 的参数 $\theta_0$ 和参数 $m$ 的最⼤似然估计。对数似然函数为：</p><script type="math/tex; mode=display">\ln p(\mathcal{D} | \theta_0,m)=-N\ln (2\pi)-\ln I_0(m)+m\sum_{n=1}^{N}\cos(\theta_n-\theta_0)\tag{2.101}</script><p>令其关于 $\theta_0$ 的导数等于零，从⽽可以得到：</p><script type="math/tex; mode=display">\theta_{0}^{ML} = \tan^{-1} \left\{\frac{\sum_{n}\sin \theta_n}{\sum_{n}\cos \theta_n} \right\}\tag{2.102}</script><p>关于 $m$ 最⼤化公式(2.101)，使⽤ $I_0^{\prime}(m)=I_1(m)$（<code>Abramowitz and Stegun</code>, 1965），从⽽可以得到：</p><script type="math/tex; mode=display">A(m_{NL})=\frac{1}{N}\sum_{n=1}^{N}\cos(\theta_{n}-\theta_{0}^{ML})\tag{2.103}</script><p>令</p><script type="math/tex; mode=display">A(m)=\frac{I_1(m)}{I_0(m)}</script><p>可以得到：</p><script type="math/tex; mode=display">A(m_{ML})=\left(\frac{1}{N}\sum_{n=1}^{N}\cos \theta_{n}\right)\cos \theta_{0}^{ML} + \left(\frac{1}{N}\sum_{n=1}^{N}\sin \theta_{n}\right)\sin \theta_{0}^{ML}\tag{2.104}</script><p>如图2.29， 函数 $A (m)$ 的图像。</p><p><img src="/images/prml_20190928090629.png" alt="Am函数"></p><h1 id="十，混合高斯模型"><a href="#十，混合高斯模型" class="headerlink" title="十，混合高斯模型"></a>十，混合高斯模型</h1><p>通过将更基本的概率分布（例如⾼斯分布）进⾏线性组合的这样的叠加⽅法，可以被形式化为概率模型，被称为<strong>混合模型</strong>（<code>mixture distributions</code>）（<code>McLachlan and Basford</code>, 1988; <code>McLachlan and Peel</code>, 2000）。</p><p>考虑 $K$ 个⾼斯概率密度的叠加，形式为：</p><script type="math/tex; mode=display">p(\boldsymbol{x}) = \sum_{k=1}^{K} \pi_{k} \mathcal{N}(\boldsymbol{x} |\boldsymbol{\mu_{k}}, \boldsymbol{\Sigma}_{k})\tag{2.105}</script><p>这被称为<strong>混合⾼斯</strong>（<code>mixture of Gaussians</code>）。 每⼀个⾼斯概率密度 $\mathcal{N}(\boldsymbol{x} |\boldsymbol{\mu_{k}}, \boldsymbol{\Sigma}_{k})$ 被称为混合分布的⼀个<strong>成分</strong>（<code>component</code>），并且有⾃⼰的均值 $\boldsymbol{\mu_{k}}$ 和协⽅差 $\boldsymbol{\Sigma}_{k}$。参数 $\pi_{k}$ 被称为<strong>混合系数</strong>（<code>mixing coefficients</code>），并且满足以下条件：</p><p>1）$\sum_{k=1}^{K} \pi_{k}=1$<br>2）$0\le \pi_{k} \le 1$</p><p>如图2.30，每个混合分量的常数概率密度轮廓线，其中三个分量分别被标记为红⾊、蓝⾊和绿⾊， 且混合系数的值在每个分量的下⽅给出。 </p><p><img src="/images/prml_ 20190928095829.png" alt="概率密度轮廓线"></p><p>如图2.31， 混合分布的边缘概率密度 $p(\boldsymbol{x})$ 的轮廓线。</p><p><img src="/images/prml_20190928095839.png" alt="边缘概率密度轮廓线"></p><p>如图2.32， 概率分布 $p(\boldsymbol{x})$ 的⼀个曲⾯图。</p><p><img src="/images/prml_20190928095852.png" alt="概率密度曲面图"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，多元高斯分布&quot;&gt;&lt;a href=&quot;#一，多元高斯分布&quot; class=&quot;headerlink&quot; title=&quot;一，多元高斯分布&quot;&gt;&lt;/a&gt;一，多元高斯
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】概率分布之变量</title>
    <link href="https://zhangbc.github.io/2019/09/29/prml_02_01/"/>
    <id>https://zhangbc.github.io/2019/09/29/prml_02_01/</id>
    <published>2019-09-29T06:26:42.000Z</published>
    <updated>2019-10-07T14:37:49.296Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，二元变量"><a href="#一，二元变量" class="headerlink" title="一，二元变量"></a>一，二元变量</h1><h2 id="1，二项分布"><a href="#1，二项分布" class="headerlink" title="1，二项分布"></a>1，二项分布</h2><p>考虑⼀个⼆元随机变量 $x \in \{0, 1\}$。 例如，$x$ 可能描述了扔硬币的结果，$x = 1$ 表⽰“正⾯”，$x = 0$ 表⽰反⾯。我们可以假设有⼀个损坏的硬币，这枚硬币正⾯朝上的概率未必等于反⾯朝上的概率。$x = 1$ 的概率被记作参数 $\mu$，因此有：</p><script type="math/tex; mode=display">p(x=1|\mu) = \mu\tag{2.1}</script><p>其中 $0\le \mu\le 1$ 。$x$ 的概率分布因此可以写成：</p><script type="math/tex; mode=display">\text {Bern}(x|\mu) = \mu^{x}(1-\mu)^{1-x}\tag{2.2}</script><p>这被叫做<strong>伯努利分布</strong>（<code>Bernoulli distribution</code>）。容易证明，这个分布是归⼀化的，并且均值和⽅差分别为：</p><script type="math/tex; mode=display">\mathbb{E}[x] = \mu\tag{2.3}</script><script type="math/tex; mode=display">\text{var}[x] = \mu(1-\mu)\tag{2.4}</script><p>如图 2.1，⼆项分布关于 $m$ 的函数的直⽅图，其中 $N = 10$ 且 $\mu = 0.25$。</p><p><img src="/images/prml_20190919231340.png" alt="⼆项分布"></p><p>假设我们有⼀个 $x$ 的观测值的数据集 $\mathcal{D} = \{x_1 ,\dots, x_N\}$。假设每次观测都是独⽴地从 $p(x | \mu)$ 中抽取的，因此可以构造关于 $\mu$ 的似然函数：</p><script type="math/tex; mode=display">p(\mathcal{D}|\mu) = \prod_{n=1}^{N}p(x_{n}|\mu) = \prod_{n=1}^{N}\mu^{x_{n}}(1-\mu)^{1-x_{n}}\tag{2.5}</script><p>其对数似然函数：</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\mu) = \sum_{n=1}^{N}\ln p(x_{n}|\mu) = \sum_{n=1}^{N}\{ x^n \ln \mu + (1-x^n) \ln (1-\mu)\}\tag{2.6}</script><p>在公式(2.6)中，令 $\ln p(\mathcal{D}|\mu)$ 关于 $\mu$ 的导数等于零，就得到了最⼤似然的估计值，也被称为<strong>样本均值</strong>（<code>sample mean</code>）：</p><script type="math/tex; mode=display">\mu_{ML} = \frac{1}{N} \sum_{n=1}^{N} x_{n}\tag{2.7}</script><p>求解给定数据集规模 $N$ 的条件下，$x = 1$ 的观测出现的数量 $m$ 的概率分布。 这被称为<strong>⼆项分布</strong> （<code>binomial distribution</code>）：</p><script type="math/tex; mode=display">\text {Bin}(m|N, \mu) = \dbinom{N}{m} \mu^{m}(1-\mu)^{N-m}\tag{2.8}</script><p>其中，</p><script type="math/tex; mode=display">\dbinom{N}{m} = \frac{N!}{(N-m)!m!}\tag{2.9}</script><p><strong>二项分布</strong> 的均值和⽅差分别为：</p><script type="math/tex; mode=display">\mathbb{E}[m] = \sum_{m=0}^{N} \text{Bin}(m|N, \mu) = N\mu\tag{2.10}</script><script type="math/tex; mode=display">\text{var}[m] = \sum_{m=0}^{N} (m-\mathbb{E}[m])^{2} \text{Bin}(m|N, \mu) = N\mu(1-\mu)\tag{2.11}</script><h2 id="2，Beta分布"><a href="#2，Beta分布" class="headerlink" title="2，Beta分布"></a>2，<code>Beta</code>分布</h2><p>首先，<strong><code>Gamma</code>函数</strong>的定义为：</p><script type="math/tex; mode=display">\Gamma(x) \equiv \int_{0}^{\infty} u^{x-1} e^{-u} \mathrm{d} u\tag{2.12}</script><p><strong><code>Gamma</code>函数</strong>具有如下性质：</p><p>1）$\Gamma(x+1) = x \Gamma(x)$<br>2）$\Gamma(1)=1$<br>3）当 $n$ 为整数时，$\Gamma(n+1) = n!$</p><p>如果我们选择⼀个正⽐于 $\mu$ 和 $(1 − \mu)$ 的幂指数的先验概率分布， 那么后验概率分布（正⽐于先验和似然函数的乘积）就会有着与先验分布相同的函数形式。这 个性质被叫做<strong>共轭性（<code>conjugacy</code>）</strong>。<br>先验分布选择<strong><code>Beta</code>分布</strong>定义为：</p><script type="math/tex; mode=display">\text {Beta}(\mu | a,b) = \frac{\Gamma{(a+b)}}{\Gamma{(a)}\Gamma{(b)}} \mu^{(a-1)}(1-\mu)^{(b-1)}\tag{2.13}</script><p>其中参数 $a$ 和 $b$ 经常被称为<strong>超参数</strong>（<code>hyperparameter</code>），均值和⽅差分别为：</p><script type="math/tex; mode=display">\mathbb{E}[\mu] = \frac{a}{a+b}\tag{2.14}</script><script type="math/tex; mode=display">\text{var}[\mu] = \frac{ab}{(a+b)^{2}(a+b+1)}\tag{2.15}</script><p>把<code>Beta</code>先验与⼆项似然函数相乘，然后归⼀化。只保留依赖于 $\mu$ 的因⼦，从而得到后验概率分布的形式为：</p><script type="math/tex; mode=display">p(\mu | m, l, a,b) = \frac{\Gamma{(m+a+l+b)}}{\Gamma{(m+a)}\Gamma{(l+b)}} \mu^{(m+a-1)}(1-\mu)^{(l+b-1)}\tag{2.16}</script><p>其中 $l = N − m$。</p><p>如图2.2～2.5： 对于不同的超参数 $a$ 和 $b$，公式(2.13)给出的<code>Beta</code>分布 $\text{Beta}(\mu | a, b)$ 关于 $\mu$ 的函数图像。</p><p><img src="/images/prml_20190920095614.png" alt="a=0.1,b=0.1"></p><p><img src="/images/prml_20190920095624.png" alt="a=1,b=1"></p><p><img src="/images/prml_20190920095635.png" alt="a=2,b=3"></p><p><img src="/images/prml_20190920095646.png" alt="a=8,b=4"></p><p><strong>贝叶斯学习过程</strong>存在⼀个共有的属性：随着我们观测到越来越多的数据，后验概率表⽰的不确定性将会持续下降。</p><p>为了说明这⼀点，我们可以⽤频率学家的观点考虑贝叶斯学习问题。考虑⼀个⼀般的贝叶斯推断问题，参数为 $\boldsymbol {\theta}$ ，并且我们观测到了⼀个数据集 $\mathcal{D}$，由联合概率分布 $p(\boldsymbol {\theta}, \mathcal{D})$ 描述，有：</p><script type="math/tex; mode=display">\mathbb{E}_{\boldsymbol {\theta}}[\boldsymbol {\theta}] = \mathbb{E}_{\mathcal{D}}[\mathbb{E}_{\boldsymbol {\theta}}[\boldsymbol {\theta}|\mathcal{D}]]\tag{2.17}</script><p>其中，</p><script type="math/tex; mode=display">\mathbb{E}_{\boldsymbol {\theta}}[\boldsymbol {\theta}] = \int p(\boldsymbol {\theta}) \boldsymbol {\theta} \mathrm{d} \boldsymbol {\theta}\tag{2.18}</script><script type="math/tex; mode=display">\mathbb{E}_{\mathcal{D}}[\mathbb{E}_{\boldsymbol {\theta}}[\boldsymbol {\theta}|\mathcal{D}]] = \int \left \{ \int \boldsymbol {\theta}p(\boldsymbol {\theta}|\mathcal{D}) \mathrm{d} \boldsymbol {\theta} \right \} p(\mathcal{D})\mathrm{d} \mathcal{D}\tag{2.19}</script><p>方差，</p><script type="math/tex; mode=display">\text{var}_{\boldsymbol {\theta}}[\boldsymbol {\theta}] = \mathbb{E}_{\mathcal{D}}[\text{var}[\boldsymbol {\theta}|\mathcal{D}]] + \text{var}_{\mathcal{D}} [\mathbb{E}_{\boldsymbol {\theta}}[\boldsymbol {\theta}|\mathcal{D}]]\tag{2.20}</script><h1 id="二，多项式变量"><a href="#二，多项式变量" class="headerlink" title="二，多项式变量"></a>二，多项式变量</h1><h2 id="1，多项式分布"><a href="#1，多项式分布" class="headerlink" title="1，多项式分布"></a>1，多项式分布</h2><p><strong>“1-of-K ”表⽰法</strong> ： 变量被表⽰成⼀个 $K$ 维向量 $\boldsymbol{x}$，向量中的⼀个元素 $x_k$ 等于1，剩余的元素等于0。注意，这样的向量 $\boldsymbol{x}$ 满足 $\sum_{k=1}^{K} x_k = 1$ ，如果我们⽤参数 $\mu_k$ 表⽰ $x_k = 1$ 的概率，那么 $\boldsymbol{x}$ 的分布：</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\boldsymbol{\mu}) = \prod_{k=1}^{K} \mu_{k}^{x_k}\tag{2.21}</script><p>其中 $\boldsymbol{\mu} = (\mu_1 ,\dots, \mu_K)^T$ ， 参数 $\mu_k$ 要满⾜ $\mu_k \ge 0$ 和 $\sum_{k} \mu_k = 1$ 。</p><p>容易看出，这个分布是归⼀化的：</p><script type="math/tex; mode=display">\sum_{\boldsymbol {x}}p(\boldsymbol{x} | \boldsymbol{\mu}) = \sum_{k=1}^{K} \mu_k = 1\tag{2.22}</script><p>并且，</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{x}|\boldsymbol{\mu}] = \sum_{\boldsymbol {x}}p(\boldsymbol{x} | \boldsymbol{\mu}) \boldsymbol{x} = (\mu_1 ,\dots, \mu_K)^T = \boldsymbol {\mu}\tag{2.23}</script><p>现在考虑⼀个有 $N$ 个独⽴观测值 $\boldsymbol {x}_1 ,\dots, \boldsymbol {x}_N$ 的数据集 $\mathcal{D}$。对应的似然函数的形式为：</p><script type="math/tex; mode=display">p(\mathcal{D}|\boldsymbol{\mu}) = \prod_{n=1}^{N} \prod_{k=1}^{K} \mu_{k}^{x_{nk}} = \prod_{k=1}^{K} \mu_{k}^{(\sum_{n}x_{nk})} = \prod_{k=1}^{K} \mu_{k}^{m_k}\tag{2.24}</script><p>看到似然函数对于 $N$ 个数据点的依赖只是通过 $K$ 个下⾯形式的量：</p><script type="math/tex; mode=display">m_k = \sum_{n}x_{nk}\tag{2.25}</script><p>它表⽰观测到 $x_k = 1$ 的次数。这被称为这个分布的<strong>充分统计量</strong>（<code>sufficient statistics</code>）。</p><p>通过<strong>拉格朗⽇乘数法</strong>容易求得最大似然函数：</p><script type="math/tex; mode=display">\mu_k^{ML} = \frac{m_k}{N}\tag{2.26}</script><p>考虑 $m_1 ,\dots , m_K$ 在参数 $\boldsymbol{\mu}$ 和观测总数 $N$ 条件下的联合分布。根据公式(2.24)，这个分布的形式为：</p><script type="math/tex; mode=display">\text{Mult}(m_1 ,\dots , m_K | \boldsymbol{\mu}, N) = \dbinom{N}{m_1 \dots  m_K}\prod_{k=1}^{K} \mu_{k}^{m_k}\tag{2.27}</script><p>这被称为<strong>多项式分布</strong>（<code>multinomial distribution</code>）。 归⼀化系数是把 $N$ 个物体分成⼤⼩为 $m_1 ,\dots , m_K$ 的 $K$ 组的⽅案总数，定义为：</p><script type="math/tex; mode=display">\dbinom{N}{m_1 \dots  m_K} = \frac{N!}{m_1!m_2! \dots m_K!}\tag{2.28}</script><p>其中，$m_k$ 满足以下限制 $\sum_{k=1}^{K} m_k = N$ 。</p><h2 id="2，狄利克雷分布"><a href="#2，狄利克雷分布" class="headerlink" title="2，狄利克雷分布"></a>2，狄利克雷分布</h2><p><strong>狄利克雷分布</strong>（<code>Dirichlet distribution</code>）或<strong>多元<code>Beta</code>分布</strong>（<code>multivariate Beta distribution</code>）是一类在实数域以正单纯形（<code>standard simplex</code>）为支撑集（<code>support</code>）的高维连续概率分布，是 <strong><code>Beta</code>分布</strong>在高维情形的推广  。狄利克雷分布是<strong>指数族分布</strong>之一，也是<strong>刘维尔分布</strong>（<code>Liouville distribution</code>）的特殊形式，将狄利克雷分布的解析形式进行推广可以得到<strong>广义狄利克雷分布</strong>（<code>generalized Dirichlet distribution</code>）和<strong>组合狄利克雷分布</strong>（<code>Grouped Dirichlet distribution</code>）。</p><p><strong>狄利克雷分布</strong>概率的归⼀化形式为：</p><script type="math/tex; mode=display">\text{Dir}(\boldsymbol{\mu}|\boldsymbol{\alpha}) = \frac{\Gamma{(\alpha_{0})}}{\Gamma{(\alpha_{1})} \dots \Gamma{(\alpha_{K})}} \prod_{k=1}^{K}\mu_{k}^{\alpha_{k-1}}\tag{2.29}</script><p>其中，$\alpha_{0}=\sum_{k=1}^{K} \alpha_{k}$ 。</p><p>如图 2.6～2.8： 在不同的参数 $\alpha_{k}$ 的情况下，单纯形上的狄利克雷分布的图像。</p><p><img src="/images/prml_20190920160005.png" alt="ak=0.1"></p><p><img src="/images/prml_20190920160012.png" alt="ak=1"></p><p><img src="/images/prml_20190920160020.png" alt="ak=10"></p><p>如图2.9～2.11： 对于不同的 $N$ 值，$N$ 个均匀分布的均值的直⽅图。</p><p><img src="/images/prml_20190920160607.png" alt="N=1"></p><p><img src="/images/prml_20190920160615.png" alt="N=2"></p><p><img src="/images/prml_20190920160621.png" alt="N=10"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，二元变量&quot;&gt;&lt;a href=&quot;#一，二元变量&quot; class=&quot;headerlink&quot; title=&quot;一，二元变量&quot;&gt;&lt;/a&gt;一，二元变量&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】总论</title>
    <link href="https://zhangbc.github.io/2019/09/19/prml_01_pandect/"/>
    <id>https://zhangbc.github.io/2019/09/19/prml_01_pandect/</id>
    <published>2019-09-19T00:49:10.000Z</published>
    <updated>2019-10-07T14:25:30.150Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，-概率论"><a href="#一，-概率论" class="headerlink" title="一， 概率论"></a>一， 概率论</h1><h2 id="1，离散型随机变量概率"><a href="#1，离散型随机变量概率" class="headerlink" title="1，离散型随机变量概率"></a>1，离散型随机变量概率</h2><p>假设随机变量 $X$ 可以取任意 的 $x_i$ ，其中 $i = 1, \dots. , M$ ，并且随机变量 $Y$ 可以取任意的 $y_j$ ，其中 $j = 1,\dots , L$。考虑 $N$ 次试验，其中我们对 $X$ 和 $Y$ 都进⾏取样， 把 $X = x_i$ 且 $Y = y_j$ 的试验的数量记作 $n_{ij}$ ，并且，把 $X$ 取值 $x_i$ （与 $Y$ 的取值⽆关）的试验的数量记作 $c_i$ ，类似地，把 $Y$ 取值 $y_j$ 的试验的数量记作 $r_j$ 。</p><p>$X$ 取值 $x_i$ 且 $Y$ 取值 $y_j$ 的概率被记作 $p(X = x_i , Y = y_j )$， 被称为 $X = x_i$ 和 $Y = y_j$ 的<strong>联合概率</strong> （<code>joint probability</code>）。它的计算⽅法为落在单元格 $i, j$ 的点的数量与点的总数的⽐值，即：</p><script type="math/tex; mode=display">p\left(X=x_{i}, Y=y_{i}\right)=\frac{n_{i j}}{N}\tag{1.5}</script><p>如图1.15所示，联合概率的计算方法。</p><p><img src="/images/prml_20190917233144.png" alt="联合概率计算"></p><p>类似地，$X$ 取值 $x_i$ （与 $Y$ 取值无关）的概率被记作 $p(X = x_i )$ ，也称为<strong>边缘概率</strong>（<code>marginal probability</code>），计算⽅法为落在列$i$上的点的数量与点的总数的⽐值，即：</p><script type="math/tex; mode=display">p\left(X=x_{i}\right)=\frac{c_{j}}{N}\tag{1.6}</script><p>由于图1.15中列 $i$ 上的实例总数就是这列的所有单元格中实例的数量之和，即$c_{i}=\sum_{j} n_{i j}$，因此根据公式(1.5)和公式(1.6)，我们可以得到概率的<strong>加和规则</strong>（<code>sun rule</code>），即：</p><script type="math/tex; mode=display">p\left(X=x_{j}\right)=\sum_{j=1}^{L} p\left(X=x_{i}, Y=y_{j}\right)\tag{1.7}</script><p>如果我们只考虑那些 $X = x_i$ 的实例， 那么这些实例中 $Y = y_j$ 的实例所占的⽐例被写成 $p(Y = y_j | X = x_i)$，被称为给定 $X = x_i$ 的 $Y = y_j$ 的<strong>条件概率</strong>（<code>conditional probability</code>），其计算⽅式为：计算落在单元格 $i, j$ 的点的数量列 $i$ 的点的数量的⽐值，即：</p><script type="math/tex; mode=display">p\left(Y=y_{j} | X=x_{i}\right)=\frac{n_{i j}}{c_{i}}\tag{1.8}</script><p>从公式(1.5)、公式(1.6)、公式(1.8)可以推导出概率的<strong>乘积规则</strong>（<code>product rule</code>），即：</p><script type="math/tex; mode=display">p\left(X=x_{i}, Y=y_{j}\right)=\frac{n_{i j}}{N}=\frac{n_{i j}}{c_{i}} \cdot \frac{c_{i}}{N}=p\left(Y=y_{j} | X=x_{i}\right) p\left(X=x_{i}\right)\tag{1.9}</script><p>根据<strong>乘积规则</strong>，以及对称性 $p(X, Y ) = p(Y, X)$，我们⽴即得到了下⾯的两个条件概率之间的关系，称为<strong>贝叶斯定理</strong>（<code>Bayes&#39; theorem</code>）即：</p><script type="math/tex; mode=display">p(Y | X)=\frac{p(X | Y) p(Y)}{p(X)}\tag{1.10}</script><p><strong>贝叶斯定理</strong>（<code>Bayes&#39; theorem</code>），在模式识别和机器学习领域扮演者中⼼⾓⾊。使⽤<strong>加和规则</strong>，贝叶斯定理中的<strong>分母</strong>可以⽤出现在分⼦中的项表⽰，这样就可以把分母看作归一常数，即：</p><script type="math/tex; mode=display">p(X)=\sum_{Y} p(X|Y) p(Y)\tag{1.11}</script><p>如果两个变量的联合分布可以分解成两个边缘分布的乘积，即 $p(X, Y) = p(X)p(Y)$， 那么我们说 $X$ 和 $Y$ <strong>相互独⽴</strong>（<code>independent</code>）。</p><h2 id="2，概率密度"><a href="#2，概率密度" class="headerlink" title="2，概率密度"></a>2，概率密度</h2><p>如果⼀个实值变量x的概率 落在区间 $(x, x + \delta x)$ 的概率由 $p(x)\delta x$ 给出（$\delta x \to 0$）， 那么 $p(x)$ 叫做 $x$ 的<strong>概率密度</strong>（<code>probability density</code>）。$x$ 位于区间 $(a, b)$ 的概率：</p><script type="math/tex; mode=display">p(x \in(a, b))=\int_{a}^{b} p(x) \mathrm{d}x\tag{1.12}</script><p>如图1.16，概率密度函数。</p><p><img src="/images/prml_20190918085229.png" alt="离散型变量的概率与概率密度函数"></p><p>由于概率是⾮负的，并且 $x$ 的值⼀定位于实数轴上得某个位置，因此概率密度⼀定满⾜下⾯两个<strong>条件</strong>：</p><p>1）$p(x) \geq 0$</p><p>2) $\int_{-\infty}^{\infty} p(x) \mathrm{d} x=1$</p><p>在变量以⾮线性的形式变化的情况下，概率密度函数通过<code>Jacobian</code>因⼦变换为与简单的函数不同的形式。</p><p>例如，假设我们考虑⼀个变量的变化 $x = g(y)$， 那么函数 $f(x)$ 就变成 了 $\tilde{f}(y)=f(g(y))$。现在让我们考虑⼀个概率密度函数 $p_x (x)$，它对应于⼀个关于新变量 $y$ 的密度函数 $p_y (y)$，对于很⼩的 $\delta x$ 的值，落在区间 $(x, x + \delta x)$ 内的观测会被变换到区间 $(y, y + \delta y)$ 中。其中 $p_{x}(x) \delta x \simeq p_{y}(y) \delta y$ ，因此有：</p><script type="math/tex; mode=display">p_{y}(y)=p_{x}(x)\left|\frac{\mathrm{d} x}{\mathrm{d} y}\right|=p_{x}(g(y))\left|g^{\prime}(y)\right|\tag{1.13}</script><p>位于区间 $(−\infty, z)$ 的 $x$ 的概率由<strong>累积分布函数</strong>（<code>cumulative distribution function</code>）给出。 定义为：</p><script type="math/tex; mode=display">P(z)=\int_{-\infty}^{z} p(x) \mathrm{d} x\tag{1.14}</script><p>如果我们有⼏个连续变量 $x_1 ,\dots , x_D$ ， 整体记作向量 $\boldsymbol{x}$， 那么我们可以定义联合概率密度 $p(\boldsymbol{x}) = p(x_1 ,\dots , x_D )$，使得 $\boldsymbol{x}$ 落在包含点 $\boldsymbol{x}$ 的⽆穷⼩体积 $\delta \boldsymbol{x}$ 的概率由 $p(\boldsymbol{x})\delta \boldsymbol{x}$ 给出。<strong>多变量概率密度</strong>必须满⾜以下<strong>条件</strong>：</p><p>1）$p(\boldsymbol{x}) \geq 0$</p><p>2) $\int p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}=1$</p><p>其中，积分必须在整个 $\boldsymbol{x}$ 空间上进⾏。</p><h2 id="3，期望和方差"><a href="#3，期望和方差" class="headerlink" title="3，期望和方差"></a>3，期望和方差</h2><p>在概率分布 $p(x)$ 下，函数 $f(x)$ 的平均值被称为 $f(x)$ 的<strong>期望</strong>（<code>expectation</code>），记作 $\mathbb{E}[f]$。对于⼀个离散变量，它的定义为：</p><script type="math/tex; mode=display">\mathbb{E}[f]=\sum_{x} p(x) f(x)\tag{1.15}</script><p>在连续变量的情形下，期望以对应的概率密度的积分的形式表⽰为：</p><script type="math/tex; mode=display">\mathbb{E}[f]=\int p(x) f(x) \mathrm{d}{x}\tag{1.16}</script><p>如果我们给定有限数量的 $N$ 个点，这些点满⾜某个概率分布或者概率密度函数， 那么期望可以通过<strong>求和</strong>的⽅式估计，因此有：</p><script type="math/tex; mode=display">\mathbb{E}[f] \simeq \frac{1}{N} \sum_{n=1}^{N} f\left(x_{n}\right)\tag{1.17}</script><p>$f(x)$ 的<strong>⽅差</strong>（<code>variance</code>）度量了 $f(x)$ 在均值 $\mathbb{E} [f(x)]$ 附近变化性的⼤⼩。被定义为：</p><script type="math/tex; mode=display">\operatorname{var}[f]=\mathbb{E}\left[(f(x)-\mathbb{E}[f(x)])^{2}\right]\tag{1.18}</script><p>将公式(1.18)中的平方项展开，即有公式(1.19)：</p><script type="math/tex; mode=display">\operatorname{var}[f]=\mathbb{E}\left[f(x)^{2}\right]-\mathbb{E}[f(x)]^{2}\tag{1.19}</script><p>特别地，我们可以考虑变量 $x$ ⾃⾝的⽅差，即有：</p><script type="math/tex; mode=display">\operatorname{var}[x]=\mathbb{E}\left[x^{2}\right]-\mathbb{E}[x]^{2}\tag{1.20}</script><p>对于两个随机变量 $x$ 和 $y$ ，<strong>协⽅差</strong>（<code>covariance</code>），表⽰在多⼤程度上 $x$ 和 $y$ 会共同变化。被定义为：</p><script type="math/tex; mode=display">\operatorname{cov}[x, y]=\mathbb{E}_{x, y}[\{x-\mathbb{E}[x]\}\{y-\mathbb{E}[y]\}]=\mathbb{E}_{x, y}[x y]-\mathbb{E}[x] \mathbb{E}[y]\tag{1.21}</script><p>显然，由公式(1.21)推知，如果 $x$ 和 $y$ <strong>相互独⽴</strong>，那么它们的<strong>协⽅差</strong>为0。</p><p>在两个随机向量 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 的情形下，协⽅差是⼀个<strong>矩阵</strong>，即有：</p><script type="math/tex; mode=display">\operatorname{cov}[\boldsymbol{x}, \boldsymbol{y}]=\mathbb{E}_{\boldsymbol{x}, \boldsymbol{y}}\left[\{\boldsymbol{x}-\mathbb{E}[\boldsymbol{x}]\}\left\{\boldsymbol{y}^{T}-\mathbb{E}\left[\boldsymbol{y}^{T}\right]\right\}\right]=\mathbb{E}_{\boldsymbol{x}, \boldsymbol{y}}\left[\boldsymbol{x} \boldsymbol{y}^{T}\right]-\mathbb{E}[\boldsymbol{x}] \mathbb{E}\left[\boldsymbol{y}^{T}\right]\tag{1.22}</script><h2 id="4，贝叶斯概率"><a href="#4，贝叶斯概率" class="headerlink" title="4，贝叶斯概率"></a>4，贝叶斯概率</h2><p>在观察到数据之前，我们有⼀些关于参数 $\boldsymbol{w}$ 的假设，这以<strong>先验概率</strong> $p(\boldsymbol{w})$ 的形式给出。观测数据 $\mathcal{D} = {t_1,\dots, t_N}$ 的效果可以通过<strong>条件概率</strong> $p(\mathcal{D} | \boldsymbol{w})$ 表达，即贝叶斯定理的形式为：</p><script type="math/tex; mode=display">p(\boldsymbol{w} | \mathcal{D})=\frac{p(\mathcal{D} | \boldsymbol{w}) p(\boldsymbol{w})}{p(\mathcal{D})}\tag{1.23}</script><p>其中， 可以⽤后验概率分布和似然函数来表达贝叶斯定理的分母，即得：</p><script type="math/tex; mode=display">p(\mathcal{D})=\int p(\mathcal{D} | \boldsymbol{w}) p(\boldsymbol{w}) \mathrm{d} \boldsymbol{w}\tag{1.24}</script><p>让我们能够通过<strong>后验概率</strong> $p(\boldsymbol{w} | \mathcal{D})$，在观测到 $\mathcal{D}$ 之后估计 $\boldsymbol{w}$ 的不确定性。公式(1.23)中 $p(\mathcal{D} | \boldsymbol{w})$ 由观测数据集 $\mathcal{D}$ 来估计，可以被看成参数向量 $\boldsymbol{w}$ 的函数，被称为<strong>似然函数</strong>（<code>likelihood function</code>）。</p><h2 id="5，高斯分布"><a href="#5，高斯分布" class="headerlink" title="5，高斯分布"></a>5，高斯分布</h2><p><strong>正态分布</strong>（<code>Normal distribution</code>），也称<strong>常态分布</strong>，又名<strong>高斯分布</strong>（<code>Gaussian distribution</code>），最早由A.棣莫弗在求<strong>二项分布</strong>的渐近公式中得到。正态分布概念是由德国的数学家和天文学家<code>Moivre</code>于1733年首次提出的。</p><p><strong>正态曲线</strong>呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为<strong>钟形曲线</strong>。</p><p>对于⼀元实值变量 $x$，⾼斯分布被定义为：</p><script type="math/tex; mode=display">\mathcal{N}\left(x | \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{\frac{1}{2}}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}\tag{1.25}</script><p>其中，$\mu$ 被叫做<strong>均值</strong>（<code>mean</code>）， $\sigma^{2}$ 被叫做<strong>⽅差</strong>（<code>variance</code>）或者<strong>方差参数</strong>。⽅差的平⽅根， 由 $\sigma$ 给定， 被叫做<strong>标准差</strong>（<code>standard deviation</code>）。 ⽅差的倒数， 记作 $\beta=\frac{1}{\sigma^{2}}$ ， 被叫做<strong>精度</strong> （<code>precision</code>）。</p><p>如图1.17，高斯分布曲线。</p><p><img src="/images/prml_20190918103504.png" alt="高斯分布曲线"></p><p>不难发现，<strong>高斯分布</strong>具有以下<strong>性质</strong>：</p><p>1）$\mathcal{N}\left(x | \mu, \sigma^{2}\right)&gt;0$</p><p>2）$\int_{-\infty}^{\infty} \mathcal{N}\left(x | \mu, \sigma^{2}\right) \mathrm{d} x=1$</p><p>3）$\mathbb{E}[x]=\int_{-\infty}^{\infty} \mathcal{N}\left(x | \mu, \sigma^{2}\right) x \mathrm{d} x=\mu$</p><p>4）$\mathbb{E}\left[x^{2}\right]=\int_{-\infty}^{\infty} \mathcal{N}\left(x | \mu, \sigma^{2}\right) x^{2} \mathrm{d} x=\mu^{2}+\sigma^{2}$</p><p>5）$\operatorname{var}[x]=\mathbb{E}\left[x^{2}\right]-\mathbb{E}[x]^{2}=\sigma^{2}$</p><p>分布的最⼤值被叫做<strong>众数</strong>。对于<strong>⾼斯分布</strong>，<strong>众数</strong>与<strong>均值</strong>恰好相等。</p><p>对 $D$ 维向量 $\boldsymbol{x}$ 的⾼斯分布，定义为：</p><script type="math/tex; mode=display">\mathcal{N}(\boldsymbol{x} | \boldsymbol{\mu}, \mathbf{\Sigma})=\frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}} \exp \left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}\tag{1.26}</script><p>其中 $D$ 维向量 $\boldsymbol{\mu}$ 被称为<strong>均值</strong>，$D \times D$ 的矩阵 $\boldsymbol{\Sigma}$ 被称为<strong>协⽅差</strong>，$|\boldsymbol{\Sigma}|$ 表⽰ $\boldsymbol{\Sigma}$ 的⾏列式。</p><p>独⽴地从相同的数据点中抽取的数据点被称为<strong>独⽴同分布</strong>（<code>independent and identically distributed</code>），通常缩写成<code>i.i.d.</code>。</p><p>假设给定一个观测的数据集由标量变量 $x$ 的 $N$ 次观测组成，写作 $<br>\mathbf{x} = (x_1,\dots, x_N)^T $ ，记向量变量 $\boldsymbol{x} = (x_1,\dots, x_N)^T$ ，假定各次观测是独⽴地从⾼斯分布中抽取的， 分布的均值 $\mu$ 和⽅差 $\sigma^{2}$ 未知， 我们想根据数据集来确定这些参数。由于数据集 $\mathbf{x}$ 是<strong>独⽴同分布</strong>的，因此给定 $\mu$ 和 $\sigma^{2}$ ，我们可以给出数据集的概率即为高斯分布的似然函数：</p><script type="math/tex; mode=display">p\left(\mathbf{x} | \mu, \sigma^{2}\right)=\prod_{n=1}^{N} \mathcal{N}\left(x_{n} | \mu, \sigma^{2}\right)\tag{1.27}</script><p>对于似然函数公式(1.27)变形可得到：</p><script type="math/tex; mode=display">\ln p\left(\mathbf{x} | \mu, \sigma^{2}\right)=-\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2}-\frac{N}{2} \ln \sigma^{2}-\frac{N}{2} \ln (2 \pi)\tag{1.28}</script><p>关于 $\mu$ ，最⼤化函数公式(1.28)，我们可以得到<strong>均值最⼤似然解</strong>：</p><script type="math/tex; mode=display">\mu_{ML} = \frac{1}{N} \sum_{n=1}^{N}x_{n}\tag{1.29}</script><p>关于 $\sigma^{2}$  ，最⼤化函数公式(1.28)，我们可以得到<strong>方差最⼤似然解</strong>：</p><script type="math/tex; mode=display">\sigma^{2}_{ML} = \frac{1}{N} \sum_{n=1}^{N} (x_n - \mu_{ML})^{2}\tag{1.30}</script><h2 id="6，考察曲线拟合问题"><a href="#6，考察曲线拟合问题" class="headerlink" title="6，考察曲线拟合问题"></a>6，考察曲线拟合问题</h2><p><strong>曲线拟合问题</strong>的<strong>⽬标</strong>是能够根据 $N$ 个输⼊ $\mathbf{x}\equiv(x_1,\dots, x_N)^T$ 组成的数据集和它们对应的⽬标值 $\mathbf{t}\equiv  (t_1,\dots, t_N)^T$ ，在给出输⼊变量 $x$ 的新值的情况下，对⽬标变量 $t$ 进⾏预测。</p><p>现假定给定 $x$ 的值， 对应的 $t$ 值服从<strong>⾼斯分布</strong>，分布的均值为 $y(x, \boldsymbol{w})$ ，由公式(1.1)给出。则其概率为:</p><script type="math/tex; mode=display">p(t|x, \boldsymbol{w}, \beta ) = \mathcal{N}\left(t | y(x, \boldsymbol{w}),  \beta^{-1}\right)\tag{1.31}</script><p>如图1.18，⾼斯条件概率分布。</p><p><img src="/images/prml_20190918151706.png" alt="⾼斯条件概率分布"></p><p>似然函数为：</p><script type="math/tex; mode=display">p\left(\mathbf{t} | \mathbf{x}, \boldsymbol{w}, \beta \right)=\prod_{n=1}^{N} \mathcal{N}\left(t | y(x, \boldsymbol{w}),  \beta^{-1}\right)\tag{1.32}</script><p>似然函数变形为：</p><script type="math/tex; mode=display">\ln p\left(\mathbf{t} | \mathbf{x}, \boldsymbol{w}, \beta \right)=-\frac{\beta}{2} \sum_{n=1}^{N}\left\{y(x_{n},\boldsymbol{w})-t_n\right\}^{2}+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)\tag{1.33}</script><p>考虑确定多项式系数的最⼤似然解（记作 $\boldsymbol{w}_{ML}$ ），这些由公式(1.33)来确定。可以使⽤最⼤似然⽅法来确定⾼斯条件分布的精度参数 $\beta$ ：</p><script type="math/tex; mode=display">\frac{1}{\beta_{ML}} = \frac{1}{N} \sum_{n=1}^{N}\left\{y(x_{n},\boldsymbol{w}_{ML})-t_n\right\}^{2}\tag{1.34}</script><p>简单起见，引⼊在多项式系数 $\boldsymbol{w}$ 上的先验分布，我们考虑下⾯形式的⾼斯分布：</p><script type="math/tex; mode=display">p(\boldsymbol{w} | \alpha)=\mathcal{N}\left(\boldsymbol{w} | \mathbf{0}, \alpha^{-1} \boldsymbol{I}\right)=\left(\frac{\alpha}{2 \pi}\right)^{\frac{M+1}{2}} \exp \left\{-\frac{\alpha}{2} \boldsymbol{w}^{T} \boldsymbol{w}\right\}\tag{1.35}</script><p>给定数据集，我们现在通过寻找最可能的 $\boldsymbol{w}$ 值（即<strong>最⼤化后验概率</strong>）来确定 $\boldsymbol{w}$ 。这种技术被称 为<strong>最⼤后验</strong>（<code>maximum posterior</code>），简称<strong><code>MAP</code></strong>。根据公式(1.35)和公式(1.33)可得最⼤化后验概率即最⼩化：</p><script type="math/tex; mode=display">\frac{\beta}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, \boldsymbol{w}\right)-t_{n}\right\}^{2}+\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}\tag{1.36}</script><h2 id="7，贝叶斯曲线拟合"><a href="#7，贝叶斯曲线拟合" class="headerlink" title="7，贝叶斯曲线拟合"></a>7，贝叶斯曲线拟合</h2><p><strong>贝叶斯⽅法</strong>就是⾃始⾄终地使⽤概率的<strong>加和规则</strong>和<strong>乘积规则</strong>。因此预测概率可以写成：</p><script type="math/tex; mode=display">p\left(t | x, \mathbf{x}, \mathbf{t} \right)=\int p(t | x, \boldsymbol{w}) p(\boldsymbol{w} | \mathbf{x}, \mathbf{t}) \mathrm{d} \boldsymbol{w}\tag{1.37}</script><p>预测分布由⾼斯的形式：</p><script type="math/tex; mode=display">p\left(t | x, \mathbf{x}, \mathbf{t} \right) = \mathcal{N}\left(t | m(x), s^{2}(x)\right)\tag{1.38}</script><p>其中，均值，</p><script type="math/tex; mode=display">m(x) = \beta \phi(x)^{T} \boldsymbol{S} \sum_{n=1}^{N}\phi(x_n)t_n\tag{1.39}</script><p>⽅差，</p><script type="math/tex; mode=display">s^{2}(x) = \beta^{-1} + \phi(x)^{T}\boldsymbol{S}\phi(x)\tag{1.40}</script><p>矩阵，</p><script type="math/tex; mode=display">\boldsymbol{S}^{-1} = \alpha \boldsymbol{I} + \beta \sum_{n=1}^{N}\phi({x_n})\phi({x_n})^{T}\tag{1.41}</script><p>其中，$\boldsymbol{I}$  是单位矩阵，向量 $\phi(x)$ 被定义为 $\phi_i (x) = x^{i} (i = 0, \dots, M)$。</p><p>如图1.19，⽤贝叶斯⽅法处理多项式曲线拟合问题得到的预测分布的结果。使⽤的多项式为 $M$ = 9，超参数被固定为 $\alpha = 5 \times 10^{-3}$ 和 $\beta = 11.1$（对应于已知的噪声⽅差）。 其中， 红⾊曲线表⽰预测概率分布的均值，红⾊区域对应于均值周围 $±1$ 标准差的范围。</p><p><img src="/images/prml_20190918151304.png" alt="贝叶斯⽅法处理多项式曲线拟合问题"></p><p>如图1.20，参数为 $S$ 的交叉验证⽅法。</p><p><img src="/images/prml_20190918151313.png" alt="交叉验证⽅法"></p><h1 id="二，模型选择"><a href="#二，模型选择" class="headerlink" title="二，模型选择"></a>二，模型选择</h1><p>为了建⽴好的模型，我们想使⽤尽可能多的可得到的数据进⾏训练。然⽽，如果验证机很⼩，它对预测表现的估计就会有⼀定的噪声。解决这种困境的⼀种⽅法是使⽤<strong>交叉验证</strong>（<code>cross validation</code>）。这种⽅法能够让可得到数据的 $\frac{S−1}{S}$ ⽤于训练，同时使⽤所有的数据来评估表现。当数据相当稀疏的时候，考虑 $S = N$ 的情况很合适，其中 $N$ 是数据点的总数。这种技术叫做<strong>留⼀法</strong>（<code>leave-one-out</code>）。</p><p>如图1.21，交叉验证。</p><p><img src="/images/prml_20190918160236.png" alt="交叉验证"></p><p><strong>交叉验证</strong>的⼀个主要的<strong>缺点</strong>是需要进⾏的训练的次数随着 $S$ ⽽增加，这对于训练本⾝很耗时的问题来说是个⼤问题。对于像交叉验证这种使⽤分开的数据来评估模型表现的⽅法来说，还 有⼀个问题：对于⼀个单⼀的模型，我们可能有多个复杂度参数（例如可能有若⼲个正则化参数）。</p><h1 id="三，维度灾难"><a href="#三，维度灾难" class="headerlink" title="三，维度灾难"></a>三，维度灾难</h1><p>如果我们有 $D$ 个输⼊变量，那么 ⼀个三阶多项式就可以写成如下的形式：</p><script type="math/tex; mode=display">y(\boldsymbol{x}, \boldsymbol{w}) = w_{0} + \sum_{i=1}^{D}w_{i}x_{i} + \sum_{i=1}^{D}\sum_{j=1}^{D}w_{ij}x_{i}x_{j} + \sum_{i=1}^{D}\sum_{j=1}^{D}\sum_{k=1}^{D}w_{ijk}x_{i}x_{j}x_{k}\tag{1.42}</script><p>随着 $D$ 的增加，独⽴的系数的数量（并⾮所有的系数都独⽴，因为变量 $x$ 之间的互换对称性）的 增长速度正⽐于$D^3$ 。</p><p>注意到，$D$ 维空间的半径为 $r$ 的球体的体积⼀定是 $r^{D}$ 的倍数，因此有：</p><script type="math/tex; mode=display">V_{D}(r) = K_{D}r^{D}\tag{1.43}</script><p>其中常数 $K_{D}$ 值依赖于D。因此要求解的体积比，即：</p><script type="math/tex; mode=display">\frac{V_{D}(1)-V_{D}(1-\epsilon)}{V_{D}(1)} = 1 - (1-\epsilon)^{D}\tag{1.44}</script><p>如图1.22，对于不同的 $D$，位于 $r = 1 − \epsilon$ 和 $r = 1$ 之间的部分与球的体积⽐。</p><p><img src="/images/prml_20190918161735.png" alt="维度D"></p><p>如图1.23，不同的维度 $D$ 中的⾼斯分布的概率密度关于半径 $r$ 的关系。</p><p><img src="/images/prml_20190918163058.png" alt="D与r的关系"></p><p><strong>维度灾难</strong>(<code>Curse of Dimensionality</code>)：通常是指在涉及到向量的计算的问题中，随着维度的增加，计算量呈指数倍增长的一种现象。</p><h1 id="四，决策论"><a href="#四，决策论" class="headerlink" title="四，决策论"></a>四，决策论</h1><h2 id="1，最⼩化错误分类率"><a href="#1，最⼩化错误分类率" class="headerlink" title="1，最⼩化错误分类率"></a>1，最⼩化错误分类率</h2><p>假定我们的⽬标很简单，即尽可能少地作出错误分类。我们需要⼀个规则来把每个 $x$ 的值分到⼀个合适的类别。这种规则将会把输⼊空间切分成不同的区域 $\mathcal{R}_{k}$ ，这种区域被称为<strong>决策区域</strong> （<code>decision region</code>）。每个类别都有⼀个决策区域，区域 $\mathcal{R}_{k}$ 中的所有点都被分到 $\mathcal{C}_{k}$ 类。决策区域间的边界被叫做<strong>决策边界</strong>（<code>decision boundary</code>）或者<strong>决策⾯</strong>（<code>decision surface</code>）。注意， 每⼀个 决策区域未必是连续的，可以由若⼲个分离的区域组成。 如果我们把属于 $\mathcal{C}_{1}$ 类的输⼊向量分到了 $\mathcal{C}_{2}$ 类（或者相反）， 那么我们就犯了⼀个错误。这种事情发⽣的概率为：</p><script type="math/tex; mode=display">\begin{aligned} p(\text { mistake }) &=p\left(\boldsymbol{x} \in \mathcal{R}_{1}, \mathcal{C}_{2}\right)+p\left(\boldsymbol{x} \in \mathcal{R}_{2}, \mathcal{C}_{1}\right) \\ &=\int_{\mathcal{R}_{1}} p\left(\boldsymbol{x}, \mathcal{C}_{2}\right) \mathrm{d} \boldsymbol{x}+\int_{\mathcal{R}_{2}} p\left(\boldsymbol{x}, \mathcal{C}_{1}\right) \mathrm{d} \boldsymbol{x} \end{aligned}\tag{1.45}</script><p>如图1.24，两个类别的联合概率分布 $p(x, \mathcal{C}_k)$ 与 $x$ 的关系。</p><p><img src="/images/prml_20190918171908.png" alt="联合概率分布与x关系"></p><p>对于更⼀般的 $K$ 类的情形，最⼤化正确率会稍微简单⼀些，即最⼤化下式：</p><script type="math/tex; mode=display">p(\text {correct}) = \sum_{k=1}^{K}p\left(\boldsymbol{x} \in \mathcal{R}_{k}, \mathcal{C}_{k}\right) = \sum_{k=1}^{K} \int_{\mathcal{R}_{k}} p\left(\boldsymbol{x}, \mathcal{C}_{k}\right) \mathrm{d} \boldsymbol{x}\tag{1.46}</script><h2 id="2，最⼩化期望损失"><a href="#2，最⼩化期望损失" class="headerlink" title="2，最⼩化期望损失"></a>2，最⼩化期望损失</h2><p><strong>损失函数</strong>也被称为<strong>代价函数</strong>（<code>cost function</code>），是对于所有可能的决策或者动作可能产⽣的损失的⼀种整体的度量。</p><p>假设对于新的 $x$ 值，真实的类别为 $\mathcal{C}_{k}$ ，我们把 $x$ 分类为 $\mathcal{C}_{j}$ （其中 $j$ 可能与 $k$ 相等，也可能不相等）。这样做的结果是，我们会造成某种程度的损失，记作 $L_{kj}$ ，它可以看成<strong>损失矩阵</strong>（<code>loss matrix</code>）的第 $k, j$ 个元素。</p><p>对于⼀个给定的输⼊向量 $\boldsymbol{x}$，我们对于真实类别的不确定性通过联合概率分布 $p(\boldsymbol{x}, \mathcal{C}_{k})$ 表⽰。因此，我们转⽽去最⼩化平均损失。平均损失根据这个联合概率分布计算，定义为：</p><script type="math/tex; mode=display">\mathbb{E}[L] = \sum_{k}\sum_{j} \int_{\mathcal{R}_{k}} L_{kj}p(\boldsymbol{x}, \mathcal{C}_{k})\mathrm{d} \boldsymbol{x}\tag{1.47}</script><h2 id="3，拒绝选项"><a href="#3，拒绝选项" class="headerlink" title="3，拒绝选项"></a>3，拒绝选项</h2><p>在发⽣分类错误的输⼊空间中，后验概率 $p(\mathcal{C}_{k} | \boldsymbol{x})$ 通常远⼩于1，或者等价地，不同类别的联合分布 $p(\boldsymbol{x}, \mathcal{C}_{k})$ 有着可⽐的值。这些区域中，类别的归属相对不确定。在某些应⽤中， 对于这种困难的情况， 避免做出决策是更合适的选择。 这样会使得模型的分类错误率降低。 这被称为<strong>拒绝选项</strong>（<code>reject option</code>）。</p><p>如图1.25，拒绝选项。</p><p><img src="/images/prml_20190918180040.png" alt="拒绝选项"></p><h2 id="4，推断和决策"><a href="#4，推断和决策" class="headerlink" title="4，推断和决策"></a>4，推断和决策</h2><p>同时解决两个问题，即简单地学习⼀个函数，将输⼊ $\boldsymbol{x}$ 直接映射为决策。这样的函数被称为<strong>判别函数</strong>（<code>discriminant function</code>）。</p><p>给出三种不同的⽅法来解决决策问题，具体如下：</p><p>a）⾸先对于每个类别 $\mathcal{C}_{k}$ ， 独⽴地确定类条件密度  $p(\boldsymbol{x} | \mathcal{C}_{k})$ ，这是⼀个推断问题。 然后， 推断先验类概率 $p(\mathcal{C}_{k})$ 。之后，使⽤贝叶斯定理求出后验类概率  $p(\mathcal{C}_{k} | \boldsymbol{x})$ 。等价地，我们可以直接对联合概率分布 $p(\boldsymbol{x} , \mathcal{C}_{k})$ 建模，然后归⼀化，得到后验概率。得到后验概率之后， 我们可以使⽤决策论来确定每个新的输⼊ $\boldsymbol{x}$ 的类别。显式地或者隐式地对输⼊以及输出进⾏建模的⽅法被称为<strong>⽣成式模型</strong>（<code>generative model</code>），因为通过取样，可以⽤来⼈⼯⽣成出输⼊空间的数据点。</p><p>b）⾸先解决确定后验类密度  $p(\mathcal{C}_{k} | \boldsymbol{x})$ 这⼀推断问题，接下来使⽤决策论来对新的输⼊ $\boldsymbol{x}$ 进⾏分类。这种直接对后验概率建模的⽅法被称为<strong>判别式模型</strong>（<code>discriminative models</code>）。</p><p>c）找到⼀个函数 $f(\boldsymbol{x})$， 被称为<strong>判别函数</strong>。 这个函数把每个输⼊ $\boldsymbol{x}$ 直接映射为类别标签。</p><h2 id="5，回归问题的损失函数"><a href="#5，回归问题的损失函数" class="headerlink" title="5，回归问题的损失函数"></a>5，回归问题的损失函数</h2><p>讨论曲线拟合问题，决策阶段包括对于每个输⼊ $\boldsymbol{x}$，选择⼀个对于 $t$ 值的具体估计 $y(\boldsymbol{x})$。假设这样做之后，我们造成了⼀个损失 $L(t, y(\boldsymbol{x}))$。平均损失（或者说期望损失）就是：</p><script type="math/tex; mode=display">\mathbb{E}[L] = \int\int L(t, y(\boldsymbol{x}, x))p(\boldsymbol{x}, t)\mathrm{d} \boldsymbol{x}\mathrm{d}{t}\tag{1.48}</script><p>回归问题中，损失函数的⼀个通常的选择是<strong>平⽅损失</strong>，定义为 $L(t, y(\boldsymbol{x})) = \{y(\boldsymbol{x}) − t\}^{2}$ 。这种情况下，期望损失函数可以写成：</p><script type="math/tex; mode=display">\mathbb{E}[L] = \int\int \{y(\boldsymbol{x}) − t\}^{2}p(\boldsymbol{x}, t)\mathrm{d} \boldsymbol{x}\mathrm{d}{t}\tag{1.49}</script><p>假设⼀个完全任意的函数 $y(\boldsymbol{x})$，我们能够形式化地使⽤变分法：</p><script type="math/tex; mode=display">\frac{\delta \mathbb{E}[L]}{\delta y(\boldsymbol{x})} = 2 \int \{y(\boldsymbol{x}) − t\}p(\boldsymbol{x}, t)\mathrm{d}{t} = 0\tag{1.50}</script><p>求解 $y(\boldsymbol{x})$，使⽤概率的<strong>加和规则</strong>和<strong>乘积规则</strong>，得到：</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\frac{\int \operatorname{tp}(\boldsymbol{x}, t) \mathrm{d} t}{p(\boldsymbol{x})}=\int t p(t | \boldsymbol{x}) \mathrm{d} t=\mathbb{E}_{t}[t | \boldsymbol{x}]\tag{1.51}</script><p>这是在 $\boldsymbol{x}$ 的条件下 $t$ 的条件均值， 被称为<strong>回归函数</strong>（<code>regression function</code>）。</p><p>如图1.26，回归函数。</p><p><img src="/images/prml_20190918195108.png" alt="回归函数"></p><p><strong>闵可夫斯基损失函数</strong>（<code>Minkowski loss</code>），它的期望为：</p><script type="math/tex; mode=display">\mathbb{E}[L_{q}] = \int\int |y(\boldsymbol{x}) − t|^{q}p(\boldsymbol{x}, t)\mathrm{d} \boldsymbol{x}\mathrm{d}{t}\tag{1.52}</script><p>如图1.27～1.30，对于不同的 $q$ 值，$L_{q} = |y − t|^{q}$ 的图像。</p><p><img src="/images/prml_20190918200648.png" alt="q=0.3"></p><p><img src="/images/prml_20190918200657.png" alt="q=1"></p><p><img src="/images/prml_20190918200706.png" alt="q=2"></p><p><img src="/images/prml_20190918200716.png" alt="q=10"></p><p>由公式(1.52)分析不难发现，当 $q = 2$ 时， 这个函数就变成了<strong>平⽅损失函数</strong>的期望。当 $q = 2$ 时，$\mathbb{E}[L_{q}]$ 的最⼩值是条件均值。当 $q = 1$ 时，$\mathbb{E}[L_{q}]$ 的最⼩值是条件中位数。当$q \to 0$ 时，$\mathbb{E}[L_{q}]$ 的最⼩值是条件众数。</p><h1 id="五，信息论"><a href="#五，信息论" class="headerlink" title="五，信息论"></a>五，信息论</h1><p>如果有两个不相关的事件 $x$ 和 $y$ ， 那么我们观察到两个事件同时发⽣时获得的信息应该等于观察到事件各⾃发⽣时获得的信息之和， 即 $h(x, y) = h(x) + h(y)$。 两个不相关事件是统计独⽴的， 因此 $p(x, y) = p(x)p(y)$。根据这两个关系，很容易看出 $h(x)$ ⼀定与 $p(x)$ 的对数有关。因此，我们有：</p><script type="math/tex; mode=display">h(x) = - \text{log}_{2}p(x)\tag{1.53}</script><p>概率分布  $p(x)$ 的期望，</p><script type="math/tex; mode=display">H[x] = - \sum_{x}p(x) \text {log}_{2}p(x)\tag{1.54}</script><p>这个重要的量被叫做随机变量 $\boldsymbol{x}$ 的<strong>熵</strong>（<code>entropy</code>）。</p><h2 id="1，关于理解熵的例子"><a href="#1，关于理解熵的例子" class="headerlink" title="1，关于理解熵的例子"></a>1，关于理解<strong>熵</strong>的例子</h2><p>考虑⼀个集合，包含 $N$ 个完全相同的物体，这些 物体要被分到若⼲个箱⼦中，使得第 $i$ 个箱⼦中有 $n_i$ 个物体。考虑把物体分配到箱⼦中的不同⽅案的数量。有 $N$ 种⽅式选择第⼀个物体，有 $(N − 1)$ 种⽅式选择第⼆个物体，以此类推。因此总 共有 $N!$ 种⽅式把 $N$ 个物体分配到箱⼦中，其中 $N!$ 表⽰乘积 $N\times(N − 1)\times \dots \times 2 \times 1$。然⽽，我们不想区分每个箱⼦内部物体的重新排列。在第 $i$ 个箱⼦中，有 $n_i !$ 种⽅式对物体重新排序，因此把 $N$ 个物体分配到箱⼦中的总⽅案数量为：</p><script type="math/tex; mode=display">W=\frac{N !}{\prod_{i} n_{i} !}\tag{1.55}</script><p>这被称为<strong>乘数</strong>（<code>multiplicity</code>）。<strong>熵</strong>被定义为通过适当的参数放缩后的对数乘数，即：</p><script type="math/tex; mode=display">H = \frac{1}{N} \ln W = \frac{1}{N} \ln N! - \frac{1}{N}\ln n_{i}!\tag{1.56}</script><p>考虑极限 $N \to \infty$ ，并且保持⽐值 $\frac{n_i}{N}$ 固定，使⽤<code>Stirling</code>的估计：</p><script type="math/tex; mode=display">\ln N! \simeq N \ln N - N\tag{1.57}</script><p>即可得：</p><script type="math/tex; mode=display">H = - \lim_{N \to \infty} \sum_{i} (\frac{n_{i}}{N}) \ln (\frac{n_{i}}{N}) = - \sum_{i} p_{i} \ln p_{i}\tag{1.58}</script><p>在概率归⼀化的限制下，使⽤<strong>拉格朗⽇乘数法</strong>可以找到<strong>熵</strong>的最⼤值。因此，我们要最⼤化：</p><script type="math/tex; mode=display">\tilde{H}=-\sum_{i} p\left(x_{i}\right) \ln p\left(x_{i}\right)+\lambda\left(\sum_{i} p\left(x_{i}\right)-1\right)\tag{1.59}</script><p>可以证明， 当所有的 $p(x_i)$ 都相等， 且值为 $p(x_i) = \frac{1}{M}$ 时， 熵取得最⼤值。 其中，$M$ 是状态 $x_i$ 的总数。此时对应的熵值为 $H = \ln M$ 。</p><h2 id="2，关于连续变量-boldsymbol-x-的概率分布-p-x-的熵"><a href="#2，关于连续变量-boldsymbol-x-的概率分布-p-x-的熵" class="headerlink" title="2，关于连续变量 $\boldsymbol{x}$ 的概率分布 $p(x)$ 的熵"></a>2，关于连续变量 $\boldsymbol{x}$ 的概率分布 $p(x)$ 的<strong>熵</strong></h2><p>⾸先把 $x$ 切分成宽度为 $\Delta$ 的箱⼦。然后假设 $p(x)$ 是连续的。<strong>均值定理</strong>（<code>mean value theorem</code>）（Weisstein, 1999）告诉我们，对于每个这样的箱⼦，⼀定存在⼀个值 $x_i$ 使得：</p><script type="math/tex; mode=display">\int_{i \Delta}^{(i+1) \Delta}p(x_i)\mathrm{d}x = p(x_i)\Delta\tag{1.60}</script><p>现在可以这样量化连续变量 $x$：只要 $x$ 落在第 $i$ 个箱⼦中，我们就把 $x$ 赋值为 $x_i$ 。因此观察到值 $x_i$ 的概率为 $p(x_i) \Delta$。这就变成了离散的分布，这种情形下<strong>熵</strong>的形式为：</p><script type="math/tex; mode=display">H \Delta = - \sum_{i}p(x_i) \Delta \ln(p(x_i) \Delta) - \ln \Delta\tag{1.61}</script><p>其中，</p><script type="math/tex; mode=display">\sum_{i}p(x_i) \Delta = 1 \tag{1.62}</script><p>考察 $\Delta \to 0$ ，由以上公式即可推得<strong>微分熵</strong>（<code>differential entropy</code>）：</p><script type="math/tex; mode=display">\lim_{\Delta \to 0}\left\{-\sum_{i} p\left(x_{i}\right) \Delta \ln p(x_i) \right\} = - \int p(x)\ln p(x) \mathrm{d}{x}\tag{1.63}</script><p>对于定义在多元连续变量（联合起来记作向量 $\boldsymbol{x}$ ）上的概率密度，<strong>微分熵</strong>为：</p><script type="math/tex; mode=display">H[\boldsymbol{x}] = - \int p(x) \ln p(x) \mathrm{d}{x}\tag{1.64}</script><p><strong>最⼤化微分熵</strong>的时候要遵循下⾯三个限制条件，即：</p><script type="math/tex; mode=display">\int_{-\infty}^{\infty}p(x) \mathrm{d}{x} = 1\tag{1.65}</script><script type="math/tex; mode=display">\int_{-\infty}^{\infty}x p(x) \mathrm{d}{x} = \mu\tag{1.66}</script><script type="math/tex; mode=display">\int_{-\infty}^{\infty}(x - \mu)^{2} p(x) \mathrm{d}{x} = \delta^{2}\tag{1.67}</script><p>在上述条件的限制下，使⽤<strong>拉格朗⽇乘数法</strong>可以找到<strong>熵</strong>的最⼤值。最终结果化：</p><script type="math/tex; mode=display">p(x) = \frac{1}{\left(2 \pi \sigma^{2}\right)^{\frac{1}{2}}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}\tag{1.68}</script><p>因此最⼤化微分熵的分布是<strong>⾼斯分布</strong>，其<strong>微分熵</strong>公式：</p><script type="math/tex; mode=display">H[x] = \frac{1}{2}\{ 1 + \ln(2\pi \delta^{2}) \}\tag{1.69}</script><h2 id="3，关于连续变量-boldsymbol-x-boldsymbol-y-的联合概率分布-p-boldsymbol-x-boldsymbol-y-的熵"><a href="#3，关于连续变量-boldsymbol-x-boldsymbol-y-的联合概率分布-p-boldsymbol-x-boldsymbol-y-的熵" class="headerlink" title="3，关于连续变量 $(\boldsymbol{x}, \boldsymbol{y})$ 的联合概率分布 $p(\boldsymbol{x}, \boldsymbol{y})$ 的熵"></a>3，关于连续变量 $(\boldsymbol{x}, \boldsymbol{y})$ 的联合概率分布 $p(\boldsymbol{x}, \boldsymbol{y})$ 的<strong>熵</strong></h2><p>假设有⼀个联合概率分布 $p(\boldsymbol{x}, \boldsymbol{y})$ ，我们从这个概率分布中抽取了⼀对 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 。如果 $\boldsymbol{x}$ 的值已知，那么需要确定对应的 $\boldsymbol{y}$ 值所需的附加的信息就是 $− \ln p(\boldsymbol{y} | \boldsymbol{x})$ 。因此，⽤来确定 $y$ 值的平均附加信息可以写成：</p><script type="math/tex; mode=display">H[\boldsymbol{x}|\boldsymbol{x}] = - \iint p(\boldsymbol{y}, \boldsymbol{x})\ln p(\boldsymbol{y}|\boldsymbol{x}) \mathrm{d}{\boldsymbol{y}} \mathrm{d}{\boldsymbol{x}}\tag{1.70}</script><p>这被称为给定 $\boldsymbol{x}$ 的情况下，$\boldsymbol{y}$ 的<strong>条件熵</strong>。使⽤乘积规则，很容易看出，条件熵满⾜下⾯的关系：</p><script type="math/tex; mode=display">H[\boldsymbol{x}, \boldsymbol{y}] = H[\boldsymbol{y}|\boldsymbol{x}] + H[\boldsymbol{x}]\tag{1.71}</script><h2 id="4，相对熵和互信息"><a href="#4，相对熵和互信息" class="headerlink" title="4，相对熵和互信息"></a>4，相对熵和互信息</h2><p>考虑某个未知的分布 $p(\boldsymbol{x})$，假定我们已经使⽤⼀个近似的分布 $q(\boldsymbol{x})$ 对它进⾏了建模。如果我们使⽤ $q(\boldsymbol{x})$ 来建⽴⼀个编码体系，⽤来把 $\boldsymbol{x}$ 的值传给接收者，那么，由于我们使⽤了 $q(\boldsymbol{x})$ ⽽不是真实分布 $p(\boldsymbol{x})$，因此在具体化 $\boldsymbol{x}$ 的值（假定我们选择了⼀个⾼效的编码系统）时，我们需要⼀些附加的信息。我们需要的平均的附加信息量（单位是<code>nat</code>）：</p><script type="math/tex; mode=display">\begin{aligned} \mathrm{KL}(p \| q) &=-\int p(\boldsymbol{x}) \ln q(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}-\left(-\int p(\boldsymbol{x}) \ln p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}\right) \\ &=-\int p(\boldsymbol{x}) \ln \left\{\frac{q(\boldsymbol{x})}{p(\boldsymbol{x})}\right\} \mathrm{d} \boldsymbol{x} \end{aligned}\tag{1.72}</script><p>这被称为分布 $p(\boldsymbol{x})$ 和分布 $q(\boldsymbol{x})$ 之间的<strong>相对熵</strong>（<code>relative entropy</code>） 或者 <strong><code>Kullback-Leibler</code>散度</strong> （<code>Kullback-Leibler divergence</code>）， 或者 <strong><code>KL</code>散度</strong>（Kullback and Leibler, 1951）。</p><p>可以证明，<strong><code>Kullback-Leibler</code>散度</strong> 满⾜ $\mathrm{KL}(p | q) \ge 0$，并且当且仅当 $p(\boldsymbol{x}) = q(\boldsymbol{x})$ 时等号成⽴。</p><p>如果⼀个函数具有如下性质：每条弦都位于函数图像或其上⽅，那么我们说这个函数是<strong>凸函数</strong>。其性质为：</p><script type="math/tex; mode=display">f(\lambda a + (1-\lambda)b) \le \lambda f(a) + (1-\lambda)f(b)\tag{1.73}</script><p>由归纳法容易知，<strong>凸函数</strong> $f(x)$ 满足 <strong><code>Jensen</code>不等式</strong>（<code>Jensen&#39;s inequality</code>）：</p><script type="math/tex; mode=display">f \left(\sum_{i=1}^{M}\lambda_{i} x_i\right) \le \sum_{i=1}^{M}\lambda_i f(x_i)\tag{1.74}</script><p>对于连续变量，<strong><code>Jensen</code>不等式</strong>：</p><script type="math/tex; mode=display">f \left(\int \boldsymbol{x} p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}\right) \le \int f(\boldsymbol{x})p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}\tag{1.75}</script><p>现在考虑由 $p(\boldsymbol{x}, \boldsymbol{y})$ 给出的两个变量 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 组成的数据集。如果变量的集合是独⽴的，那么他们的联合分布可以分解为边缘分布的乘积 $p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x})p(\boldsymbol{y})$ 。如果变量不是独⽴的，那么我们可以通过考察<strong>联合概率分布</strong>与<strong>边缘概率分布</strong>乘积之间的 <strong><code>Kullback-Leibler</code>散度</strong>来判断它们是否“接近”于相互独⽴。此时，<strong><code>Kullback-Leibler</code>散度</strong>：</p><script type="math/tex; mode=display">\begin{aligned} I[\boldsymbol{x}, \boldsymbol{y}] &\equiv \mathrm{KL}(p(\boldsymbol{x}, \boldsymbol{y}) \|p(\boldsymbol{x})q(\boldsymbol{y}))\\ &=-\iint p(\boldsymbol{x}, \boldsymbol{y}) \ln \left(\frac{p(\boldsymbol{x})q(\boldsymbol{y})}{p(\boldsymbol{x, y})}\right) \mathrm{d} \boldsymbol{x} \mathrm{d} \boldsymbol{y} \end{aligned}\tag{1.76}</script><p>这被称为变量 $\boldsymbol{x}$ 和变量 $\boldsymbol{y}$ 之间的<strong>互信息</strong>（<code>mutual information</code>）。 根据 <strong><code>Kullback-Leibler</code>散度</strong>的性质，我们看到 $I[\boldsymbol{x}, \boldsymbol{y}] \ge 0$ ，当且仅当 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 相互独⽴时等号成⽴。使⽤概率的<strong>加和规则</strong>和<strong>乘积规则</strong>，我们看到互信息和条件熵之间的关系：</p><script type="math/tex; mode=display">I[\boldsymbol{x,y}] = H[\boldsymbol{x}] - H[\boldsymbol{x|y}] = H[\boldsymbol{y}] - H[\boldsymbol{y|x}]\tag{1.77}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，-概率论&quot;&gt;&lt;a href=&quot;#一，-概率论&quot; class=&quot;headerlink&quot; title=&quot;一， 概率论&quot;&gt;&lt;/a&gt;一， 概率论&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】从回归问题引基础：多项式曲线拟合</title>
    <link href="https://zhangbc.github.io/2019/09/17/prml_01_polynomial_curve_fitting/"/>
    <id>https://zhangbc.github.io/2019/09/17/prml_01_polynomial_curve_fitting/</id>
    <published>2019-09-17T13:52:56.000Z</published>
    <updated>2019-10-07T08:59:52.344Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，-举例：多项式曲线拟合"><a href="#一，-举例：多项式曲线拟合" class="headerlink" title="一， 举例：多项式曲线拟合"></a>一， 举例：多项式曲线拟合</h1><p>假设给定一个训练集。这个训练集由 $x$ 的 $N$ 次观测组成，写作 $\mathbf{x}\equiv(x_1,\dots, x_N)^T$ ，伴随这对应的 $t$ 的观测值，记作 $\mathbf{t}\equiv (t_1,\dots, t_N)^T$。其中，输入数据集合 $\mathbf{x}$ 通过选择$x_n(n=1,\dots,N)$ 的值来生成，这些 $x_n$ 均匀分布在区间[0, 1]，目标数据集 $\mathbf{t}$ 的获得方式是：首先计算函数 $sin(2\pi x)$ 的对应的值，然后给每个点增加一个小的符合高斯分布的随机噪声，从而得到对应的 $t_n$ 的值。 我们的<strong>目标</strong>是利用这个训练集预测对于输入变量的新值 $\hat{x}$ 得到的目标变量的值 $\hat{t}$。 </p><p>如下图1.1，由 $N$ =10个数据点组成的训练集的图像，用蓝色圆圈表示。</p><p><img src="/images/prml_20190917090037.png" alt="训练集"></p><p>如图1.2，误差函数对应于每个数据点与函数 $y(x, \boldsymbol{w})$ 之间位移（绿⾊垂直线）的平⽅和（的⼀半）。</p><p><img src="/images/prml_20190917090255.png" alt="误差分析"></p><p>但是现在，我们要⽤⼀种相当⾮正式的、相当简单的⽅式来进⾏曲线拟合。特别地，将使⽤下⾯形式的多项式函数来拟合数据：</p><script type="math/tex; mode=display">y(x, \boldsymbol{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{i=0}^{M} w_{j} x^{j}\tag{1.1}</script><p>其中 $M$ 是多项式的阶数（<code>order</code>），$x^j$ 表⽰ $x$ 的 $j$ 次幂。 多项式系数 $w_0 , \dots , w_M$ 整体记作向量 $\boldsymbol{w}$。 注意，虽然多项式函数 $y(x, \boldsymbol{w})$ 是 $x$ 的⼀个⾮线性函数，它是系数 $\boldsymbol{w}$ 的⼀个线性函数。类似多项式函数的这种关于未知参数满⾜线性关系的函数有着重要的性质，被叫做<strong>线性模型</strong>。</p><p>系数的值可以通过调整多项式函数拟合训练数据的⽅式确定。 这可以通过<strong>最⼩化误差函数</strong> （<code>error function</code>）的⽅法实现。</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, \boldsymbol{w}\right)-t_{n}\right\}^{2}\tag{1.2}</script><p>我们可以通过过选择使得 $E(\boldsymbol{w})$ 尽量⼩的 $\boldsymbol{w}$ 来解决曲线拟合问题。由于误差函数是系数 $\boldsymbol{w}$ 的⼆次函数， 因此它关于系数的导数是 $\boldsymbol{w}$ 的线性函数， 所以误差函数的最⼩值有⼀个唯⼀解， 记作 $\boldsymbol{w}^*$ ，可以⽤解析的⽅式求出。最终的多项式函数由函数 $y\left(x, \boldsymbol{w}^*\right)$ 给出。</p><p>如下图1.3～1.6，不同阶数的多项式曲线，⽤红⾊曲线表⽰，拟合了图1.1中的数据集。</p><p><img src="/images/prml_20190917092654.png" alt="M=0"></p><p><img src="/images/prml_20190917092714.png" alt="M=1"></p><p><img src="/images/prml_20190917092731.png" alt="M=3"></p><p><img src="/images/prml_20190917092742.png" alt="M=9"></p><p>当 $M=9$ 时，多项式函数精确地通过了每⼀个数据点，$E(\boldsymbol{w}^*) = 0$。 然⽽， 拟合的曲线剧烈震荡，就表达函数 $sin(2\pi x)$ ⽽⾔表现很差。这种⾏为叫做<strong>过拟合</strong>（<code>over-fitting</code>）。</p><p>通常用<strong>根均⽅</strong>（<code>RMS</code>）<strong>误差</strong>来计算：</p><script type="math/tex; mode=display">E_{R M S}=\sqrt{2 E\left(\boldsymbol{w}^{*}\right) / N}\tag{1.3}</script><p>如图1.7，当M 的取值为 $3 \leq M \leq 8$ 时， 测试误差较⼩， 对于⽣成函数 $sin(2\pi x)$ 也能给出合理的模拟。</p><p><img src="/images/prml_20190917101815.png" alt="根均方误差"></p><p>如图1.8，不同阶数的多项式的系数 $\boldsymbol{w}^{*}$ 的值。观察随着多项式阶数的增加，系数的⼤⼩是如何剧烈增⼤的。</p><p><img src="/images/prml_20190917105109.png" alt="系数变化"></p><p>如图1.9～1.10，使⽤ $M = 9$ 的多项式对 $N = 15$ 个数据点和 $N = 100$ 个数据点通过最⼩化平⽅和误差函数的⽅法得到的解。</p><p><img src="/images/prml_20190917105954.png" alt="N=15"></p><p><img src="/images/prml_20190917110004.png" alt="N=100"></p><p>常⽤来控制过拟合现象的⼀种技术是<strong>正则化</strong>（<code>regularization</code>）。 这种技术涉及到给误差函数增加⼀个<strong>惩罚项</strong>，使得系数不会达到很⼤的值。这种惩罚项最简单的形式采⽤所有系数的平⽅和的形式。这推导出了误差函数的修改后的形式：</p><script type="math/tex; mode=display">\tilde{E}(\boldsymbol{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, \boldsymbol{w}\right)-t_{n}\right\}^{2}+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}\tag{1.4}</script><p>其中，系数 $\lambda$ 控制了正则化项相对于平⽅和误差项的重要性；</p><script type="math/tex; mode=display">\|\boldsymbol{w}\|^{2} \equiv \boldsymbol{w}^{T} \boldsymbol{w}=w_{0}^{2}+w_{1}^{2}+\ldots+w_{M}^{2}</script><p>通过把给定的数据中的⼀部分从测试集中分离出，来确定系数 $\boldsymbol{w}$。这个分离出来的验证集（<code>validation set</code>），也被称为<strong>拿出集</strong>（<code>hold-out set</code>），⽤来最优化模型的复杂度（$M$ 或者 $\lambda$）。</p><p>如图1.11～1.12，使⽤正则化的误差函数，⽤ $M = 9$ 的多项式拟合图中的数据集。其中正则化参数 $\lambda$ 选择了两个值，分别对应于 $\ln \lambda=-18$ 和 $\ln \lambda=0$。</p><p><img src="/images/prml_20190917152015.png" alt="ln lambda=-18"></p><p><img src="/images/prml_20190917152029.png" alt="ln lambda=0"><br>如图1.13，不同的正则化参数 $\lambda$ 下，$M$ = 9的多项式的系数 $\boldsymbol{w}^{*}$ 的值。观察随着 $\lambda$ 的增大，系数的⼤⼩是逐渐变小的。</p><p><img src="/images/prml_20190917152043.png" alt="正则参数"></p><p>如图1.14，对于 $M = 9$ 的多项式，均⽅根误差与 $\ln \lambda$ 的关系。</p><p><img src="/images/prml_20190917152101.png" alt="均⽅根误差与ln lambda的关系"></p><h1 id="二，-总结"><a href="#二，-总结" class="headerlink" title="二， 总结"></a>二， 总结</h1><p>  本小节为机器学习的入门篇，主要通过一个多项式拟合具体实例引出了线性模型相关概念，训练集的意义，误差函数，根均方差，修正误差函数等公式，正则化参数概念。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，-举例：多项式曲线拟合&quot;&gt;&lt;a href=&quot;#一，-举例：多项式曲线拟合&quot; class=&quot;headerlink&quot; title=&quot;一， 举例：多项式曲
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【资源共享】eBook分享大集合</title>
    <link href="https://zhangbc.github.io/2019/08/29/eBooks_share/"/>
    <id>https://zhangbc.github.io/2019/08/29/eBooks_share/</id>
    <published>2019-08-28T16:03:18.000Z</published>
    <updated>2019-09-29T06:16:03.059Z</updated>
    
    <content type="html"><![CDATA[<h1 id="eBook分享大集合"><a href="#eBook分享大集合" class="headerlink" title="eBook分享大集合"></a>eBook分享大集合</h1><ul><li>主要以IT领域经典书籍收藏，以备不时之需。</li><li>福利传送门：<a href="https://github.com/zhangbc/eBooks" target="_blank" rel="noopener">【GitHub】</a> 欢迎各位指点，要是能补充更是感激不尽。</li></ul><h2 id="服务器系统类"><a href="#服务器系统类" class="headerlink" title="服务器系统类"></a>服务器系统类</h2><blockquote><ol><li>Linux高性能服务器编程</li><li>Shell脚本学习指南</li><li>高级Bash脚本编程指南.3.9.1 (杨春敏 黄毅 译)</li><li>鸟哥的Linux私房菜基础篇(第3版)</li><li>深入理解计算机系统</li></ol></blockquote><h2 id="机器学习类"><a href="#机器学习类" class="headerlink" title="机器学习类"></a>机器学习类</h2><blockquote><ol><li>吴恩达深度学习教程</li><li>deepLearning深度学习(开源版)</li><li>python自然语言处理实战：核心技术与算法</li><li>机器学习方法</li><li>社交网站的数据挖掘与分析</li><li>统计学习方法</li><li>用Python进行自然语言处理</li></ol></blockquote><h2 id="算法类"><a href="#算法类" class="headerlink" title="算法类"></a>算法类</h2><blockquote><ol><li>Java数据结构和算法(第2版)</li><li>编程之法面试和算法心得</li><li>编程珠玑(第2版)</li><li>编程珠玑2</li><li>大话数据结构</li><li>计算机程序设计艺术第1卷：基本算法（第3版）</li><li>计算机程序设计艺术第2卷：半数值算法（第3版）</li><li>计算机程序设计艺术第3卷：排序与查找（第2版）</li><li>剑指offer</li><li>数据结构(C语言版).严蔚敏_吴伟民.扫描版</li><li>数据结构与算法分析(C++描述)(第3版)</li><li>算法导论(第2版)</li></ol></blockquote><h2 id="网络类"><a href="#网络类" class="headerlink" title="网络类"></a>网络类</h2><blockquote><ol><li>HTTP权威指南</li><li>TCP-IP详解卷1：协议</li><li>TCP-IP详解卷2：实现</li><li>TCP-IP详解卷3：TCP事务协议，HTTP，NNTP和UNIX域协议</li><li>图解TCP IP(第5版)</li></ol></blockquote><h2 id="程序语言类"><a href="#程序语言类" class="headerlink" title="程序语言类"></a>程序语言类</h2><ul><li>C/C++语言</li></ul><blockquote><ol><li>C++ Primer(第5版)(中文版)</li><li>C和指针</li><li>C语言程序设计</li><li>C语言的科学和艺术</li><li>modern-cpp-tutorial</li><li>大规模C++程序设计</li><li>深入体验C语言项目开发</li><li>实用C语言编程（第3版）</li></ol></blockquote><ul><li>Python语言</li></ul><blockquote><ol><li>Django Web开发指南</li><li>Python.Cookbook(第2版)中文版</li><li>Python标准库中文版</li><li>Python高级编程（法莱德）</li><li>Python核心编程(第2版)</li><li>Python灰帽子</li><li>python基础教程(第2版)</li><li>Python源码剖析</li><li>think in Python</li><li>编写高质量代码 改善Python程序的91个建议</li><li>利用Python进行数据分析</li><li>流畅的Python</li><li>深入浅出Python</li></ol></blockquote><ul><li>Java语言</li></ul><blockquote><ol><li>Head First Java 中文高清版</li><li>Java编程思想(第4版)</li><li>Java核心技术(第8版)卷I_基础知识</li><li>Java核心技术</li><li>Java入门经典</li><li>阿里巴巴Java开发手册终极版v1.3.0</li><li>设计模式之禅 秦晓波</li></ol></blockquote><ul><li>PHP语言</li></ul><blockquote><ol><li>Ajax与PHPWeb开发.pdf</li><li>PHP高级程序设计_模式、框架与测试</li><li>PHP项目开发案例全程实录</li><li>PHP与MYSQL权威指南</li></ol></blockquote><ul><li>C#/.NET语言</li></ul><blockquote><ol><li>.NET本质论</li><li>.NET应用程序架构设计 原则 模式与实践</li><li>ASP.NET MVC4 WEB编程</li><li>ASP.NET MVC4高级编程</li><li>ASP.NET MVC4框架揭秘</li><li>ASP.NET.4.0 揭秘(卷1)</li><li>ASP.NET.4.0 揭秘(卷2)</li><li>ASP.NET本质论</li><li>ASP.NET设计模式</li><li>C#本质论</li><li>C#程序开发范例宝典</li><li>C#高级编程（第7版）</li><li>C#入门经典(第3版)</li><li>C#入门经典(第5版)</li><li>C#与.NET程序员面试宝典</li><li>CLR.via.C#（第3版）</li><li>IT企业必读的200个.NET面试题</li><li>WCF服务编程</li><li>WCF全面解析（上册）</li><li>WCF全面解析（下册）</li><li>编写高质量代码改善C#程序的157个建议</li><li>大话设计模式</li></ol></blockquote><ul><li>Web技术</li></ul><blockquote><ol><li>CSS权威指南(第3版)</li><li>HTML5程序设计(第2版)</li><li>HTML5权威指南</li><li>JavaScript高级应用与实践</li><li>JavaScript权威指南(第4版)</li><li>JavaScript权威指南(第6版)</li><li>JavaScript入门经典(第4版)</li><li>jQuery权威指南</li><li>WebKit技术内幕</li><li>高性能网站建设进阶指南</li><li>论道HTML5</li><li>认识与设计：理解UI设计准则</li></ol></blockquote><h2 id="数据库类"><a href="#数据库类" class="headerlink" title="数据库类"></a>数据库类</h2><ul><li>Oracle</li></ul><blockquote><ol><li>Oracle高性能SQL引擎剖析-SQL优化与调优机制详解</li><li>PLSQL操作手册</li><li>编程艺术深入数据库体系结构</li><li>剑破冰山  Oracle开发艺术</li><li>收获，不止Oracle</li></ol></blockquote><ul><li>MySQL</li></ul><blockquote><ol><li>MySQL 5权威指南(第3版)</li><li>MYSQL必知必会</li><li>MySQL技术内幕(第4版) </li><li>MySQL技术内幕：SQL编程</li><li>MySQL技术内幕InnoDB存储引擎</li><li>MySQL性能调优与架构设计</li><li>高性能MySQL(第2版)</li><li>高性能MySQL(第3版)</li></ol></blockquote><ul><li>SQL Server</li></ul><blockquote><ol><li>SQL2005技术内幕： T-SQ程序设计</li><li>SQL2005技术内幕：存储引擎</li><li>SQL2008技术内幕：T-SQL查询</li><li>SQL2008技术内幕：T-SQL语言基础</li><li>SQLServer2008查询性能优化</li><li>SQLSERVER2008学习笔记：日常维护、深入管理、性能优化</li><li>SQL反模式</li><li>Transact-SQL权威指南</li><li>数据库索引设计与优化</li><li>数据库性能调优.原理与技术</li></ol></blockquote><ul><li>大数据类</li></ul><blockquote><ol><li>Hadoop权威指南(第2版)</li><li>MongoDB权威指南</li><li>MongoDB实战</li><li>R与Hadoop大数据分析实战</li><li>Spark大数据处理：技术、应用与性能优化</li><li>Spark快速数据处理</li></ol></blockquote><h2 id="其他系列"><a href="#其他系列" class="headerlink" title="其他系列"></a>其他系列</h2><ul><li>IT思维类</li></ul><blockquote><ol><li>编码的奥秘</li><li>编码—隐匿在计算机软硬件背后的语言上</li><li>程序员的自我修养—链接、装载与库</li><li>程序员修炼之道</li><li>代码整洁之道</li><li>高效能人士的七个习惯</li><li>计算机程序的构造和解释</li><li>浪潮之巅</li><li>全栈增长工程师指南</li><li>人月神话</li><li>数学之美(第2版)</li><li>修改代码的艺术</li><li>一万小时天才理论</li></ol></blockquote><ul><li>非书籍类</li></ul><blockquote><ol><li>C语言学习资料.exe</li></ol></blockquote><ul><li>架构设计类</li></ul><blockquote><ol><li>GOF设计模式</li><li>UML和模式应用(第3版)</li><li>分布式JAVA应用 基础与实践</li><li>精通.NET企业项目开发：最新的模式、工具与方法</li><li>领域驱动设计C#2008实现 - 问题.设计.解决方案</li><li>领域驱动设计—软件核心复杂性应对之道</li><li>领域驱动设计与模式实战</li><li>企业应用架构模式</li><li>探索CQRS和事件源</li><li>微软应用技术架构(第2版)</li><li>重构_改善既有代码的设计</li><li>重构与模式</li></ol></blockquote><ul><li>敏捷开发类</li></ul><blockquote><ol><li>Scrum敏捷软件开发</li><li>Web开发敏捷之道</li><li>Web开发敏捷之道：应用Rails进行敏捷Web开发（第4版）</li><li>测试驱动开发的3项修炼：走出TDD丛林</li><li>大规模定制模式下的敏捷产品开发</li><li>高效程序员的45个习惯：敏捷开发修炼之道</li><li>敏捷估计与规划</li><li>敏捷技能修炼-敏捷软件开发与设计的最佳实践</li><li>敏捷开发：原则、模式与实践</li><li>敏捷开发的必要技巧</li><li>敏捷开发的艺术</li><li>敏捷开发回顾：使团队更强大</li><li>敏捷开发知识体系</li><li>敏捷软件开发：原则、模式与实践(C#版)</li><li>敏捷软件开发：原则、模式与实践</li><li>敏捷无敌</li><li>敏捷武士：看敏捷高手交付卓越软件</li><li>敏捷整合开发：更快改进性能的案例与实用技术</li><li>硝烟中的Scrum和XP</li><li>应用Rails进行敏捷Web开发(第3版)</li><li>用户故事与敏捷方法</li></ol></blockquote><h2 id="LFS-100M"><a href="#LFS-100M" class="headerlink" title="LFS(100M+)"></a>LFS(100M+)</h2><ul><li>由于 <code>GitHub</code> 是gitLFS属于付费产品，免费空间有限，不作上传处理。</li><li><strong>百度云</strong>传送门：<a href="https://pan.baidu.com/s/19mBtD55fTnyYskyjJs3k4w" target="_blank" rel="noopener">【LFS_EBOOKS】</a> <strong>提取码</strong>：hpgp </li></ul><blockquote><ol><li>C#范例开发大全</li><li>C#核心开发技术从入门到精通</li><li>C语言程序设计_现代方法(第2版)</li><li>C语言入门经典(第4版)</li><li>Java核心技术(第10版)卷II_高级特性</li><li>Oracle+Database+11g数据库管理艺术</li><li>PHP 核心技术与最佳实践</li><li>SQL Server 2012编程入门经典(第4版)</li><li>SQL Server企业级平台管理实践</li><li>大话设计模式</li><li>大话数据库</li><li>大数据Spark企业级实战版</li><li>代码大全(第2版)</li><li>代码重构(C# &amp; ASP.NET版)</li><li>锋利的jquery</li><li>软件设计精要与模式</li><li>实现领域驱动设计</li><li>数据结构与算法分析Java语言描述(第3版)</li><li>算法导论(第3版)</li></ol></blockquote><h1 id="GitHub上传100M以上文件解决方案"><a href="#GitHub上传100M以上文件解决方案" class="headerlink" title="GitHub上传100M以上文件解决方案"></a>GitHub上传100M以上文件解决方案</h1><ul><li>工具下载，详见<a href="https://git-lfs.github.com/" target="_blank" rel="noopener">【官网】</a></li></ul><blockquote><p><a href="https://github.com/git-lfs/git-lfs/releases/download/v2.8.0/git-lfs-windows-v2.8.0.exe" target="_blank" rel="noopener">git-lfs-windows</a><br><a href="https://github.com/git-lfs/git-lfs/releases/download/v2.8.0/git-lfs-darwin-amd64-v2.8.0.tar.gz" target="_blank" rel="noopener">git-lfs-mac</a></p></blockquote><ul><li>基本步骤及其命令</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在项目中安装lfs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git lfs install</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需要push的文件</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git lfs track <span class="string">"程序语言类\C&amp;C++语言\C语言入门经典(第四版).（美）霍顿.pdf"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git add .gitattributes</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git lfs track <span class="string">"程序语言类\C&amp;C++语言\C语言入门经典(第四版).（美）霍顿.pdf"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git add <span class="string">"程序语言类\C&amp;C++语言\C语言入门经典(第四版).（美）霍顿.pdf"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">"[add] add lfs ebook for C."</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git push origin master</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;eBook分享大集合&quot;&gt;&lt;a href=&quot;#eBook分享大集合&quot; class=&quot;headerlink&quot; title=&quot;eBook分享大集合&quot;&gt;&lt;/a&gt;eBook分享大集合&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;主要以IT领域经典书籍收藏，以备不时之需。&lt;/li&gt;
&lt;li&gt;福
      
    
    </summary>
    
      <category term="杂七杂八" scheme="https://zhangbc.github.io/categories/others/"/>
    
    
      <category term="其他" scheme="https://zhangbc.github.io/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>【经典算法】字符串转换成整数</title>
    <link href="https://zhangbc.github.io/2019/08/28/algorithm_strings_02/"/>
    <id>https://zhangbc.github.io/2019/08/28/algorithm_strings_02/</id>
    <published>2019-08-28T15:59:47.000Z</published>
    <updated>2019-08-28T16:12:53.702Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《编程之法：面试和算法心得》的读书笔记。</p></blockquote><h4 id="算法1-3：字符串转换成整数"><a href="#算法1-3：字符串转换成整数" class="headerlink" title="算法1.3：字符串转换成整数"></a>算法1.3：字符串转换成整数</h4><ul><li><p>题目描述</p><p>  输入一个由数字组成的字符串，把它转换成整数并输出。例如:输入字符串”123”，输出整数为123。给定函数原型 <code>int StrToInt(const char *str)</code>，实现字符串转换成整数的功能，不能使用库函数<code>atoi</code>。</p></li><li><p>分析与解法</p></li></ul><blockquote><blockquote><p>思路分析：当扫描字符串的第一个字符“1”时，由于是第一位，故得到数字1；继续向后扫描到第二个字符”2“，之前已经得到数字1，在其后添加一个数字2，得到数字12，相当于前面的数字扩大了10倍然后加上刚扫描到的数字2，即：1×10+2=12。同理，扫描到第三个字符”3“，即可得到最终整数123为所求。故而，其<strong>基本思路</strong>就是：从左至右扫描字符串，把之前得到的数字乘以10，再加上当前字符表示的数字。 但是，在处理过程中，需要考虑以下问题：</p><blockquote><p>1）空指针的输入：输入的是指针，在访问空指针时程序会崩溃，需要提前判空；<br>2）正负符号：整数不仅包括数字，还有可能包括以“+”或“-”开头表示正负整数，遇到负号“-”需要做转换；<br>3）非法字符：输入的字符串中可能有不是数字的字符（如误操作其他字符），需要预先判断，碰到非法字符程序应停止转换；<br>4）整型溢出：输入的数字是以字符串的形式输入，若输入一个很长的字符串可能导致溢出。</p></blockquote></blockquote></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算法1.3：字符串转成整数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">StrToInt</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> MAX_INT = (<span class="keyword">int</span>)((<span class="keyword">unsigned</span>)~<span class="number">0</span> &gt;&gt; <span class="number">1</span>);</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> MIN_INT = -(<span class="keyword">int</span>)((<span class="keyword">unsigned</span>)~<span class="number">0</span> &gt;&gt; <span class="number">1</span>);</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> n = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判空</span></span><br><span class="line"><span class="keyword">if</span>(str == <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理空格</span></span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">isspace</span>(*str))</span><br><span class="line">&#123;</span><br><span class="line">++str;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理正负</span></span><br><span class="line"><span class="keyword">int</span> sign = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(*str == <span class="string">'+'</span> || *str == <span class="string">'-'</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(*str == <span class="string">'-'</span>)</span><br><span class="line">&#123;</span><br><span class="line">sign = <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line">str++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">isdigit</span>(*str))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> c = *str - <span class="string">'0'</span>;</span><br><span class="line"><span class="keyword">if</span>(sign &gt; <span class="number">0</span> &amp;&amp; (n &gt; MAX_INT/<span class="number">10</span> || (n == MAX_INT/<span class="number">10</span> &amp;&amp; c &gt; MAX_INT%<span class="number">10</span>)))</span><br><span class="line">&#123;</span><br><span class="line">n = MAX_INT;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span>(sign &lt; <span class="number">0</span> &amp;&amp; (n &gt; (<span class="keyword">unsigned</span>)MIN_INT/<span class="number">10</span> || (n == (<span class="keyword">unsigned</span>)MIN_INT/<span class="number">10</span> &amp;&amp; c &gt; (<span class="keyword">unsigned</span>)MIN_INT%<span class="number">10</span>)))</span><br><span class="line">&#123;</span><br><span class="line">n = MIN_INT;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">n = n *<span class="number">10</span> + c;</span><br><span class="line">str++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> sign &gt; <span class="number">0</span> ? n:-n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>算法分析：此算法难点在于处理数据溢出，其时间复杂度为 $O(n)$。</p></blockquote></blockquote><ul><li>练习题</li></ul><blockquote><blockquote><p>实现 <code>string</code> 到 <code>double</code> 的转换。</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《编程之法：面试和算法心得》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;算法1-3：字符串转换成整数&quot;&gt;&lt;a href=&quot;#算法1-3：字符串转换成整数&quot; class=&quot;headerlink&quot; title=&quot;算法1.3
      
    
    </summary>
    
      <category term="C++" scheme="https://zhangbc.github.io/categories/C/"/>
    
    
      <category term="数据结构与算法" scheme="https://zhangbc.github.io/tags/data-structure-and-algorithms/"/>
    
  </entry>
  
  <entry>
    <title>【经典算法】字符串旋转和包含算法</title>
    <link href="https://zhangbc.github.io/2019/08/08/algorithm_strings_01/"/>
    <id>https://zhangbc.github.io/2019/08/08/algorithm_strings_01/</id>
    <published>2019-08-08T15:40:40.000Z</published>
    <updated>2019-08-08T15:48:32.238Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《编程之法：面试和算法心得》的读书笔记。</p></blockquote><p>作为一名大龄青年，为了即将踏入研究生之路，特此需要做一些计算机相关基础知识的积累，以弥补算法知识，谨以此开始自己的算法学习之路。</p><h4 id="算法1-1：旋转字符串"><a href="#算法1-1：旋转字符串" class="headerlink" title="算法1.1：旋转字符串"></a>算法1.1：旋转字符串</h4><ul><li><p>题目描述</p><p>  给定一个字符串，要求把字符串前面的若干个字符移动到字符串的尾部，如把字符串“abcdef”前面的2个字符’a’和’b’移动到字符串的尾部，使得原字符串变成字符串“cdefab”。请写一个函数完成此功能，要求对长度为n的字符串操作的时间复杂度为 $O(n)$，空间复杂度为 $O(1)$。</p></li><li><p>分析与解法</p></li></ul><blockquote><p>解法一：暴力移位法</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算法1.1：旋转字符串，暴力移位法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LeftShiftOne</span><span class="params">(<span class="keyword">char</span>* strs, <span class="keyword">int</span> number)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">char</span> ch = strs[i];</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; number; i++)</span><br><span class="line">&#123;</span><br><span class="line">strs[i<span class="number">-1</span>] = strs[i];</span><br><span class="line">&#125;</span><br><span class="line">strs[i<span class="number">-1</span>] = ch;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LeftRoatateString</span><span class="params">(<span class="keyword">char</span>* strs, <span class="keyword">int</span> n, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">while</span>(m--)</span><br><span class="line">&#123;</span><br><span class="line">LeftShiftOne(strs, n);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">char</span> strs[] = <span class="string">"ABCDEFGH"</span>;</span><br><span class="line">LeftRoatateString(strs, <span class="number">8</span>, <span class="number">3</span>);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; strs &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>算法分析：针对长度为n的字符串而言，假设需要移动m个字符到字符串的尾部，总共需要移动 <code>m*n</code> 次操作，同时设立一个变量存储第一个字符，故时间复杂度为 $O(n^2)$，空间复杂度为 $O(1)$，不合题意。</p></blockquote><p>解法二：三步反转法</p><blockquote><p>思路分析：将一个字符串分成X和Y两部分，在每个部分字符串上定义反转操作，如$X^T$，即把X的所有字符反转（例如X=”abc”，则 $X^T$=”cba”），于是得到：$(X^T Y^T)^T$=$YX$。</p></blockquote></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算法1.1：旋转字符串，三步反转法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ReverseString</span><span class="params">(<span class="keyword">char</span>* str, <span class="keyword">int</span> from, <span class="keyword">int</span> to)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">while</span>(from &lt; to)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">char</span> ch = str[from];</span><br><span class="line">str[from++] = str[to];</span><br><span class="line">str[to--] = ch;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LeftReverseString</span><span class="params">(<span class="keyword">char</span>* strs, <span class="keyword">int</span> n, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">m %= n;</span><br><span class="line">ReverseString(strs, <span class="number">0</span>, m<span class="number">-1</span>);</span><br><span class="line">ReverseString(strs, m, n<span class="number">-1</span>);</span><br><span class="line">ReverseString(strs, <span class="number">0</span>, n<span class="number">-1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">char</span> strs[] = <span class="string">"ABCDEFGH"</span>;</span><br><span class="line">LeftReverseString(strs, <span class="number">8</span>, <span class="number">3</span>);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; strs &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>算法分析：针对长度为n的字符串而言，假设需要移动m个字符到字符串的尾部，总共需要移动 <code>2*n</code> 次操作，同时设立一个变量存储第一个字符，故时间复杂度为 $O(n)$，空间复杂度为 $O(1)$，符合题意。</p></blockquote></blockquote><ul><li>练习题（自己动手）</li></ul><blockquote><blockquote><ol><li>链表翻转。例如给出一个链表和一个数k，链表为1—&gt;2—&gt;3—&gt;4—&gt;5—&gt;6，k=2，则翻转后为2—&gt;1—&gt;6—&gt;5—&gt;4—&gt;3；若k=3，翻转后3—&gt;2—&gt;1—&gt;6—&gt;5—&gt;4。</li><li>编写程序在原来字符串中把字符串尾部的m个字符移动到字符串的头部，要求：长度为n的字符串操作时间复杂度为 $O(n)$，空间复杂度为 $O(1)$。例如，源字符串为 “Ilovebaofeng”，m=7时输出为：“baofengIlove”。</li><li>单词翻转。输入一个英文句子，翻转句子中单词的顺序，但是单词内字符的顺序不变，句子中单词以空格符号隔开。为简单起见，标点符号和普通字符一样处理。例如，输入”I am a student.”，输出为 “student. a am I”。</li></ol></blockquote></blockquote><h4 id="算法1-2：字符串包含"><a href="#算法1-2：字符串包含" class="headerlink" title="算法1.2：字符串包含"></a>算法1.2：字符串包含</h4><ul><li><p>题目描述</p><p>  给定两个分别由字母组成的字符串A和字符串B，字符串B的长度比字符串A短。请问，如何快速地判断字符串B中的所有字符是否都在字符串A里面？<br>为简单起见，我们规定输入的字符串只包含大写英文字母，请实现函数 bool StringContain(string &amp;A, string &amp;B)。<br>示例一：string 1：ABCD，string 2： BAD，答案为true；<br>示例二：string 1：ABCD，string 2： BCE，答案为false；<br>示例三：string 1：ABCD，string 2： AA，答案为true。</p></li><li><p>分析与解法</p></li></ul><blockquote><p>解法一：常规解法</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算法1.2：字符串包含，常规方法</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">StringContain</span><span class="params">(<span class="built_in">string</span> &amp;a, <span class="built_in">string</span> &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; b.length(); i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> j;</span><br><span class="line"><span class="keyword">for</span>(j=<span class="number">0</span>; (j &lt; a.length()) &amp;&amp; (a[j] != b[i]); j++);</span><br><span class="line"><span class="keyword">if</span>(j &gt;= a.length())</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">string</span> a = <span class="string">"ABCD"</span>;</span><br><span class="line"><span class="built_in">string</span> b = <span class="string">"AA"</span>;</span><br><span class="line"><span class="keyword">bool</span> result = StringContain(a, b);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; result &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>算法分析：这是一种最直观也是最简单的方法思路。此算法需要 $O（n*m）$ 次操作，时间开销较大。</p></blockquote><p>解法二：排序方法</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算法1.2：字符串包含，排序方法</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">StringContainSort</span><span class="params">(<span class="built_in">string</span> &amp;a, <span class="built_in">string</span> &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">sort(a.begin(), a.end());   <span class="comment">// 包含于&lt;algorithm&gt;模块内</span></span><br><span class="line">sort(b.begin(), b.end());</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> pa = <span class="number">0</span>, pb = <span class="number">0</span>; pb &lt; b.length(); pb++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">while</span>((pa &lt; a.length()) &amp;&amp; (a[pa] &lt; b[pb]))</span><br><span class="line">&#123;</span><br><span class="line">pa++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(pa &gt;= a.length() || (a[pa] &gt; b[pb]))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>算法分析：两个字符串的排序需要（常规情况）$O(m log m)+O(n log n)$ 次操作（快排算法），然后需要线性扫描 $O(m+n)$ 次操作。</p></blockquote><p>解法三： 转换成素数</p><blockquote><p>思路分析：</p><blockquote><p>1）假定有一个仅由字母组成的字符串，按照从小到大的顺序，让每个字母与一个素数唯一对应，即用26个<strong>素数</strong>分别对应于<code>A</code>~<code>Z</code>；<br>2）遍历长字符串。求得每个字符对应素数的乘积；<br>3）遍历短字符串，判断乘积能否被短字符串中的字符对应的素数整除；<br>4）输出结果。</p></blockquote></blockquote></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算法1.2：字符串包含，转换成素数</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">StringContainPrime</span><span class="params">(<span class="built_in">string</span> &amp;a, <span class="built_in">string</span> &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> <span class="built_in">array</span>[<span class="number">26</span>] = &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">23</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">37</span>, <span class="number">41</span>, </span><br><span class="line"><span class="number">43</span>, <span class="number">47</span>, <span class="number">53</span>, <span class="number">59</span>, <span class="number">61</span>, <span class="number">67</span>, <span class="number">71</span>, <span class="number">73</span>, <span class="number">79</span>, <span class="number">83</span>, <span class="number">89</span>, <span class="number">97</span>, <span class="number">101</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> f = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length(); i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> x = <span class="built_in">array</span>[a[i] - <span class="string">'A'</span>];</span><br><span class="line"><span class="keyword">if</span>(f % x)</span><br><span class="line">&#123;</span><br><span class="line">f *= x;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; b.length(); j++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> x = <span class="built_in">array</span>[b[j] - <span class="string">'A'</span>];</span><br><span class="line"><span class="keyword">if</span>(f % x)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>算法分析：算法的时间复杂度为 $O(n)$ ，最好的情况为 $O(1)$（遍历短的字符串的第一个数，与长字符串素数的乘积相除，即出现余数，便可退出程序，返回 <code>false</code>）， <code>n</code> 为长字串的长度，空间复杂度为 $O(1)$。<br><strong>注意</strong>：此方法只有理论意义，因为整数乘积很大会造成溢出风险。</p></blockquote><p>解法四：<code>Hashtable</code>方法</p><blockquote><p>思路分析：先把长字符串 <code>A</code>中的所有字符都放入一个 <code>Hashtable</code> 里，然后轮询短字符串 <code>B</code>，看短字符串 <code>B</code> 的每个字符是否都在 <code>Hashtable</code> 里，如果都存在，说明长字符串 <code>A</code> 包含短字符串 <code>B</code>， 否则，说明不包含。</p></blockquote></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算法1.2：字符串包含，Hashtable方法</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">StringContainHash</span><span class="params">(<span class="built_in">string</span> &amp;a, <span class="built_in">string</span> &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> hash = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length(); i++)</span><br><span class="line">&#123;</span><br><span class="line">hash |= (<span class="number">1</span> &lt;&lt; (a[i] - <span class="string">'A'</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; b.length(); j++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(hash &amp; (<span class="number">1</span> &lt;&lt; (b[j] - <span class="string">'A'</span>)))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>算法分析：此方法实质是用一个整数代替了<code>Hashtable</code>，空间复杂度为 $O(1)$，时间复杂度为 $O(n)$。</p></blockquote></blockquote><ul><li>练习题（自己动手）</li></ul><blockquote><blockquote><p>变位词：如果两个字符串的字符一样，但是顺序不一样，被认为是兄弟字符串，比如 <code>bad</code> 和 <code>adb</code> 即为兄弟字符串，现提供一个字符串，如何在字典中迅速找到它的兄弟字符串，请描述数据结构和查询过程。</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《编程之法：面试和算法心得》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作为一名大龄青年，为了即将踏入研究生之路，特此需要做一些计算机相关基础知识的积累，以弥补算法知识，谨以此开始自己的算法学习之路。&lt;/p&gt;
&lt;h4 id=&quot;算
      
    
    </summary>
    
      <category term="C++" scheme="https://zhangbc.github.io/categories/C/"/>
    
    
      <category term="数据结构与算法" scheme="https://zhangbc.github.io/tags/data-structure-and-algorithms/"/>
    
  </entry>
  
  <entry>
    <title>【Python编码规范】设计模式</title>
    <link href="https://zhangbc.github.io/2019/06/13/python_code91_05/"/>
    <id>https://zhangbc.github.io/2019/06/13/python_code91_05/</id>
    <published>2019-06-13T14:55:38.000Z</published>
    <updated>2019-08-08T15:33:23.828Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。</p></blockquote><p><strong>温馨提醒</strong>：在阅读本书之前，强烈建议先仔细阅读：<a href="https://legacy.python.org/dev/peps/pep-0008/" target="_blank" rel="noopener"><strong>PEP</strong>规范</a>，增强代码的可阅读性，配合优雅的<a href="http://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">pycharm</a>编辑器(开启<code>pep8</code>检查)写出规范代码，是<code>Python</code>入门的第一步。</p><h2 id="建议50：利用模块实现单例模式"><a href="#建议50：利用模块实现单例模式" class="headerlink" title="建议50：利用模块实现单例模式"></a>建议50：利用模块实现单例模式</h2><p>1）所有的变量都会绑定到模块；<br>2）模块只初始化一次；<br>3）<code>import</code>机制是线程安全的。</p><h2 id="建议51-用mixin模式让程序更加灵活"><a href="#建议51-用mixin模式让程序更加灵活" class="headerlink" title="建议51: 用mixin模式让程序更加灵活"></a>建议51: 用mixin模式让程序更加灵活</h2><p><code>模板方法模式</code>：在一个方法中定义一个算法的骨架，并将一些实现步骤延迟到子类中。</p><p><code>Python</code> 中每一个类都有一个<code>__base__</code>属性，是一个元组，用来存放所有的基类，基类在运行中可以动态改变。</p><h2 id="建议52：用发布订阅模式实现松耦合"><a href="#建议52：用发布订阅模式实现松耦合" class="headerlink" title="建议52：用发布订阅模式实现松耦合"></a>建议52：用发布订阅模式实现松耦合</h2><p> 发布订阅模式（publish/subscribe或pub/sub）是一种编程模式，消息的发送者（发布者）不会发送其消息给特定的接收者（订阅者），而是将发布的消息分为不同的类别直接发布，并不关注订阅者是谁。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">发布订阅模式实现</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> message</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line">route_table = defaultdict(list)</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sub</span><span class="params">(topic, callback)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">    :param topic:</span></span><br><span class="line"><span class="string">    :param callback:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> callback <span class="keyword">in</span> route_table[topic]:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">​</span><br><span class="line">    route_table[topic].append(callback)</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pub</span><span class="params">(topic, *args, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">    :param topic:</span></span><br><span class="line"><span class="string">    :param args:</span></span><br><span class="line"><span class="string">    :param kwargs:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> func <span class="keyword">in</span> route_table[topic]:</span><br><span class="line">        func(*args, **kwargs)</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greeting</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">    :param name:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Hello, &#123;0&#125;.'</span>.format(name)</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">​</span><br><span class="line">    sub(<span class="string">'greet'</span>, greeting)</span><br><span class="line">    pub(<span class="string">'greet'</span>, <span class="string">'LaiYonghao'</span>)</span><br><span class="line">​</span><br><span class="line">    message.sub(<span class="string">'greet'</span>, greeting)</span><br><span class="line">    message.pub(<span class="string">'greet'</span>, <span class="string">'Welcome to Python'</span>)</span><br></pre></td></tr></table></figure><h2 id="建议53：用状态模式美化代码"><a href="#建议53：用状态模式美化代码" class="headerlink" title="建议53：用状态模式美化代码"></a>建议53：用状态模式美化代码</h2><p><code>状态模式</code>：当一个对象的内在状态改变时允许改变其行为，但这个对象看起来像是改变了其类。主要用于控制一个对象状态的条件表达式过于复杂的情况，其可把状态的判断逻辑转移到表示不同状态的一系列类中，进而把复杂的判断逻辑简化。<br>@stateful修饰函数，重载了被修饰类的<strong>getattr</strong>()方法从而使得类的实例方法能调用当前状态类的方法。被@stateful修饰后的类的实例是带有状态的，能够使用curr()查询当前状态，也可以使用switch()进行状态切换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">状态模式实现</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> state <span class="keyword">import</span> switch, stateful, State, behavior</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="meta">@stateful</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">People</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">​</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Workday</span><span class="params">(State)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">​</span><br><span class="line">        default = <span class="literal">True</span></span><br><span class="line">​</span><br><span class="line"><span class="meta">        @behavior</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">day</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'work hard.'</span></span><br><span class="line">​</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Weekend</span><span class="params">(State)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">​</span><br><span class="line"><span class="meta">        @behavior</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">day</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'play harder.'</span></span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">​</span><br><span class="line">    people = People()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">8</span>):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">6</span>:</span><br><span class="line">            switch(people, People.Weekend)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">            switch(people, People.Workday)</span><br><span class="line">        people.day()</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">​</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;温馨提醒&lt;/strong&gt;：在阅读本书之前，强烈建议先仔细阅读：&lt;a href=&quot;https://legacy.py
      
    
    </summary>
    
      <category term="Python" scheme="https://zhangbc.github.io/categories/python/"/>
    
    
      <category term="Python编码规范" scheme="https://zhangbc.github.io/tags/python-coding-convention/"/>
    
  </entry>
  
  <entry>
    <title>【Python编码规范】库</title>
    <link href="https://zhangbc.github.io/2019/05/12/python_code91_04/"/>
    <id>https://zhangbc.github.io/2019/05/12/python_code91_04/</id>
    <published>2019-05-12T15:01:16.000Z</published>
    <updated>2019-05-12T15:21:55.643Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。</p></blockquote><p><strong>温馨提醒</strong>：在阅读本书之前，强烈建议先仔细阅读：<a href="https://legacy.python.org/dev/peps/pep-0008/" target="_blank" rel="noopener"><strong>PEP</strong>规范</a>，增强代码的可阅读性，配合优雅的<a href="http://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">pycharm</a>编辑器(开启<code>pep8</code>检查)写出规范代码，是<code>Python</code>入门的第一步。</p><h2 id="建议36：掌握字符串的基本用法"><a href="#建议36：掌握字符串的基本用法" class="headerlink" title="建议36：掌握字符串的基本用法"></a>建议36：掌握字符串的基本用法</h2><p><strong><code>Python</code>小技巧</strong>：<code>Python</code>遇到未闭合的小括号会自动将多行代码拼接为一行和把相邻的两个字符串字面量拼接在一起的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>st = (<span class="string">'select * '</span></span><br><span class="line"><span class="meta">... </span>      <span class="string">'from table '</span></span><br><span class="line"><span class="meta">... </span>      <span class="string">'whre field="value";'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>st</span><br><span class="line"><span class="string">'select * from table whre field="value";'</span></span><br></pre></td></tr></table></figure><ul><li>字符串用法举例：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> isinstance(<span class="string">'hello world'</span>, basestring)  <span class="comment"># basestring是str与unicode的基类</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> isinstance(<span class="string">'hello world'</span>, unicode)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> isinstance(<span class="string">'hello world'</span>, str)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> isinstance(<span class="string">u'hello world'</span>, unicode)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><ul><li><code>split()</code>的陷阱示例</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">' Hello World'</span>.split(<span class="string">' '</span>)</span><br><span class="line">[<span class="string">''</span>, <span class="string">'Hello'</span>, <span class="string">'World'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">' Hello   World'</span>.split()</span><br><span class="line">[<span class="string">'Hello'</span>, <span class="string">'World'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">' Hello   World'</span>.split(<span class="string">' '</span>)</span><br><span class="line">[<span class="string">''</span>, <span class="string">'Hello'</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">'World'</span>]</span><br></pre></td></tr></table></figure><ul><li><code>title()</code>应用示例</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> string</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>string.capwords(<span class="string">'hello  wOrld'</span>)</span><br><span class="line"><span class="string">'Hello World'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>string.capwords(<span class="string">' hello  wOrld '</span>)</span><br><span class="line"><span class="string">'Hello World'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">' hello  wOrld '</span>.title()</span><br><span class="line"><span class="string">' Hello  World '</span></span><br></pre></td></tr></table></figure><h2 id="建议37：按需选择sort-或者sorted"><a href="#建议37：按需选择sort-或者sorted" class="headerlink" title="建议37：按需选择sort()或者sorted()"></a>建议37：按需选择sort()或者sorted()</h2><p><code>sorted(iterable[, cmp[, key[, reverse]]])</code>：作用于任何可迭代对象，返回一个排序后的列表；</p><p><code>sort(cmp[, key[, reverse]]])</code>：一般作用于列表，直接修改原有列表，返回为<code>None</code>。</p><p>1）<strong>对字典进行排序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>phone_book = &#123;<span class="string">'Linda'</span>: <span class="string">'775'</span>, <span class="string">'Bob'</span>: <span class="string">'9349'</span>, <span class="string">'Carol'</span>: <span class="string">'5834'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted_pb = sorted(phone_book.iteritems(), key=itemgetter(<span class="number">1</span>))  ​<span class="comment"># 按照字典的value进行排序</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> phone_book</span><br><span class="line">&#123;<span class="string">'Linda'</span>: <span class="string">'775'</span>, <span class="string">'Bob'</span>: <span class="string">'9349'</span>, <span class="string">'Carol'</span>: <span class="string">'5834'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> sorted_pb</span><br><span class="line">[(<span class="string">'Carol'</span>, <span class="string">'5834'</span>), (<span class="string">'Linda'</span>, <span class="string">'775'</span>), (<span class="string">'Bob'</span>, <span class="string">'9349'</span>)]</span><br></pre></td></tr></table></figure><p> 2）<strong>多维<code>list</code>排序</strong><br>​<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>game_result = [[<span class="string">'Linda'</span>, <span class="number">95</span>, <span class="string">'B'</span>], [<span class="string">'Bob'</span>, <span class="number">93</span>, <span class="string">'A'</span>], [<span class="string">'Carol'</span>, <span class="number">69</span>, <span class="string">'D'</span>], [<span class="string">'zhangs'</span>, <span class="number">95</span>, <span class="string">'A'</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted_res = sorted(game_result, key=itemgetter(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># 按照学生成绩排序，成绩相同的按照等级排序</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> game_result</span><br><span class="line">[[<span class="string">'Linda'</span>, <span class="number">95</span>, <span class="string">'B'</span>], [<span class="string">'Bob'</span>, <span class="number">93</span>, <span class="string">'A'</span>], [<span class="string">'Carol'</span>, <span class="number">69</span>, <span class="string">'D'</span>], [<span class="string">'zhangs'</span>, <span class="number">95</span>, <span class="string">'A'</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> sorted_res</span><br><span class="line">[[<span class="string">'Carol'</span>, <span class="number">69</span>, <span class="string">'D'</span>], [<span class="string">'Bob'</span>, <span class="number">93</span>, <span class="string">'A'</span>], [<span class="string">'zhangs'</span>, <span class="number">95</span>, <span class="string">'A'</span>], [<span class="string">'Linda'</span>, <span class="number">95</span>, <span class="string">'B'</span>]]</span><br></pre></td></tr></table></figure></p><p>3）<strong>字典中混合<code>list</code>排序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list_dict = &#123;</span><br><span class="line"><span class="meta">... </span>    <span class="string">'Li'</span>: [<span class="string">'M'</span>, <span class="number">7</span>],</span><br><span class="line"><span class="meta">... </span>    <span class="string">'Zhang'</span>: [<span class="string">'E'</span>, <span class="number">2</span>],</span><br><span class="line"><span class="meta">... </span>    <span class="string">'Du'</span>: [<span class="string">'P'</span>, <span class="number">3</span>],</span><br><span class="line"><span class="meta">... </span>    <span class="string">'Ma'</span>: [<span class="string">'C'</span>, <span class="number">9</span>],</span><br><span class="line"><span class="meta">... </span>    <span class="string">'Zhe'</span>: [<span class="string">'H'</span>, <span class="number">7</span>]</span><br><span class="line"><span class="meta">... </span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted_ld = sorted(list_dict.iteritems(), key=<span class="keyword">lambda</span> (k, v): itemgetter(<span class="number">1</span>)(v))  <span class="comment"># 按照字典的value[m,n]中的n值排序</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> list_dict</span><br><span class="line">&#123;<span class="string">'Zhe'</span>: [<span class="string">'H'</span>, <span class="number">7</span>], <span class="string">'Zhang'</span>: [<span class="string">'E'</span>, <span class="number">2</span>], <span class="string">'Ma'</span>: [<span class="string">'C'</span>, <span class="number">9</span>], <span class="string">'Du'</span>: [<span class="string">'P'</span>, <span class="number">3</span>], <span class="string">'Li'</span>: [<span class="string">'M'</span>, <span class="number">7</span>]&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> sorted_ld</span><br><span class="line">[(<span class="string">'Zhang'</span>, [<span class="string">'E'</span>, <span class="number">2</span>]), (<span class="string">'Du'</span>, [<span class="string">'P'</span>, <span class="number">3</span>]), (<span class="string">'Zhe'</span>, [<span class="string">'H'</span>, <span class="number">7</span>]), (<span class="string">'Li'</span>, [<span class="string">'M'</span>, <span class="number">7</span>]), (<span class="string">'Ma'</span>, [<span class="string">'C'</span>, <span class="number">9</span>])]</span><br></pre></td></tr></table></figure><p>4）<strong><code>list</code>中混合字典排序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>game_result = [</span><br><span class="line"><span class="meta">... </span>    &#123;<span class="string">'name'</span>: <span class="string">'Bob'</span>, <span class="string">'wins'</span>: <span class="number">10</span>, <span class="string">'losses'</span>: <span class="number">3</span>, <span class="string">'rating'</span>: <span class="number">75</span>&#125;,</span><br><span class="line"><span class="meta">... </span>    &#123;<span class="string">'name'</span>: <span class="string">'David'</span>, <span class="string">'wins'</span>: <span class="number">3</span>, <span class="string">'losses'</span>: <span class="number">5</span>, <span class="string">'rating'</span>: <span class="number">57</span>&#125;,</span><br><span class="line"><span class="meta">... </span>    &#123;<span class="string">'name'</span>: <span class="string">'Carol'</span>, <span class="string">'wins'</span>: <span class="number">4</span>, <span class="string">'losses'</span>: <span class="number">5</span>, <span class="string">'rating'</span>: <span class="number">57</span>&#125;,</span><br><span class="line"><span class="meta">... </span>    &#123;<span class="string">'name'</span>: <span class="string">'Patty'</span>, <span class="string">'wins'</span>: <span class="number">9</span>, <span class="string">'losses'</span>: <span class="number">3</span>, <span class="string">'rating'</span>: <span class="number">71.48</span>&#125;</span><br><span class="line"><span class="meta">... </span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted_res = sorted(game_result, key=itemgetter(<span class="string">'rating'</span>, <span class="string">'name'</span>))   <span class="comment"># 按照name和rating排序</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> game_result</span><br><span class="line">[&#123;<span class="string">'wins'</span>: <span class="number">10</span>, <span class="string">'losses'</span>: <span class="number">3</span>, <span class="string">'name'</span>: <span class="string">'Bob'</span>, <span class="string">'rating'</span>: <span class="number">75</span>&#125;, &#123;<span class="string">'wins'</span>: <span class="number">3</span>, <span class="string">'losses'</span>: <span class="number">5</span>, <span class="string">'name'</span>: <span class="string">'David'</span>, <span class="string">'rating'</span>: <span class="number">57</span>&#125;, &#123;<span class="string">'wins'</span>: <span class="number">4</span>, <span class="string">'losses'</span>: <span class="number">5</span>, <span class="string">'name'</span>: <span class="string">'Carol'</span>, <span class="string">'rating'</span>: <span class="number">57</span>&#125;, &#123;<span class="string">'wins'</span>: <span class="number">9</span>, <span class="string">'losses'</span>: <span class="number">3</span>, <span class="string">'name'</span>: <span class="string">'Patty'</span>, <span class="string">'rating'</span>: <span class="number">71.48</span>&#125;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> sorted_res</span><br><span class="line">[&#123;<span class="string">'wins'</span>: <span class="number">4</span>, <span class="string">'losses'</span>: <span class="number">5</span>, <span class="string">'name'</span>: <span class="string">'Carol'</span>, <span class="string">'rating'</span>: <span class="number">57</span>&#125;, &#123;<span class="string">'wins'</span>: <span class="number">3</span>, <span class="string">'losses'</span>: <span class="number">5</span>, <span class="string">'name'</span>: <span class="string">'David'</span>, <span class="string">'rating'</span>: <span class="number">57</span>&#125;, &#123;<span class="string">'wins'</span>: <span class="number">9</span>, <span class="string">'losses'</span>: <span class="number">3</span>, <span class="string">'name'</span>: <span class="string">'Patty'</span>, <span class="string">'rating'</span>: <span class="number">71.48</span>&#125;, &#123;<span class="string">'wins'</span>: <span class="number">10</span>, <span class="string">'losses'</span>: <span class="number">3</span>, <span class="string">'name'</span>: <span class="string">'Bob'</span>, <span class="string">'rating'</span>: <span class="number">75</span>&#125;]</span><br></pre></td></tr></table></figure><h2 id="建议38：使用copy模块深拷贝对象"><a href="#建议38：使用copy模块深拷贝对象" class="headerlink" title="建议38：使用copy模块深拷贝对象"></a>建议38：使用copy模块深拷贝对象</h2><ul><li><code>浅拷贝(shallow copy)</code>：构造一个新的复合对象并将从原对象中发现的引用插入该对象中。实现方式有：工厂函数，切片操作，<code>copy</code>模块中<code>copy</code>操作等；</li><li><p><code>深拷贝(deep copy)</code>：构造一个新的复合对象，但是遇到引用会继续递归拷贝其所指向的具体内容，也就是说它会针对引用所指向的对象继续进行拷贝，因此产生的对象不受其他引用对象操作的影响。实现方式有<code>copy</code>模块中的<code>deepcopy()</code>操作。</p></li><li><p>实例</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pizza</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, size, price)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.size = size</span><br><span class="line">        self.price = price</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_pizza_info</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name, self.size, self.price</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_pizza_info</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Pizza name: &#123;0&#125;, size: &#123;1&#125;, price: &#123;2&#125;"</span>.format(self.name, self.size, self.price)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_size</span><span class="params">(self, size)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param size:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_price</span><span class="params">(self, price)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param price:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        self.price = price</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Order</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.customer_name = name</span><br><span class="line">        self.pizza_list = list()</span><br><span class="line">        self.pizza_list.append(Pizza(<span class="string">"Mushroom"</span>, <span class="number">12</span>, <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">order_more</span><span class="params">(self, pizza)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param pizza:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        self.pizza_list.append(pizza)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_name</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param name:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        self.customer_name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_oder_detail</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Customer name: &#123;0&#125;"</span>.format(self.customer_name)</span><br><span class="line">        <span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(self.pizza_list):</span><br><span class="line">            item.show_pizza_info()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_pizza</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param number:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.pizza_list[number]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">customer_one</span><span class="params">()</span>:</span></span><br><span class="line">    c1 = Order(<span class="string">"zhang San"</span>)</span><br><span class="line">    c1.order_more(Pizza(<span class="string">"seafood"</span>, <span class="number">9</span>, <span class="number">40</span>))</span><br><span class="line">    c1.order_more(Pizza(<span class="string">"fruit"</span>, <span class="number">12</span>, <span class="number">35</span>))</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"==============Customer one order info================="</span></span><br><span class="line">    c1.get_oder_detail()</span><br><span class="line"></span><br><span class="line">    c2 = copy(c1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"==============Customer two order info(copy)================="</span></span><br><span class="line">    c2.change_name(<span class="string">"Li Si"</span>)</span><br><span class="line">    c2.get_pizza(<span class="number">2</span>).change_size(<span class="number">9</span>)</span><br><span class="line">    c2.get_pizza(<span class="number">2</span>).change_price(<span class="number">30</span>)</span><br><span class="line">    c2.get_oder_detail()</span><br><span class="line"></span><br><span class="line">    c3 = deepcopy(c1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"==============Customer three order info(deepcopy)================="</span></span><br><span class="line">    c3.change_name(<span class="string">"Li Si"</span>)</span><br><span class="line">    c3.get_pizza(<span class="number">1</span>).change_size(<span class="number">10</span>)</span><br><span class="line">    c3.get_pizza(<span class="number">1</span>).change_price(<span class="number">50</span>)</span><br><span class="line">    c3.get_oder_detail()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"==============Customer one order info================="</span></span><br><span class="line">    c1.get_oder_detail()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    customer_one()</span><br></pre></td></tr></table></figure><ul><li>运行结果如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">==============Customer one order info=================</span><br><span class="line">Customer name: zhang San</span><br><span class="line">Pizza name: Mushroom, size: <span class="number">12</span>, price: <span class="number">30</span></span><br><span class="line">Pizza name: seafood, size: <span class="number">9</span>, price: <span class="number">40</span></span><br><span class="line">Pizza name: fruit, size: <span class="number">12</span>, price: <span class="number">35</span></span><br><span class="line">==============Customer two order info(copy)=================</span><br><span class="line">Customer name: Li Si</span><br><span class="line">Pizza name: Mushroom, size: <span class="number">12</span>, price: <span class="number">30</span></span><br><span class="line">Pizza name: seafood, size: <span class="number">9</span>, price: <span class="number">40</span></span><br><span class="line">Pizza name: fruit, size: <span class="number">9</span>, price: <span class="number">30</span></span><br><span class="line">==============Customer three order info(deepcopy)=================</span><br><span class="line">Customer name: Li Si</span><br><span class="line">Pizza name: Mushroom, size: <span class="number">12</span>, price: <span class="number">30</span></span><br><span class="line">Pizza name: seafood, size: <span class="number">10</span>, price: <span class="number">50</span></span><br><span class="line">Pizza name: fruit, size: <span class="number">9</span>, price: <span class="number">30</span></span><br><span class="line">==============Customer one order info=================</span><br><span class="line">Customer name: zhang San</span><br><span class="line">Pizza name: Mushroom, size: <span class="number">12</span>, price: <span class="number">30</span></span><br><span class="line">Pizza name: seafood, size: <span class="number">9</span>, price: <span class="number">40</span></span><br><span class="line">Pizza name: fruit, size: <span class="number">9</span>, price: <span class="number">30</span></span><br></pre></td></tr></table></figure><h2 id="建议39：使用Counter进行计数统计"><a href="#建议39：使用Counter进行计数统计" class="headerlink" title="建议39：使用Counter进行计数统计"></a>建议39：使用Counter进行计数统计</h2><ul><li>使用<code>dict</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_data = [<span class="string">'a'</span>, <span class="number">2</span>, <span class="string">'2'</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'2'</span>, <span class="string">'b'</span>, <span class="number">7</span>, <span class="string">'a'</span>, <span class="number">5</span>, <span class="string">'d'</span>, <span class="string">'a'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>count_frq = dict()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(some_data):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">if</span> item <span class="keyword">in</span> count_frq:</span><br><span class="line"><span class="meta">... </span>        count_frq[item] += <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>        count_frq[item] = <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>        </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> count_frq</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">1</span>, <span class="number">4</span>: <span class="number">1</span>, <span class="number">5</span>: <span class="number">2</span>, <span class="number">7</span>: <span class="number">1</span>, <span class="string">'2'</span>: <span class="number">2</span>, <span class="string">'z'</span>: <span class="number">1</span>, <span class="string">'d'</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure><ul><li>使用<code>defaultdict</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_data = [<span class="string">'a'</span>, <span class="number">2</span>, <span class="string">'2'</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'2'</span>, <span class="string">'b'</span>, <span class="number">7</span>, <span class="string">'a'</span>, <span class="number">5</span>, <span class="string">'d'</span>, <span class="string">'a'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>count_frq = defaultdict(int)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(some_data):</span><br><span class="line"><span class="meta">... </span>    count_frq[item] += <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> count_frq</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;, &#123;<span class="string">'a'</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">1</span>, <span class="number">4</span>: <span class="number">1</span>, <span class="number">5</span>: <span class="number">2</span>, <span class="number">7</span>: <span class="number">1</span>, <span class="string">'2'</span>: <span class="number">2</span>, <span class="string">'z'</span>: <span class="number">1</span>, <span class="string">'d'</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure><ul><li>使用<code>set</code>与<code>list</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_data = [<span class="string">'a'</span>, <span class="number">2</span>, <span class="string">'2'</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'2'</span>, <span class="string">'b'</span>, <span class="number">7</span>, <span class="string">'a'</span>, <span class="number">5</span>, <span class="string">'d'</span>, <span class="string">'a'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>count_set = set(some_data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>count_list = list()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(some_data):</span><br><span class="line"><span class="meta">... </span>    count_list.append((item, some_data.count(item)))</span><br><span class="line"><span class="meta">... </span>    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> count_list</span><br><span class="line">[(<span class="string">'a'</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="string">'2'</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">5</span>, <span class="number">2</span>), (<span class="string">'2'</span>, <span class="number">2</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="number">7</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">3</span>), (<span class="number">5</span>, <span class="number">2</span>), (<span class="string">'d'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">3</span>), (<span class="string">'z'</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure><ul><li>使用更为优雅的<code>Pythonic</code>方法—<code>collections.Counter</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_data = [<span class="string">'a'</span>, <span class="number">2</span>, <span class="string">'2'</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'2'</span>, <span class="string">'b'</span>, <span class="number">7</span>, <span class="string">'a'</span>, <span class="number">5</span>, <span class="string">'d'</span>, <span class="string">'a'</span>, <span class="string">'z'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> Counter(some_data)</span><br><span class="line">Counter(&#123;<span class="string">'a'</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">2</span>, <span class="string">'2'</span>: <span class="number">2</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">1</span>, <span class="number">4</span>: <span class="number">1</span>, <span class="number">7</span>: <span class="number">1</span>, <span class="string">'z'</span>: <span class="number">1</span>, <span class="string">'d'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> Counter(<span class="string">'success'</span>)</span><br><span class="line">Counter(&#123;<span class="string">'s'</span>: <span class="number">3</span>, <span class="string">'c'</span>: <span class="number">2</span>, <span class="string">'e'</span>: <span class="number">1</span>, <span class="string">'u'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> Counter(s=<span class="number">3</span>, c=<span class="number">2</span>, e=<span class="number">1</span>, u=<span class="number">1</span>)</span><br><span class="line">Counter(&#123;<span class="string">'s'</span>: <span class="number">3</span>, <span class="string">'c'</span>: <span class="number">2</span>, <span class="string">'u'</span>: <span class="number">1</span>, <span class="string">'e'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> Counter(&#123;<span class="string">'s'</span>: <span class="number">3</span>, <span class="string">'c'</span>: <span class="number">2</span>, <span class="string">'u'</span>: <span class="number">1</span>, <span class="string">'e'</span>: <span class="number">1</span>&#125;)</span><br><span class="line">Counter(&#123;<span class="string">'s'</span>: <span class="number">3</span>, <span class="string">'c'</span>: <span class="number">2</span>, <span class="string">'u'</span>: <span class="number">1</span>, <span class="string">'e'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> list(Counter(some_data).elements())</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'a'</span>, <span class="string">'a'</span>, <span class="number">2</span>, <span class="string">'b'</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="string">'2'</span>, <span class="string">'2'</span>, <span class="string">'z'</span>, <span class="string">'d'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> Counter(some_data).most_common(<span class="number">3</span>) <span class="comment"># 出现频次最高的前三个字符</span></span><br><span class="line">[(<span class="string">'a'</span>, <span class="number">3</span>), (<span class="number">5</span>, <span class="number">2</span>), (<span class="string">'2'</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure><h2 id="建议40：深入理解ConfigParser"><a href="#建议40：深入理解ConfigParser" class="headerlink" title="建议40：深入理解ConfigParser"></a>建议40：深入理解ConfigParser</h2><ul><li>实例</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ConfigParser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conf = ConfigParser.ConfigParser()</span><br><span class="line">conf.read(<span class="string">'config.ini'</span>)</span><br><span class="line"><span class="keyword">print</span> conf.get(<span class="string">'default'</span>, <span class="string">'host'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conf = ConfigParser.ConfigParser()</span><br><span class="line">conf.read(<span class="string">'config.ini'</span>)</span><br><span class="line"><span class="keyword">print</span> conf.get(<span class="string">'online'</span>, <span class="string">'conn_str'</span>)   <span class="comment"># 仅在default下</span></span><br><span class="line"></span><br><span class="line">​</span><br><span class="line">====================config.ini=========================</span><br><span class="line"></span><br><span class="line">[default]</span><br><span class="line">conn_str = %(dbn)s://%(user)s:%(pw)s@%(host)s:%(port)s/%(db)s</span><br><span class="line">dbn = msyql</span><br><span class="line">host = <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">user = root</span><br><span class="line">port = <span class="number">3306</span></span><br><span class="line">pw = xxxxxx</span><br><span class="line">db = test</span><br><span class="line"></span><br><span class="line">[online]</span><br><span class="line">conn_str = %(dbn)s://%(user)s:%(pw)s@%(host)s:%(port)s/%(db)s</span><br><span class="line">dbn = msyql</span><br><span class="line">host = <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">user = root</span><br><span class="line">port = <span class="number">3306</span></span><br><span class="line">pw = xxxxxx</span><br><span class="line">db = test</span><br></pre></td></tr></table></figure><h2 id="建议41：使用argparese处理命令行参数"><a href="#建议41：使用argparese处理命令行参数" class="headerlink" title="建议41：使用argparese处理命令行参数"></a>建议41：使用argparese处理命令行参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-v'</span>, dest=<span class="string">'verbose'</span>, action=<span class="string">'store_true'</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="keyword">print</span> args</span><br></pre></td></tr></table></figure><h2 id="建议42：使用pandas处理大型csv文件"><a href="#建议42：使用pandas处理大型csv文件" class="headerlink" title="建议42：使用pandas处理大型csv文件"></a>建议42：使用pandas处理大型csv文件</h2><p><code>csv</code>作为一种逗号分隔型值的纯文本格式文件，常见于数据库数据的导入导出、数据分析中记录的存储等。</p><p>以下列举几个与<code>csv</code>处理相关的<code>API</code>：</p><ul><li><code>csv.reader(csvfile[, dialect=&#39;excel&#39;][, fmtparam])</code>：用于<code>CSV</code>文件的读取，返回一个<code>reader</code>对象用于在<code>CSV</code>文件中进行行迭代；</li><li><code>csv.writer(csvfile, dialect=&#39;excel&#39;, **fmtparams)</code>：用于写入<code>CSV</code>文件；</li><li><code>csv.DictReader(csvfile, fieldnames=None, restKey=&#39;&#39;, restval=&#39;&#39;, dialect=&#39;excel&#39;, *args, **kwds)</code>：用于支持字典的读取；</li><li><p><code>csv.DictReader(csvfile, fieldnames=None, restval=&#39;&#39;, extrasaction=&#39;raise&#39;, dialect=&#39;excel&#39;, *args, **kwds)</code>：用于支持字典的写入。</p></li><li><p>实例</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入csv</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'csv_test.csv'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fields = [<span class="string">'Tran_date'</span>, <span class="string">'Product'</span>, <span class="string">'Price'</span>, <span class="string">'PaymentType'</span>]</span><br><span class="line">    writer = csv.DictWriter(fp, fieldnames=fields)</span><br><span class="line">    writer.writerow(dict(zip(fields, fields)))</span><br><span class="line">    data = &#123;<span class="string">'Tran_date'</span>: <span class="string">'1/2/09 6:17'</span>,</span><br><span class="line">            <span class="string">'Product'</span>: <span class="string">'Nick'</span>,</span><br><span class="line">            <span class="string">'Price'</span>: <span class="string">'1200'</span>,</span><br><span class="line">            <span class="string">'PaymentType'</span>: <span class="string">'Mastercard'</span>&#125;</span><br><span class="line">    writer.writerow(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取csv</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'csv_test.csv'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> csv.DictReader(fp):</span><br><span class="line">        <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><p><code>csv</code>使用非常简单，基本可以满足大部分需求，但是对于上百<code>MB</code>或<code>G</code>级别以上的文件处理无能为力。这种情况下，可以考虑使用<code>pandas</code>模块，它支持以下两种数据结构。</p><ul><li><code>Series</code>：是一种类似数组的带索引的一维数据结构，支持的类型与<code>NumPy</code>兼容。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pandas <span class="keyword">import</span> Series</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>obj = Series([<span class="number">1</span>, <span class="string">'a'</span>, (<span class="number">1</span>, <span class="number">2</span>), <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>obj</span><br><span class="line">a         <span class="number">1</span></span><br><span class="line">b         a</span><br><span class="line">c    (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">d         <span class="number">3</span></span><br><span class="line">dtype: object</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>obj_dic = Series(&#123;<span class="string">'Book'</span>: <span class="string">'Python'</span>, <span class="string">'Author'</span>: <span class="string">'Dan'</span>, <span class="string">'ISBN'</span>: <span class="string">'011334'</span>, <span class="string">'Price'</span>: <span class="number">25</span>&#125;, index=[<span class="string">'book'</span>, <span class="string">'Author'</span>, <span class="string">'ISBN'</span>, <span class="string">'Price'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>obj_dic</span><br><span class="line">book         NaN  <span class="comment"># 匹配失败，导致数据丢失</span></span><br><span class="line">Author       Dan</span><br><span class="line">ISBN      <span class="number">011334</span></span><br><span class="line">Price         <span class="number">25</span></span><br><span class="line">dtype: object</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>obj_dic.isnull()</span><br><span class="line">book       <span class="literal">True</span></span><br><span class="line">Author    <span class="literal">False</span></span><br><span class="line">ISBN      <span class="literal">False</span></span><br><span class="line">Price     <span class="literal">False</span></span><br><span class="line">dtype: bool</span><br></pre></td></tr></table></figure><ul><li><code>DataFrame</code>：类似于电子表格，其数据为排好序的数据列的集合，每一列都可以是不同的数据类型，类似一个二维数组，支持行和列的索引。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = &#123;<span class="string">'OrderDate'</span>: [<span class="string">'1-6-10'</span>, <span class="string">'1-23-10'</span>, <span class="string">'2-9-10'</span>, <span class="string">'2-26-10'</span>, <span class="string">'3-15-10'</span>],</span><br><span class="line"><span class="meta">... </span>        <span class="string">'Region'</span>: [<span class="string">'East'</span>, <span class="string">'Central'</span>, <span class="string">'Central'</span>, <span class="string">'West'</span>, <span class="string">'East'</span>],</span><br><span class="line"><span class="meta">... </span>        <span class="string">'Rep'</span>: [<span class="string">'Jones'</span>, <span class="string">'Kivell'</span>, <span class="string">'Jardine'</span>, <span class="string">'Gill'</span>, <span class="string">'Sorvino'</span>]&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>DataFrame(data, columns=[<span class="string">'OrderDate'</span>, <span class="string">'Region'</span>, <span class="string">'Rep'</span>])</span><br><span class="line">  OrderDate   Region      Rep</span><br><span class="line"><span class="number">0</span>    <span class="number">1</span><span class="number">-6</span><span class="number">-10</span>     East    Jones</span><br><span class="line"><span class="number">1</span>   <span class="number">1</span><span class="number">-23</span><span class="number">-10</span>  Central   Kivell</span><br><span class="line"><span class="number">2</span>    <span class="number">2</span><span class="number">-9</span><span class="number">-10</span>  Central  Jardine</span><br><span class="line"><span class="number">3</span>   <span class="number">2</span><span class="number">-26</span><span class="number">-10</span>     West     Gill</span><br><span class="line"><span class="number">4</span>   <span class="number">3</span><span class="number">-15</span><span class="number">-10</span>     East  Sorvino</span><br></pre></td></tr></table></figure><p><code>pandas</code>中处理<code>CSV</code>文件的函数主要为<code>read_csv()</code>和<code>to_csv()</code>。</p><ul><li>指定读取部分列和文件的行数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = pd.read_csv(<span class="string">'/home/projects/pythoner/quality_code/csv_test.csv'</span>, nrows=<span class="number">5</span>, usecols=[<span class="string">'Tran_date'</span>, <span class="string">'Product'</span>, <span class="string">'Price'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">     Tran_date Product  Price</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span></span><br><span class="line"><span class="number">4</span>  <span class="number">5</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span></span><br></pre></td></tr></table></figure><ul><li>设置<code>CSV</code>文件与<code>excel</code>兼容</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> csv</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dia = csv.excel()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dia.delimiter = <span class="string">","</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.read_csv(<span class="string">'/home/projects/pythoner/quality_code/csv_test.csv'</span>, dialect=dia, error_bad_lines=<span class="literal">False</span>)</span><br><span class="line">      Tran_date Product  Price PaymentType</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">2</span>   <span class="number">3</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">3</span>   <span class="number">4</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">4</span>   <span class="number">5</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">5</span>   <span class="number">6</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">6</span>   <span class="number">7</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">7</span>   <span class="number">8</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">8</span>   <span class="number">9</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">9</span>  <span class="number">10</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br></pre></td></tr></table></figure><ul><li>对文件进行分块处理并返回一个可迭代的对象</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reader = pd.read_table(<span class="string">'/home/projects/pythoner/quality_code/csv_test.csv'</span>, chunksize=<span class="number">5</span>, iterator=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>iter(reader).next()</span><br><span class="line">  Tran_date,Product,Price,PaymentType</span><br><span class="line"><span class="number">0</span>    <span class="number">1</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">1</span>    <span class="number">2</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">2</span>    <span class="number">3</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">3</span>    <span class="number">4</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">4</span>    <span class="number">5</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>iter(reader).next()</span><br><span class="line">  Tran_date,Product,Price,PaymentType</span><br><span class="line"><span class="number">5</span>    <span class="number">6</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">6</span>    <span class="number">7</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">7</span>    <span class="number">8</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">8</span>    <span class="number">9</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br><span class="line"><span class="number">9</span>   <span class="number">10</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>,Nick,<span class="number">1200</span>,Mastercard</span><br></pre></td></tr></table></figure><ul><li>当文件格式相似时，支持多个文件合并处理</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>file_list = [<span class="string">'/home/projects/pythoner/quality_code/csv_test1.csv'</span>, <span class="string">'/home/projects/pythoner/quality_code/csv_test2.csv'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dfs = [pd.read_csv(f) <span class="keyword">for</span> f <span class="keyword">in</span> file_list]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>total_df = pd.concat(dfs)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>total_df</span><br><span class="line">      Tran_date Product  Price PaymentType</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">2</span>   <span class="number">3</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">3</span>   <span class="number">4</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">4</span>   <span class="number">5</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">0</span>   <span class="number">6</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">1</span>   <span class="number">7</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">2</span>   <span class="number">8</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">3</span>   <span class="number">9</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br><span class="line"><span class="number">4</span>  <span class="number">10</span>/<span class="number">2</span>/<span class="number">09</span> <span class="number">6</span>:<span class="number">17</span>    Nick   <span class="number">1200</span>  Mastercard</span><br></pre></td></tr></table></figure><h2 id="建议43：一般情况使用ElementTree解析XML"><a href="#建议43：一般情况使用ElementTree解析XML" class="headerlink" title="建议43：一般情况使用ElementTree解析XML"></a>建议43：一般情况使用ElementTree解析XML</h2><p><code>ElementTree</code>解析<code>XML</code>具有以下特性：</p><ul><li>使用简单，将整个<code>XML</code>文件以树的形式展示，每一个元素的属性以字典的形式表示，非常方便处理；</li><li>内存上消耗明显低于<code>DOM</code>解析；</li><li>支持<code>XPath</code>查询，非常方便获取任意结点的值。</li></ul><h2 id="建议44：理解模块pickle优劣"><a href="#建议44：理解模块pickle优劣" class="headerlink" title="建议44：理解模块pickle优劣"></a>建议44：理解模块pickle优劣</h2><p>1）<code>pickle.dump(obj,file[,protocol])</code>：序列化数据到一个文件描述符。 其中：<code>protocol</code>为序列化使用的协议版本，0表示<code>ASCII</code>协议，为默认值；1表示老式的二进制协议；2表示2.3版本引入的新二进制协议。</p><p>2）<code>pickle.load()</code>：表示把文件中的对象恢复为原来的对象，这个过程也被称为<code>反序列化</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line">my_data = &#123;<span class="string">"name"</span>: <span class="string">"Python"</span>, <span class="string">"type"</span>: <span class="string">"language"</span>, <span class="string">"version"</span>: <span class="string">"2.7.6"</span>&#125;</span><br><span class="line">fp = open(<span class="string">'pickle.dat'</span>, <span class="string">'wb'</span>)</span><br><span class="line">pickle.dump(my_data, fp)</span><br><span class="line">fp.close()</span><br><span class="line">​</span><br><span class="line">fp = open(<span class="string">'pickle.dat'</span>, <span class="string">'rb'</span>)</span><br><span class="line">out = pickle.load(fp)</span><br><span class="line"><span class="keyword">print</span> out</span><br><span class="line">fp.close()</span><br></pre></td></tr></table></figure><ul><li><code>pickle</code>的优点：</li></ul><blockquote><p>1）接口简单，容易使用；<br>2）<code>pickle</code>的存储格式具有通用性，能够被不同平台的<code>Python</code>解析器共享；<br>3）支持的数据类型广泛；<br>4）<code>pickle</code>模块是可扩展的；<br>5）能够自动维护对象间的引用。</p></blockquote><ul><li><code>pickle</code>的缺点：</li></ul><blockquote><p>1）<code>pickle</code>不能保证操作的原子性；<br>2）<code>pickle</code>存在安全性问题；<br>3）<code>pickle</code>协议是<code>Python</code>特定的，不同语言之间的兼容性难以保证。</p></blockquote><h2 id="建议45：序列化的另一个不错的选择—JSON"><a href="#建议45：序列化的另一个不错的选择—JSON" class="headerlink" title="建议45：序列化的另一个不错的选择—JSON"></a>建议45：序列化的另一个不错的选择—JSON</h2><p><code>JSON</code>具有以下优势：</p><ul><li>使用简单，支持多种数据类型；仅存在两大数据结构：名称/值对的集合（对象，记录，结构，字典，散列表，键列表，关联数组等）；值的有序列表（数组，向量，列表，序列等）。</li><li>存储格式可读性好，容易修改；</li><li><code>json</code>支持跨平台跨语言操作，能够轻易被其他语言解析，存储格式较紧凑，所占空间较小；</li><li>具有较强的扩展性；</li><li><code>json</code>在序列化<code>datetime</code>时会抛出<code>TypeError</code>异常，需要对<code>json</code>本身的<code>JSONEncoder</code>进行扩展。</li></ul><h2 id="建议46：使用traceback获取栈信息"><a href="#建议46：使用traceback获取栈信息" class="headerlink" title="建议46：使用traceback获取栈信息"></a>建议46：使用traceback获取栈信息</h2><ul><li>实例</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> trackback</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">g_list = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    g_list[<span class="number">5</span>]</span><br><span class="line">    <span class="keyword">return</span> g()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> h()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">h</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">del</span> g_list[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> i()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">i</span><span class="params">()</span>:</span></span><br><span class="line">    g_list.append(<span class="string">'i'</span>)</span><br><span class="line">    <span class="keyword">print</span> g_list[<span class="number">7</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        f()</span><br><span class="line">    <span class="keyword">except</span> IndexError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Error: &#123;0&#125;'</span>.format(e)</span><br><span class="line">        traceback.print_exc()</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Error: list index out of range</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"/home/projects/pythoner/quality_code/configure_parser.py"</span>, line <span class="number">33</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    f()</span><br><span class="line">  File <span class="string">"/home/projects/pythoner/quality_code/configure_parser.py"</span>, line <span class="number">13</span>, <span class="keyword">in</span> f</span><br><span class="line">    <span class="keyword">return</span> g()</span><br><span class="line">  File <span class="string">"/home/projects/pythoner/quality_code/configure_parser.py"</span>, line <span class="number">17</span>, <span class="keyword">in</span> g</span><br><span class="line">    <span class="keyword">return</span> h()</span><br><span class="line">  File <span class="string">"/home/projects/pythoner/quality_code/configure_parser.py"</span>, line <span class="number">22</span>, <span class="keyword">in</span> h</span><br><span class="line">    <span class="keyword">return</span> i()</span><br><span class="line">  File <span class="string">"/home/projects/pythoner/quality_code/configure_parser.py"</span>, line <span class="number">27</span>, <span class="keyword">in</span> i</span><br><span class="line">    <span class="keyword">print</span> g_list[<span class="number">7</span>]</span><br><span class="line">IndexError: list index out of range</span><br></pre></td></tr></table></figure><h2 id="建议47：使用logging记录日志信息"><a href="#建议47：使用logging记录日志信息" class="headerlink" title="建议47：使用logging记录日志信息"></a>建议47：使用logging记录日志信息</h2><p>1，<strong>日志级别</strong></p><div class="table-container"><table><thead><tr><th style="text-align:left">Level</th><th style="text-align:left">使用情形</th></tr></thead><tbody><tr><td style="text-align:left">DEBUG</td><td style="text-align:left">详细的信息，在追踪问题时使用</td></tr><tr><td style="text-align:left">INFO</td><td style="text-align:left">正常的信息</td></tr><tr><td style="text-align:left">WARNING</td><td style="text-align:left">一些不可预见的问题发生，或者将要发生，如磁盘空间低等，但不影响程序的运行</td></tr><tr><td style="text-align:left">ERROR</td><td style="text-align:left">由于某些严重的问题，程序中的一些功能受到影响</td></tr><tr><td style="text-align:left">CRITICAL</td><td style="text-align:left">严重的错误，或者程序本身不能继续运行</td></tr></tbody></table></div><p>2， <strong><code>logging lib</code>的四个主要对象</strong></p><ul><li><strong><code>logger</code></strong>：程序信息输出的接口，分散在不同的代码中，使得程序可以在运行时记录相应的信息，并根据设置的日志级别或者<code>filter</code>来决定哪些信息需要输出，并将这些信息分发到其关联的<code>handler</code>。</li><li><strong><code>Handler</code></strong>：用来处理信息的输出，可以将信息输出到控制台、文件或者网络。</li><li><strong><code>Formatter</code></strong>：决定<code>log</code>信息的格式。</li><li><strong><code>Filter</code></strong>：决定哪些信息需要输出，可以被<code>handler</code>和<code>logger</code>使用，支持层次关系。</li></ul><p><code>logging.basicConfig([**kwargs])</code> 提供对日志系统的基本配置，默认使用<code>StreamHandler</code>和<code>Formatter</code>并添加到<code>root logger</code>。字典参数列表如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left">格式</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">filename</td><td style="text-align:left">指定FileHandler的文件名，而不是默认的StreamHandler</td></tr><tr><td style="text-align:left">filemode</td><td style="text-align:left">打开文件的模式，默认为‘a’</td></tr><tr><td style="text-align:left">format</td><td style="text-align:left">输出格式字符串</td></tr><tr><td style="text-align:left">datefmt</td><td style="text-align:left">日期格式</td></tr><tr><td style="text-align:left">level</td><td style="text-align:left">设置root logger的日志级别</td></tr><tr><td style="text-align:left">stream</td><td style="text-align:left">指定StreamHandler，若与filename冲突，忽略stream</td></tr></tbody></table></div><ul><li>实例</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_logger</span><span class="params">(file_name, level=logging.INFO)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    设置日志文件输出</span></span><br><span class="line"><span class="string">    :param file_name: 文件名称</span></span><br><span class="line"><span class="string">    :param level: 日志严重级别 ==&gt; CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG &gt; NOTSET</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger = logging.getLogger()</span><br><span class="line">    logger.setLevel(level)</span><br><span class="line">    file_handler = logging.FileHandler(file_name, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">    file_handler.setLevel(level)</span><br><span class="line">    formatter = logging.Formatter(<span class="string">"%(asctime)s %(name)s %(levelname)s [line %(lineno)s]: %(message)s"</span>)</span><br><span class="line">    file_handler.setFormatter(formatter)</span><br><span class="line">    logger.addHandler(file_handler)</span><br></pre></td></tr></table></figure><p>3，<strong>使用建议</strong></p><blockquote><p>1）尽量为<code>logging</code>取一个名字而不是采用默认，<code>eg</code>：<code>logger=logging.getLogger(__name__)；</code><br>2）为了方便找出问题，<code>logging</code>的名字建议以模块或者<code>class</code>命名；<br>3）<code>logging</code>只是线程安全的，不支持多进程写入同一个日志文件。</p></blockquote><h2 id="建议48：使用threading模块编写多线程程序"><a href="#建议48：使用threading模块编写多线程程序" class="headerlink" title="建议48：使用threading模块编写多线程程序"></a>建议48：使用threading模块编写多线程程序</h2><ul><li><code>Python</code>多线程支持两种方式创建：</li></ul><blockquote><p>1）通过继承<code>Thread</code>类，重写其<code>run()</code>方法(不是<code>start()</code>方法)；不支持守护线程；<br>2）创建<code>threading.Thread</code>对象,在它的初始化函数（<code>__init__()</code>）中将可调用对象作为参数传入。</p></blockquote><h2 id="建议49：使用Queue使多线程编程更加安全"><a href="#建议49：使用Queue使多线程编程更加安全" class="headerlink" title="建议49：使用Queue使多线程编程更加安全"></a>建议49：使用Queue使多线程编程更加安全</h2><p><code>Python</code>中的<code>Queue</code>模块提供了以下队列：</p><ul><li><strong><code>Queue.Queue(maxsize)</code></strong>：先进先出，<code>maxsize</code>为队列大小，其值为非正数时为无限循环队列；</li><li><strong><code>Queue.LifoQueue(maxsize)</code></strong>：后进先出，相当于栈；</li><li><p><strong><code>Queue.PriorityQueue(maxsize)</code></strong>：优先级队列。</p></li><li><p>生产-消费者模式实现<code>demo</code>：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line">WRITE_LOCK = threading.Lock()</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    生产-消费者模式（生产者）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, queue, condition, name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">        :param queue:</span></span><br><span class="line"><span class="string">        :param condition:</span></span><br><span class="line"><span class="string">        :param name:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">​</span><br><span class="line">        super(Producer, self).__init__()</span><br><span class="line">        self.queue = queue</span><br><span class="line">        self.name = name</span><br><span class="line">        self.condition = condition</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Producer &#123;0&#125; started."</span>.format(self.name)</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">​</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">global</span> WRITE_LOCK</span><br><span class="line">            self.condition.acquire()  <span class="comment"># 获取锁对象</span></span><br><span class="line">            <span class="keyword">if</span> self.queue.full():</span><br><span class="line">                <span class="keyword">with</span> WRITE_LOCK:</span><br><span class="line">                    <span class="keyword">print</span> <span class="string">'Queue is full, producer wait!'</span></span><br><span class="line">                self.condition.wait()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                value = random.randint(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line">                <span class="keyword">with</span> WRITE_LOCK:</span><br><span class="line">                    <span class="keyword">print</span> <span class="string">"&#123;name&#125; put value: &#123;value&#125; into queue."</span>\</span><br><span class="line">                        .format(name=self.name, value=value)</span><br><span class="line">                    self.queue.put(<span class="string">"&#123;0&#125;: &#123;1&#125;"</span>.format(self.name, value))</span><br><span class="line">                    self.condition.notify()</span><br><span class="line">            self.condition.release()</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Consumer</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    生产-消费者模式（消费者）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, queue, condition, name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">        :param queue:</span></span><br><span class="line"><span class="string">        :param condition:</span></span><br><span class="line"><span class="string">        :param name:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">​</span><br><span class="line">        super(Consumer, self).__init__()</span><br><span class="line">        self.queue = queue</span><br><span class="line">        self.name = name</span><br><span class="line">        self.condition = condition</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Consumer &#123;0&#125; started."</span>.format(self.name)</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">​</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">global</span> WRITE_LOCK</span><br><span class="line">            self.condition.acquire()  <span class="comment"># 获取锁对象</span></span><br><span class="line">            <span class="keyword">if</span> self.queue.empty():</span><br><span class="line">                <span class="keyword">with</span> WRITE_LOCK:</span><br><span class="line">                    <span class="keyword">print</span> <span class="string">'Queue is empty, consumer wait!'</span></span><br><span class="line">                self.condition.wait()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                value = self.queue.get()</span><br><span class="line">                <span class="keyword">with</span> WRITE_LOCK:</span><br><span class="line">                    <span class="keyword">print</span> <span class="string">"&#123;name&#125; get value: &#123;value&#125; from queue."</span>\</span><br><span class="line">                        .format(name=self.name, value=value)</span><br><span class="line">                    self.condition.notify()</span><br><span class="line">            self.condition.release()</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">​</span><br><span class="line">    qe = Queue.Queue(<span class="number">10</span>)</span><br><span class="line">    con = threading.Condition()</span><br><span class="line">    producer_1 = Producer(qe, con, <span class="string">"P1"</span>)</span><br><span class="line">    producer_1.start()</span><br><span class="line">    <span class="comment"># producer_2 = Producer(qe, con, "P2")</span></span><br><span class="line">    <span class="comment"># producer_2.start()</span></span><br><span class="line">​</span><br><span class="line">    consumer_1 = Consumer(qe, con, <span class="string">"C1"</span>)</span><br><span class="line">    consumer_1.start()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《编写高质量代码-改善Python程序的91个建议》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;温馨提醒&lt;/strong&gt;：在阅读本书之前，强烈建议先仔细阅读：&lt;a href=&quot;https://legacy.py
      
    
    </summary>
    
      <category term="Python" scheme="https://zhangbc.github.io/categories/python/"/>
    
    
      <category term="Python编码规范" scheme="https://zhangbc.github.io/tags/python-coding-convention/"/>
    
  </entry>
  
  <entry>
    <title>【Python爬虫实例】Python解决521反爬方案</title>
    <link href="https://zhangbc.github.io/2019/05/05/python_anti_spider_521/"/>
    <id>https://zhangbc.github.io/2019/05/05/python_anti_spider_521/</id>
    <published>2019-05-05T15:49:14.000Z</published>
    <updated>2019-05-05T16:09:48.352Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考文献：<a href="https://github.com/xiantang/Spider/blob/master/Anti_Anti_Spider_521/pass_521.py" target="_blank" rel="noopener">https://github.com/xiantang/Spider/blob/master/Anti_Anti_Spider_521/pass_521.py</a></p></blockquote><h2 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h2><p><code>Python</code>在爬虫方面的优势，想必业界无人不知，随着互联网信息时代的的发展，<code>Python</code>爬虫日益突出的地位越来越明显，爬虫与反爬虫愈演愈烈。下面分析一例关于返回<code>HTTP</code>状态码为<code>521</code>的案例。</p><h2 id="案例准备"><a href="#案例准备" class="headerlink" title="案例准备"></a>案例准备</h2><ul><li>案例网站：<a href="https://www.yidaiyilu.gov.cn" target="_blank" rel="noopener">【中国一带一路官网】</a>， 以抓取文章<a href="https://www.yidaiyilu.gov.cn/xwzx/gnxw/87373.htm" target="_blank" rel="noopener">【“一带一路”建设成果图鉴丨陆海内外联动，湖北推动产能合作纵深推进】</a>为例，进行深度剖析。</li></ul><h2 id="案例剖析"><a href="#案例剖析" class="headerlink" title="案例剖析"></a>案例剖析</h2><p>1） 浏览器访问<a href="https://www.yidaiyilu.gov.cn/xwzx/gnxw/87373.htm" target="_blank" rel="noopener">【“一带一路”建设成果图鉴丨陆海内外联动，湖北推动产能合作纵深推进】</a>：<br><img src="/images/python_anti_spider_521_url_yidaiyilu_20190505.png" alt="URL访问"></p><p>2）写<code>ython</code>代码访问，查看<code>http(s)</code>返回状态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">USER_AGENT = UserAgent()</span><br><span class="line">ua = USER_AGENT.random</span><br><span class="line">url = <span class="string">r'https://www.yidaiyilu.gov.cn/xwzx/gnxw/87373.htm'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"www.yidaiyilu.gov.cn"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: ua</span><br><span class="line">&#125;</span><br><span class="line">rs = requests.session()</span><br><span class="line">resp = rs.get(url)</span><br><span class="line">print(resp.status_code)</span><br><span class="line">print(resp.text)</span><br></pre></td></tr></table></figure><p>不幸的是，返回的<code>http</code>的状态码却是<code>501</code>，<code>text</code>为一段混淆的<code>js</code>代码。</p><p><img src="/images/python_anti_spider_521_requests_20190505.png" alt="request_501"></p><p>3）百度查资料，推荐为文首的<a href="https://github.com/xiantang/Spider/blob/master/Anti_Anti_Spider_521/pass_521.py" target="_blank" rel="noopener">【参考文献】</a></p><p>继续参照资料修改代码，<code>Python</code>执行<code>JS</code>首选<code>execjs</code>，<code>pip</code>安装如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install PyExecJS</span><br></pre></td></tr></table></figure></p><p>将请求到的<code>js</code>执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text_521 = <span class="string">''</span>.join(re.findall(<span class="string">'&lt;script&gt;(.*?)&lt;/script&gt;'</span>, resp.text))</span><br><span class="line">func_return = text_521.replace(<span class="string">'eval'</span>, <span class="string">'return'</span>)</span><br><span class="line">content = execjs.compile(func_return)</span><br><span class="line">print(content.call(<span class="string">'f'</span>))</span><br></pre></td></tr></table></figure><p>将返回的结果<code>print</code>发现还是一段<code>JS</code>，标准格式化（<a href="https://www.html.cn/tool/js_beautify/" target="_blank" rel="noopener">【格式化<code>Javascript</code>工具】</a>），结果如下所示：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> _2i = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">        setTimeout(<span class="string">'location.href=location.pathname+location.search.replace(/[\?|&amp;]captcha-challenge/,\'\')'</span>, <span class="number">1500</span>);</span><br><span class="line">        <span class="built_in">document</span>.cookie = <span class="string">'__jsl_clearance=1557019601.296|0|'</span> + (<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">                    <span class="keyword">var</span> _2i = [([(-~[] &lt;&lt; -~[])] * (((+!+&#123;&#125;) + [(-~[] &lt;&lt; -~[])] &gt;&gt; (-~[] &lt;&lt; -~[]))) + []), (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + [<span class="number">3</span> - ~(+!+&#123;&#125;) - ~(+!+&#123;&#125;)], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + [<span class="number">5</span>], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + [~~<span class="string">''</span>], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]), [-~~~!&#123;&#125; + [~~</span><br><span class="line">                                []</span><br><span class="line">                            ] - (-~~~!&#123;&#125;)], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + [-~&#123;&#125; - ~[-~&#123;&#125; - ~&#123;&#125;]], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]), [-~(+!+&#123;&#125;)], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + ([(-~[] &lt;&lt; -~[])] * (((+!+&#123;&#125;) + [(-~[] &lt;&lt; -~[])] &gt;&gt; (-~[] &lt;&lt; -~[]))) + []), (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + (-~[-~&#123;&#125; - ~&#123;&#125;] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]), (((-~[] &lt;&lt; -~[]) &lt;&lt; (-~[] &lt;&lt; -~[])) + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]), [<span class="number">3</span> - ~(+!+&#123;&#125;) - ~(+!+&#123;&#125;)], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + (((-~[] &lt;&lt; -~[]) &lt;&lt; (-~[] &lt;&lt; -~[])) + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]), [<span class="number">5</span>],</span><br><span class="line">                            [-~&#123;&#125; - ~[-~&#123;&#125; - ~&#123;&#125;]], (-~&#123;&#125; + [] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]) + [-~(+!+&#123;&#125;)], (-~[-~&#123;&#125; - ~&#123;&#125;] + [</span><br><span class="line">                                []</span><br><span class="line">                            ][<span class="number">0</span>]), [~~<span class="string">''</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        _1d = <span class="built_in">Array</span>(_2i.length);</span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">var</span> _5 = <span class="number">0</span>; _5 &lt; _2i.length; _5++) &#123;</span><br><span class="line">                        _1d[_2i[_5]] = [<span class="string">'Bz'</span>, (-~[-~&#123;&#125; - ~&#123;&#125;] + [</span><br><span class="line">                                    []</span><br><span class="line">                                ][<span class="number">0</span>]), [&#123;&#125; + [] + [</span><br><span class="line">                                    []</span><br><span class="line">                                ][<span class="number">0</span>]][<span class="number">0</span>].charAt(-~~~!&#123;&#125;), <span class="string">'DR'</span>, ([(-~[] &lt;&lt; -~[])] / (+!<span class="regexp">/!/</span>) + [] + [</span><br><span class="line">                                    []</span><br><span class="line">                                ][<span class="number">0</span>]).charAt(-~[-~~~!&#123;&#125; - ~(-~[] - ~&#123;&#125; - ~&#123;&#125;)]) + (+[(+!+&#123;&#125;), (+!+&#123;&#125;)] + []).charAt((+!+&#123;&#125;)), [</span><br><span class="line">                                    [][</span><br><span class="line">                                        []</span><br><span class="line">                                    ] + [] + [</span><br><span class="line">                                        []</span><br><span class="line">                                    ][<span class="number">0</span>]</span><br><span class="line">                                ][<span class="number">0</span>].charAt(-~&#123;&#125; - ~[-~&#123;&#125; - ~&#123;&#125;]), <span class="string">'qM'</span>, (((-~[] &lt;&lt; -~[]) &lt;&lt; (-~[] &lt;&lt; -~[])) + [</span><br><span class="line">                                    []</span><br><span class="line">                                ][<span class="number">0</span>]) + (+[(+!+&#123;&#125;), (+!+&#123;&#125;)] + []).charAt((+!+&#123;&#125;)) + (-~&#123;&#125;</span><br><span class="line">                                    /~~<span class="string">''</span>+[]+[[]][<span class="number">0</span>]).charAt((+!<span class="regexp">/!/</span>)),<span class="string">'S'</span>,<span class="string">'g%'</span>,(((-~[]&lt;&lt;-~[])&lt;&lt;(-~[]&lt;&lt;-~[]))+[[]][<span class="number">0</span>]),<span class="string">'HxXL'</span>,[[][[]]+[]+[[]][<span class="number">0</span>]][<span class="number">0</span>].charAt(-~&#123;&#125;-~[-~&#123;&#125;-~&#123;&#125;]),<span class="string">'D'</span>,[-~(+!+&#123;&#125;)],<span class="string">'T%'</span>,<span class="string">'YW'</span>,[&#123;&#125;+[]+[[]][<span class="number">0</span>]][<span class="number">0</span>].charAt(-~~~!&#123;&#125;),<span class="string">'vw'</span>][_5]&#125;;<span class="keyword">return</span> _1d.join(<span class="string">''</span>)&#125;)()+<span class="string">';Expires=Sun, 05-May-19 02:26:41 GMT;Path=/;</span></span><br><span class="line"><span class="string">                                    '</span>&#125;;<span class="keyword">if</span>((<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="keyword">try</span>&#123;<span class="keyword">return</span> !!<span class="built_in">window</span>.addEventListener;&#125;<span class="keyword">catch</span>(e)&#123;<span class="keyword">return</span> <span class="literal">false</span>;&#125;&#125;)())&#123;<span class="built_in">document</span>.addEventListener(<span class="string">'</span></span><br><span class="line"><span class="string">                                    DOMContentLoaded '</span>,_2i,<span class="literal">false</span>)&#125;<span class="keyword">else</span>&#123;<span class="built_in">document</span>.attachEvent(<span class="string">'</span></span><br><span class="line"><span class="string">                                    onreadystatechange '</span>,_2i)&#125;</span><br></pre></td></tr></table></figure><p>4）修改与浏览器相关的代码，然后放入浏览器的<code>console</code>进行调试。</p><p><img src="/images/python_anti_spider_521_js_debug_20190505.png" alt="JS执行结果"></p><p><strong>注意</strong>，在调试过程中，不难发现，<code>js</code>变量是动态生成的。最初还嵌套有<code>document.createElement(&#39;div&#39;)</code>，<code>Python</code>的<code>execjs</code>包不支持处理这类代码，需要做相应处理。</p><p>5）综上分析，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> execjs</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YiDaiYiLuSpider</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    中国一带一路网（521反爬）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    USER_AGENT = UserAgent()</span><br><span class="line">    ua = USER_AGENT.random</span><br><span class="line">    url = <span class="string">r'https://www.yidaiyilu.gov.cn/xwzx/gnxw/87373.htm'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Host"</span>: <span class="string">"www.yidaiyilu.gov.cn"</span>,</span><br><span class="line">        <span class="string">"User-Agent"</span>: ua</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_text521</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        rs = requests.session()</span><br><span class="line">        resp = rs.get(url=cls.url, headers=cls.headers)</span><br><span class="line">        text_521 = <span class="string">''</span>.join(re.findall(<span class="string">'&lt;script&gt;(.*?)&lt;/script&gt;'</span>, resp.text))</span><br><span class="line">        cookie_id = <span class="string">'; '</span>.join([<span class="string">'='</span>.join(item) <span class="keyword">for</span> item <span class="keyword">in</span> resp.cookies.items()])</span><br><span class="line">        <span class="keyword">return</span> cookie_id, text_521</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_cookies</span><span class="params">(cls, func)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param func:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        func_return = func.replace(<span class="string">'eval'</span>, <span class="string">'return'</span>)</span><br><span class="line">        content = execjs.compile(func_return)</span><br><span class="line">        eval_func = content.call(<span class="string">'f'</span>)</span><br><span class="line">        var = str(eval_func.split(<span class="string">'='</span>)[<span class="number">0</span>]).split(<span class="string">' '</span>)[<span class="number">1</span>]</span><br><span class="line">        rex = <span class="string">r"&gt;(.*?)&lt;/a&gt;"</span></span><br><span class="line">        rex_var = re.findall(rex, eval_func)[<span class="number">0</span>]</span><br><span class="line">        mode_func = eval_func.replace(<span class="string">'document.cookie='</span>, <span class="string">'return '</span>).replace(<span class="string">';if((function()&#123;try&#123;return !!window.addEventListener;&#125;'</span>, <span class="string">''</span>). \</span><br><span class="line">            replace(<span class="string">"catch(e)&#123;return false;&#125;&#125;)())&#123;document.addEventListener('DOMContentLoaded',"</span> + var + <span class="string">",false)&#125;"</span>, <span class="string">''</span>). \</span><br><span class="line">            replace(<span class="string">"else&#123;document.attachEvent('onreadystatechange',"</span> + var + <span class="string">")&#125;"</span>, <span class="string">''</span>).\</span><br><span class="line">            replace(<span class="string">r"setTimeout('location.href=location.pathname+location.search.replace(/[\?|&amp;]captcha-challenge/,\'\')',1500);"</span>, <span class="string">''</span>).\</span><br><span class="line">            replace(<span class="string">'return return'</span>, <span class="string">'return'</span>).\</span><br><span class="line">            replace(<span class="string">"document.createElement('div')"</span>, <span class="string">'"https://www.yidaiyilu.gov.cn/"'</span>).\</span><br><span class="line">            replace(<span class="string">r"&#123;0&#125;.innerHTML='&lt;a href=\'/\'&gt;&#123;1&#125;&lt;/a&gt;';&#123;0&#125;=&#123;0&#125;.firstChild.href;"</span>.format(var, rex_var), <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">        content = execjs.compile(mode_func)</span><br><span class="line">        cookies_js = content.call(var)</span><br><span class="line">        __jsl_clearance = cookies_js.split(<span class="string">';'</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> __jsl_clearance</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawler</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        url = <span class="string">r'https://www.yidaiyilu.gov.cn/zchj/sbwj/87255.htm'</span></span><br><span class="line">        cookie_id, text_521 = cls.get_text521()</span><br><span class="line">        __jsl_clearance = cls.generate_cookies(text_521)</span><br><span class="line">        cookies = <span class="string">"&#123;0&#125;;&#123;1&#125;;"</span>.format(cookie_id, __jsl_clearance)</span><br><span class="line">        cls.headers[<span class="string">"Cookie"</span>] = cookies</span><br><span class="line">        print(cls.headers)</span><br><span class="line">        res = requests.get(url=url, headers=cls.headers)</span><br><span class="line">        res.encoding = <span class="string">'utf-8'</span></span><br><span class="line">        print(res.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    YiDaiYiLuSpider.crawler()</span><br></pre></td></tr></table></figure><p>运行结果如下：<br><img src="/images/python_anti_spider_521_js_result_20190505.png" alt="运行结果"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;参考文献：&lt;a href=&quot;https://github.com/xiantang/Spider/blob/master/Anti_Anti_Spider_521/pass_521.py&quot; target=&quot;_blank&quot; rel=&quot;noopener
      
    
    </summary>
    
      <category term="Python" scheme="https://zhangbc.github.io/categories/python/"/>
    
    
      <category term="Python爬虫实例" scheme="https://zhangbc.github.io/tags/python-crawler/"/>
    
  </entry>
  
</feed>
