<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>天堂的鸽子</title>
  
  <subtitle>天道酬勤</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangbc.github.io/"/>
  <updated>2020-05-15T04:25:13.985Z</updated>
  <id>https://zhangbc.github.io/</id>
  
  <author>
    <name>Bocheng Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【Leetcode刷题】560. 和为K的子数组</title>
    <link href="https://zhangbc.github.io/2020/05/15/leetcode-560/"/>
    <id>https://zhangbc.github.io/2020/05/15/leetcode-560/</id>
    <published>2020-05-15T03:34:47.000Z</published>
    <updated>2020-05-15T04:25:13.985Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://leetcode-cn.com/problems/subarray-sum-equals-k/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/subarray-sum-equals-k/</a></p></blockquote><h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><blockquote><p>给定一个整数数组和一个整数 $k$，你需要找到该数组中和为 $k$ 的连续的子数组的个数。</p></blockquote><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><blockquote><p>方法一：此题最容易想到的暴力解法即枚举法，从数组的第一个元素开始，累加求和 <code>sum</code> 直到数组的最后一个元素结束（数组是无序，需要求的是连续的子数组，千万不能满足找到了第一个子数组就跳出循环，这是很容易忽略的地方），用一个整型变量<code>counts</code> 记录 <code>sum == k</code> 的个数，然后对第 <code>2～n</code> 做同样的处理，最后把每次循环后得到的 <code>counts</code> 累加便是本题的答案。注意此方法解题需要注意的几个特例（需要处理的细节问题）：</p><blockquote><p>1）子数组可能只含一个元素，如 <code>[1,1,3] 3</code>；</p><p>2）子数组可能就是整个数组，如 <code>[28,54,7,-70,22,65,-6] 100</code>；</p><p>3）以某个元素开始的满足条件的子数组可能不止一个，如 <code>[0,0,0,0,0,0,0,0,0,0] 0</code>。</p></blockquote><p>方法二：前缀和+哈希优化法，此方法最不容易想到。对方法一进行优化分析，定义一个新的整型数组 <code>pre</code> ，用 <code>pre[i]</code> 记录数组的前 <code>i</code> 项和，容易得到递推公式：</p><script type="math/tex; mode=display">pre[i] = pre[i-1]+nums[i]</script><p>那么在 <code>[j ... i]</code> 中，和为 <code>k</code> 为的条件可表示为：</p><script type="math/tex; mode=display">pre[i]-pre[j-1]==k</script><p>所以考虑以 <code>i</code> 结尾的和为 <code>k</code> 的连续子数组个数时只要统计有多少个前缀和为 <code>pre[i]-k</code> 的 <code>pre[j]</code> 即可。为了简化 <code>pre</code> 操作可以用 <code>hashMap</code> 存储，定义为 <code>map</code>，<code>key</code> 为和值 <code>pre[i]</code>，<code>value</code> 为对应 <code>pre[i]</code> 出现的次数，从左往右边更新 <code>map</code> 边计算答案，那么以 <code>i</code> 结尾的答案 <code>map[pre[i]−k]</code> 即可在 $O(1)$ 时间内得到，最终答案即为所有下标结尾的和为 <code>k</code> 的子数组个数之和。</p></blockquote><h1 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h1><ul><li>方法一：暴力枚举法，时间复杂度为 $O(n^2)$，空间复杂度为 $O(1)$。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">subarraySum</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums == <span class="keyword">null</span> || nums.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            count += getSums(nums, i, k);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">getSums</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> i, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> counts = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> sum = nums[i];</span><br><span class="line">        <span class="keyword">if</span> (sum == k) &#123;</span><br><span class="line">            counts++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (i &lt; nums.length - <span class="number">1</span>) &#123;</span><br><span class="line">            sum += nums[++i];</span><br><span class="line">            <span class="keyword">if</span> (sum == k) &#123;</span><br><span class="line">                counts++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> counts;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="/images/leetcode_20200515012859.png" alt="暴力枚举法"></p><ul><li>方法二：前缀和+哈希优化，时间复杂度为 $O(n)$，空间复杂度为 $O(n)$。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">subarraySum</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>, pre = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums == <span class="keyword">null</span> || nums.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;Integer, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">16</span>);</span><br><span class="line">        map.put(<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            pre += nums[i];</span><br><span class="line">            <span class="keyword">if</span> (map.containsKey(pre - k)) &#123;</span><br><span class="line">                count += map.get(pre - k);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            map.put(pre, map.getOrDefault(pre, <span class="number">0</span>) + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="/images/leetcode_20200515102934.png" alt="前缀和哈希优化"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/subarray-sum-equals-k/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://leetcode-cn.com/pro
      
    
    </summary>
    
      <category term="Java" scheme="https://zhangbc.github.io/categories/java/"/>
    
    
      <category term="leetcode" scheme="https://zhangbc.github.io/tags/leetcode/"/>
    
      <category term="数据结构与算法" scheme="https://zhangbc.github.io/tags/data-structure-and-algorithms/"/>
    
  </entry>
  
  <entry>
    <title>【Leetcode刷题】9.回文数</title>
    <link href="https://zhangbc.github.io/2020/05/14/leetcode-09/"/>
    <id>https://zhangbc.github.io/2020/05/14/leetcode-09/</id>
    <published>2020-05-14T05:59:59.000Z</published>
    <updated>2020-05-15T04:25:38.448Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原题链接：<a href="https://leetcode-cn.com/problems/palindrome-number/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/palindrome-number/</a></p></blockquote><h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><blockquote><p>判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数。</p></blockquote><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><blockquote><p>由于整数的特殊性，如果为负数，则易知不是回文数（因为整数的末尾不可能出现符号）。</p><p>方法一：此题最容易想到的就是把数字转成字符串 <code>str</code>，然后用双指针法(<code>low</code>，<code>high</code>)进行首尾遍历，当 <code>str[low] != str[high]</code> 说明不是回文；否则进行下一轮，<code>low++，high--</code> ，直到<code>low &gt;= high</code> 循环结束。</p><p>方法二：方法一是常规的字符串解法，没有充分利用本题的整数这一条件，回文数是指整数的前部分（负数除外）和后半部分形成对称，也就是说只要将整数拆成位数相等（如果整数的奇数位去中间位）的两部分并且将后半部分翻转与前半部分相比，如相等则说明是回文数，否则不是。需要处理的问题有：1）整数翻转，定义一个新的整型变量 <code>reverse</code>，循环每次取 <code>x</code> 的个位数，然后 <code>reverse = reverse * 10 + x % 10</code>，同时需要将 <code>x = x / 10</code>，这样可以将<code>x</code>进行翻转；2）拆分整数<code>x</code>为两部分，本题只需要判断前半部分和翻转后的后半分部分是否相等即可，所以循环的终止条件是 <code>x &lt;= reverse</code>，当整数 <code>x</code> 为偶数位时两数相等；当整数 <code>x</code> 为奇数位时 <code>x &lt; reverse</code>，因为多出的中间位被添加到 <code>reverse</code> 的末尾，所以最终判断是否为回文数的条件应该为 <code>x == reverse || x == reverse / 10</code> ；3）注意特殊情况，如果 <code>x</code> 的个位数为0，那么易知整数不是回文数，2）中的最终判断条件会判定 <code>x = 10</code> 为 <code>true</code> ，这是最容易忽略的一点。</p></blockquote><h1 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h1><ul><li>方法一：转字符串法，时间复杂度为 $O(n/2)$，空间复杂度为 $O(1)$ 。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        String value = String.valueOf(x);</span><br><span class="line">        <span class="keyword">int</span> low = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> high = value.length() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (low &lt; high) &#123;</span><br><span class="line">            <span class="keyword">if</span> (value.charAt(low) != value.charAt(high)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            low++;</span><br><span class="line">            high--;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="/images/leetcode_20200514125544.png" alt="转字符串法"></p><ul><li>方法二：反转后半部分数字，时间复杂度为 $O(log_{10}n)$，空间复杂度为 $O(1)$。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> flag = x &lt; <span class="number">0</span> || (x % <span class="number">10</span> ==<span class="number">0</span> &amp;&amp; x != <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> reverse = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (x &gt; reverse) &#123;</span><br><span class="line">            reverse = reverse * <span class="number">10</span> + x % <span class="number">10</span>;</span><br><span class="line">            x /= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x == reverse || x == reverse / <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="/images/leetcode_20200514121403.png" alt="反转后半部分数字"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;原题链接：&lt;a href=&quot;https://leetcode-cn.com/problems/palindrome-number/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://leetcode-cn.com/pr
      
    
    </summary>
    
      <category term="Java" scheme="https://zhangbc.github.io/categories/java/"/>
    
    
      <category term="leetcode" scheme="https://zhangbc.github.io/tags/leetcode/"/>
    
      <category term="数据结构与算法" scheme="https://zhangbc.github.io/tags/data-structure-and-algorithms/"/>
    
  </entry>
  
  <entry>
    <title>【Leetcode刷题】题3.数组中重复的数字</title>
    <link href="https://zhangbc.github.io/2020/05/08/leetcode-lcof-03/"/>
    <id>https://zhangbc.github.io/2020/05/08/leetcode-lcof-03/</id>
    <published>2020-05-08T15:12:08.000Z</published>
    <updated>2020-05-15T04:25:17.582Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原题链接： <a href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof" target="_blank" rel="noopener">https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof</a></p></blockquote><h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><blockquote><p>在一个长度为 $n$ 的数组 $nums$ 里的所有数字都在 $0$ ~ $n-1$ 的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复，也不知道每个数字重复几次。请找出数组中任意一个重复的数字。</p></blockquote><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><blockquote><p>通过阅读提干，不难发现，题目要求是从已知数组中找到重复元素即可。这里列出三种解题思路仅作参考：</p><blockquote><p>1）最容易想到的算法自然是先排序后通过依次遍历并与后一个元素相比较找出第一个重复元素即可，易想到但是算法代价大不可取，最优时间复杂度为 $O(nlogn)$ ；</p><p>2）利用数据结构—<code>hash</code>结构，依次遍历数组中每一个元素，同时将元素放入<code>hash</code>中，在放入前先做判断：如果在<code>hash</code>存在则说明找到重复元素，否则放入<code>hash</code>中，直到遍历完所有元素为止，其时间复杂度为 $O(n)$ ，空间复杂度为 $O(n)$ ；</p><p>3）充分利用已知条件“数组里的所有数字都在 $0$ ~ $n-1$ 的范围内”，通过交换元素位置比较可以发现是否有重复元素，为本题最优解，其时间复杂度为 $O(n)$ ：</p><blockquote><p>（1）原地改动数组，空间复杂度为 $O(1)$ ；</p><p>（2）定义新数组，空间复杂度为 $O(n)$ 。</p></blockquote></blockquote></blockquote><h1 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h1><ul><li>方法一：复制数组，时间复杂度为 $O(n)$ ，空间复杂度为 $O(n)$ 。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findRepeatNumber</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            arr[nums[i]]++;</span><br><span class="line">            <span class="keyword">if</span> (arr[nums[i]] &gt; <span class="number">1</span> ) &#123;</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="/images/leetcode_20200508234733.png" alt="复制数组"></p><ul><li>方法二：原地修改数组，时间复杂度为 $O(n)$ ，空间复杂度为 $O(1)$ 。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findRepeatNumber</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] != i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[nums[i]] == nums[i]) &#123;</span><br><span class="line">                    <span class="keyword">return</span> nums[i];</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">int</span> temp = nums[nums[i]];</span><br><span class="line">                nums[nums[i]] = nums[i];</span><br><span class="line">                nums[i] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="/images/leetcode_20200508234657.png" alt="原地修改数组"></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p>《剑指offer》(第2版)</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;原题链接： &lt;a href=&quot;https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https:/
      
    
    </summary>
    
      <category term="Java" scheme="https://zhangbc.github.io/categories/java/"/>
    
    
      <category term="leetcode" scheme="https://zhangbc.github.io/tags/leetcode/"/>
    
      <category term="数据结构与算法" scheme="https://zhangbc.github.io/tags/data-structure-and-algorithms/"/>
    
      <category term="剑指offer" scheme="https://zhangbc.github.io/tags/prove-offer/"/>
    
  </entry>
  
  <entry>
    <title>【数据库测试工具】认识Sysbench</title>
    <link href="https://zhangbc.github.io/2020/04/12/db_sysbench/"/>
    <id>https://zhangbc.github.io/2020/04/12/db_sysbench/</id>
    <published>2020-04-12T00:54:04.000Z</published>
    <updated>2020-04-12T03:59:10.376Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文基于课堂PPT笔记整理，主要介绍一下Sysbench及其简单使用，实验代码部分在代码中有重点注释，不另作说明。 </p></blockquote><h4 id="一，基准测试"><a href="#一，基准测试" class="headerlink" title="一，基准测试"></a>一，基准测试</h4><p>什么是数据库的基准测试？</p><blockquote><p>数据库的基准测试是对数据库的性能指标进行定量的、可复现的、可对比的测试。</p></blockquote><p>数据库的基准测试有何作用？</p><blockquote><p>对数据库的基准测试的作用，就是分析在当前的配置下（包括硬件配置、OS、数据库设置等），数据库的性能表现，从而找出MySQL的性能阈值，并根据实际系统的要求调整配置。</p></blockquote><p>基准测试可以理解为针对系统的一种压力测试。但基准测试不关心业务逻辑，更加简单、直接、易于测试，数据可以由工具生成，不要求真实；而压力测试一般考虑业务逻辑(如购物车业务)，要求真实的数据。</p><p>数据库的基准测试有哪些测试指标？</p><blockquote><p>1）TPS/QPS：衡量吞吐量；</p><p>2）响应时间：包括平均响应时间、最小响应时间、最大响应时间、时间百分比等，其中时间百分比参考意义较大，如前95%的请求的最大响应时间；</p><p>3）并发量：同时处理的查询请求的数量。</p></blockquote><h4 id="二，Sysbench简介"><a href="#二，Sysbench简介" class="headerlink" title="二，Sysbench简介"></a>二，Sysbench简介</h4><p>Sysbench是跨平台的基准测试工具，支持多线程，支持多种数据库；主要包括以下几种测试：</p><blockquote><p>cpu性能</p><p>磁盘io性能</p><p>调度程序性能</p><p>内存分配及传输速度</p><p>POSIX线程性能</p><p>数据库性能(OLTP基准测试)</p></blockquote><p>Sysbench基本命令一览表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">☁  ~  sysbench --<span class="built_in">help</span></span><br><span class="line">Usage:</span><br><span class="line">  sysbench [options]... [testname] [<span class="built_in">command</span>]</span><br><span class="line"></span><br><span class="line">Commands implemented by most tests: prepare run cleanup <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">General options:</span><br><span class="line">  --threads=N                     number of threads to use [1]</span><br><span class="line">  --events=N                      <span class="built_in">limit</span> <span class="keyword">for</span> total number of events [0]</span><br><span class="line">  --time=N                        <span class="built_in">limit</span> <span class="keyword">for</span> total execution time <span class="keyword">in</span> seconds [10]</span><br><span class="line">  --forced-shutdown=STRING        number of seconds to <span class="built_in">wait</span> after the --time <span class="built_in">limit</span> before forcing shutdown, or <span class="string">'off'</span> to <span class="built_in">disable</span> [off]</span><br><span class="line">  --thread-stack-size=SIZE        size of stack per thread [64K]</span><br><span class="line">  --rate=N                        average transactions rate. 0 <span class="keyword">for</span> unlimited rate [0]</span><br><span class="line">  --report-interval=N             periodically report intermediate statistics with a specified interval <span class="keyword">in</span> seconds. 0 disables intermediate reports [0]</span><br><span class="line">  --report-checkpoints=[LIST,...] dump full statistics and reset all counters at specified points <span class="keyword">in</span> time. The argument is a list of comma-separated values representing the amount of time <span class="keyword">in</span> seconds elapsed from start of <span class="built_in">test</span> when report checkpoint(s) must be performed. Report checkpoints are off by default. []</span><br><span class="line">  --debug[=on|off]                <span class="built_in">print</span> more debugging info [off]</span><br><span class="line">  --validate[=on|off]             perform validation checks <span class="built_in">where</span> possible [off]</span><br><span class="line">  --<span class="built_in">help</span>[=on|off]                 <span class="built_in">print</span> <span class="built_in">help</span> and <span class="built_in">exit</span> [off]</span><br><span class="line">  --version[=on|off]              <span class="built_in">print</span> version and <span class="built_in">exit</span> [off]</span><br><span class="line">  --config-file=FILENAME          File containing <span class="built_in">command</span> line options</span><br><span class="line">  --tx-rate=N                     deprecated <span class="built_in">alias</span> <span class="keyword">for</span> --rate [0]</span><br><span class="line">  --max-requests=N                deprecated <span class="built_in">alias</span> <span class="keyword">for</span> --events [0]</span><br><span class="line">  --max-time=N                    deprecated <span class="built_in">alias</span> <span class="keyword">for</span> --time [0]</span><br><span class="line">  --num-threads=N                 deprecated <span class="built_in">alias</span> <span class="keyword">for</span> --threads [1]</span><br><span class="line"></span><br><span class="line">Pseudo-Random Numbers Generator options:</span><br><span class="line">  --rand-type=STRING random numbers distribution &#123;uniform,gaussian,special,pareto&#125; [special]</span><br><span class="line">  --rand-spec-iter=N number of iterations used <span class="keyword">for</span> numbers generation [12]</span><br><span class="line">  --rand-spec-pct=N  percentage of values to be treated as <span class="string">'special'</span> (<span class="keyword">for</span> special distribution) [1]</span><br><span class="line">  --rand-spec-res=N  percentage of <span class="string">'special'</span> values to use (<span class="keyword">for</span> special distribution) [75]</span><br><span class="line">  --rand-seed=N      seed <span class="keyword">for</span> random number generator. When 0, the current time is used as a RNG seed. [0]</span><br><span class="line">  --rand-pareto-h=N  parameter h <span class="keyword">for</span> pareto distribution [0.2]</span><br><span class="line"></span><br><span class="line">Log options:</span><br><span class="line">  --verbosity=N verbosity level &#123;5 - debug, 0 - only critical messages&#125; [3]</span><br><span class="line"></span><br><span class="line">  --percentile=N       percentile to calculate <span class="keyword">in</span> latency statistics (1-100). Use the special value of 0 to <span class="built_in">disable</span> percentile calculations [95]</span><br><span class="line">  --histogram[=on|off] <span class="built_in">print</span> latency histogram <span class="keyword">in</span> report [off]</span><br><span class="line"></span><br><span class="line">General database options:</span><br><span class="line"></span><br><span class="line">  --db-driver=STRING  specifies database driver to use (<span class="string">'help'</span> to get list of available drivers) [mysql]</span><br><span class="line">  --db-ps-mode=STRING prepared statements usage mode &#123;auto, <span class="built_in">disable</span>&#125; [auto]</span><br><span class="line">  --db-debug[=on|off] <span class="built_in">print</span> database-specific debug information [off]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Compiled-in database drivers:</span><br><span class="line">  mysql - MySQL driver</span><br><span class="line"></span><br><span class="line">mysql options:</span><br><span class="line">  --mysql-host=[LIST,...]          MySQL server host [localhost]</span><br><span class="line">  --mysql-port=[LIST,...]          MySQL server port [3306]</span><br><span class="line">  --mysql-socket=[LIST,...]        MySQL socket</span><br><span class="line">  --mysql-user=STRING              MySQL user [sbtest]</span><br><span class="line">  --mysql-password=STRING          MySQL password []</span><br><span class="line">  --mysql-db=STRING                MySQL database name [sbtest]</span><br><span class="line">  --mysql-ssl[=on|off]             use SSL connections, <span class="keyword">if</span> available <span class="keyword">in</span> the client library [off]</span><br><span class="line">  --mysql-ssl-cipher=STRING        use specific cipher <span class="keyword">for</span> SSL connections []</span><br><span class="line">  --mysql-compression[=on|off]     use compression, <span class="keyword">if</span> available <span class="keyword">in</span> the client library [off]</span><br><span class="line">  --mysql-debug[=on|off]           trace all client library calls [off]</span><br><span class="line">  --mysql-ignore-errors=[LIST,...] list of errors to ignore, or <span class="string">"all"</span> [1213,1020,1205]</span><br><span class="line">  --mysql-dry-run[=on|off]         Dry run, pretend that all MySQL client API calls are successful without executing them [off]</span><br><span class="line"></span><br><span class="line">Compiled-in tests:</span><br><span class="line">  fileio - File I/O <span class="built_in">test</span></span><br><span class="line">  cpu - CPU performance <span class="built_in">test</span></span><br><span class="line">  memory - Memory <span class="built_in">functions</span> speed <span class="built_in">test</span></span><br><span class="line">  threads - Threads subsystem performance <span class="built_in">test</span></span><br><span class="line">  mutex - Mutex performance <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">See <span class="string">'sysbench &lt;testname&gt; help'</span> <span class="keyword">for</span> a list of options <span class="keyword">for</span> each <span class="built_in">test</span>.</span><br></pre></td></tr></table></figure><h4 id="三，Mac安装使用Sysbench"><a href="#三，Mac安装使用Sysbench" class="headerlink" title="三，Mac安装使用Sysbench"></a>三，Mac安装使用Sysbench</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">☁  ~  brew install sysbench</span><br><span class="line"><span class="comment"># 添加路径</span></span><br><span class="line">☁  ~  <span class="built_in">export</span> LDFLAGS=<span class="string">"-L/usr/local/opt/mysql@5.7/lib"</span></span><br><span class="line">☁  ~  <span class="built_in">export</span> CPPFLAGS=<span class="string">"-I/usr/local/opt/mysql@5.7/include"</span></span><br><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">☁  ~  sysbench --version</span><br><span class="line"><span class="comment"># 测试自带lua脚本</span></span><br><span class="line">☁  ~  sysbench --<span class="built_in">test</span>=/usr/<span class="built_in">local</span>/Cellar/sysbench/1.0.19/share/sysbench/tests/include/oltp_legacy/oltp.lua</span><br><span class="line">WARNING: the --<span class="built_in">test</span> option is deprecated. You can pass a script name or path on the <span class="built_in">command</span> line without any options.</span><br><span class="line">sysbench 1.0.19 (using bundled LuaJIT 2.1.0-beta2)</span><br><span class="line">☁  ~  sysbench /usr/<span class="built_in">local</span>/Cellar/sysbench/1.0.19/share/sysbench/tests/include/oltp_legacy/oltp.lua</span><br><span class="line">sysbench 1.0.19 (using bundled LuaJIT 2.1.0-beta2)</span><br><span class="line"><span class="comment"># 测试cpu</span></span><br><span class="line">☁  ~  sysbench cpu --cpu-max-prime=20000 --threads=3 run</span><br><span class="line"><span class="comment"># 测试file，run是执行正式的测试，prepare是为测试提前准备数据，cleanup是在测试完成后对数据库进行清理</span></span><br><span class="line">☁  ~  sysbench fileio --threads=16 --file-total-size=1G --file-test-mode=rndrw prepare</span><br><span class="line">☁  ~  sysbench fileio --threads=16 --file-total-size=1G --file-test-mode=rndrw run</span><br><span class="line">☁  ~  sysbench fileio --threads=16 --file-total-size=1G --file-test-mode=rndrw cleanup</span><br><span class="line"><span class="comment"># 测试threads</span></span><br><span class="line">☁  ~  sysbench threads --threads=64 --thread-yields=100 --thread-locks=2 run</span><br><span class="line"><span class="comment"># 测试memory</span></span><br><span class="line">☁  ~  sysbench memory --memory-block-size=8k --memory-total-size=40G run</span><br><span class="line"><span class="comment"># OLTP测试</span></span><br><span class="line">☁  ~  sysbench /usr/<span class="built_in">local</span>/Cellar/sysbench/1.0.19/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-table-engine=innodb --oltp-tables-count=3 --oltp-table-size=100000 --mysql-user=root --mysql-password=xxxxxxx --mysql-socket=/tmp/mysql.sock prepare</span><br><span class="line">☁  ~  sysbench /usr/<span class="built_in">local</span>/Cellar/sysbench/1.0.19/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-table-engine=innodb --oltp-tables-count=3 --oltp-table-size=100000 --mysql-user=root --mysql-password=xxxxxx --mysql-socket=/tmp/mysql.sock --max-time=60 --max-requests=0 --num-threads=8 --report-interval=10 run</span><br><span class="line">WARNING: --num-threads is deprecated, use --threads instead</span><br><span class="line">WARNING: --max-time is deprecated, use --time instead</span><br><span class="line">sysbench 1.0.19 (using bundled LuaJIT 2.1.0-beta2)</span><br><span class="line"></span><br><span class="line">Running the <span class="built_in">test</span> with following options:</span><br><span class="line">Number of threads: 8</span><br><span class="line">Report intermediate results every 10 second(s)</span><br><span class="line">Initializing random number generator from current time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Initializing worker threads...</span><br><span class="line"></span><br><span class="line">Threads started!</span><br><span class="line"></span><br><span class="line">[ 10s ] thds: 8 tps: 658.76 qps: 13182.94 (r/w/o: 9229.30/2635.43/1318.21) lat (ms,95%): 26.68 err/s: 0.00 reconn/s: 0.00</span><br><span class="line">[ 20s ] thds: 8 tps: 717.41 qps: 14350.05 (r/w/o: 10045.00/2870.13/1434.91) lat (ms,95%): 22.69 err/s: 0.00 reconn/s: 0.00</span><br><span class="line">[ 30s ] thds: 8 tps: 745.71 qps: 14914.47 (r/w/o: 10439.89/2983.15/1491.43) lat (ms,95%): 21.11 err/s: 0.00 reconn/s: 0.00</span><br><span class="line">[ 40s ] thds: 8 tps: 754.39 qps: 15085.46 (r/w/o: 10560.23/3016.45/1508.78) lat (ms,95%): 21.11 err/s: 0.00 reconn/s: 0.00</span><br><span class="line">[ 50s ] thds: 8 tps: 687.47 qps: 13749.32 (r/w/o: 9624.39/2749.98/1374.94) lat (ms,95%): 24.83 err/s: 0.00 reconn/s: 0.00</span><br><span class="line">[ 60s ] thds: 8 tps: 620.78 qps: 12417.29 (r/w/o: 8692.61/2483.12/1241.56) lat (ms,95%): 27.17 err/s: 0.00 reconn/s: 0.00</span><br><span class="line">SQL statistics:</span><br><span class="line">    queries performed:</span><br><span class="line">        <span class="built_in">read</span>:                            585984</span><br><span class="line">        write:                           167424</span><br><span class="line">        other:                           83712</span><br><span class="line">        total:                           837120</span><br><span class="line">    transactions:                        41856  (697.33 per sec.)</span><br><span class="line">    queries:                             837120 (13946.58 per sec.)</span><br><span class="line">    ignored errors:                      0      (0.00 per sec.)</span><br><span class="line">    reconnects:                          0      (0.00 per sec.)</span><br><span class="line"></span><br><span class="line">General statistics:</span><br><span class="line">    total time:                          60.0215s</span><br><span class="line">    total number of events:              41856</span><br><span class="line"></span><br><span class="line">Latency (ms):</span><br><span class="line">         min:                                    3.54</span><br><span class="line">         avg:                                   11.47</span><br><span class="line">         max:                                  138.32</span><br><span class="line">         95th percentile:                       23.95</span><br><span class="line">         sum:                               479897.13</span><br><span class="line"></span><br><span class="line">Threads fairness:</span><br><span class="line">    events (avg/stddev):           5232.0000/18.17</span><br><span class="line">    execution time (avg/stddev):   59.9871/0.01</span><br><span class="line">☁  ~  sysbench /usr/<span class="built_in">local</span>/Cellar/sysbench/1.0.19/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-table-engine=innodb --oltp-tables-count=3 --oltp-table-size=100000 --mysql-user=root --mysql-password=xxxxxxx --mysql-socket=/tmp/mysql.sock --report-interval=10 cleanup</span><br></pre></td></tr></table></figure><h4 id="四，安装并使用TPCC-MYSQL"><a href="#四，安装并使用TPCC-MYSQL" class="headerlink" title="四，安装并使用TPCC-MYSQL"></a>四，安装并使用TPCC-MYSQL</h4><p>Tpcc-mysql默认自带的数据库比较大，约10G，请实验前保证磁盘空间充足。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源码</span></span><br><span class="line">☁  tools git <span class="built_in">clone</span> https://github.com/Percona-Lab/tpcc-mysql.git</span><br><span class="line"><span class="comment"># 配置mysql_config</span></span><br><span class="line">☁  ~  sudo ln -s /usr/<span class="built_in">local</span>/opt/mysql@5.7/bin/mysql_config /usr/<span class="built_in">local</span>/bin/mysql_config</span><br><span class="line"><span class="comment"># Bulid binaries</span></span><br><span class="line">☁  tools  <span class="built_in">cd</span> tpcc-mysql/src/</span><br><span class="line">☁  src [master] ⚡  make</span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">☁  ~  mysqladmin -uroot -p create tpcc1000</span><br><span class="line">☁  ~  mysql -uroot -p  tpcc1000 &lt; /home/tools/tpcc-mysql/create_table.sql</span><br><span class="line">☁  ~  mysql -uroot -p  tpcc1000 &lt; /home/tools/tpcc-mysql/add_fkey_idx.sql</span><br><span class="line">☁  tpcc-mysql [master] ⚡  ./tpcc_load -h127.0.0.1 -d tpcc1000 -uroot -pxxxxxx -w 100</span><br><span class="line"><span class="comment"># Start benchmark</span></span><br><span class="line">☁  tpcc-mysql [master] ⚡  ./tpcc_start -h 127.0.0.1 -p 3306 -d tpcc1000 -uroot -p <span class="string">"xxxxxxx"</span> -w 100 -c 10 -r 100 -l 300 -i 20 -f /var/<span class="built_in">log</span>/tpcc_mysql.log -t /var/<span class="built_in">log</span>/tpcc_mysql.rtx</span><br><span class="line">....</span><br><span class="line">&lt;Constraint Check&gt; (all must be [OK])</span><br><span class="line"> [transaction percentage]</span><br><span class="line">        Payment: 43.48% (&gt;=43.0%) [OK]</span><br><span class="line">   Order-Status: 4.35% (&gt;= 4.0%) [OK]</span><br><span class="line">       Delivery: 4.35% (&gt;= 4.0%) [OK]</span><br><span class="line">    Stock-Level: 4.35% (&gt;= 4.0%) [OK]</span><br><span class="line"> [response time (at least 90% passed)]</span><br><span class="line">      New-Order: 0.00%  [NG] *</span><br><span class="line">        Payment: 30.49%  [NG] *</span><br><span class="line">   Order-Status: 34.91%  [NG] *</span><br><span class="line">       Delivery: 12.76%  [NG] *</span><br><span class="line">    Stock-Level: 0.00%  [NG] *</span><br><span class="line"></span><br><span class="line">&lt;TpmC&gt;</span><br><span class="line">                 4061.400 TpmC</span><br></pre></td></tr></table></figure><h4 id="五，遇到的问题及其解决方案"><a href="#五，遇到的问题及其解决方案" class="headerlink" title="五，遇到的问题及其解决方案"></a>五，遇到的问题及其解决方案</h4><p>问题描述：ld: library not found for -lrt </p><blockquote><p>解决方案：在<code>src/Makefile</code>中去掉 <code>lrt</code>，产生的错误按照提示去解决即可。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LIBS=       `mysql_config --libs_r` <span class="comment"># -lrt</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文基于课堂PPT笔记整理，主要介绍一下Sysbench及其简单使用，实验代码部分在代码中有重点注释，不另作说明。 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;一，基准测试&quot;&gt;&lt;a href=&quot;#一，基准测试&quot; class=&quot;header
      
    
    </summary>
    
      <category term="数据库技术" scheme="https://zhangbc.github.io/categories/database/"/>
    
    
      <category term="数据库实践" scheme="https://zhangbc.github.io/tags/db-practice/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】《基于深度学习的中文命名实体识别研究》阅读笔记</title>
    <link href="https://zhangbc.github.io/2020/03/01/paper_01/"/>
    <id>https://zhangbc.github.io/2020/03/01/paper_01/</id>
    <published>2020-03-01T04:29:30.000Z</published>
    <updated>2020-03-01T08:20:00.262Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者及其单位：北京邮电大学，张俊遥，2019年6月，硕士论文</p></blockquote><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>实验数据：来源于网络公开的新闻文本数据；用随机欠采样和过采样的方法解决分类不均衡问题；使用<code>BIO</code>格式的标签识别5类命名实体，标注11种标签。</p><p>学习模型：基于<code>RNN-CRF</code>框架，提出<code>Bi-GRU-Attention</code>模型；基于改进的<code>ELMo</code>可移植模型。</p><h1 id="一，绪论"><a href="#一，绪论" class="headerlink" title="一，绪论"></a>一，绪论</h1><h2 id="1，研究背景及意义"><a href="#1，研究背景及意义" class="headerlink" title="1，研究背景及意义"></a>1，研究背景及意义</h2><blockquote><p>研究背景主要介绍的是时代背景及<code>NER</code>的应用领域。</p></blockquote><h2 id="2，研究现状"><a href="#2，研究现状" class="headerlink" title="2，研究现状"></a>2，研究现状</h2><blockquote><p>1）基于规则和词典的方法；</p><p>2）基于统计的方法：语言的<code>N</code>元模型，隐马尔科夫模型，最大熵模型，条件随机场，支持向量机，决策树，基于转换的学习方法；</p><p>3）基于深度学习的方法：基于双向循环神经网络与条件随机场结合的框架；基于标签转移与窗口滑动的方法；注意力机制(<code>Attention</code>)；</p><p>4）基于迁移学习的方法。</p></blockquote><p>面临挑战：</p><blockquote><p>1）中文命名实体界限难划分；</p><p>2）中文命名实体结构更多样复杂；</p><p>3）中文命名实体分类标准不同，划分标注结果不同。</p></blockquote><h2 id="3，研究内容"><a href="#3，研究内容" class="headerlink" title="3，研究内容"></a>3，研究内容</h2><blockquote><p>1）数据集收集与预处理；</p><p>2）基于双向循环神经网络与条件随机场模型的研究；</p><p>3）基于<code>ELMo</code>的可移植模型研究。</p></blockquote><h1 id="二，相关技术"><a href="#二，相关技术" class="headerlink" title="二，相关技术"></a>二，相关技术</h1><h2 id="1，基于循环神经网络方法的技术"><a href="#1，基于循环神经网络方法的技术" class="headerlink" title="1，基于循环神经网络方法的技术"></a>1，基于循环神经网络方法的技术</h2><blockquote><p>1）神经单元结构：循环是指一个神经单元的计算是按照时间顺序展开依次进行的过程。具有记忆特征，常用来处理与序列相关的问题。</p><p>2）循环神经网络的发展：<code>LSTM</code>取代<code>CNN</code>，主要是解决<code>CNN</code>单元的反向传播的计算问题。</p><p>3）深层网络搭建：<code>Dropout</code>常被用作防止模型过拟合，减少网络冗余度，增加模型鲁棒性；批量归一化策略是批量梯度下降算法过程的一项操作；<code>clip</code>是一种有效控制梯度爆炸的算法。</p><p>4）目标函数，即损失函数，衡量经过模型计算的预测结果和事实上的结果之间的差距。如：平方差，交叉熵，<code>softmax</code>。</p><p>5）注意力机制：论文研究了在<code>LSTM</code>中引入注意力机制。</p><p>6）<code>Adam</code>优化算法：适合解决梯度稀疏或噪音较高的优化问题。</p></blockquote><h2 id="2，基于迁移学习方法的技术"><a href="#2，基于迁移学习方法的技术" class="headerlink" title="2，基于迁移学习方法的技术"></a>2，基于迁移学习方法的技术</h2><blockquote><p>1）基本思想：</p><blockquote><p>（1）预训练的两种基本思路：</p><blockquote><p>a）基于共同表示形式的思路：电子文本大多以某种向量形式（词，句，段，文本）表示输入到网络中，如<code>ELMo</code>模型。</p><p>b）基于网络微调的思想：借鉴机器视觉领域的模型思想，在预训练好的模型上加入针对任务的功能层，在对后几层进行结构和参数设置的精调。</p></blockquote></blockquote><p>2）语言模型：双向语言模型</p><p>3）词向量技术：<code>One-hot</code>向量，稀疏向量和稠密向量。</p><blockquote><p>（1）基于统计的方法</p><blockquote><p>a）基于共现矩阵的方法：在设定的窗口大小内，统计了一个句子中词语前后相邻出现的次数，使用这个次数构成的向量当作词向量，这个向量比较稀疏。</p><p>b）奇异值分解的方法：可以看作一种降维过程，把稀疏矩阵压缩为稠密矩阵的过程。</p></blockquote><p>（2）基于语言模型的方法：</p><blockquote><p>a）跳字模型（<code>skip-gram</code>）：使用一个词来预测上下文词语；</p><p>b）连续词袋模型（<code>CBOW</code>）：使用周围词语预测中心词；</p><p>c）<code>ELMo</code>模型：词向量表达过程是动态的，即一词多义下的词向量完全不同。</p></blockquote></blockquote><p>4）混淆矩阵：数据科学，数据分析和机器学习中统计分类的实际结果和预测结果的表格表示。</p></blockquote><h1 id="三，命名实体识别任务与数据集"><a href="#三，命名实体识别任务与数据集" class="headerlink" title="三，命名实体识别任务与数据集"></a>三，命名实体识别任务与数据集</h1><h2 id="1，命名实体识别任务"><a href="#1，命名实体识别任务" class="headerlink" title="1，命名实体识别任务"></a>1，命名实体识别任务</h2><blockquote><p>1）定义：命名实体识别属于序列标注类问题，分为三大类（实体类，数量类，时间类），七小类（人名，地名，组织名，日期，时间，货币或者百分比）。</p><p>2）任务过程：准确划分出命名实体的边界，并将命名实体进行正确的分类。</p><p>3）判别标准：（1）准确划分出命名实体的边界；（2）命名实体的标注分类正确；（3）命名实体内部位置标注有序。</p><script type="math/tex; mode=display">准确率=\frac{标注结果正确的数量}{标注结果的数量}\times{100\%} \\召回率=\frac{标注命名实体正确的数量}{标注命名实体的数量}\times{100\%}\\F_1=\frac{(\beta^{2}+1)\times 准确率\times 召回率}{(\beta^{2}\times 准确率) + 召回率}\times{100\%}</script></blockquote><h2 id="2，数据集收集与处理"><a href="#2，数据集收集与处理" class="headerlink" title="2，数据集收集与处理"></a>2，数据集收集与处理</h2><blockquote><p>1）数据源：本论文数据来源于搜狗实验室公开的2012年6月到7月期间的国内外国际、体育、社会、娱乐等18类新闻文本。</p><p>2）数据处理：<code>jieba</code>+盘古工具，本文研究<code>NER</code>分为五类：人名（58136），地名（87412），机构名（5142），时间（75491），数量（148392）。数据集（句子个数）分：训练集（197828），验证集（8994），测试集（3485）。</p></blockquote><h1 id="四，基于改进的神经网络与注意力机制结合的研究"><a href="#四，基于改进的神经网络与注意力机制结合的研究" class="headerlink" title="四，基于改进的神经网络与注意力机制结合的研究"></a>四，基于改进的神经网络与注意力机制结合的研究</h1><h2 id="1，RNN-CRF框架"><a href="#1，RNN-CRF框架" class="headerlink" title="1，RNN-CRF框架"></a>1，RNN-CRF框架</h2><blockquote><p>1）框架结构：以<code>Bi-LSTM-CRF</code>模型为例，包括字嵌入层（字量化表示，输入到神经网络），<code>Bi-LSTM</code>神经网络层（双向网络记录了上下文信息，据此共同训练计算当前的字的新向量表示，其输出字或词的向量维度与神经单元数量有关），<code>CRF</code>层（进行进一步标签顺序的规则学习）。</p><p>2）模型原理：将输入的语句转换为词向量，然后输入到<code>LSTM</code>网络计算，接着在<code>CRF</code>层中计算输出标签，根据定义的目标函数计算损失，使用梯度下降等算法更新模型中的参数。</p></blockquote><h2 id="2，改进与设计"><a href="#2，改进与设计" class="headerlink" title="2，改进与设计"></a>2，改进与设计</h2><blockquote><p>1）改进的思想与结构设计：改进思路就是简化神经单元结构，本文使用双向的<code>GRU</code>结构代替<code>LSTM</code>单元结构，使用神经网络与注意力机制结合。</p><p>2）改进的模型设计</p></blockquote><h2 id="3，实验与分析"><a href="#3，实验与分析" class="headerlink" title="3，实验与分析"></a>3，实验与分析</h2><blockquote><p>1）实验思路是以<code>Bi-LSTM-CRF</code>为基础，并进行网络优化，对比本文提出的<code>Bi-GRU-Attention</code>模型。</p></blockquote><p>实验一：<code>Bi-LSTM</code>网络参数</p><div class="table-container"><table><thead><tr><th>参数名称</th><th>数值</th></tr></thead><tbody><tr><td>batch_size</td><td>20</td></tr><tr><td>max_num_steps</td><td>20</td></tr><tr><td>优化器</td><td>Admin</td></tr><tr><td>初始学习率</td><td>0.001</td></tr><tr><td>衰减率</td><td>0.8</td></tr><tr><td>clip</td><td>5</td></tr><tr><td>one-hot_dim</td><td>11</td></tr></tbody></table></div><p>实验二：<code>GRU-Attention</code>模型实验参数</p><div class="table-container"><table><thead><tr><th>参数</th><th>数值</th></tr></thead><tbody><tr><td>batch_size</td><td>20</td></tr><tr><td>char_dim</td><td>100</td></tr><tr><td>max_num_steps</td><td>20</td></tr><tr><td>神经单元数</td><td>128</td></tr><tr><td>优化器</td><td>Adam</td></tr><tr><td>初始学习率</td><td>0.001</td></tr><tr><td>衰减率</td><td>0.8</td></tr><tr><td>one-hot_dim</td><td>11</td></tr><tr><td>epoch</td><td>100</td></tr></tbody></table></div><p>实验结果如下：</p><div class="table-container"><table><thead><tr><th>分类/F1/模型</th><th>Bi-LSTM-CRF</th><th>Bi-LSTM-Attention</th><th>Bi-GRU-CRF</th><th>Bi-GRU-Attention</th></tr></thead><tbody><tr><td>人名</td><td>82.32%</td><td>82.45%</td><td>82.22%</td><td>82.42%</td></tr><tr><td>地名</td><td>89.97%</td><td>90.19%</td><td>89.93%</td><td>91.06%</td></tr><tr><td>机构名</td><td>91.94%</td><td>91.96%</td><td>91.94%</td><td>91.95%</td></tr><tr><td>数量</td><td>94.98%</td><td>95.06%</td><td>95.01%</td><td>95.26%</td></tr><tr><td>时间</td><td>96.05%</td><td>96.14%</td><td>96.06%</td><td>96.14%</td></tr></tbody></table></div><h1 id="五，基于ELMo的可移植模型研究"><a href="#五，基于ELMo的可移植模型研究" class="headerlink" title="五，基于ELMo的可移植模型研究"></a>五，基于ELMo的可移植模型研究</h1><h2 id="1，改进的ELMo模型设计"><a href="#1，改进的ELMo模型设计" class="headerlink" title="1，改进的ELMo模型设计"></a>1，改进的<code>ELMo</code>模型设计</h2><blockquote><p><code>ELMo</code>模型在2018年由<code>Peter</code>提出，<code>Peter</code>团队使用双层的循环神经网络实现模型的预先训练。本章基于<code>Peter</code>的<code>ELMo</code>模型设计，提出了直通结构，实现词向量的提前训练模型。</p><p>1）模型原理：<code>Peters</code>使用<code>CNN-BIG-LSTM</code>网络实现模型，使用卷积神经网络实现字符编码，使用两层双向循环神经网络实现词向量的训练模型。</p><p>2）改进与设计：本文使用改进的<code>ELMo</code>预先训练模型包含输入层，卷积神经网络7层，双向神经网络2层，输出层结构。</p></blockquote><h2 id="2，基于ELMo的嵌入式模型设计"><a href="#2，基于ELMo的嵌入式模型设计" class="headerlink" title="2，基于ELMo的嵌入式模型设计"></a>2，基于<code>ELMo</code>的嵌入式模型设计</h2><blockquote><p>1）连接结构：在模型嵌入的衔接层中，本文使用维度映射的方法，将不同维度的输入输出维度进行统一。</p><p>2）模型设计：本文的嵌入<code>ELMo</code>模型，包含<code>ELMo</code>层，衔接层，神经网络层，注意力层和输出调整层结构。</p></blockquote><h2 id="3，实验"><a href="#3，实验" class="headerlink" title="3，实验"></a>3，实验</h2><p>实验参数配置如下：</p><p>1）<code>ELMo</code>模型实验参数</p><div class="table-container"><table><thead><tr><th>参数名称</th><th>数值</th></tr></thead><tbody><tr><td>word_dim</td><td>100</td></tr><tr><td>char_dim</td><td>50</td></tr><tr><td>activation</td><td>ReLU</td></tr><tr><td>每层神经单元数目</td><td>512</td></tr><tr><td>优化器</td><td>Adam</td></tr><tr><td>初始学习率</td><td>0.001</td></tr><tr><td>lr_decay</td><td>0.8</td></tr><tr><td>clip</td><td>3</td></tr></tbody></table></div><p>2）卷积神经网络参数</p><div class="table-container"><table><thead><tr><th>卷积层</th><th>输出词向量维度</th><th>过滤器个数</th></tr></thead><tbody><tr><td>conv1</td><td>32</td><td>32</td></tr><tr><td>conv2</td><td>32</td><td>32</td></tr><tr><td>conv3</td><td>64</td><td>64</td></tr><tr><td>conv4</td><td>128</td><td>128</td></tr><tr><td>conv5</td><td>256</td><td>256</td></tr><tr><td>conv6</td><td>512</td><td>512</td></tr><tr><td>conv7</td><td>1024</td><td>1024</td></tr></tbody></table></div><p>3）移植模型实验参数</p><div class="table-container"><table><thead><tr><th>参数名称</th><th>数值</th></tr></thead><tbody><tr><td>batch_size</td><td>20</td></tr><tr><td>char_dim</td><td>100</td></tr><tr><td>max_num_steps</td><td>20</td></tr><tr><td>神经单元数</td><td>128</td></tr><tr><td>优化器</td><td>Adam</td></tr><tr><td>初始化学习率</td><td>0.001</td></tr><tr><td>clip</td><td>5</td></tr><tr><td>dropout</td><td>0.1</td></tr><tr><td>one-hot_dim</td><td>11</td></tr><tr><td>epoch</td><td>100</td></tr></tbody></table></div><p>实验结果对比：</p><div class="table-container"><table><thead><tr><th>分类/F1/模型</th><th>Bi-LSTM-CRF</th><th>Bi-GRU-Attention</th><th>改进的ELMo嵌入模型</th></tr></thead><tbody><tr><td>人名</td><td>82.32%</td><td>82.42%</td><td>83.14%</td></tr><tr><td>地名</td><td>89.97%</td><td>91.06%</td><td>92.36%</td></tr><tr><td>机构名</td><td>91.94%</td><td>91.95%</td><td>93.02%</td></tr><tr><td>数量</td><td>94.98%</td><td>95.26%</td><td>96.13%</td></tr><tr><td>时间</td><td>96.05%</td><td>96.14%</td><td>96.55%</td></tr></tbody></table></div><h1 id="六，总结与展望"><a href="#六，总结与展望" class="headerlink" title="六，总结与展望"></a>六，总结与展望</h1><h2 id="1，总结"><a href="#1，总结" class="headerlink" title="1，总结"></a>1，总结</h2><blockquote><p>本文主要研究了基于深度学习的中文命名实体识别任务，提出了<code>Bi-GRU-Attention</code>模型减少训练时间，提升模型准确率；提出了基于改进的<code>ELMo</code>可移植模型，用于应对少量数据集和快速移植不同场景的问题。</p></blockquote><h2 id="2，不足与发展趋势"><a href="#2，不足与发展趋势" class="headerlink" title="2，不足与发展趋势"></a>2，不足与发展趋势</h2><blockquote><p>1）公开的权威的中文文本数据集不足；</p><p>2）可以划分更细的领域或分类，分别涉及分类器；</p><p>3）基于迁移学习的多任务模型研究是热点。</p></blockquote><p><strong>阅读心得</strong>：绪论内容相对详细，结构中规中矩，美中不足缺乏对研究对象现状的介绍，国内外研究现状，要解决的问题以及达到的预期效果未尽阐述。技术要点论述详尽，本文设计实验充分且多角度论证，扩展实验与改进设计也具有一定创新性。通过本篇论文研究学习，在<code>NER</code>领域收获颇多，很多知识有待弥补，如<code>ELMo</code>模型，迁移学习方面需要加强学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;作者及其单位：北京邮电大学，张俊遥，2019年6月，硕士论文&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;实验数据：
      
    
    </summary>
    
      <category term="科研笔记" scheme="https://zhangbc.github.io/categories/research-mote/"/>
    
    
      <category term="paper" scheme="https://zhangbc.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>【大数据基础】Hadoop入门--HDFS</title>
    <link href="https://zhangbc.github.io/2019/11/27/Hadoop_02/"/>
    <id>https://zhangbc.github.io/2019/11/27/Hadoop_02/</id>
    <published>2019-11-27T06:16:49.000Z</published>
    <updated>2020-03-01T05:42:13.314Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《Hadoop大数据处理基础与实践》的读书笔记。</p></blockquote><h1 id="一，Hadoop-概述"><a href="#一，Hadoop-概述" class="headerlink" title="一，Hadoop 概述"></a>一，Hadoop 概述</h1><h2 id="1，Hadoop-来源和动机"><a href="#1，Hadoop-来源和动机" class="headerlink" title="1，Hadoop 来源和动机"></a>1，<code>Hadoop</code> 来源和动机</h2><p><code>Hadoop</code> 用 <code>Java</code> 语言开发，是对 <code>Google</code> 的 <code>MapReduce</code>、<code>GFS</code>(<code>Google File SZstem</code>)和 <code>Bigtable</code> 等核心技术的开源实现，由 <code>Apache</code> 软件基会支持，是以 <code>Hadoop</code> 分布式文件系统 (<code>Hadoop Distributed File System,HDFS</code>)和 <code>MapReduce</code>(<code>Google MapReduce</code>)为核心,以及 一些支持 <code>Hadoop</code> 的其他子项目的通用工具组成的分布式计算系统，主要用于海量数据(大于 1<code>TB</code>)高效的存储、管理和分析。</p><p><img src="/images/hadoop_20191112144932.png" alt="Hadoop Logo"></p><p><code>Hadoop</code> 最起源于 <code>Nutch</code>。<code>Nutch</code> 是基于 <code>Java</code> 实现的开源搜索引擎，2002年由 <code>Doug Cutting</code> 领衔的 <code>Yahoo</code> 团队开发。2003年，<code>Google</code> 在 <code>SOSP</code>(操作系统原理会议)上发表了有关 <code>GFS</code>(<code>Google File SZstem</code>，<code>Google</code> 文件系统)分布式存储系统的论文；2004 年，<code>Google</code> 在 <code>OSDI</code>（操作系统设计与实现会议）上发表了有关 <code>MapReduce</code> 分布式处理技术的论文。</p><p><img src="/images/hadoop_20191112145704.png" alt="Hadoop发展历程"></p><p><img src="/images/hadoop_20191112145746.png" alt="Hadoop发展时序图"></p><p><code>Hadoop</code> 是基于以下思想设计的：</p><blockquote><p>（1）可以通过普通机器组成的服务器群来分发以及处理数据，这些服务器群总计可达数千个节点，使高性能服务成本极度低（<code>Economical</code>）。</p><p>（2）极度减小服务器节点ܾ失效导致的问题，不会因某个服务器节点ܾ失效导致工作不能正常进行，能实现该方式的原因是 <code>Hadoop</code> 能自动地维护数据的多份复制，并且在任务失败后能自动地重新部署计算任务，实现了工作可靠性（<code>Reliable</code>）和弹性扩容能力（<code>Scalable</code>）。</p><p>（3）能高效率（<code>Efficient</code>）地存储和处理千兆字节（<code>PB</code>）的数据，通过分发数据，<code>Hadoop</code> 可以在数据所在的节点上并行地处理它们，这使得处理非常的快速。</p><p>（4）文件不会被频繁写入和修改；机柜内的数据传输速度大于机柜间的数据传输速度； 海量数据的情况下移动计算比移动数据更高效（<code>Moving Computation is Cheaper than Moving Data</code>）。</p></blockquote><h2 id="2，Hadoop-体系架构"><a href="#2，Hadoop-体系架构" class="headerlink" title="2，Hadoop 体系架构"></a>2，<code>Hadoop</code> 体系架构</h2><p><code>Hadoop</code> 实现了对大数据进行分布式并行处理的系统框架，是一种数据并行方法。由实现数据分析的 <code>MapReduce</code> 计算框架和实现数据存储的分布式文件系统 <code>HDFS</code> 有机结合组成，它自动把应用程序分成许多小的工作单元，并把这些单元放到集群中的相应节点上执行，而分布式文件系统 <code>HDFS</code> 负责各个节点上的数据的存储，实现高吞吐率的数据读写。<code>Hadoop</code> 的基础架构如图所示。</p><p><img src="/images/hadoop_20191118224333.png" alt="Hadoop基础架构"></p><p><code>MapReduce</code> 的主要吸引力在于：它支持使用廉价的计算机集群对规模达到 <code>PB</code> 级的数据集进行分布式并行计算，是一种编程模型。它由 <code>Map</code> 函数和 <code>Reduce</code> 函数构成，分别完成任务的分解与结果的汇总。<code>MapReduce</code> 的用途是进行批量处理，而不是进行实时查查询，特别不适用于交互式应用。</p><p><code>HDFS</code> 中的数据具有“<strong>一次写，多次读</strong>”的特征，即保证一个文件在一时刻只能被一个调用者执行写操作，但可以被多个调用者执行读操作。目前，<code>Hadoop</code> 已经发展成为包含很多项目的集合，形成了一个以 <code>Hadoop</code> 为中心的生态系（<code>Hadoop Ecosystem</code>），如图所示。</p><p><img src="/images/hadoop_20191112151532.png" alt="Hadoop生态系统圈"></p><blockquote><ul><li><code>ETL Tools</code> 是构建数据仓库的重要环节，由一系列数据仓库采集工具构成。</li><li><code>BI Reporting</code>（<code>Business Intelligence Reporting</code>，商业智能报表）能提供综合报告、数据分析和数据集成等功能。</li><li><code>RDBMS</code> 是关系型数据库管理系统。<code>RDBMS</code> 中的数据存储在被称为表（<code>table</code>）的数据库中。表是相关的记录的集合，它由列和行组成，是一种二维关系表。</li><li><code>Pig</code> 是数据处理脚本，提供相应的数据流（<code>Data Flow</code>）语言和运行环境，实现数据转换（使用管道）和实验性研究（如快速原型），适用于数据准备阶段。<code>Pig</code> 运行在由 <code>Hadoop</code> 基本架构构建的集群上。</li><li><code>Hive</code> 是基于平面文件而构建的分布式数据仓库，擅长于数据展示，由 <code>Facebook</code> 贡献。<code>Hive</code> 管理存储在 <code>HDFS</code> 中的数据，提供了基于 <code>SQL</code> 的查询语言（由运行时的引擎翻译成 <code>MapReduce</code> 作业）查询数据。<code>Hive</code> 和 <code>Pig</code> 都是建立在 <code>Hadoop</code> 基本架构之上的，可以用来从数据库中提取信息，交给 <code>Hadoop</code> 处理。</li><li><code>Sqoop</code> 是数据接口，完成 <code>HDFS</code> 和关系型数据库中的数据相̈转移的工具。</li><li><code>HBase</code> 是类ͪ于 <code>Google BigTable</code> 的分布式列数据库。<code>HBase</code> 和 <code>Avro</code> 于 2010 年 5 月成为顶级 <code>Apache</code> 项目。<code>HBase</code> 支持 <code>MapReduce</code> 的并行计算和点查询（随机读取）。</li><li><code>Avro</code> 是一种新的数据序列化（<code>serialization</code>）格式和传输工具，主要用来取代 <code>Hadoop</code> 基本架构中原有的 <code>IPC</code> 机制。</li><li><code>Zookeeper</code> 用于构建分布式应用，是一种分布式锁设施，提供类ͪ 似 <code>Google Chubby</code>（主要用于解决分布式一致性问题）的功能，它是基于 <code>HBase</code> 和 <code>HDFS</code> 的，由 <code>Facebook</code> 贡献。</li><li><code>Ambari</code> 是最新加入 <code>Hadoop</code> 的项目， <code>Ambari</code> 项目旨在将监控和管理等核心功能加入 <code>Hadoop</code> 项目。<code>Ambari</code> 可帮助系统管理员部署和配置 <code>Hadoop</code>、升级集群以及监控服务。</li><li><code>Flume</code> 是 <code>Cloudera</code> 提供的一个高可用的、高可靠的、分布式的海量日志采集、聚合和传输的系统，<code>Flume</code> 支持在日志系统中定制各类数据发送方，用于收集数据；同时，<code>Flume</code> 提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</li><li><code>Mahout</code> 是机器学习和数据挖掘的一个分布式框架，区别于其他的开源数据挖掘软件，它是基于 <code>Hadoop</code> 之上的；<code>Mahout</code> 用 <code>MapReduce</code> 实现了部分数据挖掘算法，解决了并行挖掘的问题，所以 <code>Hadoop</code> 的优势就是 <code>Mahout</code> 的优势。</li></ul></blockquote><p>关系型数据库与<code>MapReduce</code>的比较如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th>关系型数据库</th><th><code>MapRedude</code></th></tr></thead><tbody><tr><td style="text-align:left">数据大小</td><td>GB</td><td>PB</td></tr><tr><td style="text-align:left">数据存取</td><td>交互式和批处理</td><td>批处理</td></tr><tr><td style="text-align:left">更新</td><td>多次读/写</td><td>一次写入，多次读取</td></tr><tr><td style="text-align:left">事务</td><td>ACID</td><td>无</td></tr><tr><td style="text-align:left">结构</td><td>写时模式</td><td>读时模式</td></tr><tr><td style="text-align:left">完整性</td><td>高</td><td>低</td></tr><tr><td style="text-align:left">横向扩展</td><td>非线性</td><td>线性</td></tr></tbody></table></div><h1 id="二，HDFS技术"><a href="#二，HDFS技术" class="headerlink" title="二，HDFS技术"></a>二，HDFS技术</h1><h2 id="1，HDFS-的特点"><a href="#1，HDFS-的特点" class="headerlink" title="1，HDFS 的特点"></a>1，<code>HDFS</code> 的特点</h2><p>1）简单一致性：对 <code>HDFS</code> 的大部分应用都是一次写入多次读（只能有一个 <code>writer</code>，但可以有多个 <code>reader</code>）， 如搜索引擎程序，一个文件写入后就不能修改了。因此写入 <code>HDFS</code> 的文件不能修改或编辑， 如果一定要进行这样的操作，只能在 <code>HDFS</code> 外修改好了再上传；</p><p>2）故障检测和自动恢复：<code>HDFS</code> 具有容错性（<code>fault-tolerant</code>），能够自动检测故障并迅速恢复，因此用户察觉不到明显的中断；</p><p>3）流式数据访问：<code>Hadoop</code> 的访问模式是一次写多次读，而读可以在不同的节点的冗余副本读，所以读数据的时间相应可以非常短，非常适合大数据读取。运行在 <code>HDFS</code> 上的程序必须是流式访问数据集，接着长时间在大数据集上进行各类分析，所以 <code>HDFS</code> 的设计旨在提高数据吞吐量，而不是用户交互型的小数据；</p><p>4）支持超大文件：由于更高的访问吞吐量，<code>HDFS</code> 支持 <code>GB</code> 级甚至 <code>TB</code> 级的文件存储，但如果存储大量小文件的话对主节点的内存影响会很大；</p><p>5）优化的读取：由于 <code>HDFS</code> 集群往往是建立在跨多个机架（<code>RACK</code>）的集群机器上的，而同一个机架节点间的网络带宽要优于不同机架数据块进行复制。</p><h2 id="2，HDFS-架构"><a href="#2，HDFS-架构" class="headerlink" title="2，HDFS 架构"></a>2，<code>HDFS</code> 架构</h2><p><code>HDFS</code> 是一个典型的主从架构，一个主节点或者说是元数据节点（<code>MetadataNode</code>）负责系统命名空间（<code>NameSpace</code>）的管理、客户端文件操作的控制和存储任务的管理分配，多个从节点或者说是数据节点（<code>DataNode</code>）提供真实文件数据的物理支持，系统架构如图所示。</p><p><img src="/images/hadoop_20191112164611.png" alt="HDFS架构图"></p><p>在 <code>HDFS</code> 上，块默认为64 <code>MB</code>。在 <code>HDFS</code> 上的文件被划分成多个64 <code>MB</code> 的大块（<code>Chunk</code>）作为独立储存单元。与单机分布式文件系统不同的是，不满一个块大小的数据不会占据整个块空间，也就是这个块空间还可以给其他数据共享。设置块大小目的是把寻址时间占所有传输数据所用的时间最小化，即增大实际传输数据的时间。</p><ul><li>检查 <code>HDFS</code> 系统上 <code>input</code> 目录下的数据块的健康状况</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /input -blocks -files</span><br></pre></td></tr></table></figure><p><code>HDFS</code> 集群有两种按照主（<code>master</code>）从（<code>slave</code>）模式划分的节点：元数据节点（<code>MetadataNode</code>） 和数据节点（<code>DataNode</code>）。</p><p>元数据节点负责管理整个集群的命名空间，并且为所有文件和目录维护了一个树状结构的元数据信息，而元数据信息被持久化到本地磁盘上分别对应了两种文件：文件系统镜像文件（<code>FsImage</code>）和编辑日志文件（<code>EditsLog</code>）。文件系统镜像文件存储所有关于命名空间的信息，编辑日志文件存储所有事务的记录。一般会配置两个目录来存储这两个文件，分别是本地磁盘和网络文件系统（<code>NFS</code>），防止元数据节点所在节点磁盘损坏后数据丢失。元数据节点在磁盘上的存储结构如下所示。</p><p><img src="/images/hadoop_20191112172402.png" alt="元数据节点在磁盘上的存储结构"></p><p>编辑日志文件会随着事务操作的增加而增大，所以需要把编辑日志文件合并到文件系统镜像文件当中去，这个操作就由<strong>辅助元数据节点</strong>（<code>Secondary MetadataNode</code>）完成。</p><p><strong>辅助元数据节点</strong>主要工作是周期性地把文件系统镜像文件与编辑日志文件合并，然后清空旧的编辑日志文件。</p><p><img src="/images/hadoop_20191118235328.png" alt="辅助元数据节点工作原理"></p><p>辅助元数据节点加载磁盘上的文件系统镜像文件和编辑日志文件，在内存中合并后成为新的文件系统镜像文件，然后写到磁盘上，这个过程称作<strong>保存点</strong>（<code>CheckPoint</code>），合并生成的文件为 <code>fsimage.ckpt</code>。</p><p>当元数据节点启动时，会将文件系统镜像载入内存，并执行编辑日志文件中的各项操作，然后开始监控 <code>RPC</code> 和 <code>HTTP</code> 请求，此时会进入到一种特殊状态，即<strong>安全模式状态</strong>（<code>Safe Mode</code>）。</p><ul><li>查看系统是否处于安全模式：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode get</span><br></pre></td></tr></table></figure><ul><li>进入安全模式：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs dfsadmin -safemode enter</span><br></pre></td></tr></table></figure><ul><li>离开安全模式：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs dfsadmin -safemode level</span><br></pre></td></tr></table></figure><ul><li>在执行某个脚本之前先退出安全模式：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs dfsadmin -safemode <span class="built_in">wait</span></span><br></pre></td></tr></table></figure><p>在 <code>HDFS</code> 中，<code>ReplicationTargetChooser</code> 类是负责实现为新分配的数据块ࠬ寻找最优存储位置的。总体说，数据块的分配工作和备份的数量、申请的客户端地址，已注册的数据服务器位置密切相关。其算法基本思路是只考量静态位置信息，优先照顾写入者的速度，让多份备份分配到不同的机架去。此外，<code>HDFS</code> 中的 <code>Balancer</code> 类是为了实现动态的负载调整而存在的。 <code>Balancer</code> 类派生于 <code>Tool</code> 类，这说明它是以一个独立的进程存在的，可以独立的运行和配置。它运行有 <code>NamenodeProtocol</code> 和 <code>ClientProtocol</code> 两个协议，与主节点进行通信，获取各个数据服务器的负载状况，从而进行调整。主要的调整其实就是一个操作，将一个数据块从一个服务器搬迁到另一个服务器上。</p><p>使用负载均衡的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop balance [-threshold&lt;threshold&gt;]  <span class="comment"># []为可选参数，默认阈值为10%，代表磁盘容量的百分比。</span></span><br></pre></td></tr></table></figure><p>对分布式文件系统而言，没有利用͈价值的数据块备份，就是<strong>垃圾</strong>。基本上， 所有的垃圾都可以视为两类：</p><blockquote><p>一类是由系统正常逻辑产生的，如一个文件被删除了，所有相关度数据块都沦为了垃圾，或一个数据块被负载均衡器移动了，原始数据块也不幸成了垃圾。</p><p>另一类垃圾是由于系统的一些异常症状产生的，如一个数据服务器停机了一段，重启之后发现其上的一个数据块已经在其他服务器上重新增加了此数据块的备份，它上面的备份因过期而ܾ失去了价值，就需要当作垃圾来处理。</p></blockquote><h2 id="3，HDFSShell命令"><a href="#3，HDFSShell命令" class="headerlink" title="3，HDFSShell命令"></a>3，<code>HDFSShell</code>命令</h2><p> <code>hdfs URI</code> 格式：<code>scheme://authority:path</code>。</p><p>其中，<code>scheme</code> 表示协议名，可以是 <code>file</code> 或 <code>HDFS</code>，前者是本地文件，后者是分布式文件；<code>authority</code> 表示集群所在的命名空间；<code>path</code> 表示文件或者目录的路径。</p><ul><li><code>hdfs dfs</code>命令大全</li></ul><p><img src="/images/hadoop_20191119100234.png" alt="HDFS文件系统命令"></p><ul><li><code>hdfs dfsadmin</code> 命令大全</li></ul><p><img src="/images/hadoop_20191119100802.png" alt="dfsadmin命令大全"></p><ul><li><code>namenode</code>命令大全</li></ul><p><img src="/images/hadoop_20191119101049.png" alt="namenode命令"></p><ul><li><code>fsck</code>命令大全</li></ul><p><img src="/images/hadoop_20191119102026.png" alt="fsck命令"></p><ul><li><code>pipes</code>命令大全</li></ul><p><img src="/images/hadoop_20191119101656.png" alt="pipes命令"></p><ul><li><code>job</code>命令大全</li></ul><p><img src="/images/hadoop_20191119101826.png" alt="job命令"></p><h2 id="4，HDFS中的Java-API的使用"><a href="#4，HDFS中的Java-API的使用" class="headerlink" title="4，HDFS中的Java API的使用"></a>4，<code>HDFS</code>中的<code>Java API</code>的使用</h2><p><code>Hadoop</code>的文件系统如下：</p><div class="table-container"><table><thead><tr><th>文件系统</th><th>URI方案</th><th>Java实现</th><th>定义</th></tr></thead><tbody><tr><td>Local</td><td>file</td><td>fs.LocalFileSystem</td><td>支持有客户端校验和本地文件系统。带有校验和的本地系统文件在<code>fs.LocalFileSystem</code>中实现</td></tr><tr><td>HDFS</td><td>hdfs</td><td>hdfs.DistributionFileSystem</td><td><code>Hadoop</code>的分布式文件系统</td></tr><tr><td>HFTP</td><td>hftp</td><td>hdfs.HftpFileSystem</td><td>支持通过<code>HTTP</code>方式以只读方式访问<code>HDFS</code>，<code>distcp</code>经常用在不同的<code>HDFS</code>集群间复制数据</td></tr><tr><td>HSFTP</td><td>hsftp</td><td>hdfs.HsftpFileSystem</td><td>支持通过<code>HTTPS</code>方式以只读方式访问<code>HDFS</code></td></tr><tr><td>HAR</td><td>har</td><td>fs.HarFileSystem</td><td>构建在<code>Hadoop</code>文件系统之上，对文件进行归档，以减少<code>NameNode</code>的内存使用</td></tr><tr><td>KFS</td><td>kfs</td><td>fs.kfs.KosmosFileSystem</td><td><code>Cloundstore</code>（前身是<code>Kosmos</code>文件系统）文件系统是类似于<code>HDFS</code>和<code>Google</code>的<code>GFS</code>文件系统，使用<code>C++</code>编写</td></tr><tr><td>FTP</td><td>ftp</td><td>fs.ftp.FtpFileSystem</td><td>由<code>FTP</code>服务器支持的文件系统</td></tr><tr><td>S3(本地)</td><td>s3n</td><td>fs.s3native.NativeS3FileSystem</td><td>基于<code>Amazon S3</code>的文件系统</td></tr><tr><td>S3(基于块)</td><td>s3</td><td>fs.s3.NativeS2FileSystem</td><td>基于<code>Amazon S3</code>的文件系统，以块格式存储解决了<code>S3</code>的<code>5GB</code>文件大小限制</td></tr></tbody></table></div><h2 id="5，RPC通信"><a href="#5，RPC通信" class="headerlink" title="5，RPC通信"></a>5，<code>RPC</code>通信</h2><p><code>RPC</code>（<code>Remote Procedure Call Protocol</code>）即远程调用协议，是一台计算机通过跨越底层网络协议（<code>TCP</code>、<code>UDP</code> 等）调用另一台计算机的子程序或者服务所遵守的协议标准。其主要特点如下：</p><blockquote><p>1）透明性：远程调用其他机器上的程序，对用户来说就像是调用本地方法一样；</p><p>2）高性能：<code>RPC server</code> 能够并发处理多个来自 <code>Client</code> 的请求；</p><p>3）可控性：<code>JDK</code> 中已经提供了一个 <code>RPC</code> 框架——<code>RMI</code>，但是该 <code>PRC</code> 框架过于重量级 并且可控之处比较少，所以 <code>Hadoop RPC</code> 实现了自定义的 <code>PRC</code> 框架。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《Hadoop大数据处理基础与实践》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，Hadoop-概述&quot;&gt;&lt;a href=&quot;#一，Hadoop-概述&quot; class=&quot;headerlink&quot; title=&quot;一，Hadoo
      
    
    </summary>
    
      <category term="大数据技术" scheme="https://zhangbc.github.io/categories/big-data/"/>
    
    
      <category term="Hadoop" scheme="https://zhangbc.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>【大数据基础】Hadoop2.2.0集群环境搭建</title>
    <link href="https://zhangbc.github.io/2019/11/14/Hadoop_01/"/>
    <id>https://zhangbc.github.io/2019/11/14/Hadoop_01/</id>
    <published>2019-11-14T12:05:21.000Z</published>
    <updated>2020-02-29T11:57:33.696Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《Hadoop大数据处理基础与实践》的读书笔记。</p></blockquote><h1 id="一，准备环境"><a href="#一，准备环境" class="headerlink" title="一，准备环境"></a>一，准备环境</h1><ul><li><code>PC</code>基本配置如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">处理器：Intel(R) Core(TM) i5-3230M CPU @ 2.6GHz 2.60GHz</span><br><span class="line">安装内存（RAM）: 12.0GB</span><br><span class="line">系统类型：64位操作系统</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop_20191114132209.PNG" alt="PC基本配置"></p><ul><li>初始化四台<code>Ubuntu-14.04_x64</code>虚拟机，配置如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">内存：2GB</span><br><span class="line">处理器：1</span><br><span class="line">硬盘：40G</span><br><span class="line">网络适配器：NAT</span><br><span class="line">系统：Linux ubuntu 4.4.0-142-generic #168~14.04.1-Ubuntu SMP Sat Jan 19 11:26:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop_20191114153212.PNG" alt="虚拟机配置"></p><ul><li>修改系统时区</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~ sudo timedatectl <span class="built_in">set</span>-timezone <span class="string">"Asia/Shanghai"</span></span><br></pre></td></tr></table></figure><ul><li>为方便使用建议如下配置：</li></ul><blockquote><p>安装<a href="https://github.com/robbyrussell/oh-my-zsh" target="_blank" rel="noopener"><code>oh-my-zsh</code></a>插件；</p><p>设置<code>VIM</code>行号；</p><p>安装<code>SSH</code>插件服务；</p><p>安装<code>vsftpd</code>插件服务并加以配置，方便文件上传下载；</p><p>在<code>PC</code>上安装<code>XSHELL</code>客户端；</p><p>在<code>PC</code>上安装<code>FTP</code>客户端。</p></blockquote><ul><li>需要的软件：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apache-tomcat-7.0.52.tar.gz  链接：http://pan.baidu.com/s/1nvjjd6T  密码：6ft0（暂不需要）</span><br><span class="line">jdk-7u51-linux-x64.gz  链接：http://pan.baidu.com/s/1dFFT1GP  密码：cc5t</span><br><span class="line">solr-4.7.0.tgz  链接：http://pan.baidu.com/s/1ge3xPLp  密码：s53p（暂不需要）</span><br><span class="line">zookeeper-3.4.5.tar.gz  链接：http://pan.baidu.com/s/1qYD5iBq  密码：zgq9（暂不需要）</span><br><span class="line">hadoop-2.2.0-x64.tar.gz https://download.csdn.net/download/wwyymmddbb/10203840</span><br><span class="line">http://down.csdnxz.top/files/csdn/2019/11/13/hadoop-2.2.0-x64.tar.gz</span><br></pre></td></tr></table></figure><ul><li>在虚拟机做如下步骤：创建目录，存储工具包<code>/home/zhangbocheng</code>，并利用<code>FTP</code>上传相关软件包。</li></ul><p><img src="/images/hadoop_20191114153217.PNG" alt="FTP"></p><p><img src="/images/hadoop_20191114153216.PNG" alt="软件包"></p><h1 id="二，安装单机环境"><a href="#二，安装单机环境" class="headerlink" title="二，安装单机环境"></a>二，安装单机环境</h1><ul><li>安装<code>Java1.7.0</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">~ mkdir java</span><br><span class="line">~ <span class="built_in">cd</span> java   </span><br><span class="line">➜  java tar -xf /home/zhangbocheng/jdk-7u51-linux-x64.gz </span><br><span class="line">➜  java ln -s jdk1.7.0_51 JDK</span><br><span class="line">➜  java vi ~/.bashrc </span><br><span class="line">➜  java vi ~/.zshrc</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="variable">$HOME</span>/java/java</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line">:wq            </span><br><span class="line">➜  java <span class="built_in">source</span> ~/.bashrc </span><br><span class="line">➜  java <span class="built_in">source</span> ~/.zshrc</span><br><span class="line">➜  java java -version </span><br><span class="line">java version <span class="string">"1.7.0_51"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_51-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop_20191114153219.PNG" alt="Java"></p><ul><li>安装<code>Hadoop2.2.0</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tar -xf /home/zhangbocheng/hadoop-2.2.0-x64.tar.gz</span><br><span class="line">➜  ~ mv hadoop-2.2.0 hadoop2.2.0    </span><br><span class="line">➜  ~ mkdir hadoop2.2.0/hdfs  </span><br><span class="line">➜  ~ mkdir hadoop2.2.0/hdfs/name</span><br><span class="line">➜  ~ mkdir hadoop2.2.0/hdfs/data</span><br><span class="line">➜  ~ mkdir hadoop2.2.0/logs     </span><br><span class="line">➜  ~ mkdir hadoop2.2.0/tmp</span><br></pre></td></tr></table></figure><ul><li>配置环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ vi .zshrc </span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="variable">$HOME</span>/java/jdk</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/tool.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">➜  ~ <span class="built_in">source</span> .zshrc </span><br><span class="line">➜  ~ <span class="built_in">echo</span> <span class="variable">$CLASSPATH</span></span><br><span class="line">/home/zhangbc/java/jdk/lib/tool.jar:/home/zhangbc/java/jdk/lib/dt.jar</span><br><span class="line">➜  ~ vi .zshrc </span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="variable">$HOME</span>/hadoop2.2.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_LOG_DIR=<span class="variable">$HADOOP_HOME</span>/logs</span><br><span class="line"><span class="built_in">export</span> YARN_LOG_DIR=<span class="variable">$HADOOP_LOG_DIR</span></span><br><span class="line">➜  ~ <span class="built_in">source</span> .zshrc </span><br><span class="line">➜  ~ hadoop version</span><br><span class="line">Hadoop 2.2.0</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2014-09-21T22:41Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From <span class="built_in">source</span> with checksum 79e53ce7994d1628b240f09af91e1af4</span><br><span class="line">This <span class="built_in">command</span> was run using /home/zhangbc/hadoop2.2.0/share/hadoop/common/hadoop-common-2.2.0.jar</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop_20191114153239.PNG" alt="Hadoop"></p><ul><li>修改<code>Hadoop2.2.0</code>配置文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查并修改以下三个文件中JAVA_HOME的值</span></span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/hadoop-env.sh </span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/yarn-env.sh </span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/mapred-env.sh <span class="comment"># 只需要去掉注释加以修改</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span> <span class="comment"># 错误</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/zhangbc/java/jdk <span class="comment"># 正确</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加集群的slave节点</span></span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/slaves </span><br><span class="line"><span class="comment"># localhost</span></span><br><span class="line">slave_1</span><br><span class="line">slave_2</span><br><span class="line">slave_3</span><br><span class="line"></span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/core-site.xml </span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000/&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;设定namenode的主机名及其端口&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/zhangbc/hadoop2.2.0/tmp/hadoop-<span class="variable">$&#123;user.name&#125;</span>&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;存储临时文件&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:50070&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;设定NameNode地址及其端口&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;slave_1:50090&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;设定SecondNameNode地址及其端口&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;设定HDFS存储文件的副本个数，默认为3&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:///home/zhangbc/hadoop2.2.0/hdfs/name&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;设置NameNode用来持续存储命名空间和交换日志的本地文件系统路径&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:///home/zhangbc/hadoop2.2.0/hdfs/data&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;设置DataNode在本地存储文件的目录列表&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:///home/zhangbc/hadoop2.2.0/hdfs/namesecondary&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;设置SecondaryNameNode存储临时镜像的本地文件系统路径，</span><br><span class="line">            若这是一个用逗号分隔的列表，则镜像会冗余复制到所有目录&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;是否允许网页浏览HDFS文件&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.stream-buffer-size&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;131072&lt;/value&gt;</span><br><span class="line">        &lt;descrption&gt;默认为4KB,作为Hadoop的缓冲区，用于Hapdoop读写HDFS的文件，</span><br><span class="line">        还有map的输出都用到了这个缓冲区容量，131072=128KB&lt;/descrption&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/mapred-site.xml </span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">➜  ~ vi hadoop2.2.0/etc/hadoop/yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8032&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8030&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8031&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8033&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>修改主机名称（<strong>千万不要含有下划线<code>_</code></strong>）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ sudo hostname master   <span class="comment"># 只对当前状态生效</span></span><br><span class="line">[sudo] password <span class="keyword">for</span> zhangbc: </span><br><span class="line">➜  ~ hostname</span><br><span class="line">master</span><br><span class="line">➜  ~ sudo vi /etc/hostname <span class="comment"># 永久修改</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">master</span><br><span class="line">:wq</span><br></pre></td></tr></table></figure><ul><li>关闭防火墙</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ service ufw status</span><br><span class="line">ufw start/running</span><br><span class="line">➜  ~ sudo service ufw stop</span><br><span class="line">[sudo] password <span class="keyword">for</span> zhangbc: </span><br><span class="line">ufw stop/waiting</span><br><span class="line">➜  ~ service ufw status   </span><br><span class="line">ufw stop/waiting</span><br></pre></td></tr></table></figure><h1 id="三，克隆VM"><a href="#三，克隆VM" class="headerlink" title="三，克隆VM"></a>三，克隆<code>VM</code></h1><p>通过 <code>VMware Workstation</code>工具，关闭当前虚拟机，对其克隆三台虚拟机作为从机使用。</p><p>克隆方法选择“<strong>创建完整克隆(F)</strong>”，如图所示：</p><p><img src="/images/hadoop_20191114153339.PNG" alt="创建完整克隆"></p><h1 id="四，搭建集群"><a href="#四，搭建集群" class="headerlink" title="四，搭建集群"></a>四，搭建集群</h1><ul><li>修改三台从机<code>slave</code>的<code>host</code>，并再重启使之生效。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ sudo vi /etc/hostname</span><br><span class="line">➜  ~ sudo vi /etc/hosts</span><br><span class="line">➜  ~ sudo reboot</span><br></pre></td></tr></table></figure><ul><li>对所有集群中的服务器进行检查，关闭防火墙并禁止掉。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ sudo service ufw status</span><br><span class="line">[sudo] password <span class="keyword">for</span> zhangbc: </span><br><span class="line">ufw start/running</span><br><span class="line">➜  ~ sudo service ufw stop  </span><br><span class="line">ufw stop/waiting</span><br><span class="line">➜  ~ sudo service ufw status</span><br><span class="line">ufw stop/waiting</span><br><span class="line">➜  ~ sudo ufw <span class="built_in">disable</span> </span><br><span class="line">Firewall stopped and disabled on system startup</span><br></pre></td></tr></table></figure><ul><li>对所有集群中的服务器绑定<code>hostname</code>与<code>IP</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ sudo vi /etc/hosts</span><br><span class="line"> 192.168.71.128  master</span><br><span class="line"> 192.168.71.129  slave_1</span><br><span class="line"> 192.168.71.130  slave_2</span><br><span class="line"> 192.168.71.131  slave_3</span><br></pre></td></tr></table></figure><ul><li>对所有集群中的服务器创建<code>SSH</code>密钥，完成相关验证，注意保留原有的其他密钥，以备他用</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  .ssh mv id_rsa id_rsa_git</span><br><span class="line">➜  .ssh mv id_rsa.pub id_rsa_git.pub</span><br><span class="line">➜  .ssh ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span><br><span class="line"><span class="comment"># 生成authorized_keys</span></span><br><span class="line">➜  .ssh cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line"><span class="comment"># 设置权限</span></span><br><span class="line">➜  .ssh sudo chmod 600 authorized_keys </span><br><span class="line"><span class="comment"># ssh登录本机，并退出</span></span><br><span class="line">➜  .ssh ssh localhost </span><br><span class="line">The authenticity of host <span class="string">'localhost (127.0.0.1)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is b6:fa:8d:2b:2d:0d:e4:fd:4f:44:ed:37:3f:79:b6:ce.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>localhost<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Welcome to Ubuntu 14.04.6 LTS (GNU/Linux 4.4.0-142-generic x86_64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> * Documentation:  https://help.ubuntu.com/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">New release '</span>16.04.6 LTS<span class="string">' available.</span></span><br><span class="line"><span class="string">Run '</span><span class="keyword">do</span>-release-upgrade<span class="string">' to upgrade to it.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Your Hardware Enablement Stack (HWE) is supported until April 2019.</span></span><br><span class="line"><span class="string">Last login: Wed Nov 13 20:17:41 2019 from 192.168.71.1</span></span><br><span class="line"><span class="string">➜  ~ exit</span></span><br><span class="line"><span class="string">Connection to localhost closed.</span></span><br><span class="line"><span class="string">➜  .ssh</span></span><br></pre></td></tr></table></figure><ul><li>配置各个节点之间免密登录</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将slave_1节点rsa通过ssh-copy-id分别复制到master，slave_2，slave_3</span></span><br><span class="line">➜  ~ ssh-copy-id -i ~/.ssh/id_rsa.pub master</span><br><span class="line">➜  ~ ssh-copy-id -i ~/.ssh/id_rsa.pub slave_2</span><br><span class="line">➜  ~ ssh-copy-id -i ~/.ssh/id_rsa.pub slave_3</span><br><span class="line"><span class="comment"># 验证登录</span></span><br><span class="line">➜  ~ ssh master</span><br><span class="line">➜  ~ ssh slave_2</span><br><span class="line">➜  ~ ssh slave_3</span><br><span class="line"><span class="comment"># 其他节点同步骤</span></span><br></pre></td></tr></table></figure><h1 id="五，Hadoop启动与测试"><a href="#五，Hadoop启动与测试" class="headerlink" title="五，Hadoop启动与测试"></a><code>五，Hadoop</code>启动与测试</h1><ul><li><p>格式化文件系统</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ hdfs namenode -format</span><br><span class="line">19/11/13 21:57:48 INFO namenode.NameNode: STARTUP_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = master/192.168.71.128</span><br><span class="line">STARTUP_MSG:   args = [-format]</span><br><span class="line">STARTUP_MSG:   version = 2.2.0</span><br><span class="line">.........</span><br><span class="line">19/11/13 21:57:55 INFO util.ExitUtil: Exiting with status 0  <span class="comment"># 表示成功</span></span><br><span class="line">.........</span><br></pre></td></tr></table></figure></li><li><p>启动<code>HDFS</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">zhangbc@master:~$ start-dfs.sh </span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">master: starting namenode, logging to /home/zhangbc/hadoop2.2.0/logs/hadoop-zhangbc-namenode-master.out</span><br><span class="line">slave_1: starting datanode, logging to /home/zhangbc/hadoop2.2.0/logs/hadoop-zhangbc-datanode-slave_1.out</span><br><span class="line">slave_3: starting datanode, logging to /home/zhangbc/hadoop2.2.0/logs/hadoop-zhangbc-datanode-slave_3.out</span><br><span class="line">slave_2: starting datanode, logging to /home/zhangbc/hadoop2.2.0/logs/hadoop-zhangbc-datanode-slave_2.out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">zhangbc@master:~$ jps</span><br><span class="line">6524 Jps</span><br><span class="line">5771 NameNode</span><br><span class="line">zhangbc@slave_1:~$ jps</span><br><span class="line">4919 Jps</span><br><span class="line">4818 DataNode</span><br><span class="line">zhangbc@slave_2:~$ jps</span><br><span class="line">4919 Jps</span><br><span class="line">4801 DataNode</span><br><span class="line">zhangbc@slave_3:~$ jps</span><br><span class="line">4705 DataNode</span><br><span class="line">4800 Jps</span><br></pre></td></tr></table></figure></li></ul><p><code>WEB</code>验证：<a href="http://192.168.71.128:50070" target="_blank" rel="noopener">http://192.168.71.128:50070</a></p><p><img src="/images/hadoop_20191114133212.PNG" alt="web_dfs"></p><ul><li>启动<code>Yarn</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">zhangbc@master:~$ start-yarn.sh </span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/zhangbc/hadoop2.2.0/logs/yarn-zhangbc-resourcemanager-master.out</span><br><span class="line">slave_2: starting nodemanager, logging to /home/zhangbc/hadoop2.2.0/logs/yarn-zhangbc-nodemanager-slave_2.out</span><br><span class="line">slave_1: starting nodemanager, logging to /home/zhangbc/hadoop2.2.0/logs/yarn-zhangbc-nodemanager-slave_1.out</span><br><span class="line">slave_3: starting nodemanager, logging to /home/zhangbc/hadoop2.2.0/logs/yarn-zhangbc-nodemanager-slave_3.out</span><br><span class="line"></span><br><span class="line">zhangbc@master:~$ jps</span><br><span class="line">5771 NameNode</span><br><span class="line">6642 Jps</span><br><span class="line">zhangbc@slave_1:~$ jps</span><br><span class="line">5099 Jps</span><br><span class="line">4818 DataNode</span><br><span class="line">5011 NodeManager</span><br><span class="line">zhangbc@slave_2:~$ jps</span><br><span class="line">5101 Jps</span><br><span class="line">5016 NodeManager</span><br><span class="line">4801 DataNode</span><br><span class="line">zhangbc@slave_2:~$ jps</span><br><span class="line">5101 Jps</span><br><span class="line">5016 NodeManager</span><br><span class="line">4801 DataNode</span><br></pre></td></tr></table></figure><p><code>WEB</code>验证：<a href="http://192.168.71.128:8088" target="_blank" rel="noopener">http://192.168.71.128:8088</a></p><p><img src="/images/hadoop_20191114133210.png" alt="web_yarn"></p><ul><li>管理<code>JobHistory Server</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zhangbc@master:~$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">starting historyserver, logging to /home/zhangbc/hadoop2.2.0/logs/mapred-zhangbc-historyserver-master.out</span><br><span class="line">zhangbc@master:~$ mr-jobhistory-daemon.sh stop historyserver</span><br></pre></td></tr></table></figure><p><code>WEB</code>验证：<a href="http://192.168.71.128:19888" target="_blank" rel="noopener">http://192.168.71.128:19888</a></p><p><img src="/images/hadoop_20191114153340.png" alt="web_job"></p><ul><li>集群验证</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">zhangbc@master:~$ hdfs dfs -mkdir -p /data/wordscount</span><br><span class="line">zhangbc@master:~$ hdfs dfs -mkdir -p /output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据目录</span></span><br><span class="line">zhangbc@master:~$ hdfs dfs -ls /data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传本地文件</span></span><br><span class="line">zhangbc@master:~$ hdfs dfs -put hadoop2.2.0/etc/hadoop/core-site.xml /data/wordscount</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">zhangbc@master:~$ hadoop jar hadoop2.2.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /data/wordscount /output/wordscount</span><br><span class="line">...............................</span><br><span class="line">19/11/14 13:04:45 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1573705994579_0001/</span><br><span class="line">19/11/14 13:04:45 INFO mapreduce.Job: Running job: job_1573705994579_0001</span><br><span class="line">19/11/14 13:04:59 INFO mapreduce.Job: Job job_1573705994579_0001 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></span><br><span class="line">19/11/14 13:04:59 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/11/14 13:05:14 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/11/14 13:05:27 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/11/14 13:05:27 INFO mapreduce.Job: Job job_1573705994579_0001 completed successfully</span><br><span class="line">19/11/14 13:05:27 INFO mapreduce.Job: Counters: 43</span><br><span class="line">............................................</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看运行结果</span></span><br><span class="line">zhangbc@master:~$ hdfs dfs -cat /output/wordscount/part-r-00000</span><br></pre></td></tr></table></figure><h1 id="六，安装过程中遇到的问题及其解决方案"><a href="#六，安装过程中遇到的问题及其解决方案" class="headerlink" title="六，安装过程中遇到的问题及其解决方案"></a>六，安装过程中遇到的问题及其解决方案</h1><ul><li>问题1：上传文件报错</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">zhangbc@master:~$ hdfs dfs -put hadoop2.2.0/etc/hadoop/core-site.xml /data/wordcount</span><br><span class="line">19/11/14 10:13:24 WARN hdfs.DFSClient: DataStreamer Exception</span><br><span class="line">org.apache.hadoop.ipc.RemoteException(java.io.IOExcept、ion): File /data/wordcount/core-site.xml._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded <span class="keyword">in</span> this operation.</span><br><span class="line">at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:555)</span><br><span class="line">at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:387)</span><br><span class="line">at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos<span class="variable">$ClientNamenodeProtocol</span><span class="variable">$2</span>.callBlockingMethod(ClientNamenodeProtocolProtos.java:59582)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Server</span><span class="variable">$ProtoBufRpcInvoker</span>.call(ProtobufRpcEngine.java:585)</span><br><span class="line">at org.apache.hadoop.ipc.RPC<span class="variable">$Server</span>.call(RPC.java:928)</span><br><span class="line">at org.apache.hadoop.ipc.Server<span class="variable">$Handler</span><span class="variable">$1</span>.run(Server.java:2048)</span><br><span class="line">at org.apache.hadoop.ipc.Server<span class="variable">$Handler</span><span class="variable">$1</span>.run(Server.java:2044)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">at org.apache.hadoop.ipc.Server<span class="variable">$Handler</span>.run(Server.java:2042)</span><br><span class="line"></span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1347)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1300)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Invoker</span>.invoke(ProtobufRpcEngine.java:206)</span><br><span class="line">at com.sun.proxy.<span class="variable">$Proxy9</span>.addBlock(Unknown Source)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:606)</span><br><span class="line">at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)</span><br><span class="line">at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</span><br><span class="line">at com.sun.proxy.<span class="variable">$Proxy9</span>.addBlock(Unknown Source)</span><br><span class="line">at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:330)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSOutputStream<span class="variable">$DataStreamer</span>.locateFollowingBlock(DFSOutputStream.java:1226)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSOutputStream<span class="variable">$DataStreamer</span>.nextBlockOutputStream(DFSOutputStream.java:1078)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSOutputStream<span class="variable">$DataStreamer</span>.run(DFSOutputStream.java:514)</span><br><span class="line">put: File /data/wordcount/core-site.xml._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded <span class="keyword">in</span> this operation.</span><br><span class="line">19/11/14 10:13:24 ERROR hdfs.DFSClient: Failed to close file /data/wordcount/core-site.xml._COPYING_</span><br><span class="line">org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /data/wordcount/core-site.xml._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded <span class="keyword">in</span> this operation.</span><br><span class="line">at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:555)</span><br><span class="line">at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:387)</span><br><span class="line">at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos<span class="variable">$ClientNamenodeProtocol</span><span class="variable">$2</span>.callBlockingMethod(ClientNamenodeProtocolProtos.java:59582)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Server</span><span class="variable">$ProtoBufRpcInvoker</span>.call(ProtobufRpcEngine.java:585)</span><br><span class="line">at org.apache.hadoop.ipc.RPC<span class="variable">$Server</span>.call(RPC.java:928)</span><br><span class="line">at org.apache.hadoop.ipc.Server<span class="variable">$Handler</span><span class="variable">$1</span>.run(Server.java:2048)</span><br><span class="line">at org.apache.hadoop.ipc.Server<span class="variable">$Handler</span><span class="variable">$1</span>.run(Server.java:2044)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">at org.apache.hadoop.ipc.Server<span class="variable">$Handler</span>.run(Server.java:2042)</span><br><span class="line"></span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1347)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1300)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Invoker</span>.invoke(ProtobufRpcEngine.java:206)</span><br><span class="line">at com.sun.proxy.<span class="variable">$Proxy9</span>.addBlock(Unknown Source)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:606)</span><br><span class="line">at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)</span><br><span class="line">at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</span><br><span class="line">at com.sun.proxy.<span class="variable">$Proxy9</span>.addBlock(Unknown Source)</span><br><span class="line">at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:330)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSOutputStream<span class="variable">$DataStreamer</span>.locateFollowingBlock(DFSOutputStream.java:1226)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSOutputStream<span class="variable">$DataStreamer</span>.nextBlockOutputStream(DFSOutputStream.java:1078)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSOutputStream<span class="variable">$DataStreamer</span>.run(DFSOutputStream.java:514)</span><br></pre></td></tr></table></figure><p><strong>主要原因</strong>是重新格式化文件系统，导致<code>master</code>节点下的<code>hadoop2.2.0/hdfs/name/current/VERSION</code>中的<code>clusterID</code>和<code>Slave</code>节点下的<code>hadoop2.2.0/hdfs/data/current/VERSION</code>中的<code>clusterID</code>不一致。在浏览器输入<a href="http://192.168.71.128:50070" target="_blank" rel="noopener">master:50070</a>可以发现<code>Live Nodes</code>为0。</p><p><strong>解决方案</strong>是修改<code>master</code>节点下的<code>clusterID</code>使之与<code>Slave</code>节点下的<code>clusterID</code>一致，然后重启服务即可。</p><ul><li>问题2：执行<code>JAR</code>报错问题</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Container launch failed <span class="keyword">for</span> container_1573700741821_0001_01_000007 : java.lang.IllegalArgumentException: Does not contain a valid host:port authority: slave_1:33775</span><br></pre></td></tr></table></figure><p><strong>主要原因</strong>：<code>Hadoop nodemanager</code>结点主机名不能带下划线<code>_</code>。</p><p><strong>解决方案</strong>：修改主机名称。</p><ul><li>问题3：绑定主机名引起的问题：<code>sudo: unable to resolve host master</code></li></ul><p><strong>解决方案</strong>如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ sudo vi /etc/hosts</span><br><span class="line"></span><br><span class="line">  1 127.0.0.1       localhost</span><br><span class="line">  2 127.0.1.1       ubuntu</span><br><span class="line">  3 127.0.1.1       master</span><br><span class="line">:wq!</span><br></pre></td></tr></table></figure><p>通过本次实验，对集群概念有个基本的认识，在搭建过程中遇到问题不算太多，主要是对局域网组建缺乏认识深度，本集群环境可以进一步扩展，如动态增减节点，借助<code>Zookeeper</code>技术加以融合等在企业中是比较常见的做法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《Hadoop大数据处理基础与实践》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，准备环境&quot;&gt;&lt;a href=&quot;#一，准备环境&quot; class=&quot;headerlink&quot; title=&quot;一，准备环境&quot;&gt;&lt;/a&gt;一，准备环
      
    
    </summary>
    
      <category term="大数据技术" scheme="https://zhangbc.github.io/categories/big-data/"/>
    
    
      <category term="Hadoop" scheme="https://zhangbc.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>【Java实践】Kettle从一次实验说起</title>
    <link href="https://zhangbc.github.io/2019/11/12/java_kettle_01/"/>
    <id>https://zhangbc.github.io/2019/11/12/java_kettle_01/</id>
    <published>2019-11-12T04:16:52.000Z</published>
    <updated>2019-11-12T06:10:04.935Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一，安装Kettle"><a href="#一，安装Kettle" class="headerlink" title="一，安装Kettle"></a>一，安装Kettle</h1><h2 id="1，关于简易安装Kettle"><a href="#1，关于简易安装Kettle" class="headerlink" title="1，关于简易安装Kettle"></a>1，关于简易安装Kettle</h2><p>第一次接触<code>kettle</code>（以前只是听过罢了），摸索了几天，在<code>mac</code>源码安装失败，转而快速安装。在<code>mac</code>上安装最新版<code>kettle</code>并成功启动代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">☁  ~  brew install kettle</span><br><span class="line">☁  ~  <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/kettle/8.2.0.0-342/</span><br><span class="line">☁  8.2.0.0-342  <span class="built_in">cd</span> libexec</span><br><span class="line">☁  libexec  spoon.sh</span><br></pre></td></tr></table></figure><h2 id="2，关于源码尝试安装kettle"><a href="#2，关于源码尝试安装kettle" class="headerlink" title="2，关于源码尝试安装kettle"></a>2，关于源码尝试安装kettle</h2><ul><li><a href="https://github.com/pentaho/pentaho-kettle" target="_blank" rel="noopener">【Kettle源码下载】：https://github.com/pentaho/pentaho-kettle</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/pentaho/pentaho-kettle</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">git <span class="built_in">clone</span> git@github.com:pentaho/pentaho-kettle.git</span><br></pre></td></tr></table></figure><ul><li>设置 <code>setting.xml</code></li></ul><p>将  <code>setting.xml</code> 参见： <a href="https://raw.githubusercontent.com/pentaho/maven-parent-poms/master/maven-support-files/settings.xml" target="_blank" rel="noopener">settings.xml</a>  在你的<code>Maven</code>启动目录<code>/.m2</code>中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">☁  pentaho-kettle [master] ⚡  ll /Users/zhangbocheng/.m2</span><br><span class="line">total 8</span><br><span class="line">drwxr-xr-x  97 zhangbocheng  staff  3104 11  8 17:28 repository</span><br><span class="line">-rw-r--r--   1 zhangbocheng  staff  2345 11  8 20:10 setting.xml</span><br></pre></td></tr></table></figure><ul><li>安装</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">☁  pentaho-kettle [master] mvn clean install &gt;&gt; /Users/zhangbocheng/Desktop/kettle.log</span><br></pre></td></tr></table></figure><ul><li>关于<code>error.log</code></li></ul><p>未设置 <code>setting.xml</code>报错问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">.....................................</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD FAILURE</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 47:49 min</span><br><span class="line">[INFO] Finished at: 2019-11-08T17:44:01+08:00</span><br><span class="line">[INFO] Final Memory: 230M/985M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[ERROR] Failed to execute goal on project pdi-ce: Could not resolve dependencies for project org.pentaho.di:pdi-ce:pom:9.0.0.0-SNAPSHOT: Could not transfer artifact org.hitachivantara.karaf.assemblies:client:zip:9.0.0.0-20191107.125717-160 from/to pentaho-public (http://nexus.pentaho.org/content/groups/omni/): Failed to transfer file http://nexus.pentaho.org/content/groups/omni/org/hitachivantara/karaf/assemblies/client/9.0.0.0-SNAPSHOT/client-9.0.0.0-20191107.125717-160.zip with status code 502 -&gt; [Help 1]</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.</span><br><span class="line">[ERROR] Re-run Maven using the -X switch to enable full debug logging.</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] For more information about the errors and possible solutions, please read the following articles:</span><br><span class="line">[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] After correcting the problems, you can resume the build with the command</span><br><span class="line">[ERROR]   mvn &lt;goals&gt; -rf :pdi-ce</span><br></pre></td></tr></table></figure><p>设置 <code>setting.xml</code>后，就一直处在等待中。</p><h1 id="二，实验案例"><a href="#二，实验案例" class="headerlink" title="二，实验案例"></a>二，实验案例</h1><p>关于课程实验，第一次需要亲手搭建<code>Kettle</code>，这算是一次比较有意思的工程实践机会，花最少的时间来认识认识比较流行而且强大的<code>ETL</code>工具之一—<code>Kettle</code>。</p><h2 id="1，关于实验题目"><a href="#1，关于实验题目" class="headerlink" title="1，关于实验题目"></a>1，关于实验题目</h2><p>任务描述：用<code>kettle</code>完成下列实验，结果存储到<code>MySQL</code>（或者<code>CSV</code>）。已知<code>Excel</code>文件，包含列（姓名，年龄，身份证号码，性别，挂号日期时间，门诊号），数据若干。</p><blockquote><p>生成数据1，包含列（日，性别，儿童/青年/中年/老年，人次），其中儿童/青年/中年/老年的年龄段自己定义；<br>生成数据2，包含列（省份，hour，人次）</p></blockquote><p>第一次接触<code>kettle</code>，力求简单，仅考虑输入输出均为<code>Excel</code>，首先按照题目要求捏造一批数据，如下图所示：</p><p><img src="/images/kettle_20191111230208.png" alt="元数据"></p><p><code>Excel</code>字段说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">姓名：字符串</span><br><span class="line">年龄：整型</span><br><span class="line">身份证号码：字符串</span><br><span class="line">性别：字符串</span><br><span class="line">挂号日期时间：日期时间型</span><br><span class="line">门诊号：整型</span><br></pre></td></tr></table></figure><p>进入安装目录<code>/usr/local/Cellar/kettle/8.2.0.0-342/libexec</code>启动<code>kettle</code>:</p><p><img src="/images/kettle_20191111230516.png" alt="welcome"></p><p>根据实验要求，其实所涉及的问题仅仅是输入和输出，转换（分组统计）。创建任务之初，有必要先百度<code>or Google</code>看看<code>kettle</code>的输入输出是如何实现的？</p><h2 id="2，实例预热"><a href="#2，实例预热" class="headerlink" title="2，实例预热"></a>2，实例预热</h2><p>最容易实现的简单案例就是生成随机数，并存储到<code>txt</code>文件。</p><p>1）新建一个转换保存为<code>test_random</code>（后缀为<code>.ktr</code>）通过拖拽插件方式，在核心对象-&gt;输入和输出分别拖拽“生成随机数”和“文本文件输出”两个按钮，然后点击“生成随机数”并按下<code>sheft</code>键，用鼠标指向“文本文件输出”，以生成剪头，表示数据流向。如下图：</p><p><img src="/images/kettle_20191111232316.png" alt="test_random"></p><p>2）编辑输入流，即“生成随机数”按钮，如图所示：</p><p><img src="/images/kettle_20191111232402.png" alt="生成随机数"></p><p>关于支持的随机数据类型有：</p><p><img src="/images/kettle_20191111232421.png" alt="随机数据类型"></p><p>3）然后编辑输出流，即“文本文件输出”按钮，如图所示：</p><p><img src="/images/kettle_20191111232535.png" alt="文本文件输出"></p><p>输出文件名支持预览模式，即点击图中“显示文件名…”按钮：</p><p><img src="/images/kettle_20191111232550.png" alt="显示文件名"></p><p>4）最后执行，看看结果。</p><p><img src="/images/kettle_20191111234356.png" alt="log"></p><p><img src="/images/kettle_20191111234347.png" alt="preview_data"></p><p><img src="/images/kettle_20191111234407.png" alt="text"></p><h2 id="3，实验步骤"><a href="#3，实验步骤" class="headerlink" title="3，实验步骤"></a>3，实验步骤</h2><p>通过上述简单实验，我们知道了输入输出流的基本操作，下面开始进入正题。</p><p>1）将上述实验中的输入输出全部改为<code>Excel</code>。进行相关配置说明如下：</p><p><code>Excel</code>输入：</p><p>在文件选项下，表格类型根据实际进行适配（<code>xls or xlsx</code>），在文件或目录后，点击“浏览”选择自己的源数据文件，然后点击“添加”；</p><p>在工作表选项下，点击“获取工作表名称…”添加工作表，即<code>Excel</code>中的<code>sheet</code>；</p><p>在字段选项下，点击“获取来自头部数据的字段…”自动获取字段，由于原<code>Excel</code>中整型数据转入会变成浮点型，所以需要进行更改，如图所示：</p><p><img src="/images/kettle_20191112095739.png" alt="字段配置"></p><p>最后可以进行预览。</p><p><img src="/images/kettle_20191112100005.png" alt="预览数据"></p><p><code>Excel</code>输出：只需要配置输出文件名即可，其他均为默认。</p><p><img src="/images/kettle_20191112100334.png" alt="Excel输出"></p><p>2）接下来需要处理的就行核心步骤，即转换。首先针对<code>生成数据1</code>进行分析，由于<code>kettle</code>中分组需要首先进行排序，从而需要处理的点有：</p><blockquote><p>（1）将挂号日期时间截取到日；</p><p>（2）对年龄按照一定标准进行转换（自己定义）；</p><p>（3）按照待分组的字段进行排序；</p><p>（4） 进行分组统计。</p></blockquote><p>按照上述思路，在“转换”和“统计”核心对象中，分别找到对应组件，完成基本数据流节点配置，如图所示：</p><p><img src="/images/kettle_20191112101832.png" alt="数据流节点"></p><p>在“字段选择”组件中，对时间进行处理。在元数据选项中，需要对<code>Date</code>进行转换成<code>String</code>，格式设置为<code>yyyy-MM-dd</code>,同时可以对字段进行更名操作。另外还可以对字段进行选择，修改，移除。如图所示：</p><p><img src="/images/kettle_20191112103403.png" alt="时间处理"></p><p>注意，这里如果不将时间设置为<code>String</code>，进行一个小实验可以可以发现，最后存储的依然是带时间的日期，本次实验过程中在这个坎纠结了，错误地以为是<code>kettle</code>不支持多关键字（两个以上）排序，如下图所示：</p><p><img src="/images/keetle_20191112104001.png" alt="error1"></p><p><img src="/images/keetle_20191112104011.png" alt="error2"></p><p>经过与各位大佬沟通确认，<code>kettle</code>是不可能不支持对多关键的排序的，对此深信不疑，那么问题就从<code>kettle</code>本身存在的可能<code>bug</code>消失了，对一个小白而言，不熟悉<code>kettle</code>本身应遵守的规则，这是致命的，只能对怀疑的其他种种可能进行逐一实验了。期间怀疑过待排序关键字的顺序问题，测试发现都不是问题的根本原因，整个过程下来只有对日期做过预处理，而且从错误中发现，引起错排的唯一合理解释就是日期按照预处理之前的原始数据的日期时间型排序的。单独对日期设计实验，如果对预处理生效，那么输出也是预期结果。</p><ul><li>验证日期实验</li></ul><p>输入流，如图所示：</p><p><img src="/images/kettle_20191112105608.png" alt="日期输入流"></p><p>假设日期类型不改成<code>String</code>，如图所示：</p><p><img src="/images/kettle_20191112110354.png" alt="Date"></p><p>输出流，结果预览，如图所示：</p><p><img src="/images/kettle_20191112110430.png" alt="结果预览"></p><p>输出流，<code>Excel</code>输出，如图所示：</p><p><img src="/images/kettle_20191112110535.png" alt="Excel输出"></p><p>验证实验室结果发现，预览数据并没有存储到输出<code>Excel</code>中去，然后尝试转换为<code>String</code>，输出便一致了。再次验证，<code>kettle</code>对日期类数据处理有待提高。</p><p>在“数值范围”组件中，对年龄进行处理，划分标准自己定义（如下定义可能存在瑕疵）如图所示。</p><p><img src="/images/kettle_20191112111401.png" alt="年龄处理"></p><p>在“排序记录”组件中，按照生成数据要求，需要对日期，性别，年龄段进行来袭，如图所示。</p><p><img src="/images/kettle_20191112112231.png" alt="排序记录"></p><p>在“分组”组件中，进行分组统计，如图所示。</p><p><img src="/images/kettle_20191112112450.png" alt="分组"></p><p>3）执行，结果如图所示。</p><p><img src="/images/kettle_20191112112719.png" alt="运行结果"></p><p><img src="/images/kettle_20191112112916.png" alt="Excel输出">ß</p><h2 id="4，实验二简要说明"><a href="#4，实验二简要说明" class="headerlink" title="4，实验二简要说明"></a>4，实验二简要说明</h2><p>针对<code>生成数据2</code>进行分析，需要处理的点有：</p><blockquote><p>（1）将挂号日期时间设置<code>String</code>，由于不能直接从预设格式中提取日，需要采取字符串截取；</p><p>（2）对日期和身份证进行字符串截取，分别提取日和省份代码（身份证前两位）；</p><p>（3）按照待分组的字段进行排序；</p><p>（4）对省份和时间段进行值映射；</p><p>（4） 进行分组统计。</p></blockquote><p>整体设计数据流图，如图所示：</p><p><img src="/images/kettle_20191112115812.png" alt="整体设计数据流图"></p><p>在“剪切字符串”组件，设置如下：</p><p><img src="/images/kettle_20191112115803.png" alt="剪切字符串"></p><p>在“省份值映射”和“时间值映射”组件中，分别设置如下：</p><p><img src="/images/kettle_20191112120229.png" alt="省份值映射"></p><p><img src="/images/kettle_20191112120242.png" alt="时间值映射"></p><p>运行结果，如图所示：</p><p><img src="/images/kettle_20191112120513.png" alt="运行结果previewdata"></p><p><img src="/images/kettle_20191112120550.png" alt="运行结果excel"></p><h1 id="三，总结"><a href="#三，总结" class="headerlink" title="三，总结"></a>三，总结</h1><p>通过本次实验，初步认识了一下强大的<code>ETL</code>工具之<code>kettle</code>，要想获取更多知识就得更多实验，从错误中反思学到的远比从成功中收获更多。作为工具，只有多多实验才能更好的掌握好它，印证了那句经典—“实践出真知”。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一，安装Kettle&quot;&gt;&lt;a href=&quot;#一，安装Kettle&quot; class=&quot;headerlink&quot; title=&quot;一，安装Kettle&quot;&gt;&lt;/a&gt;一，安装Kettle&lt;/h1&gt;&lt;h2 id=&quot;1，关于简易安装Kettle&quot;&gt;&lt;a href=&quot;#1，关于简易安
      
    
    </summary>
    
      <category term="Java" scheme="https://zhangbc.github.io/categories/java/"/>
    
    
      <category term="Java实践" scheme="https://zhangbc.github.io/tags/java-practice/"/>
    
  </entry>
  
  <entry>
    <title>【NLP基础】常见的距离公式说明</title>
    <link href="https://zhangbc.github.io/2019/11/08/nlp_01/"/>
    <id>https://zhangbc.github.io/2019/11/08/nlp_01/</id>
    <published>2019-11-08T09:23:10.000Z</published>
    <updated>2019-11-09T04:03:33.286Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零，基本知识预备"><a href="#零，基本知识预备" class="headerlink" title="零，基本知识预备"></a>零，基本知识预备</h1><p>在二维平面中，设有两个向量 $\overrightarrow{a}=(x_1,y_1)$ , $\overrightarrow{b}=(x_2,y_2)$ ，$\theta$ 为 $\overrightarrow{a}$ 和 $\overrightarrow{b}$ 的夹角，则有：</p><p>1）$\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的<strong>数量积</strong>（又称<strong>点积</strong>）为</p><script type="math/tex; mode=display">\overrightarrow{a}.\overrightarrow{b}=|\overrightarrow{a}||\overrightarrow{b}|\cos\theta\tag{1.1}</script><p>2）$\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的<strong>向量积</strong>（又称<strong>叉积</strong>或<strong>外积</strong>）为</p><script type="math/tex; mode=display">\overrightarrow{a}\times \overrightarrow{b} = \overrightarrow{c}\tag{1.2}</script><p>其中，$\overrightarrow{c}$ 的模长为</p><script type="math/tex; mode=display">|\overrightarrow{c}|=|\overrightarrow{a}||\overrightarrow{b}|\sin\theta\tag{1.3}</script><p>方向为：$\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的向量积 $\overrightarrow{c}$ 的方向与这两个向量所在平面垂直，且遵守<strong>右手定则</strong> 即：若坐标系是满足右手定则的，当右手的四指从 $\overrightarrow{a}$ 以不超过180度的转角转向 $\overrightarrow{b}$ 时，竖起的大拇指指向是 $\overrightarrow{c}$ 的方向。</p><p>3）若 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 共线，且 $\overrightarrow{b}\ne0$ 则存在唯一的 $\lambda$ 使得  $\overrightarrow{a}=\lambda\overrightarrow{b}$ ，即有</p><script type="math/tex; mode=display">x_1y_2=x_2y_1\tag{1.4}</script><p>4）若 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 垂直，则 $\overrightarrow{a}.\overrightarrow{b}=0$ ，即有</p><script type="math/tex; mode=display">x_1x_2+y_1y_2=0\tag{1.5}</script><p>设 $n$ 维向量 $\overrightarrow{V}=(v_1,v_2,\dots,v_n)$ 则向量 $\overrightarrow{V}$ 的<strong>模长</strong>为</p><script type="math/tex; mode=display">|\overrightarrow{V}|=\sqrt{v_1^2+v_2^2+\dots+v_n^2}\tag{1.6}</script><h1 id="一，余弦距离"><a href="#一，余弦距离" class="headerlink" title="一，余弦距离"></a>一，余弦距离</h1><h2 id="1，定义"><a href="#1，定义" class="headerlink" title="1，定义"></a>1，定义</h2><p><strong>余弦距离</strong>，又称为<strong>余弦相似性</strong>或<strong>余弦相似度</strong>，是通过计算两个向量的夹角余弦值来评估他们的相似度。</p><p>在二维平面中，向量 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的余弦相似性可以表示为：</p><script type="math/tex; mode=display">\cos\theta=\frac{\overrightarrow{a}.\overrightarrow{b}}{|\overrightarrow{a}||\overrightarrow{b}|}=\frac{x_1x_2+y_1y_2}{\sqrt{x_1^2+y_1^2}\sqrt{x_2^2+y_2^2}}\tag{1.7}</script><p>给定两个 $n$ 维向量 $\overrightarrow{A}$ 与 $\overrightarrow{B}$ 的余弦相似性可以表示为：</p><script type="math/tex; mode=display">\text{similarrity}=\cos\boldsymbol{\theta}=\frac{\overrightarrow{A}.\overrightarrow{B}}{|\overrightarrow{A}||\overrightarrow{B}|}=\frac{\sum_{i=1}^{n}A_iB_i}{\sqrt{\sum_{i=1}^{n}(A_i)^2}\sqrt{\sum_{i=1}^{n}(B_i)^2}}\tag{1.8}</script><p>其中，$A_i$ 和 $B_i$ 分别表示向量 $\overrightarrow{A}$ 与 $\overrightarrow{B}$ 的分量。</p><h2 id="2，代码实现"><a href="#2，代码实现" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> pdist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cosine_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算余弦距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vec1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">    vec2 = [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解</span></span><br><span class="line">    dist1 = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))</span><br><span class="line">    print(<span class="string">"根据公式求解(Numpy),余弦距离测试结果为: "</span>, dist1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解</span></span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line">    dist2 = <span class="number">1</span> - pdist(vec, <span class="string">'cosine'</span>)</span><br><span class="line">    print(<span class="string">"根据公式求解(Scipy),余弦距离测试结果为: "</span>,dist2[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li>测试结果均为：0.9688639316269662</li></ul><h2 id="3，相关应用"><a href="#3，相关应用" class="headerlink" title="3，相关应用"></a>3，相关应用</h2><p>1）在信息检索中，每个词项被赋予不同的维度，而一个维度由一个向量表示，其各个维度上的值对应于该词项在文档中出现的频率。余弦相似度因此可以给出两篇文档在其主题方面的相似度；</p><p>2）通常用于文本挖掘中的文件比较；</p><p>3）在数据挖掘领域中，会用到它来度量集群内部的凝聚力。</p><h1 id="二，欧式距离"><a href="#二，欧式距离" class="headerlink" title="二，欧式距离"></a>二，欧式距离</h1><h2 id="1，定义-1"><a href="#1，定义-1" class="headerlink" title="1，定义"></a>1，定义</h2><p><strong>欧几里得度量</strong>（<code>euclidean metric</code>）（也称<strong>欧氏距离</strong>）是一个通常采用的距离定义，指在 $m$ 维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）。在二维和三维空间中的欧氏距离就是两点之间的实际距离。</p><p>在二维平面中，设有两个向量 $\overrightarrow{a}=(x_1,y_1)$ , $\overrightarrow{b}=(x_2,y_2)$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的欧式距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\tag{1.9}</script><p>$\overrightarrow{a}$ 的欧式距离 $d_{1}$ 为</p><script type="math/tex; mode=display">d_{1}=\sqrt{x_1^2+y_1^2}\tag{1.10}</script><p>在三维空间中，设有两个向量 $\overrightarrow{a}=(x_1,y_1,z_1)$ , $\overrightarrow{b}=(x_2,y_2,z_2)$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的欧式距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}\tag{1.11}</script><p> $\overrightarrow{a}$ 的欧式距离 $d_{1}$ 为</p><script type="math/tex; mode=display">d_{1}=\sqrt{x_1^2+y_1^2+z_1^2}\tag{1.12}</script><p>在 $n$ 维空间中，设有两个向量 $\overrightarrow{a}=(x_1,x_2，\dots,x_n)$ , $\overrightarrow{b}=(y_1,y_2,\dots,y_n)$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的欧式距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\dots+(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}\tag{1.13}</script><p> $\overrightarrow{a}$ 的欧式距离 $d_{1}$ 为</p><script type="math/tex; mode=display">d_{1}=\sqrt{x_1^2+x_2^2+\dots+x_n^2}=\sqrt{\sum_{i=1}^{n}x_i^2}\tag{1.14}</script><p> $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的欧式距离 $d_{12}$ 向量式表示为</p><script type="math/tex; mode=display">d_{12}=\sqrt{(\overrightarrow{a}-\overrightarrow{b})(\overrightarrow{a}-\overrightarrow{b})^T}\tag{*}</script><h2 id="2，代码实现-1"><a href="#2，代码实现-1" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_euclidean_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算欧式距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.mat([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    vec2 = np.mat([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解1</span></span><br><span class="line">    dist1 = np.sqrt(np.sum(np.square(vec1 - vec2)))</span><br><span class="line">    print(<span class="string">"根据公式求解1(Numpy),欧式距离测试结果为: "</span>, dist1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解</span></span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line">    dist2 = pdist(vec)</span><br><span class="line">    print(<span class="string">"根据公式求解(Scipy),欧式距离测试结果为: "</span>,dist2[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解2</span></span><br><span class="line">    dist3 = np.sqrt((vec1 - vec2) * (vec1 - vec2).T)</span><br><span class="line">    print(<span class="string">"根据公式求解2(Numpy),欧式距离测试结果为: "</span>, dist3[<span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li>测试结果均为：8.0</li></ul><h2 id="3，相关应用-1"><a href="#3，相关应用-1" class="headerlink" title="3，相关应用"></a>3，相关应用</h2><p>所谓<strong>欧氏距离变换</strong>，是指对于一张二值图像（在此我们假定白色为前景色，黑色为背景色），将前景中的像素的值转化为该点到达最近的背景点的距离。<br>欧氏距离变换在数字图像处理中的应用范围很广泛，尤其对于图像的骨架提取，是一个很好的参照。</p><h1 id="三，曼哈顿距离"><a href="#三，曼哈顿距离" class="headerlink" title="三，曼哈顿距离"></a>三，曼哈顿距离</h1><h2 id="1，定义-2"><a href="#1，定义-2" class="headerlink" title="1，定义"></a>1，定义</h2><p><strong>出租车几何</strong>或<strong>曼哈顿距离</strong>（<code>Manhattan Distance</code>）是由十九世纪的赫尔曼·闵可夫斯基所创词汇，是种使用在几何度量空间的几何学用语，用以标明两个点在标准坐标系上的绝对轴距总和。</p><p>如图1.1，曼哈顿距离示意图。</p><p><img src="/images/nlp_20191108224222.png" alt="曼哈顿距离"></p><p>图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。曼哈顿距离——两点在南北方向上的距离加上在东西方向上的距离，即 $d(i,j)=|x_i-x_j|+|y_i-y_j|$ 。对于一个具有正南正北、正东正西方向规则布局的城镇街道，从一点到达另一点的距离正是在南北方向上旅行的距离加上在东西方向上旅行的距离，因此，<strong>曼哈顿距离</strong>又称为<strong>出租车距离</strong>。曼哈顿距离不是距离不变量，当坐标轴变动时，点间的距离就会不同。曼哈顿距离示意图在早期的计算机图形学中，屏幕是由像素构成，是整数，点的坐标也一般是整数，原因是浮点运算很昂贵，很慢而且有误差，如果直接使用 $AB$ 的欧氏距离(欧几里德距离：在二维和三维空间中的欧氏距离的就是两点之间的距离），则必须要进行浮点运算，如果使用 $AC$ 和 $CB$ ，则只要计算加减法即可，这就大大提高了运算速度，而且不管累计运算多少次，都不会有误差。</p><p>在二维平面中，设有两个向量 $\overrightarrow{a}=(x_1,y_1)$ , $\overrightarrow{b}=(x_2,y_2)$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的曼哈顿距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=|x_1-x_2|+|y_1-y_2|\tag{1.15}</script><p>在 $n$ 维空间中，设有两个向量 $\overrightarrow{a}=(x_1,x_2，\dots,x_n)$ , $\overrightarrow{b}=(y_1,y_2,\dots,y_n)$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的曼哈顿距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=|x_1-y_1|+|x_2-y_2|+\dots+|x_n-y_n|=\sum_{i=1}^{n}|x_i-y_i|\tag{1.16}</script><h2 id="2，代码实现-2"><a href="#2，代码实现-2" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_city_block_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算曼哈顿距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.mat([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    vec2 = np.mat([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解</span></span><br><span class="line">    dist1 = np.sum(np.abs(vec1 - vec2))</span><br><span class="line">    print(<span class="string">"根据公式求解(Numpy),曼哈顿距离测试结果为: "</span>, dist1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解</span></span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line">    dist2 = pdist(vec, <span class="string">'cityblock'</span>)</span><br><span class="line">    print(<span class="string">"根据公式求解(Scipy),曼哈顿距离测试结果为: "</span>,dist2[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li>测试结果均为：16.0</li></ul><h1 id="四，闵可夫斯基距离"><a href="#四，闵可夫斯基距离" class="headerlink" title="四，闵可夫斯基距离"></a>四，闵可夫斯基距离</h1><h2 id="1，定义-3"><a href="#1，定义-3" class="headerlink" title="1，定义"></a>1，定义</h2><p><strong>闵氏空间</strong>指狭义相对论中由一个时间维和三个空间维组成的时空，为俄裔德国数学家闵可夫斯基(<code>H.Minkowski</code>,1864-1909)最先表述。他的平坦空间（即假设没有重力，曲率为零的空间）的概念以及表示为特殊距离量的几何学是与狭义相对论的要求相一致的。闵可夫斯基空间不同于牛顿力学的平坦空间。<br>设 $n$ 维空间中有两点坐标  $X=(x_1,x_2，\dots,x_n)$ , $Y=(y_1,y_2,\dots,y_n)$，$p$ 为一个变参，<strong>闵式距离</strong>定义为</p><script type="math/tex; mode=display">d_{12}=\sqrt[p]{\sum_{i=1}^{n}|x_i-y_i|^p}=\left(\sum_{i=1}^{n}|x_i-y_i|^p\right)^{\frac{1}{p}}\tag{1.17}</script><p><strong>注意</strong>：</p><blockquote><ol><li>闵氏距离与特征参数的量纲有关，有不同量纲的特征参数的闵氏距离常常是无意义的；</li><li>闵氏距离没有考虑特征参数间的相关性；</li><li>闵氏距离不是一种距离，而是一组距离的定义；</li><li>当 $p=1$ 时，得到绝对值距离，也叫<strong>曼哈顿距离</strong>（<code>Manhattan distance</code>）、<strong>出租汽车距离</strong>或<strong>街区距离</strong>（<code>city block distance</code>）；</li><li>当 $p=2$ 时，得到<strong>欧几里德距离</strong>（<code>Euclidean distance</code>），就是两点之间的直线距离（简称<strong>欧氏距离</strong>）；</li><li>当 $p\to\infty$ 时，得到<strong>切比雪夫距离</strong>（<code>Chebyshev distance</code>）。</li></ol></blockquote><h2 id="2，代码实现-3"><a href="#2，代码实现-3" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_minkowski_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算明可夫斯基距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.mat([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    vec2 = np.mat([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解, p=1</span></span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line">    dist1 = pdist(vec, <span class="string">'minkowski'</span>, p=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">"当p=1时,曼哈顿距离测试结果为: "</span>,dist1[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解, p=2</span></span><br><span class="line">    dist2 = np.sqrt(np.sum(np.square(vec1 - vec2)))</span><br><span class="line">    print(<span class="string">"当p=2时,欧式距离测试结果为: "</span>, dist2)</span><br></pre></td></tr></table></figure><ul><li>测试结果为：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当p=1时,曼哈顿距离测试结果为:  16.0</span><br><span class="line">当p=2时,欧式距离测试结果为:  8.0</span><br></pre></td></tr></table></figure><h1 id="五，切比雪夫距离"><a href="#五，切比雪夫距离" class="headerlink" title="五，切比雪夫距离"></a>五，切比雪夫距离</h1><h2 id="1，定义-4"><a href="#1，定义-4" class="headerlink" title="1，定义"></a>1，定义</h2><p>在数学中，<strong>切比雪夫距离</strong>（<code>Chebyshev distance</code>）也称 $L\infty$ <strong>度量</strong>，是向量空间中的一种度量，二个点之间的距离定义是其各坐标数值差绝对值的最大值。<br>在二维平面中，设有两个向量 $\overrightarrow{a}=(x_1,y_1)$ , $\overrightarrow{b}=(x_2,y_2)$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的切比雪夫距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=\max(|x_1-x_2|,|y_1-y_2|)\tag{1.18}</script><p>在 $n$ 维空间中，设有两个向量 $\overrightarrow{a}=(x_1,x_2，\dots,x_n)$ , $\overrightarrow{b}=(y_1,y_2,\dots,y_n)$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的切比雪夫距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=\max(|x_1-y_1|,|x_2-y_2|,\dots,|x_n-y_n|)=\underset{i}{\max}(|x_i-y_i|)\tag{1.19}</script><p>等价表示为</p><script type="math/tex; mode=display">d_{12}=\lim_{k\to\infty}\left(\sum_{i=1}^{n}|x_i-y_i|^k\right)^{\frac{1}{k}}\tag{1.20}</script><h2 id="2，代码实现-4"><a href="#2，代码实现-4" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_chebyshev_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算切比雪夫距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.mat([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    vec2 = np.mat([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解</span></span><br><span class="line">    dist1 = np.max(np.abs(vec1 - vec2))</span><br><span class="line">    print(<span class="string">"根据公式求解(Numpy),切比雪夫距离测试结果为: "</span>, dist1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解</span></span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line">    dist2 = pdist(vec, <span class="string">'chebyshev'</span>)</span><br><span class="line">    print(<span class="string">"根据公式求解(Scipy),切比雪夫距离测试结果为: "</span>,dist2[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li>测试结果均为：4.0</li></ul><h2 id="3，相关应用-2"><a href="#3，相关应用-2" class="headerlink" title="3，相关应用"></a>3，相关应用</h2><p>1）国际象棋棋盘上二个位置间的切比雪夫距离是指王要从一个位子移至另一个位子需要走的步数。由于王可以往斜前或斜后方向移动一格，因此可以较有效率的到达目的的格子；</p><p>2）会用在仓储物流中；</p><p>3）对一个网格（例如棋盘），和一点的切比雪夫距离为1的点为此点的 <strong><code>Moore</code>型邻居</strong>（<code>Moore neighborhood</code>）。</p><h1 id="六，杰卡德距离"><a href="#六，杰卡德距离" class="headerlink" title="六，杰卡德距离"></a>六，杰卡德距离</h1><h2 id="1，定义-5"><a href="#1，定义-5" class="headerlink" title="1，定义"></a>1，定义</h2><p><strong>杰卡德距离</strong>(<code>Jaccard Distance</code>) 是用来衡量两个集合差异性的一种指标，它是杰卡德相似系数的补集，被定义为1减去<code>Jaccard</code>相似系数。而<strong>杰卡德相似系数</strong>(<code>Jaccard similarity coefficient</code>)，也称<strong>杰卡德指数</strong>(<code>Jaccard Index</code>)，是用来衡量两个集合相似度的一种指标。</p><p><strong><code>Jaccard</code>相似指数</strong> 用来度量两个集合 $A,B$ 之间的相似性，定义为两个集合 $A,B$ 交集的元素在 $A,B$ 并集中所占的比例，即</p><script type="math/tex; mode=display">J(A,B)=\frac{|A\cap B|}{|A\cup B|}\tag{1.21}</script><p><strong><code>Jaccard</code>距离</strong> 用来度量两个集合 $A,B$ 之间的差异性，它是<code>Jaccard</code>的相似系数的补集，被定义为1减去<code>Jaccard</code>相似系数；用两个集合 $A,B$ 中不同元素占所有元素的比例来衡量两个集合的区分度。即</p><script type="math/tex; mode=display">J_{\delta}(A,B)=1-J(A,B)=\frac{|A\cup B|-|A\cap B|}{|A\cup B|}\tag{1.22}</script><h2 id="2，代码实现-5"><a href="#2，代码实现-5" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_jaccard_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算杰卡德距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    v1 = np.random.random(<span class="number">10</span>) &gt; <span class="number">0.5</span></span><br><span class="line">    v2 = np.random.random(<span class="number">10</span>) &gt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.asarray(v1, np.int32)</span><br><span class="line">    vec2 = np.asarray(v2, np.int32)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"vec1 = "</span>, vec1)</span><br><span class="line">    print(<span class="string">"vec2 = "</span>, vec2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解</span></span><br><span class="line">    up = np.double(np.bitwise_and((vec1 != vec2), np.bitwise_or(vec1 != <span class="number">0</span>, vec2 != <span class="number">0</span>)).sum())</span><br><span class="line">    down = np.double(np.bitwise_or(vec1 != <span class="number">0</span>, vec2 != <span class="number">0</span>).sum())</span><br><span class="line">    dist1 = up / down</span><br><span class="line">    print(<span class="string">"根据公式求解(Numpy),杰卡德距离测试结果为: "</span>, dist1)</span><br></pre></td></tr></table></figure><ul><li>测试结果为：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vec1 =  [1 0 1 0 1 0 0 1 0 1]</span><br><span class="line">vec2 =  [0 0 0 1 0 1 1 0 1 1]</span><br><span class="line">根据公式求解(Numpy),杰卡德距离测试结果为:  0.8888888888888888</span><br><span class="line">根据公式求解(Scipy),杰卡德距离测试结果为:  0.8888888888888888</span><br></pre></td></tr></table></figure><h1 id="七，汉明距离"><a href="#七，汉明距离" class="headerlink" title="七，汉明距离"></a>七，汉明距离</h1><h2 id="1，定义-6"><a href="#1，定义-6" class="headerlink" title="1，定义"></a>1，定义</h2><p>在信息论中，两个等长字符串之间的<strong>汉明距离</strong>是两个字符串对应位置的不同字符的个数。换句话说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。例如：</p><blockquote><p>1011101 与 1001001 之间的汉明距离是 2。<br>2143896 与 2233796 之间的汉明距离是 3。<br>“toned” 与 “roses” 之间的汉明距离是 3。</p></blockquote><h2 id="2，代码实现-6"><a href="#2，代码实现-6" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_hamming_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算汉明距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    v1 = np.random.random(<span class="number">10</span>) &gt; <span class="number">0.5</span></span><br><span class="line">    v2 = np.random.random(<span class="number">10</span>) &gt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.asarray(v1, np.int32)</span><br><span class="line">    vec2 = np.asarray(v2, np.int32)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"vec1 = "</span>, vec1)</span><br><span class="line">    print(<span class="string">"vec2 = "</span>, vec2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解</span></span><br><span class="line">    dist1 = np.mean(vec1 != vec2)</span><br><span class="line">    print(<span class="string">"根据公式求解(Numpy),汉明距离测试结果为: "</span>, dist1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解</span></span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line">    dist2 = pdist(vec, <span class="string">'hamming'</span>)</span><br><span class="line">    print(<span class="string">"根据公式求解(Scipy),汉明距离测试结果为: "</span>,dist2[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li>测试结果为：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vec1 =  [0 0 0 1 0 0 1 0 1 1]</span><br><span class="line">vec2 =  [0 1 1 1 1 1 0 1 1 0]</span><br><span class="line">根据公式求解(Numpy),汉明距离测试结果为:  0.7</span><br><span class="line">根据公式求解(Scipy),汉明距离测试结果为:  0.7</span><br></pre></td></tr></table></figure><h2 id="3，相关应用-3"><a href="#3，相关应用-3" class="headerlink" title="3，相关应用"></a>3，相关应用</h2><p>1）用于信号处理，表明一个信号变成另一个信号需要的最小操作（替换位），实际中就是比较两个比特串有多少个位不一样，简洁的操作时就是两个比特串进行异或之后包含1的个数；</p><p>2）在图像处理领域也有广泛的应用，是比较二进制图像非常有效的手段。</p><h1 id="八，标准化欧式距离"><a href="#八，标准化欧式距离" class="headerlink" title="八，标准化欧式距离"></a>八，标准化欧式距离</h1><h2 id="1，定义-7"><a href="#1，定义-7" class="headerlink" title="1，定义"></a>1，定义</h2><p><strong>标准化欧式距离</strong>的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。</p><p>假设样本集 $X$ 的数学期望或均值(<code>mean</code>)为 $m$ ，标准差(<code>standard deviation</code>，方差开根)为 $s$ ，那么 $X$ 的“标准化变量” $X^{*}$ 表示为：</p><script type="math/tex; mode=display">X^{*}=\frac{X-m}{s}\tag{1.23}</script><p>而且标准化变量的数学期望为0，方差为1。</p><p>在 $n$ 维空间中，设有两个向量 $\overrightarrow{a}=(x_{11},x_{12}，\dots,x_{1n})$ , $\overrightarrow{b}=(y_{21},y_{22},\dots,y_{2n})$ ，则 $\overrightarrow{a}$ 与 $\overrightarrow{b}$ 的标准化欧式距离 $d_{12}$ 为</p><script type="math/tex; mode=display">d_{12}=\sqrt{\sum_{k=1}^{n}\left(\frac{x_{1k}-x_{2k}}{s_k}\right)^2}\tag{1.24}</script><h2 id="2，代码实现-7"><a href="#2，代码实现-7" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_standard_euclidean_distance</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算标准化欧式距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    vec2 = np.array([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解</span></span><br><span class="line">    sk = np.var(vec, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    dist1 = np.sqrt(((vec1 - vec2) ** <span class="number">2</span> / sk).sum())</span><br><span class="line">    print(<span class="string">"根据公式求解(Numpy),标准化欧式距离测试结果为: "</span>, dist1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解</span></span><br><span class="line">    dist2 = pdist(vec, <span class="string">'seuclidean'</span>)</span><br><span class="line">    print(<span class="string">"根据公式求解(Scipy),标准化欧式距离测试结果为: "</span>,dist2[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li>测试结果均为： 2.8284271247461903</li></ul><h2 id="3，相关应用-4"><a href="#3，相关应用-4" class="headerlink" title="3，相关应用"></a>3，相关应用</h2><p>标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。</p><h1 id="九，皮尔逊积矩相关系数"><a href="#九，皮尔逊积矩相关系数" class="headerlink" title="九，皮尔逊积矩相关系数"></a>九，皮尔逊积矩相关系数</h1><h2 id="1，定义-8"><a href="#1，定义-8" class="headerlink" title="1，定义"></a>1，定义</h2><p>在统计学中，<strong>皮尔逊积矩相关系数</strong>（<code>Pearson product-moment correlation coefficient</code>，又称作<code>PPMCC</code>或<code>PCCs</code>, 常用<code>r</code>或<code>Pearson&#39;s r</code>表示）用于度量两个变量 $X$ 和 $Y$ 之间的相关（线性相关），其值介于-1与1之间。<br>在自然科学领域中，该系数广泛用于度量两个变量之间的相关程度。它是由卡尔·皮尔逊从弗朗西斯·高尔顿在19世纪80年代提出的一个相似却又稍有不同的想法演变而来。这个相关系数也称作“ <strong>皮尔森相关系数<code>r</code></strong> ”。</p><p>两个变量 $X$ 和 $Y$ 之间的皮尔逊相关系数定义为两个变量 $X$ 和 $Y$ 之间的协方差和标准差的商：</p><script type="math/tex; mode=display">\rho_{X,Y}=\frac{\text{cov}(X,Y)}{\delta_X\delta_Y}=\frac{\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]}{\delta_X\delta_Y}\tag{1.25}</script><p>估算样本的协方差和标准差，可得到样本相关系数(样本皮尔逊系数)，常用英文小写字母 $r$ 代表：</p><script type="math/tex; mode=display">r=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^{n}(X_i-\bar{X})^2}\sqrt{\sum_{i=1}^{n}(Y_i-\bar{Y})}}\tag{1.26}</script><p>$r$ 亦可由 $(X_i,Y_i)$ 样本点的标准分数均值估计，得到等价的表达式：</p><script type="math/tex; mode=display">r=\frac{1}{n-1}\sum_{i=1}^{n}\left(\frac{X_i-\bar{X}}{\delta_X}\right)\left(\frac{Y_i-\bar{Y}}{\delta_Y}\right)\tag{1.27}</script><p>其中 $\frac{X_i-\bar{X}}{\delta_X},\bar{X},\delta_{X}$ 分别是对 $X_i$ 样本的标准分数、样本平均值和样本标准差。</p><h2 id="2，代码实现-8"><a href="#2，代码实现-8" class="headerlink" title="2，代码实现"></a>2，代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pearson_product_moment_correlation_coefficient</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算皮尔逊积矩相关系数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vec1 = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    vec2 = np.array([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式求解</span></span><br><span class="line">    vec1_ = vec1 - np.mean(vec1)</span><br><span class="line">    vec2_ = vec2 - np.mean(vec2)</span><br><span class="line">    dist1 = np.dot(vec1_, vec2_) / (np.linalg.norm(vec1_) * np.linalg.norm(vec2_))</span><br><span class="line">    print(<span class="string">"根据公式求解(Numpy),皮尔逊积矩相关系数测试结果为: "</span>, dist1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据scipy库求解</span></span><br><span class="line">    vec = np.vstack([vec1, vec2])</span><br><span class="line">    dist2 = np.corrcoef(vec)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    print(<span class="string">"根据公式求解(Scipy),皮尔逊积矩相关系数测试结果为: "</span>,dist2)</span><br></pre></td></tr></table></figure><ul><li>测试结果为：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">根据公式求解(Numpy),皮尔逊积矩相关系数测试结果为:  0.9999999999999998</span><br><span class="line">根据公式求解(Scipy),皮尔逊积矩相关系数测试结果为:  1.0</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;零，基本知识预备&quot;&gt;&lt;a href=&quot;#零，基本知识预备&quot; class=&quot;headerlink&quot; title=&quot;零，基本知识预备&quot;&gt;&lt;/a&gt;零，基本知识预备&lt;/h1&gt;&lt;p&gt;在二维平面中，设有两个向量 $\overrightarrow{a}=(x_1,y_1)$ ,
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="NLP基础" scheme="https://zhangbc.github.io/tags/NLP%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】相关向量机</title>
    <link href="https://zhangbc.github.io/2019/10/24/prml_07_02/"/>
    <id>https://zhangbc.github.io/2019/10/24/prml_07_02/</id>
    <published>2019-10-24T11:15:58.000Z</published>
    <updated>2019-10-24T14:46:07.351Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，⽤于回归的-RVM"><a href="#一，⽤于回归的-RVM" class="headerlink" title="一，⽤于回归的 RVM"></a>一，⽤于回归的 <strong><code>RVM</code></strong></h1><p><strong>相关向量机</strong>（<code>relevance vector machine</code>）或者 <strong><code>RVM</code></strong>（<code>Tipping</code>, 2001）是⼀个⽤于回归问题和分类问题的贝叶斯稀疏核⽅法，它具有许多 <strong><code>SVM</code></strong> 的特征，同时避免了 <strong><code>SVM</code></strong> 的主要的局限性。此外，通常会产⽣更加稀疏的模型，从⽽使得在测试集上的速度更快，同时保留了可⽐的泛化误差。</p><p>给定⼀个输⼊向量 $\boldsymbol{x}$ 的情况下， 实值⽬标变量t的条件概率分布，形式为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w},\beta) = \mathcal{N}(t|y(\boldsymbol{x}),\beta^{-1})\tag{7.27}</script><p>其中 $\beta=\sigma^{-2}$ 是噪声精度（噪声⽅差的倒数），均值是由⼀个线性模型给出，形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\sum_{i=1}^{M}w_i\phi_i(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x})</script><p>模型带有固定⾮线性基函数 $\phi_i(\boldsymbol{x})$ ，通常包含⼀个常数项，使得对应的权参数表⽰⼀个“偏置”。</p><p>基函数由核给出，训练集的每个数据点关联着⼀个核。⼀般的表达式可以写成与 <strong><code>SVM</code></strong> 相类似的形式</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\sum_{n=1}^{N}w_nk(\boldsymbol{x},\boldsymbol{x}_n)+b\tag{7.28}</script><p>其中 $b$ 是⼀个偏置参数。在⽬前的问题中， 参数的数量为 $M=N+1$ 。$y(\boldsymbol{x})$ 与 <strong><code>SVM</code></strong> 的预测模型具有相同的形式，唯⼀的差别是系数 $a_n$ 在这⾥被记作 $w_n$ 。</p><p>假设有输⼊向量 $\boldsymbol{x}$ 的 $N$ 次观测，将这些观测聚集在⼀起，记作数据矩阵 $\boldsymbol{X}$ ，它的第 $n$ ⾏是 $\boldsymbol{x}_n^{T}$ ，其中 $n=1,\dots,N$ 。对应的⽬标值为 $\mathbf{t}=(t_1,\dots,t_N)^T$ 。因此，似然函数为</p><script type="math/tex; mode=display">p(\mathbf{t}|\boldsymbol{X},\boldsymbol{w},\beta)=\prod_{n=1}^{N}p(t_n|\boldsymbol{x}_n,\boldsymbol{w},\beta)\tag{7.29}</script><p>权值先验的形式为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\boldsymbol{\alpha})=\prod_{i=1}^{N}\mathcal{N}(w_i|0,\alpha_{i}^{-1})\tag{7.30}</script><p>其中 $\alpha_i$ 表⽰对应参数 $w_i$ 的精度，$\boldsymbol{\alpha}$ 表⽰ $(\alpha_1,\dots,\alpha_M)^T$ 。</p><p>权值的后验概率分布为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\mathbf{t},\boldsymbol{X},\boldsymbol{\alpha},\beta)=\mathcal{N}(\boldsymbol{w}|\boldsymbol{m},\boldsymbol{\Sigma})\tag{7.31}</script><p>其中，均值和⽅差为</p><script type="math/tex; mode=display">\boldsymbol{m}=\beta\boldsymbol{\Sigma}\boldsymbol{\Phi}^{T}\mathbf{t}\\\boldsymbol{\Sigma}=(\boldsymbol{A}+\beta\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}</script><p>其中，$\boldsymbol{\Phi}$ 是 $N\times M$ 的设计矩阵，元素为 $\Phi_{ni}=\phi_i(\boldsymbol{x}_n)$（ $i=1,\dots,N$ ）， 且 $\boldsymbol{A}= \text{diag}(\alpha_i)$ 。</p><p>$\boldsymbol{\alpha}$ 和 $\beta$ 的值可以使⽤第⼆类最⼤似然法（也被称为<strong>证据近似</strong>）来确定。这种⽅法中，最⼤化边缘似然函数，边缘似然函数通过对权向量积分的⽅式得到，即</p><script type="math/tex; mode=display">p(\mathbf{t}|\boldsymbol{X},\boldsymbol{\alpha},\beta)=\int p(\mathbf{t}|\boldsymbol{X},\boldsymbol{w},\beta)p(\boldsymbol{w}|\boldsymbol{\alpha})\mathrm{d}\boldsymbol{w}\tag{7.32}</script><p>由于这表⽰两个⾼斯分布的卷积，因此可以计算求得对数边缘似然函数，形式为</p><script type="math/tex; mode=display">\begin{aligned}\ln p(\mathbf{t}|\boldsymbol{X},\boldsymbol{\alpha},\beta)&=\ln \mathcal{N}(\mathbf{t}|\boldsymbol{0},\boldsymbol{C})\\&=-\frac{1}{2}\{N\ln(2\pi)+\ln|\boldsymbol{C}|+\mathbf{t}^{T}\boldsymbol{C}^{-1}\mathbf{t}\}\end{aligned}\tag{7.33}</script><p>其中 $\mathbf{t}= (t_1,\dots,t_N)^{T}$，并且定义了 $N \times N$ 的矩阵 $\boldsymbol{C}$ ，形式为</p><script type="math/tex; mode=display">\boldsymbol{C}=\beta^{-1}\boldsymbol{I}+\boldsymbol{\Phi}\boldsymbol{A}^{-1}\boldsymbol{\Phi}^{T}</script><p>现在的⽬标是关于超参数 $\boldsymbol{\alpha}$ 和 $\beta$ 最⼤化公式。 </p><p>⽅法一，简单地令要求解的边缘似然函数的导数等于零，然后得到了下⾯的重估计⽅程</p><script type="math/tex; mode=display">a_{i}^{新}=\frac{\gamma_i}{m_i^2}\tag{7.34}</script><script type="math/tex; mode=display">(\beta^{新})^{-1}=\frac{\|\mathbf{t}-\boldsymbol{\Phi}\boldsymbol{m}\|^{2}}{N-\sum_{i}\gamma_i}\tag{7.35}</script><p>其中 $m_i$ 是公式(7.31)定义的后验均值 $\boldsymbol{m}$ 的第 $i$ 个分量。$\gamma_i$ 度量了对应的参数 $w_i$ 由数据确定的效果，定义为</p><script type="math/tex; mode=display">\gamma_{i}=1-\alpha_i\Sigma_{ii}</script><p>其中 $\Sigma_{ii}$ 是公式(7.31)给出的后验协⽅差 $\boldsymbol{\Sigma}$ 的第 $i$ 个对角元素。<br>因此，学习过程按照下⾯的步骤进⾏：选择 $\boldsymbol{\alpha}$ 和 $\beta$ 的初始值，分别使⽤公式(7.31)计算后验概率的均值和协⽅差，然后交替地重新估计超参数、重新估计后验均值和协⽅差，直到满⾜⼀个合适的收敛准则。</p><p>⽅法二，使⽤ <strong><code>EM</code>算法</strong>，这两种寻找最⼤化证据的超参数值的⽅法在形式上是等价的。</p><p>在公式(7.28)给出的模型中，对应于剩下的⾮零权值的输⼊ $\boldsymbol{x}_n$ 被称为<strong>相关向量</strong>（<code>relevance vector</code>），因为它们是通过⾃动相关性检测的⽅法得到的，类似于 <strong><code>SVM</code></strong> 中的⽀持向量。通过⾃动相关性检测得到概率模型的稀疏性的⽅法是⼀种相当通⽤的⽅法，可以应⽤于任何表⽰成基函数的可调节线性组合形式的模型。</p><p>对于⼀个新的输⼊ $\boldsymbol{x}$ ，可以计算 $t$ 上的预测分布为</p><script type="math/tex; mode=display">\begin{aligned}p(t|\boldsymbol{x},\boldsymbol{X},\mathbf{t},\boldsymbol{\alpha}^{*},\beta^{*})&=\int p(t|\boldsymbol{x},\boldsymbol{w},\beta^{*})p(\boldsymbol{w}|\boldsymbol{X},\mathbf{t},\boldsymbol{\alpha}^{*},\beta^{*})\mathrm{d}\boldsymbol{w}\\&=\mathcal{N}(t|\boldsymbol{m}^{T}\boldsymbol{\Phi}(\boldsymbol{x}),\sigma^{2}(\boldsymbol{x}))\end{aligned}\tag{7.36}</script><p>因此预测均值由公式(7.27)给出，其中 $\boldsymbol{w}$ 被设置为后验均值 $\boldsymbol{m}$ ，预测分布的⽅差为</p><script type="math/tex; mode=display">\sigma^{2}(\boldsymbol{x})=(\beta^{*})^{-1}+\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\Sigma}\boldsymbol{\phi}(\boldsymbol{x})\tag{7.37}</script><p>与 <strong><code>RVM</code></strong> 相⽐， <strong><code>SVM</code></strong> 的⼀个主要缺点是训练过程涉及到优化⼀个⾮凸的函数，并且与⼀个效果相似的 <strong><code>SVM</code></strong> 相⽐，训练时间要更长。对于有 $M$ 个基函数的模型，<strong><code>RVM</code></strong> 需要对⼀个 $M \times M$ 的矩阵求逆，这通常需要 $O(M^3)$ 次操作。在类似 <strong><code>SVM</code></strong> 的模型(7.28)这⼀具体情形下，有 $M = N + 1$ 。 存在训练 <strong><code>SVM</code></strong> 的⾼效⽅法，其计算代价⼤致是 $N$ 的⼆次函数。在 <strong><code>RVM</code></strong> 的情况下，总可以在开始时将基函数的数量设置为⼩于 $N+1$ 。在相关向量机中，控制模型复杂度的参数以及噪声⽅差⾃动由⼀次训练过程确定，⽽在⽀持向量机中，参数 $C$ 和 $\epsilon$（或者 $\nu$ ）通常使⽤交叉验证的⽅法确定，这涉及到多次训练过程。</p><p>如图7.10，使用与图7.9相同的数据集和相同的⾼斯核进⾏ <strong><code>RVM</code></strong> 回归的说明。 <strong><code>RVM</code></strong> 预测分布的均值⽤红⾊曲线表⽰，预测分布的⼀个标准差的位置⽤阴影区域表⽰。此外，数据点⽤绿⾊表⽰，相关向量⽤蓝⾊圆圈标记。</p><p><img src="/images/prml_20191024000043.png" alt="RVM回归的说明"></p><h1 id="二，稀疏性分析"><a href="#二，稀疏性分析" class="headerlink" title="二，稀疏性分析"></a>二，稀疏性分析</h1><p>考虑⼀个数据集，这个数据集由 $N = 2$ 个观测 $t_1$ 和 $t_2$ 组成。有⼀个模型，它有⼀个基函数 $\phi(\boldsymbol{x})$ ，超参数为 $\alpha$ ，以及⼀个各向同性的噪声，精度为 $\beta$ 。边缘似然函数为 $p(\mathbf{t}|\alpha,\beta)=\mathcal{N}(\mathbf{t}|\boldsymbol{0},\boldsymbol{C})$ ，其中协⽅差矩阵的形式为</p><script type="math/tex; mode=display">\boldsymbol{C}=\frac{1}{\beta}\boldsymbol{I}+\frac{1}{\alpha}\boldsymbol{\varphi}\boldsymbol{\varphi}^{T}\tag{7.38}</script><p>其中 $\boldsymbol{\varphi}$ 表⽰ $N$ 维向量 $(\phi(\boldsymbol{x}_1),\phi(\boldsymbol{x}_2))^{T}$ ，$\mathbf{t}=(t_1,t_2)^{T}$。</p><p>如图7.11～7.12，贝叶斯线性回归模型的稀疏性的原理说明。图中给出了⽬标值的⼀组训练向量，形式为 $\mathbf{t}=(t_1,t_2)^{T}$ ，⽤叉号表⽰，模型有⼀个基向量  $\boldsymbol{\varphi}=(\phi(\boldsymbol{x}_1),\phi(\boldsymbol{x}_2))^{T}$ ，它与⽬标数据向量 $\mathbf{t}$ 的对齐效果很差。图7.11中，我们看到⼀个只有各向同性的噪声的模型，因此 $\boldsymbol{C}=\beta^{−1}\boldsymbol{I}$ ，对应于 $\alpha = \infty$ ，$\beta$ 被设置为概率最⾼的值。图7.12中，我们看到了同样的模型，但是 $\alpha$ 的值变成了有限值。在两种情况下，红⾊椭圆都对应于单位马⽒距离，$|\boldsymbol{C}|$ 对于两幅图的取值相同，⽽绿⾊虚线圆表⽰由项 $\beta^{−1}$ 产⽣的噪声的贡献。我们看到 $\alpha$ 的任意有限值减⼩了观测数据的概率，因此对于概率最⾼的解，基向量被移除。</p><p><img src="/images/prml_20191024000450.png" alt="各向同性的噪声的模型"></p><p><img src="/images/prml_20191024000456.png" alt="各向同性的噪声的模型_有限值"></p><p>对于涉及到 $M$ 个基函数的⼀般情形， 考察稀疏性的原理。<br>⾸先写出由公式(7.33)定义的矩阵 $\boldsymbol{C}$ 中来⾃ $\alpha_i$ 的贡献，即</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{C}&=\beta^{-1}\boldsymbol{I}+\sum_{j\ne{i}}\alpha_j^{-1}\boldsymbol{\varphi}_j\boldsymbol{\varphi}_j^T+\alpha_i^{-1}\boldsymbol{\varphi}_i\boldsymbol{\varphi}_i^T\\&=\boldsymbol{C}_{-i}+\alpha_i^{-1}\boldsymbol{\varphi}_i\boldsymbol{\varphi}_i^{T}\end{aligned}\tag{7.39}</script><p>其中 $\boldsymbol{\varphi}_i$ 表⽰矩阵 $\boldsymbol{\Phi}$ 的第 $i$ 列，即 $N$ 维向量，元素为 $(\phi_i(\boldsymbol{x}_1),\dots,\phi_i(\boldsymbol{x}_N))$ 。这与 $\boldsymbol{\phi}_n$ 不同，它表⽰的是 $\boldsymbol{\Phi}$ 的第 $n$ ⾏。 矩阵 $\boldsymbol{C}_{−i}$ 表⽰将基函数 $i$ 的贡献删除之后的矩阵 $\boldsymbol{C}$ 。矩阵 $\boldsymbol{C}$ 的⾏列式和逆矩阵可以写成</p><script type="math/tex; mode=display">|\boldsymbol{C}|=|\boldsymbol{C}_{-i}|(1+a_i^{-1}\boldsymbol{\varphi}_i^T\boldsymbol{C}_{-i}^{-1}\boldsymbol{\varphi}_i)\tag{7.40}</script><script type="math/tex; mode=display">\boldsymbol{C}^{-1}=\boldsymbol{C}_{-i}^{-1}-\frac{\boldsymbol{C}_{-i}^{-1}\boldsymbol{\varphi}_i\boldsymbol{\varphi}_i^T\boldsymbol{C}_{-i}^{-1}}{\alpha_i+\boldsymbol{\varphi_i}^{T}\boldsymbol{C}_{-i}^{-1}\boldsymbol{\varphi}_i}\tag{7.41}</script><p>对数边缘似然函数为</p><script type="math/tex; mode=display">L(\boldsymbol{\alpha})=L(\boldsymbol{\alpha}_{-i})+\lambda(\alpha_i)\tag{7.42}</script><p>其中 $L(\boldsymbol{\alpha}_{-i})$ 是省略了基函数 $\boldsymbol{\varphi}_i$ 的对数边缘似然函数，$\lambda(\alpha_i)$ 被定义为</p><script type="math/tex; mode=display">\lambda(\alpha_i)=\frac{1}{2}\left[\ln\alpha_i-\ln(\alpha_i+s_i)+\frac{q_i^2}{\alpha_i+s_i}\right]</script><p>包含了所有依赖于 $\alpha_i$ 的项。引⼊两个量</p><script type="math/tex; mode=display">s_i=\boldsymbol{\varphi}_i^T\boldsymbol{C}_{-i}^{-1}\boldsymbol{\varphi}_i\\q_i=\boldsymbol{\varphi}_i^T\boldsymbol{C}_{-i}^{-1}\mathbf{t}</script><p>$s_i$ 被称为<strong>稀疏度</strong>（<code>sparsity</code>），$q_i$ 被称为的 $\boldsymbol{\varphi}_i$ <strong>质量</strong>（<code>quality</code>），并且 $s_i$ 的值相对于 $q_i$ 的值较⼤意味着基函数 $\boldsymbol{\varphi}_i$ 更可能被模型剪枝掉。“稀疏度”度量了基函数 $\boldsymbol{\varphi}_i$ 与 模型中其他基函数重叠的程度，“质量”度量了基向量 $\boldsymbol{\varphi}_i$ 与误差向量之间的对齐程度，其中误差向量是训练值 $\mathbf{t}=(t_1,\dots,t_N)^T$ 与会导致 $\boldsymbol{\varphi}_i$ 从模型中被删除掉的预测向量 $\mathbf{y}_{-i}$ 之间的差值（<code>Tipping and Faul</code>, 2003）。</p><p>在边缘似然函数关于 $\alpha_i$ 的驻点处，导数</p><script type="math/tex; mode=display">\frac{\mathrm{d}\lambda(\alpha_i)}{\mathrm{d}\alpha_i}=\frac{\alpha_i^{-1}s_i^2-(q_i^2-s_i)}{2(\alpha_i+s_i)^{2}}\tag{7.43}</script><p>等于零。有两种可能形式的解，$\alpha_i\ge 0$ ，如果 $q_i^2 &lt; s_i$ ，那么 $\alpha_i\to\infty$ 提供了⼀个解。相反，如果 $q_i^2 &gt; s_i$ ，可以解出 $\alpha_i$ ，得</p><script type="math/tex; mode=display">\alpha_i=\frac{s_i^2}{q_i^2-s_i}\tag{7.44}</script><p>如图7.13～7.14，对数边缘似然 $\lambda(\alpha_i)$ 与 $\ln\alpha_i$ 的图像。图7.13中，单⼀的最⼤值出现在有限的 $\alpha_i$ 处，此时 $q_i^2=4$ 且 $s_i=1$（从⽽ $q_i^2 &gt; s_i$ ）。图7.14中，最⼤值位于 $\alpha_i=\infty$ 的位置，此时 $q_i^2=1$ 且 $s_i=2$（从⽽ $q_i^2 &lt; s_i$ ）。</p><p><img src="/images/prml_20191024002345.png" alt="对数边缘似然q与s_1"></p><p><img src="/images/prml_20191024002352.png" alt="对数边缘似然q与s_2"></p><p>最终的顺序稀疏贝叶斯学习算法描述：</p><blockquote><p>1）如果求解回归问题，初始化 $\beta$  。</p><p>2）使⽤⼀个基函数 $\boldsymbol{\varphi}_1$ 进⾏初始化，⽤公式(7.44)确定超参数 $\alpha_1$ ，其余的 $j \ne 1$ 的超参数 $\alpha_j$ 被初始化为⽆穷⼤，从⽽只有 $\boldsymbol{\varphi}_1$ 被包含在模型中。</p><p>3）对于所有基函数，计算 $\boldsymbol{\Sigma}$ 和 $\boldsymbol{m}$ ，以及 $q_i$ 和 $s_i$ 。<br>4）选择⼀个候选的基函数 $\boldsymbol{\varphi}_i$ 。</p><p>5.1）如果 $q_i&gt;s_i$ 且 $\alpha_i&lt;\infty$ ，从⽽基向量 $\boldsymbol{\varphi}_i$ 已经被包含在了模型中，那么使⽤公式(7.44)更新 $\alpha_i$ 。</p><p>5.2）如果 $q_i&gt;s_i$ 且 $\alpha_i=\infty$ ，那么将 $\boldsymbol{\varphi}_i$ 添加到模型中，使⽤公式(7.44)计算 $\alpha_i$ 。</p><p>5.3） 如果 $q_i\le s_i$ 且 $\alpha_i&lt;\infty$ ，那么从模型中删除基函数 $\boldsymbol{\varphi}_i$ ，令 $\alpha_i=\infty$ 。</p><p>6）如果求解回归问题，更新 $\beta$ 。</p><p>7）如果收敛，则算法终⽌，否则回到第3）步。</p></blockquote><p>在实际应⽤中，</p><script type="math/tex; mode=display">S_i=\boldsymbol{\varphi}_i^T\boldsymbol{C}^{-1}\boldsymbol{\varphi}_i\\Q_i=\boldsymbol{\varphi}_i^T\boldsymbol{C}^{-1}\mathbf{t}</script><p>质量和稀疏性变量可以表⽰为</p><script type="math/tex; mode=display">q_i=\frac{\alpha_iQ_i}{a_i-S_i}\\s_i=\frac{\alpha_iS_i}{a_i-S_i}\tag{7.45}</script><p>当 $\alpha_i=\infty$ 时，有 $q_i=Q_i$ 以及 $s_i=S_i$ ，有</p><script type="math/tex; mode=display">Q_i=\beta\boldsymbol{\varphi}_i^{T}\mathbf{t}-\beta^{2}\boldsymbol{\varphi}_i^{T}\boldsymbol{\Phi}\boldsymbol{\Sigma}\boldsymbol{\Phi}^{T}\mathbf{t}\\S_i=\beta\boldsymbol{\varphi}_i^{T}\boldsymbol{\varphi}_i-\beta^{2}\boldsymbol{\varphi}_i^{T}\boldsymbol{\Phi}\boldsymbol{\Sigma}\boldsymbol{\Phi}^{T}\boldsymbol{\varphi}_i\tag{7.46}</script><p>其中 $\boldsymbol{\Phi}$ 和 $\boldsymbol{\Sigma}$ 只涉及到对应于有限的超参数 $\alpha_i$ 的基向量。在每个阶段，需要的计算量为 $O(M^3)$ ，其中 $M$ 是模型中激活的基向量的数量，通常⽐训练模式的数量 $N$ 要⼩得多。</p><h1 id="三，RVM-⽤于分类"><a href="#三，RVM-⽤于分类" class="headerlink" title="三，RVM ⽤于分类"></a>三，<strong><code>RVM</code></strong> ⽤于分类</h1><p>考虑⼆分类问题，⽬标变量是⼆值变量 $t\in\{0,1\}$ 。这个模型现在的形式为基函数的线性组合经过 <code>logistic sigmoid</code>函数的变换，即</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})=\sigma(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}))\tag{7.47}</script><p>在 <strong><code>RVM</code></strong> 中， 模型使⽤的是 <strong><code>ARD</code>先验</strong> (7.30)，其中每个权值参数有⼀个独⽴的精度超参数。</p><p>⾸先，初始化超参数向量 $\boldsymbol{\alpha}$ 。对于这个给定的 $\boldsymbol{\alpha}$ 值，对其后验概率建⽴⼀个⾼斯近似，从⽽得到了对边缘似然的⼀个近似。这个近似后的边缘似然函数的最⼤化就引出了对 $\boldsymbol{\alpha}$ 值的重新估计，并且这个过程不断重复，直到收敛。</p><p>对于固定的 $\boldsymbol{\alpha}$ 值，$\boldsymbol{w}$ 的后验概率分布的众数可以通过最⼤化下式得到</p><script type="math/tex; mode=display">\begin{aligned}\ln p(\boldsymbol{w}|\mathbf{t},\boldsymbol{\alpha})&=\ln\{p(\mathbf{t}|\boldsymbol{w})p(\boldsymbol{w}|\boldsymbol{\alpha})\}-\ln p(\mathbf{t}|\boldsymbol{\alpha})\\&=\sum_{n=1}^{N}\{t_n\ln{y_n}+(1-t_n)\ln(1-y_n)\}-\frac{1}{2}\boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}+常数\end{aligned}\tag{7.48}</script><p>其中 $\boldsymbol{A}=\text{diag}(\alpha_i)$ 。最⼤化可以使⽤迭代重加权最⼩平⽅（<code>IRLS</code>）⽅法完成。对于这个算法，需要求出对数后验概率分布的梯度向量和<code>Hessian</code>矩阵，分别为</p><script type="math/tex; mode=display">\nabla\ln{p(\boldsymbol{w}|\mathbf{t},\boldsymbol{\alpha})}=\boldsymbol{\Phi}^{T}(\mathbf{t}-\mathbf{y})-\boldsymbol{A}\boldsymbol{w}\tag{7.49}</script><script type="math/tex; mode=display">\nabla\nabla\ln{p(\boldsymbol{w}|\mathbf{t},\boldsymbol{\alpha})}=-(\boldsymbol{\Phi}^{T}\boldsymbol{B}\boldsymbol{\Phi}+\boldsymbol{A})\tag{7.50}</script><p>其中 $\boldsymbol{B}$ 是⼀个 $N \times N$ 的对角矩阵，元素为 $b_n=y_n (1−y_n)$ 。向量 $\mathbf{y}=(y_1,\dots,y_N)^T$ ，矩阵 $\boldsymbol{\Phi}$ 是设计矩阵，元素为 $\Phi_{ni} = \phi_i(\boldsymbol{x}_n)$ 。在<code>IRLS</code>算法收敛的位置，负<code>Hessian</code>矩阵表⽰后验概率分布的⾼斯近似的协⽅差矩阵的逆矩阵。后验概率的⾼斯近似的众数，对应于⾼斯近似的均值，得到的拉普拉斯近似的均值和⽅差的形式为</p><script type="math/tex; mode=display">\boldsymbol{w}^{*}=\boldsymbol{A}^{-1}\boldsymbol{\Phi}^{T}(\mathbf{t}-\mathbf{y})\\\boldsymbol{\Sigma}=(\boldsymbol{\Phi}^{T}\boldsymbol{B}\boldsymbol{\Phi}+\boldsymbol{A})^{-1}</script><p>现在使⽤这个拉普拉斯近似来计算边缘似然函数，有</p><script type="math/tex; mode=display">\begin{aligned}p(\mathbf{t}|\boldsymbol{\alpha})&=\int{p(\mathbf{t}|\boldsymbol{w})p(\boldsymbol{w}|\boldsymbol{\alpha})}\mathrm{d}\boldsymbol{w}\\&\simeq{p(\mathbf{t}|\boldsymbol{w}^{*})p(\boldsymbol{w}^{*}|\boldsymbol{\alpha})(2\pi)^{\frac{M}{2}}|\boldsymbol{\Sigma}|^{\frac{1}{2}}}\end{aligned}\tag{7.51}</script><p>令边缘似然函数关于 $\alpha_i$ 的导数等于零，有</p><script type="math/tex; mode=display">-\frac{1}{2}(w^{*})^{2}+\frac{1}{2\alpha_i}-\frac{1}{2}\Sigma_{ii}=0</script><p>定义 $\gamma_i=1−\alpha_i\Sigma_{ii}$ ，整理，可得</p><script type="math/tex; mode=display">a_{i}^{新}=\frac{\gamma_i}{(w_i^{*})^2}\tag{7.52}</script><p>如果定义</p><script type="math/tex; mode=display">\hat{\mathbf{t}}=\boldsymbol{\Phi}\boldsymbol{w^{*}}+\boldsymbol{B}^{-1}(\mathbf{t}-\boldsymbol{y})\tag{7.53}</script><p>那么可以将近似对数边缘似然函数写成</p><script type="math/tex; mode=display">\ln p(\mathbf{t}|\boldsymbol{\alpha})=-\frac{1}{2}\{N\ln(2\pi)+\ln|\boldsymbol{C}|+(\hat{\mathbf{t}})^{T}\boldsymbol{C}^{-1}\hat{\mathbf{t}}\}\tag{7.54}</script><p>其中</p><script type="math/tex; mode=display">\boldsymbol{C}=\boldsymbol{B}+\boldsymbol{\Phi}\boldsymbol{A}\boldsymbol{\Phi}^{T}</script><p>如图7.15～7.16，相关向量机应⽤于⼈⼯数据集的说明。图7.15给出了决策边界和数据点，相关向量⽤圆圈标记出。图7.16画出了由 <strong><code>RVM</code></strong> 给出的后验概率分布，其中红⾊（蓝⾊）所占的⽐重表⽰数据点属于红⾊（蓝⾊）类别的概率。</p><p><img src="/images/prml_20191024092656.png" alt="相关向量机应⽤于⼈⼯数据集的说明1"></p><p><img src="/images/prml_20191024092703.png" alt="相关向量机应⽤于⼈⼯数据集的说明2"></p><p>对于 $K &gt; 2$ 个类别的情形，使⽤相关概率⽅法，有 $K$ 个线性模型，形式为</p><script type="math/tex; mode=display">a_k=\boldsymbol{w}_k^T\boldsymbol{x}\tag{7.55}</script><p>模型使⽤<code>softmax</code>函数进⾏组合</p><script type="math/tex; mode=display">y_k(\boldsymbol{x})=\frac{\exp(a_k)}{\sum_{j}\exp(a_j)}\tag{7.56}</script><p>对数似然函数为</p><script type="math/tex; mode=display">\ln{p(\boldsymbol{T}|\boldsymbol{w}_1,\dots,\boldsymbol{w}_k)}=\prod_{n=1}^{N}\prod_{k=1}^{K}y_{nk}^{t_{nk}}\tag{7.57}</script><p>其中，对于每个数据点 $n$，$t_{nk}$ 的表⽰⽅式是“<code>1-of-K</code>”的形式，$\boldsymbol{T}$ 是⼀个矩阵，元素为 $t_{nk}$ 。</p><p>相关向量机的主要缺点是，与 <strong><code>SVM</code></strong> 相⽐，训练时间相对较长。但是，<strong><code>RVM</code></strong> 避免了通过交叉验证确定模型复杂度的过程，从⽽补偿了训练时间的劣势。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，⽤于回归的-RVM&quot;&gt;&lt;a href=&quot;#一，⽤于回归的-RVM&quot; class=&quot;headerlink&quot; title=&quot;一，⽤于回归的 RVM&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】最大边缘分类器</title>
    <link href="https://zhangbc.github.io/2019/10/24/prml_07_01/"/>
    <id>https://zhangbc.github.io/2019/10/24/prml_07_01/</id>
    <published>2019-10-24T11:13:37.000Z</published>
    <updated>2019-10-24T14:01:18.335Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，最大边缘分类器"><a href="#一，最大边缘分类器" class="headerlink" title="一，最大边缘分类器"></a>一，最大边缘分类器</h1><p>考察线性模型的⼆分类问题，线性模型的形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x})+b\tag{7.1}</script><p>其中 $\boldsymbol{\phi}(\boldsymbol{x})$ 表⽰⼀个固定的特征空间变换，并且显式地写出了偏置参数 $b$ 。训练数据集由 $N$ 个输⼊向量 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ 组成，对应的⽬标值为 $t_1,\dots,t_N$ ，其中 $t_n\in\{−1, 1\}$ ， 新的数据点 $\boldsymbol{x}$ 根据 $y(\boldsymbol{x})$ 的符号进⾏分类。</p><p>现阶段，假设训练数据集在特征空间中是线性可分的，即根据定义，存在⾄少⼀个参数 $\boldsymbol{w}$ 和 $b$ 的选择⽅式，使得对于 $t_n = +1$ 的点，函数(7.1)都满⾜ $y(\boldsymbol{x}_n)&gt;0$ ，对于 $t_n = −1$ 的点，都有 $y(\boldsymbol{x}_n)<0$ ，从⽽对于所有训练数据点，都有 $t_ny(\boldsymbol{x}_n)>0$ 。</0$></p><p>如果有多个能够精确分类训练数据点的解，那么应该尝试寻找泛化错误最⼩的那个解。 <strong>⽀持向量机</strong>解决这个问题的⽅法是：引⼊<strong>边缘（<code>margin</code>）</strong>的概念，这个概念被<strong>定义</strong>为决策边界与任意样本之间的最⼩距离，如图7.1所⽰。</p><p><img src="/images/prml_20191023102328.png" alt="边缘"></p><p>如图7.2，最⼤化边缘会⽣成对决策边界的⼀个特定的选择，这个决策边界的位置由数据点的⼀个⼦集确定，被称为<strong>⽀持向量</strong>，⽤圆圈表⽰。</p><p><img src="/images/prml_20191023102337.png" alt="⽀持向量"></p><p>在⽀持向量机中，决策边界被选为使边缘最⼤化的那个决策边界。</p><p>点 $\boldsymbol{x}$ 距离由 $y(\boldsymbol{x})=0$ 定义的超平⾯的垂直距离为 $\frac{|y(\boldsymbol{x})|}{||\boldsymbol{w}||}$ ，其中 $y(\boldsymbol{x})$ 的函数形式由公式(7.1)给出，我们感兴趣的是那些能够正确分类所有数据点的解，即对于所有的 $n$ 都有 $t_ny(\boldsymbol{x}_n)&gt;0$ ，因此点 $\boldsymbol{x}_n$ 距离决策⾯的距离为</p><script type="math/tex; mode=display">\frac{t_ny(\boldsymbol{x}_n)}{\|\boldsymbol{w}\|}=\frac{t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)}{\|\boldsymbol{w}\|}\tag{7.2}</script><p>边缘由数据集⾥垂直距离最近的点 $\boldsymbol{x}_n$ 给出，希望最优化参数 $\boldsymbol{w}$ 和 $b$ ，使得这个距离能够最⼤化。因此最⼤边缘解可以通过下式得到：</p><script type="math/tex; mode=display">\underset{\boldsymbol{w}, b}{\arg \max}\left\{\frac{1}{\|\boldsymbol{w}\|} \min _{n}\left[t_{n}\left(\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)+b\right)\right]\right\}\tag{7.3}</script><p>注意到如果进⾏重新标度 $\boldsymbol{w}\to\kappa\boldsymbol{w}$ 以及 $b\to\kappa{b}$ ， 那么任意点 $\boldsymbol{x}_n$ 距离决策⾯的距离 $\frac{t_ny(\boldsymbol{x}_n)}{||\boldsymbol{w}||}$ 不会发⽣改变。利用这个性质，对于距离决策⾯最近的点，令</p><script type="math/tex; mode=display">t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)=1</script><p>在这种情况下，所有的数据点会满⾜限制</p><script type="math/tex; mode=display">t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)\ge1,    n=1\dots,N</script><p>这被称为<strong>决策超平⾯的标准表⽰</strong>。 对于使上式取得等号的数据点，我们说<strong>限制被激活</strong>（<code>active</code>），对于其他的数据点，我们说<strong>限制未激活</strong>（<code>inactive</code>）。根据定义，总会存在⾄少⼀个激活限制，因为总会有⼀个距离最近的点，并且⼀旦边缘被最⼤化，会有⾄少两个激活的限制。这样，最优化问题就简化为了最⼤化 $||\boldsymbol{w}||^{-1}$ ，这等价于最⼩化 $||\boldsymbol{w}||^2$ ，因此我们要在上述限制条件下，求解最优化问题</p><script type="math/tex; mode=display">\underset{\boldsymbol{w},b}{\arg\min}\frac{1}{2}\|\boldsymbol{w}\|^{2}</script><p>为了解决这个限制的最优化问题，引⼊拉格朗⽇乘数 $a_n\ge0$ 。每个限制条件都对应着⼀个乘数 $a_n$ ，从⽽可得下⾯的拉格朗⽇函数</p><script type="math/tex; mode=display">L(\boldsymbol{w},b,\boldsymbol{a})=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{n=1}^{N}a_n\{t_n(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)+b)-1\}\tag{7.4}</script><p>其中 $\boldsymbol{a} = (a_1,\dots,a_N)^{T}$ 。令 $L(\boldsymbol{w},b,\boldsymbol{a})$ 关于 $\boldsymbol{w}$ 和 $b$ 的导数等于零，有</p><script type="math/tex; mode=display">\boldsymbol{w}=\sum_{n=1}^{N}a_nt_n\boldsymbol{\phi})(\boldsymbol{x}_n)\\\sum_{n=1}^{N}a_nt_n=0</script><p>使⽤这两个条件从 $L(\boldsymbol{w},b,\boldsymbol{a})$ 中消去 $\boldsymbol{a}$ 和 $b$ ，就得到了最⼤化边缘问题的对偶表⽰（<code>dual representation</code>），其中要关于 $\boldsymbol{a}$ 最⼤化</p><script type="math/tex; mode=display">\tilde{L}(\boldsymbol{a})=\sum_{n=1}^{N}a_n-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_na_mt_nt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\tag{7.5}</script><p>其中 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})$，限制条件为</p><script type="math/tex; mode=display">a_n\ge0,n=1\dots,N\\\sum_{n=1}^{N}a_nt_n=0</script><p>通过使⽤公式消去 $\boldsymbol{w}$ ，$y(\boldsymbol{x})$ 可以根据参数 $\{a_n\}$ 和核函数表⽰，即</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\sum_{n=1}^{N}a_nt_nk(\boldsymbol{x},\boldsymbol{x}_n)+b\tag{7.6}</script><p>满足如下性质：</p><script type="math/tex; mode=display">a_n\ge0\\t_ny(\boldsymbol{x}_n)-1\ge0\\a_n\{t_ny(\boldsymbol{x}_n)-1\}=0</script><p>因此对于每个数据点，要么 $a_n = 0$ ，要么 $t_n y(\boldsymbol{x}_n) = 1$ 。任何使得 $a_n = 0$ 的数据点都不会出现在公式(7.5)的求和式中，因此对新数据点的预测没有作⽤。剩下的数据点被称为<strong>⽀持向量</strong>（<code>support vector</code>）。</p><p>解决了⼆次规划问题，找到了 $\boldsymbol{a}$ 的值之后，注意到⽀持向量  $\boldsymbol{x}_n$ 满⾜ $t_ny(\boldsymbol{x}_n)=1$，就可以确定阈值参数 $b$ 的值，可得</p><script type="math/tex; mode=display">t_n\left(\sum_{m\in{\mathcal{S}}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)+b\right)=1\tag{7.7}</script><p>其中 $\mathcal{S}$ 表⽰⽀持向量的下标集合。 ⾸先乘以 $t_n$ ，使⽤ $t_n^2=1$ 的性质，然后对于所有的⽀持向量，整理⽅程，解出 $b$ ，可得</p><script type="math/tex; mode=display">b=\frac{1}{N_\mathcal{S}}\sum_{n\in{\mathcal{S}}}\left(t_n-\sum_{m\in{\mathcal{S}}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\right)\tag{7.8}</script><p>其中 $N_\mathcal{S}$ 是⽀持向量的总数。</p><p> 对于接下来的模型⽐较，可以将最⼤边缘分类器⽤带有简单⼆次正则化项的最⼩化误差函数表⽰，形式为</p><script type="math/tex; mode=display">\sum_{n=1}^{N}E_{\propto}(y(\boldsymbol{x}_n)t_n-1)+\lambda\|\boldsymbol{w}\|^{2}</script><p>其中$E_{\infty}(z)$ 是⼀个函数，当 $z\ge0$ 时，函数值为零，其他情况下函数值为 $\infty$ 。</p><p>如图7.3，⼆维空间中来⾃两个类别的⼈⼯⽣成数据的例⼦。图中画出了具有⾼斯核函数的⽀持向量机的得到的常数 $y(\boldsymbol{x}_n)$ 的轮廓线。同时给出的时决策边界、边缘边界以及⽀持向量。</p><p><img src="/images/prml_20191024104707.png" alt="具有⾼斯核函数的⽀持向量机"></p><h1 id="二，重叠类分布"><a href="#二，重叠类分布" class="headerlink" title="二，重叠类分布"></a>二，重叠类分布</h1><p>在实际中，类条件分布可能重叠，这种情况下对训练数据的精确划分会导致较差的泛化能⼒。</p><p>引⼊<strong>松弛变量</strong>（<code>slack variable</code>）$\xi_n\ge 0$ ，其中 $n = 1,\dots, N$ ，每个训练数据点都有⼀个<strong>松弛变量</strong>（<code>Bennett</code>, 1992; <code>Cortes and Vapnik</code>, 1995）。对于位于正确的边缘边界内部的点或者边界上的点，$\xi_n=0$ ，对于其他点，$\xi_n=|t_n−y(\boldsymbol{x}_n)|$ 。 因此，对于位于决策边界 $y(\boldsymbol{x}n)=0$ 上的点，$\xi_n=1$ ，并且 $\xi_n&gt;1$ 的点就是被误分类的点。 从而分类的限制条件为</p><script type="math/tex; mode=display">t_ny(\boldsymbol{x}_n)\ge1-\xi_n,n=1,\dots,N</script><p>其中松弛变量被限制为满⾜ $\xi_n \ge 0$ 。$\xi_n = 0$ 的数据点被正确分类，要么位于边缘上，要么在边缘的正确⼀侧。$0 &lt; \xi_n\le 1$ 的点位于边缘内部，但是在决策边界的正确⼀侧。$\xi_n &gt; 1$ 的点位于决策边界的错误⼀侧，是被错误分类的点。这种⽅法有时被描述成放宽边缘的硬限制，得到⼀个<strong>软边缘</strong>（<code>soft margin</code>），并且允许⼀些训练数据点被错分。</p><p>如图7.4，松弛变量 $\xi_n \ge 0$ 的说明。圆圈标记的数据点是⽀持向量。</p><p><img src="/images/prml_20191023105522.png" alt="松弛变量"></p><p> 现在的⽬标是最⼤化边缘，同时以⼀种⽐较柔和的⽅式惩罚位于边缘边界错误⼀侧的点。于是最⼩化</p><script type="math/tex; mode=display">C\sum_{n=1}^{N}\xi_n+\frac{1}{2}\|\boldsymbol{w}_n\|^{2}</script><p>其中参数 $C&gt;0$ 控制了松弛变量惩罚与边缘之间的折中。由于任何被误分类的数据点都有 $\xi_n&gt;1$ ，因此 $\sum_{n}\xi_n$ 是误分类数据点数量的上界。于是，参数 $C$ 类似于（作⽤相反的）正则化系数，因为它控制了最⼩化训练误差与模型复杂度之间的折中。</p><p>现在想要在限制条件以及 $\xi_n \ge 0$ 的条件下最⼩化式，对应的拉格朗⽇函数为</p><script type="math/tex; mode=display">L(\boldsymbol{w},b,\boldsymbol{\xi},\boldsymbol{a},\boldsymbol{\mu})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C\sum_{n=1}^{N}\xi_n-\sum_{n=1}^{N}a_n\{t_ny(\boldsymbol{x}_n)-1+\xi_n\}-\sum_{n=1}^{N}\mu_n\xi_n\tag{7.9}</script><p>其中 $\{a_n \ge 0\}$ 和 $\{\mu_n \ge 0\}$ 是拉格朗⽇乘数。对应的 <strong><code>KKT</code></strong> 条件为</p><script type="math/tex; mode=display">a_n\ge0\\t_ny(\boldsymbol{x}_n)-1+\xi_n\ge0\\a_n(t_ny(\boldsymbol{x}_n)-1+\xi_n)=0\\\mu_n\ge0\\\xi_n\ge0\\\mu_n\xi_n=0</script><p>其中 $n = 1,\dots, N$ 。</p><p>现在对 $\boldsymbol{w}$ , $b$ 和 $\{\xi_n\}$ 进⾏最优化，有</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial\boldsymbol{w}}=0\Rightarrow\boldsymbol{w}=\sum_{n=1}^{N}a_nt_n\boldsymbol{\phi}(\boldsymbol{x}_n)\\\frac{\partial{L}}{\partial{b}}=0\Rightarrow\sum_{n=1}^{N}a_nt_n=0\\\frac{\partial{L}}{\partial{\xi_n}}=0\Rightarrow{a_n}=C-\mu_n\tag{7.10}</script><p>从而，</p><script type="math/tex; mode=display">\tilde{L}(\boldsymbol{a})=\sum_{n=1}^{N}a_n-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_na_mt_nt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\tag{7.11}</script><p>关于对偶变量 $\{a_n\}$ 最⼤化公式(7.11)时必须要满⾜以下限制</p><script type="math/tex; mode=display">0\le{a_n}\le{C}\\\sum_{n=1}^{N}a_nt_n=0</script><p>其中 $n = 1,\dots, N$ 。 第一个公式被称为<strong>盒限制</strong>（<code>box constraint</code>）。</p><p>对于数据点的⼀个⼦集，有 $a_n = 0$ ，在这种情况下这些数据点对于预测模型没有贡献；剩余的数据点组成了⽀持向量。这些数据点满⾜ $a_n &gt; 0$ ，必须满⾜</p><script type="math/tex; mode=display">t_ny(\boldsymbol{x}_n)=1-\xi_n\tag{7.12}</script><p>如果 $a_n &lt; C$ ，那么 $\mu_n &gt; 0$ ，则有 $\xi_n = 0$ ，从⽽这些点位于边缘上；$a_n = C$ 的点位于边缘内部，并且如果 $\xi_n \le 1$ 则被正确分类，如果 $\xi_n &gt; 1$ 则分类错误。</p><p> 为确定公式(7.1)中的参数 $b$ ，注意到 $0&lt;a_n&lt;C$ 的⽀持向量满⾜ $\xi_n = 0$ 即 $t_ny(\boldsymbol{x}_n)=1$ ，因此就满⾜</p><script type="math/tex; mode=display">t_n\left(\sum_{m\in{S}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)+b\right)=1\tag{7.13}</script><p>通过求平均的⽅式得</p><script type="math/tex; mode=display">b=\frac{1}{N_\mathcal{M}}\sum_{n\in{\mathcal{M}}}\left(t_n-\sum_{m\in{\mathcal{S}}}a_mt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\right)\tag{7.14}</script><p>其中 $\mathcal{M}$ 表⽰满⾜ $0 &lt; a_n &lt; C$ 的数据点的下标的集合。</p><p>⽀持向量机的另⼀种等价形式， 被称为 $\nu-SVM$，由<code>Schölkopf et al.</code>（2000）提出。它涉及到最⼩化</p><script type="math/tex; mode=display">\tilde{L}(\boldsymbol{a})=-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_na_mt_nt_mk(\boldsymbol{x}_n,\boldsymbol{x}_m)\tag{7.15}</script><p>限制条件为</p><script type="math/tex; mode=display">0\le{a_n}\le\frac{1}{N}\\\sum_{n=1}^{N}a_nt_n=0\\\sum_{n=1}^{N}a_n\ge\nu</script><p>这种⽅法的<strong>优点</strong>是，参数 $\nu$ 代替了参数 $C$ ，它既可以被看做<strong>边缘错误</strong>（<code>margin error</code>）（$\xi_n &gt; 0$ 的点，因此就是位于边缘边界错误⼀侧的数据点，它可能被误分类也可能没被误分类）的上界， 也可以被看做⽀持向量⽐例的下界。</p><p>如图7.5，$\nu-SVM$ 应⽤于⼆维不可分数据集的例⼦，圆圈表⽰⽀持向量。这⾥使⽤了形如 $\exp(-\gamma| \boldsymbol{x}−\boldsymbol{x}^{\prime}|^{2})$ 的⾼斯核，且 $\gamma = 0.45$ 。</p><p><img src="/images/prml_20191024113744.png" alt="ν-SVM"></p><p>⼀种最流⾏的训练⽀持向量机的⽅法被称为<strong>顺序最⼩化优化</strong>（<code>sequential minimal optimization</code>），或者称为  <strong><code>SMO</code></strong>（<code>Platt</code>, 1999），这种⽅法考虑了分块⽅法的极限情况，每次只考虑两个拉格朗⽇乘数。</p><p>考虑⼀个简单的⼆阶多项式核，⽤它的分量进⾏展开</p><script type="math/tex; mode=display">\begin{aligned}k(\boldsymbol{x},\boldsymbol{z})&=(1+\boldsymbol{x}^{T}\boldsymbol{z})^{2}\\&=(1+x_1z_1+x_2z_2)^{2}\\&=1+2x_1z_1+2x_2z_2+x_1^2z_1^2+2x_1z_1x_2z_2+x_2^2z_2^2\\&=(1,\sqrt{2}x_1,\sqrt{2}x_2,x_1^2,\sqrt{2}x_1x_2,x_2^2)(1,\sqrt{2}z_1,\sqrt{2}z_2,z_1^2,\sqrt{2}z_1z_2,z_2^2)^{T}\\&=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{z})\end{aligned}\tag{7.16}</script><p>于是这个核函数表⽰六维特征空间中的⼀个内积， 其中输⼊空间到特征空间的映射由向量函数 $\boldsymbol{\phi}(\boldsymbol{x})$ 描述，然⽽对这些特征加权的系数被限制为具体的形式。因此，原始⼆维空间  $\boldsymbol{x}$ 中的 任意点集都会被限制到这个六维特征空间中的⼆维⾮线性流形中。</p><p><strong>⽀持向量机不提供概率输出，⽽是对新的输⼊进⾏分类决策。</strong> <code>Veropoulos et al.</code>（1999）讨论了对 <strong><code>SVM</code></strong> 的修改，使其能控制假阳性和假阴性之间的折中。然⽽，如果希望把 <strong><code>SVM</code></strong> ⽤作较⼤的概率系统中的⼀个模块，那么需要对于新的输 ⼊ $\boldsymbol{x}$ 的类别标签 $t$ 的概率预测。为了解决这个问题，<code>Platt</code>（2000）提出了使⽤ <code>logistic sigmoid</code> 函数拟合训练过的⽀持向量机的输出的⽅法。具体来说，需要求解的条件概率被假设具有下⾯的形式</p><script type="math/tex; mode=display">p(t=1|\boldsymbol{x})=\sigma(Ay(\boldsymbol{x})+B)\tag{7.17}</script><p>其中 $y(\boldsymbol{x})$ 由公式(7.1)定义， 参数 $A$ 和 $B$ 的值通过最⼩化交叉熵误差函数的⽅式确定。 交叉熵误差函数根据由 $y(\boldsymbol{x}_n)$ 和 $t_n$ 组成的训练数据集定义。⽤于拟合<code>sigmoid</code>函数的数据需要独⽴于训练原始 <strong><code>SVM</code></strong> 的数据，为了避免严重的过拟合现象。</p><h1 id="三，与-logistic-回归的关系"><a href="#三，与-logistic-回归的关系" class="headerlink" title="三，与 logistic 回归的关系"></a>三，与 <strong><code>logistic</code></strong> 回归的关系</h1><p>对于位于边缘边界正确⼀侧的数据点，即满⾜ $y_nt_n\ge1$ 的数据点，有 $\xi_n = 0$ ，对于剩余的数据点，有 $\xi_n = 1 − y_nt_n$ 。因此⽬标函数可以写成（忽略整体的具有可乘性的常数）</p><script type="math/tex; mode=display">\sum_{n=1}^{N}E_{SV}(y_nt_n)+\lambda\|\boldsymbol{w}\|^{2}</script><p>其中 $\lambda = (2C)^{−1}$ ，$E_{SV} (·)$ 是<strong>铰链（<code>hinge</code>）误差函数</strong>，定义为</p><script type="math/tex; mode=display">E_{SV}(y_nt_n)=[1-y_nt_n]_{+}</script><p>其中 $[·]_+$ 表⽰正数部分。</p><p>如图7.6，⽀持向量机使⽤的“铰链”误差函数的图像，⽤蓝⾊表⽰。同时画出的还有<code>logistic</code>回归的误差函数，使⽤因⼦ $\frac{1}{\ln(2)}$ 重新放缩，从⽽通过点 $(0, 1)$ ，⽤红⾊表⽰，还画出了误分类误差函数（⿊⾊）和平⽅误差函数（绿⾊）。</p><p><img src="/images/prml_20191023112729.png" alt="铰链误差函数"></p><p>考虑<code>logistic</code>回归模型，发现⽐较⽅便的做法是对⽬标变量 $t\in\{0, 1\}$ 进⾏操作。为了与⽀持向量机进⾏对⽐，⾸先使⽤⽬标变量 $t\in\{−1,1\}$ 重写最⼤似然<code>logistic</code>回归函数。注意到 $p(t=1|y)=\sigma(y)$ ，其中 $y(\boldsymbol{x})$ 由公式(7.1)给出，$\sigma(y)$ 是的 <code>logistic sigmoid</code>函数。因此有 $p(t=−1|y)=1−\sigma(y)=\sigma(−y)$ ，从而</p><script type="math/tex; mode=display">p(t|y)=\sigma(yt)\tag{7.18}</script><p>从这个式⼦中可以通过对似然函数取负对数的⽅式构造⼀个误差函数。带有正则化项的误差函数的形式为</p><script type="math/tex; mode=display">\sum_{n=1}^{N}E_{LR}(y_nt_n)+\lambda\|\boldsymbol{w}\|^{2}</script><p>其中</p><script type="math/tex; mode=display">E_{LR}(y_nt_n)=\ln(1+\exp(-yt))</script><p><code>logistic</code>误差函数与铰链损失都可以看成对误分类误差函数的连续近似。有时⽤于解决分类问题的另⼀个连续近似的误差函数是平⽅和误差函数。但是，它具有下⾯的<strong>性质</strong>：它会着重强调那些被正确分类的在正确的⼀侧距离决策边界较远的点。</p><h1 id="四，多类-SVM"><a href="#四，多类-SVM" class="headerlink" title="四，多类 SVM"></a>四，多类 <strong><code>SVM</code></strong></h1><p>将多个两类 <strong><code>SVM</code></strong> 组合构造多类分类器的⼀种常⽤的⽅法（<code>Vapnik</code>, 1998）是构建 $K$ 个独⽴的 <strong><code>SVM</code></strong> ，其中第 $k$ 个模型 $y_k(\boldsymbol{x})$ 在训练时，使⽤来⾃类别 $\mathcal{C}_k$ 的数据作为正例，使⽤来⾃剩余的 $K − 1$ 个类别的数据作为负例。这被称为“<strong>1对剩余</strong>”（<code>one-versus-the-rest</code>）⽅法。然⽽使⽤独⽴的分类器进⾏决策会产⽣不相容的结果，其中⼀个输⼊会同时被分配到多个类别中，这个问题有时可以这样解决：对于新的输⼊ $\boldsymbol{x}$ ，使⽤下式做预测</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\underset{k}{\max}y_k(\boldsymbol{x})\tag{7.19}</script><p>不幸的是，这种启发式的⽅法会产⽣⼀个<strong>问题</strong>：不同的分类器是在不同的任务上进⾏训练的，⽆法保证不同分类器产⽣的实数值 $y_k(\boldsymbol{x})$ 具有恰当的标度。</p><p>“<strong>1对剩余</strong>”⽅法的另⼀个<strong>问题</strong>是训练集合不平衡。</p><p><code>Lee et al.</code>（2001）提出了“1对剩余”⽅法的⼀种变体。这种变体修改了⽬标值，使得正例类别的⽬标值为 $+1$，负例类别的⽬标值为 $−\frac{1}{K−1}$ 。</p><p><code>Weston and Watkins</code>（1999）定义了⼀个单⼀⽬标函数⽤来同时训练所有的 $K$ 个 <strong><code>SVM</code></strong> ，基于的是最⼤化每个类别与其余剩余类别的边缘。然⽽，这会导致训练过程变慢，因为这种⽅法需要求解的不是 $N$ 个数据点上的 $K$ 个独⽴的最优化问题（整体代价为 $O(KN^2)$），⽽是要求解⼀个规模为 $(K − 1)N$ 的单⼀的最优化问题，整体代价为 $O(K^2N^2)$ 。</p><p>另⼀种⽅法是在所有可能的类别对之间训练 $\frac{K(K−1)}{2}$ 个不同的⼆分类 <strong><code>SVM</code></strong> ，然后将测试数据点分到具有最⾼“投票数”的类别中去。这种⽅法有时被称为“<strong>1对1</strong>”（<code>one-versus-one</code>）。</p><p>后⼀个问题可以通过将每对分类器组织成有向⽆环图的⽅式解决，这就产⽣了 <strong><code>DAGSVM</code></strong> （<code>Platt et al.</code>, 2000）。对于 $K$ 个类别， <strong><code>DAGSVM</code></strong> 共有 $\frac{K(K−1)}{2}$ 个分类器。每次对新的测试点分类时，只需要 $K − 1$ 对分类器进⾏计算。选定的分类器是根据遍历图的路径确定的。</p><p><code>Dietterich and Bakiri</code>（1995）提出了⼀种不同的⽅法解决多分类问题。这种⽅法基于的是误差-修正输出编码，并且被<code>Allwein et al.</code>（2000）⽤到⽀持向量机中，这种⽅法可以被看做“1对1”投票⽅法的⼀个推⼴。这种⽅法中，⽤来训练各个分类器的类别划分的⽅式更加⼀般，$K$ 个类别本⾝被表⽰为选定的两类分类器产⽣的响应的集合。结合⼀套合适的解码⽅法，这种⽅法对于错误以及各个分类器的输出的歧义性具有鲁棒性。</p><h1 id="五，回归问题的-SVM"><a href="#五，回归问题的-SVM" class="headerlink" title="五，回归问题的 SVM"></a>五，回归问题的 <strong><code>SVM</code></strong></h1><p>在简单的线性回归模型中， 最⼩化⼀个正则化的误差函数</p><script type="math/tex; mode=display">\frac{1}{2}\sum_{n=1}^{N}\{y_n-t_n\}^{2}+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}</script><p>为了得到稀疏解，⼆次误差函数被替换为⼀个 $\epsilon$ <strong>-不敏感误差函数</strong>（<code>ϵ-insensitive error function</code>） （<code>Vapnik</code>, 1995）。如果预测 $y(\boldsymbol{x})$ 和⽬标 $t$ 之间的差的绝对值⼩于 $\epsilon$，那么这个误差函数给出的误差等于零，其中 $\epsilon &gt; 0$ 。 $\epsilon$ <strong>-不敏感误差函数</strong> 的⼀个简单的例⼦</p><script type="math/tex; mode=display">E_{\epsilon}(y(\boldsymbol{x})-t) =\begin{cases} 0,  & 如果|y(\boldsymbol{w})-t|<\epsilon \\|y(\boldsymbol{x})-t|-\epsilon, & 其他情况\end{cases}\tag{7.20}</script><p>它在不敏感区域之外，会有⼀个与误差相关联的线性代价。</p><p>如图7.7，$\epsilon$ -不敏感误差函数（红⾊）的图像。在不敏感区域之外，误差函数值随着距离线性增⼤。作为对⽐，同时给出了⼆次误差函数（绿⾊）。</p><p><img src="/images/prml_20191023113908.png" alt="不敏感误差函数"></p><p>于是最⼩化正则化的误差函数，形式为</p><script type="math/tex; mode=display">C\sum_{n=1}^{N}E_{\epsilon}(y(\boldsymbol{x})-t_n)+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}</script><p>其中 $y(\boldsymbol{x})$ 由公式(7.1)给出。</p><p>通过引⼊松弛变量的⽅式，可以重新表达最优化问题。对于每个数据点 $\boldsymbol{x}_n$ ，现在需要两个松弛变量 $\xi_n\ge0$ 和 $\hat{\xi}_n\ge0$ ， 其中 $\xi_n &gt; 0$ 对应于 $t_n &gt; y(\boldsymbol{x}_n)+\epsilon$ 的数据点，$\hat{\xi}&gt;0$ 对应于 $t_n &lt; y(\boldsymbol{x}_n)−\epsilon$ 的数据点。</p><p>如图7.8，<strong><code>SVM</code></strong> 回归的说明。图中画出了回归曲线以及 $\epsilon$-不敏感“管道”。同时给出的是松弛变量 $\xi$ 和 $\hat{\xi}$ 的例⼦。 对于 $\epsilon$ -管道上⽅的点，$\xi&gt;0$ 且 $\hat{\xi}=0$ ，对于 $\epsilon$ -管道下⽅的点，$\xi=0$ 且 $\hat{\xi}&gt;0$， 对于 $\epsilon$ -管道内部的点，$\xi=0$ 且 $\hat{\xi}=0$ 。</p><p><img src="/images/prml_20191023114837.png" alt="SVM回归的说明"></p><p>⽬标点位于 $\epsilon$-管道内的条件是 $y_n−\epsilon \le t_n \le y_n+\epsilon$ ，其中 $y_n=y(\boldsymbol{x}_n)$ 。引⼊松弛变量使得数据点能够位于管道之外，只要松弛变量不为零即可。对应的条件变为</p><script type="math/tex; mode=display">t_n\le y(\boldsymbol{x}_n)+\epsilon+\xi_n\\t_n\ge y(\boldsymbol{x}_n)-\epsilon-\hat{\xi}_n</script><p>⽀持向量回归的误差函数可以写成</p><script type="math/tex; mode=display">C\sum_{n=1}^{N}(\xi_n+\hat{\xi}_n)+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}</script><p>引⼊拉格朗⽇乘数 $a_n\ge0$ , $\hat{a}_n\ge0$ , $\mu_n\ge0$ 以及 $\hat{\mu}_n\ge0$ ，然后最优化拉格朗⽇函数</p><script type="math/tex; mode=display">\begin{aligned}L&=C\sum_{n=1}^{N}(\xi_n+\hat{\xi}_n)+\frac{\lambda}{2}\|\boldsymbol{w}\|^{2}-\sum_{n=1}^{N}(\mu_n\xi_n+\hat{\mu}_n\hat{\xi}_n)\\&-\sum_{n=1}^{N}a_n(\epsilon+\xi_n+y_n-t_n)-\sum_{n=1}^{N}\hat{a}_n(\epsilon+\hat{\xi}_n-y_n+t_n)\end{aligned}\tag{7.21}</script><p>令拉格朗⽇函数关于 $\boldsymbol{w}$ , $b$ , $\xi_n$ 和 $\hat{\xi}_n$ 的导数为零，有</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial\boldsymbol{w}}=0\Rightarrow\boldsymbol{w}=\sum_{n=1}^{N}(a_n-\hat{a}_n)\boldsymbol{\phi}(\boldsymbol{x}_n)\\\frac{\partial{L}}{\partial{b}}=0\Rightarrow\sum_{n=1}^{N}(a_n-\hat{a}_n)=0\\\frac{\partial{L}}{\partial{\xi_n}}=0\Rightarrow{a_n}=C-\mu_n\\\frac{\partial{L}}{\partial{\hat{\xi}_n}}=0\Rightarrow{\hat{a}_n}=C-\hat{\mu}_n</script><p>对偶问题涉及到关于 $\{a_n\}$ 和 $\{\hat{a}_n\}$ 最⼤化</p><script type="math/tex; mode=display">\begin{aligned}\tilde{L}(\boldsymbol{a},\hat{\boldsymbol{a}})&=-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(a_n-\hat{a}_n)(a_m-\hat{a}_m)k(\boldsymbol{x}_n,\boldsymbol{x}_m)\\&-\epsilon\sum_{n=1}^{N}(a_n+\hat{a}_n)+\sum_{n=1}^{N}(a_n-\hat{a}_n)t_n\end{aligned}\tag{7.22}</script><p>其中核 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})$ 。</p><p>从而盒限制</p><script type="math/tex; mode=display">0\le a_n \le C\\0\le \hat{a}_n \le C</script><p>对于新的输⼊变量，可以使⽤下式进⾏预测</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\sum_{n=1}^{N}(a_n-\hat{a}_n)k(\boldsymbol{x},\boldsymbol{x}_n)+b\tag{7.23}</script><p>对应的<code>Karush-Kuhn-Tucker</code>（<code>KKT</code>）条件说明了在解的位置，对偶变量与限制的乘积必须等于零，形式为</p><script type="math/tex; mode=display">a_n(\epsilon+\xi_n+y_n-t_n)=0\\\hat{a}_n(\epsilon+\hat{\xi}_n-y_n+t_n)=0\\(C-a_n)\xi_n=0\\(C-\hat{a}_n)\hat{\xi}_n=0</script><p>考虑⼀个数据点，满⾜ $0 &lt; a_n &lt; C$ 。根据公式，⼀定有$\xi_n=0$ ，$\epsilon + y_n − t_n = 0$ 。使⽤公式(7.1)，然后求解 $b$ ，有</p><script type="math/tex; mode=display">\begin{aligned}b&=t_n-\epsilon-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)\\&=t_n-\epsilon-\sum_{m=1}^{N}(a_m-\hat{a}_m)k(\boldsymbol{x}_n,\boldsymbol{x}_m)\end{aligned}\tag{7.24}</script><p>与分类问题的情形相同，有另⼀种⽤于回归的 <strong><code>SVM</code></strong> 的形式。这种形式的 <strong><code>SVM</code></strong> 中，控制复杂度的参数有⼀个更加直观的意义（<code>Schölkopf et al.</code>, 2000）。特别地，我们不固定不敏感区域 $\epsilon$ 的宽度，⽽是固定位于管道外部的数据点的⽐例 $\nu$ ，涉及到最⼤化</p><script type="math/tex; mode=display">\begin{aligned}\tilde{L}(\boldsymbol{a},\hat{\boldsymbol{a}})&=-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(a_n-\hat{a}_n)(a_m-\hat{a}_m)k(\boldsymbol{x}_n,\boldsymbol{x}_m)\\&+\sum_{n=1}^{N}(a_n-\hat{a}_n)t_n\end{aligned}\tag{7.25}</script><p>限制条件为</p><script type="math/tex; mode=display">0\le a_n \le \frac{C}{N}\\0\le \hat{a}_n \le \frac{C}{N}\\\sum_{n=1}^{N}(a_n-\hat{a}_n)=0\\\sum_{n=1}^{N}(a_n+\hat{a}_n)\le\nu C</script><p>如图7.9，$\nu-SVM$ 回归应⽤到⼈⼯⽣成的正弦数据集上的说明，<strong><code>SVM</code></strong> 使⽤了⾼斯核。预测分布曲线为红⾊曲线，$\epsilon$ -不敏感管道对应于阴影区域。此外，数据点⽤绿⾊表⽰，⽀持向量⽤蓝⾊圆圈标记。</p><p><img src="/images/prml_20191023201238.png" alt="SVM回归应⽤到⼈⼯⽣成的正弦数据集上的说明"></p><h1 id="六，计算学习理论"><a href="#六，计算学习理论" class="headerlink" title="六，计算学习理论"></a>六，计算学习理论</h1><p>历史上， ⽀持向量机⼤量地使⽤⼀个被称为<strong>计算学习理论</strong>（<code>computational learning theory</code>）的理论框架进⾏分析。这个框架有时候也被称为<strong>统计学习理论</strong>（<code>statistical learning theory</code>）（<code>Anthony and Biggs</code>, 1992; <code>Kearns and Vazirani</code>, 1994; <code>Vapnik</code>, 1995; <code>Vapnik</code>, 1998）。 这个框架起源于<code>Valiant</code>（1984），他建⽴了概率近似正确（<code>probably approximately correct</code>）或者称为 <strong><code>PAC</code></strong> 的学习框架。<strong><code>PAC</code></strong> 学习框架的⽬标是理解为两个给出较好的泛化能⼒，需要多⼤的数据集。</p><p>假设从联合概率分布 $p(\boldsymbol{x},\boldsymbol{t})$ 中抽取⼀个⼤⼩为 $N$ 的数据集 $\mathcal{D}$ ，其中 $\boldsymbol{x}$ 是输⼊变量，$\boldsymbol{t}$ 表⽰类别标签。我们把注意⼒集中于“⽆噪声”的情况，即类别标签由某个（未知的）判别函数 $\boldsymbol{t} = \boldsymbol{g}(\boldsymbol{x})$ 确定。在<code>PAC</code>学习中，空间 $\mathcal{F}$ 是⼀个以训练集 $\mathcal{D}$ 为基础的函数组成的空间，从空间 $\mathcal{F}$ 中抽取⼀个函数 $\boldsymbol{f}(\boldsymbol{x};\mathcal{D})$，如果它的期望错误率⼩于某个预先设定的阈值 $\epsilon$ ，即</p><script type="math/tex; mode=display">\mathbb{E}_{\boldsymbol{x},\boldsymbol{t}}[I(\boldsymbol{f}(\boldsymbol{x};\mathcal{D})\ne\boldsymbol{t})]<\epsilon\tag{7.26}</script><p>那么就说函数 $\boldsymbol{f}(\boldsymbol{x};\mathcal{D})$ 具有较好的泛化能⼒。 其中 $I(·)$ 是⽰性函数，期望是关于概率分布 $p(\boldsymbol{x},\boldsymbol{t})$ 的期望。 式⼦左侧的项是⼀个随机变量， 因为它依赖于训练数据集 $\mathcal{D}$ 。<strong><code>PAC</code></strong> 框架要求，对于从概率分布 $p(\boldsymbol{x},\boldsymbol{t})$ 中随机抽取的数据集 $\mathcal{D}$ ，公式(7.26)成⽴的概率要⼤于 $1−\delta$ 。 这 ⾥ $\delta$ 是另⼀个预先设定的参数，术语“<strong>概率近似正确</strong>”来⾃于下⾯的要求：以⼀个较⾼的概 率（⼤于 $1−\delta$ ）， 使得错误率较⼩（⼩于 $\epsilon$ ）。 对于⼀个给定的模型空间 $\mathcal{F}$ ， 以及给定的参数 $\epsilon$ 和 $\delta$ ， <strong><code>PAC</code></strong> 学习的⽬标是提供满⾜这个准则所需的最⼩数据集规模 $N$ 的界限。在 <strong><code>PAC</code></strong> 学习中， ⼀个<strong>关键的量</strong>是 <strong><code>Vapnik-Chervonenkis</code>维度</strong> （<code>Vapnik-Chervonenkis dimension</code>），或者被称为 <strong><code>VC</code>维度</strong> ，它提供了函数空间复杂度的⼀个度量，使得 <strong><code>PAC</code></strong> 框架能够扩展到包含⽆穷多个函数的空间。</p><p>⼀种提升 <strong><code>PAC</code></strong> 界限的紧致程度的⽅法是  <strong><code>PAC</code></strong> -贝叶斯框架（<code>PAC-Bayesian framework</code>）（<code>McAllester</code>, 2003）， 它考虑了空间 $\mathcal{F}$ 上的函数的概率分布情况， 有些类似于贝叶斯⽅法中的先验概率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，最大边缘分类器&quot;&gt;&lt;a href=&quot;#一，最大边缘分类器&quot; class=&quot;headerlink&quot; title=&quot;一，最大边缘分类器&quot;&gt;&lt;/a&gt;一，最
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】核方法</title>
    <link href="https://zhangbc.github.io/2019/10/22/prml_06/"/>
    <id>https://zhangbc.github.io/2019/10/22/prml_06/</id>
    <published>2019-10-22T14:41:02.000Z</published>
    <updated>2019-10-23T02:11:03.890Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，对偶表示"><a href="#一，对偶表示" class="headerlink" title="一，对偶表示"></a>一，对偶表示</h1><p>有这样⼀类模式识别的技术：训练数据点或者它的⼀个⼦集在预测阶段仍然保留并且被使⽤。许多线性参数模型可以被转化为⼀个等价的“<strong>对偶表⽰</strong>”。对偶表⽰中，预测的基础也是在训练数据点处计算的<strong>核函数</strong>（<code>kernel function</code>）的线性组合。对于基于固定⾮线性特征空间（<code>feature space</code>）映射 $\boldsymbol{\phi}(\boldsymbol{x})$ 的模型来说，核函数由下⾯的关系给出。</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{x^{\prime}})\tag{6.1}</script><p>通过考虑公式(6.1)中特征空间的恒等映射 $\boldsymbol{\phi}(\boldsymbol{x})=\boldsymbol{x}$ ，就得到了核函数的⼀个最简单的例⼦，此时 $k\boldsymbol{(x}, \boldsymbol{x}^{\prime}) =\boldsymbol{x}^{T}\boldsymbol{x}^{\prime}$ ，把这个称为<strong>线性核</strong>。</p><p>许多核函数只是参数的差值的函数，即 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})=k(\boldsymbol{x}−\boldsymbol{x}^{\prime})$ ，这被称为<strong>静⽌核</strong>（<code>stationary kernel</code>），因为核函数对于输⼊空间的平移具有不变性。另⼀种核函数是<strong>同质核</strong>（<code>homogeneous kernel</code>），也被称为<strong>径向基函数</strong>（<code>radial basis function</code>），它只依赖于参数之间的距离（通常是欧⼏⾥得距离）的⼤⼩，即 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})=k(|\boldsymbol{x}−\boldsymbol{x}^{\prime}|)$ 。</p><p>考虑⼀个线性模型，它的参数通过最⼩化正则化的平⽅和误差函数来确定，正则化的平⽅和误差函数为</p><script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}\{\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)-t_n\}^{2}+\frac{\lambda}{2}\boldsymbol{w}^{T}\boldsymbol{w}\tag{6.2}</script><p>其中 $\lambda\ge0$ 。如果令 $J(\boldsymbol{w})$ 关于 $\boldsymbol{w}$ 的梯度等于零，那么看到 $\boldsymbol{w}$ 的解是向量 $\boldsymbol{\phi}(\boldsymbol{x}_n)$ 的线性组合的形式，系数是 $\boldsymbol{w}$ 的函数，形式为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w}&=-\frac{1}{\lambda}\sum_{n=1}^{N}\{\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)-t_n\}\boldsymbol{\phi}(\boldsymbol{x}_n)\\&=\sum_{n=1}^{N}a_n\boldsymbol{\phi}(\boldsymbol{x}_n)=\boldsymbol{\Phi}^{T}\boldsymbol{a}\end{aligned}\tag{6.3}</script><p>其中 $\boldsymbol{\Phi}$ 是设计矩阵，第 $n$ ⾏为  $\boldsymbol{\phi}(\boldsymbol{x}_n)^{T}$ ，向量 $\boldsymbol{a}=(a_1,\dots,a_N)^{T}$ ，并且</p><script type="math/tex; mode=display">a_n=-\frac{1}{\lambda}\{\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x})-t_n\}</script><p>现在不直接对参数向量 $\boldsymbol{w}$ 进⾏操作，⽽是使⽤参数向量 $\boldsymbol{a}$ 重新整理最⼩平⽅算法，得到⼀个<strong>对偶表⽰</strong>（<code>dual representation</code>）。如果将 $\boldsymbol{w}=\boldsymbol{\Phi}^{T}\boldsymbol{a}$ 代⼊ $J(\boldsymbol{w})$ ，那么可以得到</p><script type="math/tex; mode=display">J(\boldsymbol{a})=\frac{1}{2}\boldsymbol{a}^{T}\boldsymbol{\Phi}\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\boldsymbol{\Phi}^{T}\boldsymbol{a}-\boldsymbol{a}^{T}\boldsymbol{\Phi}\boldsymbol{\Phi}^{T}\mathbf{t}+\frac{1}{2}\mathbf{t}^{T}\mathbf{t}+\frac{\lambda}{2}\boldsymbol{a}^{T}\boldsymbol{\Phi}\boldsymbol{\Phi}^{T}\boldsymbol{a}\tag{6.4}</script><p>其中 $\mathbf{t}=(t_1,\dots, t_N)^{T}$ 。现在定义 <strong><code>Gram</code>矩阵</strong> $\boldsymbol{K}=\boldsymbol{\Phi}\boldsymbol{\Phi}^{T}$ ，它是⼀个 $N \times N$ 的对称矩阵，元素为</p><script type="math/tex; mode=display">K_{nm}=\boldsymbol{\phi}(\boldsymbol{x}_n)^{T}\boldsymbol{\phi}(\boldsymbol{x}_m)=k(\boldsymbol{x}_n,\boldsymbol{x}_m)</script><p>使⽤<code>Gram</code>矩阵， 平⽅和误差函数可以写成</p><script type="math/tex; mode=display">J(\boldsymbol{a})=\frac{1}{2}\boldsymbol{a}^{T}\boldsymbol{K}\boldsymbol{K}\boldsymbol{a}-\boldsymbol{a}^{T}\boldsymbol{K}\mathbf{t}+\frac{1}{2}\mathbf{t}^{T}\mathbf{t}+\frac{\lambda}{2}\boldsymbol{a}^{T}\boldsymbol{K}\boldsymbol{a}\tag{6.5}</script><p>有，</p><script type="math/tex; mode=display">\boldsymbol{a}=(\boldsymbol{K}+\lambda\boldsymbol{I}_{N})^{-1}\mathbf{t}\tag{6.6}</script><p>如果将这个代⼊线性回归模型中，对于新的输⼊ $\boldsymbol{x}$ ，得到了下⾯预测</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x})=\boldsymbol{a}^{T}\boldsymbol{\Phi}\boldsymbol{\phi}(\boldsymbol{x})=\boldsymbol{k}(\boldsymbol{x})^{T}(\boldsymbol{K}+\lambda\boldsymbol{I}_N)^{-1}\mathbf{t}\tag{6.7}</script><p>其中定义了向量 $\boldsymbol{k}(\boldsymbol{x})$ ，它的元素为 $k_n(\boldsymbol{x})=k(\boldsymbol{x}_n,\boldsymbol{x})$ 。因此看到对偶公式使得最⼩平⽅ 问题的解完全通过核函数 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})$ 表⽰。这被称为<strong>对偶公式</strong>，因为 $\boldsymbol{a}$ 的解可以被表⽰为 $\boldsymbol{\phi}(\boldsymbol{x})$ 的线性组合，从⽽可以使⽤参数向量 $\boldsymbol{w}$ 恢复出原始的公式。</p><p>在对偶公式中，通过对⼀个 $N \times N$ 的矩阵求逆来确定参数向量 $\boldsymbol{a}$ ，⽽在原始参数空间公式中， 我们要对⼀个 $M \times M$ 的矩阵求逆来确定 $\boldsymbol{w}$ 。对偶公式的<strong>优点</strong>是，它可以完全通过核函数  $k(\boldsymbol{x},\boldsymbol{x}^{\prime})$ 来表⽰。于是可以直接针对核函数进⾏计算，避免了显式地引⼊特征向量 $\boldsymbol{\phi}(\boldsymbol{x})$ ，这使得可以隐式地使⽤⾼维特征空间，甚⾄⽆限维特征空间。</p><h1 id="二，构造核"><a href="#二，构造核" class="headerlink" title="二，构造核"></a>二，构造核</h1><p>为了利⽤核替换，需要能够构造合法的核函数。⼀种⽅法是选择⼀个特征空间映射 $\boldsymbol{\phi}(\boldsymbol{x})$ ，然后使⽤这个映射寻找对应的核。⼀维空间的核函数被定义为</p><script type="math/tex; mode=display">k(x,x^{\prime})=\boldsymbol{\phi}(x)^{T}\boldsymbol{\phi}（x^{\prime}=\sum_{i=1}^{M}\phi_{i}(x)\phi_i(x^{\prime})\tag{6.8}</script><p>其中 $\phi_i(x)$ 是基函数。</p><p>如图6.1～6.3，从对应的基函数集合构建核函数的例⼦。在每⼀图中，下部分给出了由公式(6.8)定义的核函数 $k(x,x^{\prime})$ ，它是 $x$ 的函数，$x^{\prime}$ 的值⽤红⾊叉号表⽰，⽽上部分给出了对应的基函数，分别是多项式基函数（图6.1）、⾼斯基函数（图6.2）、<code>logistic sigmoid</code>基函数（图6.3）。</p><p><img src="/images/prml_20191020154924.png" alt="多项式基函数"></p><p><img src="/images/prml_20191020154933.png" alt="⾼斯基函数"></p><p><img src="/images/prml_20191020154943.png" alt="logistic sigmoid基函数"></p><p>另⼀种⽅法是直接构造核函数。在这种情况下，必须确保核函数是合法的，即它对应于某个（可能是⽆穷维）特征空间的标量积。考虑下⾯的核函数</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{z})=(\boldsymbol{x}^{T}\boldsymbol{z})^{2}\tag{6.9}</script><p>如果取⼆维输⼊空间 $\boldsymbol{x}=(x_1,x_2)$ 的特殊情况，那么展开这⼀项，于是得到对应的⾮线性特征映射</p><script type="math/tex; mode=display">\begin{aligned}k(\boldsymbol{x},\boldsymbol{z})&=(\boldsymbol{x}^{T}\boldsymbol{z})^{2}\\&=(x_1z_1+x_2z_2)^{2}\\&=x_1^2z_1^2+2x_1z_1x_2z_2+x_2^2z_2^2\\&=(x_1^2,\sqrt{2}x_1x_2,x_2^2)(z_1^2,\sqrt{2}z_1z_2,z_2^2)^{T}\\&=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{\phi}(\boldsymbol{z})\end{aligned}</script><p>更⼀般地， 需要找到⼀种更简单的⽅法检验⼀个函数是否是⼀个合法的核函数，⽽不需要显⽰地构造函数 $\boldsymbol{\phi}(\boldsymbol{x})$ 。 核函数  $k(\boldsymbol{x},\boldsymbol{x}^{\prime})$ 是⼀个合法的核函数的<strong>充分必要条件</strong>是<code>Gram</code>矩阵（元素由 $k(\boldsymbol{x}_n,\boldsymbol{x}_m)$ 给出）在所有的集合 ${\boldsymbol{x}_n}$ 的选择下都是<strong>半正定的</strong>（<code>Shawe-Taylor and Cristianini</code>, 2004）。</p><p>构造新的核函数的⼀个强⼤的⽅法是使⽤简单的核函数作为基本的模块来构造。给定合法的核  $k_1(\boldsymbol{x},\boldsymbol{x}^{\prime})$ 和  $k_2(\boldsymbol{x},\boldsymbol{x}^{\prime})$ ，下⾯的新核也是合法的</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=ck_1(\boldsymbol{x},\boldsymbol{x}^{\prime})\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=f(\boldsymbol{x})k_1(\boldsymbol{x},\boldsymbol{x}^{\prime})f(\boldsymbol{x}^{\prime})\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=q(k_1(\boldsymbol{x},\boldsymbol{x}^{\prime}))\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\exp(k_1(\boldsymbol{x},\boldsymbol{x}^{\prime}))\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=k_1(\boldsymbol{x},\boldsymbol{x}^{\prime})+k_2(\boldsymbol{x},\boldsymbol{x}^{\prime})\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=k_1(\boldsymbol{x},\boldsymbol{x}^{\prime})k_2(\boldsymbol{x},\boldsymbol{x}^{\prime})\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=k_3(\boldsymbol{\phi}(\boldsymbol{x}),\boldsymbol{\phi}(\boldsymbol{x}^{\prime}))\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x}^{\prime}\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=k_a(\boldsymbol{x}_a,\boldsymbol{x}_a^{\prime})+k_b(\boldsymbol{x}_b,\boldsymbol{x}_b^{\prime})\\k(\boldsymbol{x},\boldsymbol{x}^{\prime})=k_a(\boldsymbol{x}_a,\boldsymbol{x}_a^{\prime})k_n(\boldsymbol{x}_n,\boldsymbol{x}_n^{\prime})\tag{*}\\</script><p>其中 $c&gt;0$ 是⼀个常数，$f(·)$ 是任意函数，$q(·)$ 是⼀个系数⾮负的多项式，$\boldsymbol{\phi}(\boldsymbol{x})$ 是⼀个从 $\boldsymbol{x}$ 到 $\mathbb{R}^{M}$ 的函数，$k_3(·, ·)$ 是 $\mathbb{R}^{M}$ 中的⼀个合法的核， $\boldsymbol{A}$ 是⼀个对称半正定矩阵， $\boldsymbol{x}_a$ 和 $\boldsymbol{x}_b$ 是变量（未必互斥），且 $\boldsymbol{x}=(\boldsymbol{x}_a, \boldsymbol{x}_b)$ 。$k_a$ 和 $k_b$ 是各⾃空间的合法的核函数。</p><p>另⼀个经常使⽤的核函数的形式为</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\exp\left(-\frac{\|\boldsymbol{x}-\boldsymbol{x}^{\prime}\|^{2}}{2\sigma^{2}}\right)\tag{6.10}</script><p>这个被称为<strong>⾼斯核</strong>。这是⼀个合法的核，把平⽅项展开</p><script type="math/tex; mode=display">\|\boldsymbol{x}-\boldsymbol{x}^{\prime}\|^{2}=\boldsymbol{x}^{T}\boldsymbol{x}+(\boldsymbol{x}^{\prime})^{T}\boldsymbol{x}^{\prime}-2\boldsymbol{x}^{T}\boldsymbol{x}^{\prime}</script><p>从而，</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\exp\left(-\frac{\boldsymbol{x}^{T}\boldsymbol{x}}{2\sigma^{2}}\right)\exp\left(\frac{\boldsymbol{x}^{T}\boldsymbol{x}^{\prime}}{\sigma^{2}}\right)\exp\left(-\frac{(\boldsymbol{x}^{\prime})^{T}\boldsymbol{x}^{\prime}}{2\sigma^{2}}\right)\tag{6.11}</script><p>⾼斯核并不局限于使⽤欧⼏⾥得距离。如果使⽤公式(6.10)的平方项展开式中的核替换，将 $\boldsymbol{x}^{T}\boldsymbol{x}^{\prime}$ 替换为⼀个⾮线性核 $\kappa(\boldsymbol{x},\boldsymbol{x}^{\prime})$ ，有</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\exp\left\{-\frac{1}{2\sigma^{2}}(\kappa(\boldsymbol{x},\boldsymbol{x})+\kappa(\boldsymbol{x}^{\prime},\boldsymbol{x}^{\prime})-2\kappa(\boldsymbol{x},\boldsymbol{x}^{\prime}))\right\}\tag{6.12}</script><p><strong>核观点</strong>的⼀个重要的<strong>贡献</strong>是可以扩展到符号化的输⼊，⽽不是简单的实数向量。</p><p>考虑⼀个固定的集合，定义⼀个⾮向量空间，这个空间由这个集合的所有可能的⼦集构成。 如果 $A_1$ 和 $A_2$ 是两个这样的⼦集，那么核的⼀个简单的选择可以是</p><script type="math/tex; mode=display">k(A_1,A_2)=2^{|A_1\cap{A_2}|}\tag{6.13}</script><p>其中 $A_1 \cap A_2$ 表⽰集合 $A_1$ 和 $A_2$ 的交集，$|A|$ 表⽰$A$ 的元素的数量。 </p><p>构造核的另⼀个强⼤的⽅法是从⼀个概率⽣成式模型开始构造（<code>Haussler</code>, 1999），这使得可以在⼀个判别式的框架中使⽤⽣成式模型。</p><p>给定⼀个⽣成式模型 $p(\boldsymbol{x})$ ，可以定义⼀个核</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=p(\boldsymbol{x})p(\boldsymbol{x}^{\prime})\tag{6.14}</script><p>对这类核进行扩展，扩展的⽅法是考虑不同概率分布的乘积的加和，带有正的权值系数 $p(i)$ ，形式为</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\sum_{i}p(\boldsymbol{x}|i)p(\boldsymbol{x}^{\prime}|i)p(i)\tag{6.15}</script><p>如果两个输⼊ $\boldsymbol{x}$ 和 $\boldsymbol{x}^{\prime}$ 在⼀⼤类的不同分量下都有较⼤的概率，那么这两个输⼊将会使核函数输出较⼤的值，因此就表现出<strong>相似性</strong>。在⽆限求和的极限情况下，也可以考虑下⾯形式的核函数</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\int{p(\boldsymbol{x}|\boldsymbol{z})}p(\boldsymbol{x}^{\prime}|\boldsymbol{z})p(\boldsymbol{z})\mathrm{d}\boldsymbol{z}\tag{6.16}</script><p>其中 $\boldsymbol{z}$ 是⼀个连续潜在变量。</p><p>现在假设数据由长度为 $L$ 的有序序列组成，即⼀个观测为 $\boldsymbol{X}=\{x_1,\dots,x_L\}$ 。对于这种序列， ⼀个流⾏的⽣成式模型是隐马尔科夫模型，它把概率 $p(\boldsymbol{X})$ 表⽰为对应的隐含状态序列 $\boldsymbol{Z}=\{z_1,\dots,z_L\}$ 上的积分或求和。可以使⽤这种⽅法定义⼀个核函数来度量两个序列 $\boldsymbol{X}$ 和 $\boldsymbol{X}^{\prime}$ 的相似度。定义核函数的⽅法是扩展混合表⽰，得到</p><script type="math/tex; mode=display">k(\boldsymbol{X},\boldsymbol{X}^{\prime})=\sum_{\boldsymbol{Z}}p(\boldsymbol{X}|\boldsymbol{Z})p(\boldsymbol{X}^{\prime}|\boldsymbol{Z})p(\boldsymbol{Z})\tag{6.17}</script><p>从⽽两个观测序列都通过相同的隐含序列 $\boldsymbol{Z}$ ⽣成。</p><p>另⼀个使⽤⽣成式模型定义核函数的⽅法被称为 <strong><code>Fisher</code>核</strong>（<code>Jaakkola and Haussler</code>, 1999）。考虑⼀个参数⽣成式模型 $p(\boldsymbol{x}|\boldsymbol{\theta})$ ，其中 $\boldsymbol{\theta}$ 表⽰参数的向量，⽬标是找到⼀个核，度量这个⽣成式模型的两个输⼊变量 $\boldsymbol{x}$ 和 $\boldsymbol{x}^{\prime}$ 之间的相似性。<code>Jaakkola and Haussler</code>（1999）考虑关于 $\boldsymbol{\theta}$ 的梯度，它定义了“特征”空间的⼀个向量，这个特征空间的维度与 $\boldsymbol{\theta}$ 的维度相同。特别地，考虑<code>Fisher</code>得分</p><script type="math/tex; mode=display">g(\boldsymbol{\theta},\boldsymbol{x})=\nabla_{\boldsymbol{\theta}}\ln{p(\boldsymbol{x}|\boldsymbol{\theta})}\tag{6.18}</script><p>根据<code>Fisher</code>得分，<strong><code>Fisher</code>核</strong> 被定义为</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x})^{T}\boldsymbol{F}^{-1}\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x}^{\prime})\tag{6.19}</script><p>其中， $\boldsymbol{F}$ 是 <strong><code>Fisher</code>信息矩阵</strong>（<code>Fisher information matrix</code>），定义为</p><script type="math/tex; mode=display">\boldsymbol{F}=\mathbb{E}_\boldsymbol{x}[\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x})\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x})^{T}]\tag{6.20}</script><p>其中，期望是在概率分布 $p(\boldsymbol{x}|\boldsymbol{\theta})$ 下关于 $\boldsymbol{x}$ 的期望。</p><p>在实际应⽤中，通常计算<code>Fisher</code>信息矩阵是不可⾏的。⼀种⽅法是把<code>Fisher</code>信息的定义中的期望替换为<strong>样本均值</strong>，可得</p><script type="math/tex; mode=display">\boldsymbol{F}\simeq\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x}_n)\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x}_n)^{T}\tag{6.21}</script><p>这是<code>Fisher</code>得分的协⽅差矩阵，因此<code>Fisher</code>核对应于这些分数的⼀个漂⽩。更简单地，可以省略<code>Fisher</code>信息矩阵，使⽤⾮不变核</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x})^{T}\boldsymbol{g}(\boldsymbol{\theta},\boldsymbol{x}^{\prime})\tag{6.22}</script><p>核函数的最后的⼀个例⼦是<code>sigmoid</code>核，定义为</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\tanh(a\boldsymbol{x}^{T}\boldsymbol{x}^{\prime}+b)\tag{6.23}</script><h1 id="三，径向基函数⽹络"><a href="#三，径向基函数⽹络" class="headerlink" title="三，径向基函数⽹络"></a>三，径向基函数⽹络</h1><h2 id="1，径向基函数"><a href="#1，径向基函数" class="headerlink" title="1，径向基函数"></a>1，径向基函数</h2><p>径向基函数中，每⼀个基函数只依赖于样本和中⼼ $\boldsymbol{\mu}_j$ 之间的径向距离（通常是欧⼏⾥得距离），即 $\phi_j(\boldsymbol{x})=h(|\boldsymbol{x}−\boldsymbol{\mu}_j|)$ 。历史上，径向基函数被⽤来进⾏精确的<strong>函数内插</strong>（<code>Powell</code>, 1987）。给定⼀组输⼊向量 $\{\boldsymbol{x}_1,\dots,\boldsymbol{x}_N\}$ 以及对应的⽬标值 $\{t_1,\dots,t_N\}$ ，⽬标是找到⼀个光滑的函数 $f(\boldsymbol{x})$ ， 它能够精确地拟合每个⽬标值，即对于 $n = 1,\dots, N$ ，都有 $f(\boldsymbol{x}_n)=t_n$ 。可以这样做：将 $f(\boldsymbol{x})$ 表⽰为径向基函数的线性组合，每个径向基函数都以数据点为中⼼，即</p><script type="math/tex; mode=display">f(\boldsymbol{x})=\sum_{n=1}^{N}w_nh(\|\boldsymbol{x}-\boldsymbol{x}_n\|)\tag{6.24}</script><p>系数 $\{w_n\}$ 的值由最⼩平⽅⽅法求出。</p><p>对径向基函数的展开来⾃<strong>正则化理论</strong>（<code>Poggio and Girosi</code>, 1990; <code>Bishop</code>, 1995<code>a</code>）。对于⼀个使⽤微分算符定义的带有正则化项的平⽅和误差函数，最优解可以通过对算符的<code>Green</code>函数（类似于离散矩阵的特征向量）进⾏展开，每个数据点有⼀个基函数。如果微分算符是各向同性的，那么<code>Green</code>函数只依赖于与对应的数据点的径向距离。由于正则化项的存在，因此解不再精确地对训练数据进⾏内插。</p><p>径向基函数的另⼀个研究动机来源于<strong>输⼊变量</strong>（⽽不是⽬标变量）具有噪声时的内插问题（<code>Webb</code>, 1994; <code>Bishop</code>, 1995<code>a</code>）。如果输⼊变量 $\boldsymbol{x}$ 上的噪声由⼀个服从分布 $\nu(\boldsymbol{ξ})$ 的变量 $\boldsymbol{ξ}$ 描述，那么平⽅和误差函数变成</p><script type="math/tex; mode=display">E=\frac{1}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n+\boldsymbol{\xi})-t_n\}^{2}\nu(\boldsymbol{\xi})\mathrm{d}\boldsymbol{\xi}\tag{6.25}</script><p>使⽤变分法，可以关于函数 $y(\boldsymbol{x})$ 进⾏最优化，得到</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\sum_{n=1}^{N}t_nh(\boldsymbol{x}-\boldsymbol{x}_n)\tag{6.26}</script><p>其中基函数为</p><script type="math/tex; mode=display">h(\boldsymbol{x}-\boldsymbol{x}_n)=\frac{\nu(\boldsymbol{x}-\boldsymbol{x}_n)}{\sum_{n=1}^{N}\nu(\boldsymbol{x}-\boldsymbol{x}_n)}\tag{6.27}</script><p>这是⼀个以每个数据点为中⼼的基函数，这被称为 <strong><code>Nadaraya-Watson</code>模型</strong> 。</p><p>如图6.4，⼀组⾼斯基函数的图像。</p><p><img src="/images/prml_20191020165147.png" alt="⾼斯基函数"></p><p>如图6.5，⼀组⾼斯基函数对应的归⼀化的基函数的图像。</p><p><img src="/images/prml_20191020165157.png" alt="⾼斯基函数对应的归⼀化的基函数"></p><p>选择基函数中⼼的⼀种最简单的⽅法是使⽤数据点的⼀个随机选择的⼦集。⼀个更加系统化的⽅法被称为<strong>正交最⼩平⽅</strong>（<code>Chen et al.</code>, 1991）。这是⼀个顺序选择的过程，在每⼀个步骤中，被选择作为基函数的下⼀个数据点对应于能够最⼤程度减⼩平⽅和误差的数据点。</p><h2 id="2，Nadaraya-Watson模型"><a href="#2，Nadaraya-Watson模型" class="headerlink" title="2，Nadaraya-Watson模型"></a>2，<code>Nadaraya-Watson</code>模型</h2><p>假设有⼀个训练集 $\{\boldsymbol{x}_n, t_n\}$ ，我们使⽤<code>Parzen</code>密度估计来对联合分布 $p(\boldsymbol{x},t)$ 进⾏建模，即</p><script type="math/tex; mode=display">p(\boldsymbol{x},t)=\frac{1}{N}\sum_{n=1}^{N}f(\boldsymbol{x}-\boldsymbol{x}_n,t-t_n)\tag{6.28}</script><p>其中 $f(\boldsymbol{x},t)$ 是分量密度函数，每个数据点都有⼀个以数据点为中⼼的这种分量。现在要找到回归函数 $y(\boldsymbol{x})$ 的表达式，对应于以输⼊变量为条件的⽬标变量的条件均值，其表达式为</p><script type="math/tex; mode=display">\begin{aligned}y(\boldsymbol{x})&=\mathbb{E}[t|\boldsymbol{x}]\\&=\int_{-\infty}^{\infty}tp(t|\boldsymbol{x})\mathrm{d}t\\&=\frac{\int{tp(\boldsymbol{x}|t)}\mathrm{d}t}{\int{p(\boldsymbol{x}|t)}\mathrm{d}t}\\&=\frac{\sum_{n}\int{tf(\boldsymbol{x}-\boldsymbol{x}_n,t-t_n)}\mathrm{d}t}{\sum_{m}\int{f(\boldsymbol{x}-\boldsymbol{x}_m,t-t_m)}\mathrm{d}t}\end{aligned}\tag{6.29}</script><p>现在假设分量的密度函数的均值为零，即</p><script type="math/tex; mode=display">\int_{-\infty}^{\infty}f(\boldsymbol{x},t)t\mathrm{d}t=0</script><p>对所有的 $\boldsymbol{x}$ 都成⽴。使⽤⼀个简单的变量替换，有</p><script type="math/tex; mode=display">\begin{aligned}y(\boldsymbol{x})&=\frac{\sum_{n}g(\boldsymbol{x}-\boldsymbol{x}_n)t_n}{\sum_{m}g(\boldsymbol{x}-\boldsymbol{x}_m)}\\&=\sum_{n}k(\boldsymbol{x},\boldsymbol{x}_n)t_n\end{aligned}\tag{6.30}</script><p>这被称为 <strong><code>Nadaraya-Watson</code>模型</strong>，或者称为<strong>核回归（<code>kernel regression</code>）</strong> （<code>Nadaraya</code>, 1964; <code>Watson</code>, 1964）。对于⼀个局部核函数，<strong>性质</strong>为：给距离 $\boldsymbol{x}$ 较近的数据点 $\boldsymbol{x}_n$ 较⾼的权重。其中 $n, m = 1,\dots, N$ ，且核函数 $k(\boldsymbol{x},\boldsymbol{x}_n)$ 为</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}_n)=\frac{g(\boldsymbol{x}-\boldsymbol{x}_n)}{\sum_{m}g(\boldsymbol{x}-\boldsymbol{x}_m)}</script><p>其中，</p><script type="math/tex; mode=display">g(\boldsymbol{x})=\int_{-\infty}^{\infty}f(\boldsymbol{x},t)\mathrm{d}t</script><script type="math/tex; mode=display">\sum_{n=1}^{N}k(\boldsymbol{x},\boldsymbol{x}_n)=1</script><p>事实上，这个模型不仅定义了条件期望，还定义了整个条件概率分布</p><script type="math/tex; mode=display">\begin{aligned}p(t|\boldsymbol{x})&=\frac{p(t,\boldsymbol{x})}{\int{p(t,\boldsymbol{x})}\mathrm{d}t}\\&=\frac{\sum_{n}f(\boldsymbol{x}-\boldsymbol{x}_n,t-t_n)}{\sum_{m}\int{f(\boldsymbol{x}-\boldsymbol{x}_m,t-t_m)}\mathrm{d}t}\end{aligned}\tag{6.31}</script><p>如图6.6，使⽤各向同性的⾼斯核的<code>Nadaraya-Watson</code>核回归模型的说明。数据集为正弦数据集。原始的正弦函数由绿⾊曲线表⽰，数据点由蓝⾊的点表⽰，每个数据点是⼀个各向同性的⾼斯核的中⼼。得到的回归函数，由条件均值给出，⽤红线表⽰。同时给出的还有条件概率分布 $p(t|x)$ 的两个标准差的区域，⽤红⾊阴影表⽰。在每个数据点周围的蓝⾊椭圆给出了对应的核的⼀个标准差轮廓线。由于⽔平轴和垂直轴的标度不同，这些轮廓线似乎不是圆形的。</p><p><img src="/images/prml_20191020171651.png" alt="核回归模型"></p><h1 id="四，⾼斯过程"><a href="#四，⾼斯过程" class="headerlink" title="四，⾼斯过程"></a>四，⾼斯过程</h1><h2 id="1，重新考虑线性回归问题"><a href="#1，重新考虑线性回归问题" class="headerlink" title="1，重新考虑线性回归问题"></a>1，重新考虑线性回归问题</h2><p>考虑⼀个模型 $M$ 由向量 $\boldsymbol{\phi}(\boldsymbol{x})$ 的元素给出的 $M$ 个固定基函数的线性组合，即</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x})\tag{6.32}</script><p>其中 $\boldsymbol{x}$ 是输⼊向量，$\boldsymbol{w}$ 是 $M$ 维权向量。</p><p>考虑 $\boldsymbol{w}$ 上的⼀个先验概率分布，这个分布是⼀个各向同性的⾼斯分布，形式为</p><script type="math/tex; mode=display">p(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\boldsymbol{I})\tag{6.33}</script><p>它由⼀个超参数 $\alpha$ 控制，这个超参数表⽰分布的精度（⽅差的倒数）。</p><p>如图6.7，⾼斯核的⾼斯过程的样本。</p><p><img src="/images/prml_20191020172013.png" alt="⾼斯核的⾼斯过程"></p><p>如图6.8，指数核的⾼斯过程的样本。</p><p><img src="/images/prml_20191020172021.png" alt="指数核的⾼斯过程"></p><p>在实际应⽤中， 我们希望计算这个函数在某个具体的 $\boldsymbol{x}$ 处的函数值，例如在训练数据点 $\boldsymbol{x}_1,\dots ,\boldsymbol{x}_N$ 处的函数值，感兴趣的是函数值 $y(\boldsymbol{x}_1),\dots , y(\boldsymbol{x}_N)$ 的概率分布。把函数值的集合记作向量 $\mathbf{y}$ ，它的元素为 $y_n = y(\boldsymbol{x}_n )$ ，其中 $n = 1,\dots ,N$ ，这个向量等于</p><script type="math/tex; mode=display">\mathbf{y}=\mathbf{\Phi}\boldsymbol{w}\tag{6.34}</script><p>其中 $\mathbf{\Phi}$ 是设计矩阵，元素为 $\Phi_{nk}=\phi_k(\boldsymbol{x}_n)$ 。</p><p>⾸先，注意到 $\mathbf{y}$ 是由 $\boldsymbol{w}$ 的元素给出的服从⾼斯分布的变量的线性组合，因此它本⾝是服从⾼斯分布。 于是，只需要找到它的均值和⽅差。根据公式(6.33)，均值和⽅差为</p><script type="math/tex; mode=display">\mathbb{E}[\mathbf{y}]=\mathbf{\Phi}\mathbb{E}[\boldsymbol{w}]=\boldsymbol{0}\\\text{cov}[\mathbf{y}]=\mathbb{E}[\mathbf{y}\mathbf{y}^{T}]=\mathbf{\Phi}\mathbb{E}[\boldsymbol{w}\boldsymbol{w}^{T}]\mathbf{\Phi}^{T}=\frac{1}{\alpha}\mathbf{\Phi}\mathbf{\Phi}^{T}=\boldsymbol{K}</script><p>其中，$\boldsymbol{K}$ 是<code>Gram</code>矩阵，元素为</p><script type="math/tex; mode=display">k_{nm}=k(\boldsymbol{x}_n,\boldsymbol{x}_m)=\frac{1}{\alpha}\boldsymbol{\phi}(\boldsymbol{x}_n)^{T}\boldsymbol{\phi}(\boldsymbol{x}_m)</script><p>$k(\boldsymbol{x},\boldsymbol{x}^{\prime})$ 是核函数。</p><p>通常来说，<strong>⾼斯过程</strong>被定义为函数 $y(\boldsymbol{x})$ 上的⼀个概率分布， 使得在任意点集 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$  处计算的 $y(\boldsymbol{x})$ 的值的集合联合起来服从⾼斯分布。在输⼊向量 $\boldsymbol{x}$ 是⼆维的情况下， 这也可以被称为<strong>⾼斯随机场</strong>（<code>Gaussian random field</code>）。更⼀般地，可以⽤⼀种合理的⽅式为 $y(\boldsymbol{x}_1),\dots,y(\boldsymbol{x}_N)$ 赋予⼀个联合的概率分布，来确定⼀个<strong>随机过程</strong>（<code>stochastic process</code>）$y(\boldsymbol{x})$ 。</p><p>⾼斯随机过程的⼀个<strong>关键点</strong>是 $N$ 个变量 $y_1,\dots,y_N$ 上的联合概率分布完全由⼆阶统计（即均值和协⽅差）确定。在⼤部分应⽤中，关于 $y(\boldsymbol{x})$ 的均值没有任何先验的知识，因此根据对称性，令其等于零。这等价于基函数的观点中，令权值 $p(\boldsymbol{w}|\alpha)$ 的先验概率分布的均值等于零。之后，⾼斯过程的确定通过给定两个 $\boldsymbol{x}$ 处的函数值 $y(\boldsymbol{x})$ 的协⽅差来完成。这个协⽅差由核函数确定</p><script type="math/tex; mode=display">\mathbb{E}[y(\boldsymbol{x}_n)y(\boldsymbol{x}_m)]=k(\boldsymbol{x}_n,\boldsymbol{x}_m)\tag{6.35}</script><p>我们也可以直接定义核函数，⽽不是间接地通过选择基函数。图6.8给出了对于两个不同的核函数，由⾼斯过程产⽣的函数的样本。第⼀个核函数是⾼斯核，第⼆个核函数是指数核，定义为</p><script type="math/tex; mode=display">k(x,x^{\prime})=\exp(-\theta|x-x^{\prime}|)\tag{6.36}</script><p>对应于<code>Ornstein-Uhlenbeck</code>过程，这个随机过程最开始由<code>Uhlenbeck and Ornstein</code>（1993）提出，⽤来描述<strong>布朗运动</strong>。</p><h2 id="2，⽤于回归的⾼斯过程"><a href="#2，⽤于回归的⾼斯过程" class="headerlink" title="2，⽤于回归的⾼斯过程"></a>2，⽤于回归的⾼斯过程</h2><p>为了把⾼斯过程模型应⽤于回归问题，需要考虑观测⽬标值的噪声，形式为</p><script type="math/tex; mode=display">t_n=y_n+\epsilon_n\tag{6.37}</script><p>其中 $y_n=y(\boldsymbol{x}_n)$ ，$\epsilon_n$ 是⼀个随机噪声变量，它的值对于每个观测 $n$ 是独⽴的。考虑服从⾼斯分布的噪声过程，即</p><script type="math/tex; mode=display">p(t_n|y_n)=\mathcal{N}(t_n|y_n,\beta^{-1})\tag{6.38}</script><p>其中 $\beta$ 是⼀个超参数，表⽰噪声的精度。由于噪声对于每个数据点是独⽴的，因此以 $\mathbf{y}=(y_1,\dots,y_N)^{T}$ 为条件， ⽬标值 $\mathbf{t}=(t_1,\dots,t_N)^{T}$ 的联合概率分布是⼀个各向同性的⾼斯分布，形式为</p><script type="math/tex; mode=display">p(\mathbf{t}|\mathbf{y})=\mathcal{N}(\mathbf{t}|\mathbf{y},\beta^{-1}\boldsymbol{I}_N)\tag{6.39}</script><p>其中 $\boldsymbol{I}_N$ 表⽰⼀个 $N \times N$ 的单位矩阵。 根据⾼斯过程的定义， 边缘概率分布 $p(\mathbf{y})$ 是⼀个⾼斯分布，均值为零，协⽅差由<code>Gram</code>矩阵 $\boldsymbol{K}$ 定义，即</p><script type="math/tex; mode=display">p(\mathbf{y})=\mathcal{N}(\mathbf{y}|\mathbf{0},\boldsymbol{K})\tag{6.40}</script><p>确定 $\boldsymbol{K}$ 的核函数通常被选择成能够表⽰下⾯的性质：对于相似的点 $\boldsymbol{x}_n$ 和 $\boldsymbol{x}_m$ ，对应的值 $y(\boldsymbol{x}_n)$ 和 $y(\boldsymbol{x}_m)$ 的相关性要⼤于不相似的点。</p><p>为了找到以输⼊值 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ 为条件的边缘概率分布 $p(\mathbf{t})$ ，需要对 $\mathbf{y}$ 积分，$\mathbf{t}$ 的边缘概率分布为</p><script type="math/tex; mode=display">p(\mathbf{t})=\int p(\mathbf{t}|\mathbf{y})p(\mathbf{y})\mathrm{d}\mathbf{y}=\mathcal{N}(\mathbf{t}|\mathbf{0},\boldsymbol{C})\tag{6.41}</script><p>其中协⽅差矩阵 $\boldsymbol{C}$ 的元素为</p><script type="math/tex; mode=display">C(\boldsymbol{x}_n,\boldsymbol{x}_m)=k(\boldsymbol{x}_n,\boldsymbol{x}_m)+\beta^{-1}\delta_{nm}</script><p>对于⾼斯过程回归，⼀个⼴泛使⽤的核函数的形式为指数项的⼆次型加上常数和线性项，即</p><script type="math/tex; mode=display">k(\boldsymbol{x}_n,\boldsymbol{x}_m)=\theta_{0}\exp\left\{-\frac{\theta_1}{2}\|\boldsymbol{x}_n-\boldsymbol{x}_m\|^{2}\right\}+\theta_2+\theta_3\boldsymbol{x}_n^{T}\boldsymbol{x}_m\tag{6.42}</script><p>如图6.9～6.14，由协⽅差函数定义的⾼斯过程先验的样本。每张图上⽅的标题表⽰ $(\theta_0,\theta_1,\theta_2,\theta_3)$ 。</p><p><img src="/images/prml_20191021101710.png" alt="1-4"></p><p><img src="/images/prml_20191021101719.png" alt="9-4"></p><p><img src="/images/prml_20191021101728.png" alt="1-64"></p><p><img src="/images/prml_20191021101736.png" alt="1-0.25"></p><p><img src="/images/prml_20191021101743.png" alt="1-4-10"></p><p><img src="/images/prml_20191021101753.png" alt="1-4-5"></p><p>假设 $\mathbf{t}_N = (t_1,\dots,t_N)^{T}$ ，对应于输⼊值 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ ，组成观测训练集，并且⽬标是对于新的输⼊向量 $\boldsymbol{x}_{N+1}$ 预测⽬标变量 $t_{N+1}$ ，要求计算预测分布 $p(t_{N+1}|\mathbf{t}_N)$ 。为了找到条件分布 $p(t_{N+1}|\mathbf{t})$ ， ⾸先写下联合概率分布 $p(\mathbf{t}_{N+1})$ ， 其中 $\mathbf{t}_{N+1}$ 表⽰向量 $(t_1,\dots,t_N,t_{N+1})^{T}$ ，然后求条件概率分布。</p><p>如图6.15，⾼斯过程回归的原理说明，其中只有⼀个训练点和⼀个测试点， 红⾊椭圆表⽰联合概率分布 $p(t_1,t_2)$ 的轮廓线。$t_1$ 是训练数据点，以 $t_1$ 为条件（蓝⾊直线），得到了 $p(t_2|t_1)$ 。绿⾊曲线表⽰它关于 $t_2$ 的函数。</p><p><img src="/images/prml_20191021101819.png" alt="⾼斯过程回归的原理"></p><p>如图6.16，⾼斯过程的数据点 $\{t_n\}$ 的取样的说明。蓝⾊曲线给出了函数上的⾼斯过程先验的⼀个样本函数，红点表⽰计算函数在⼀组输⼊值 $\{x_n\}$ 上计算得到的函数值 $y_n$ 。对应的 $\{t_n\}$ 的值，⽤绿⾊表⽰，可以通过对每个 $\{y_n\}$ 添加独⽴噪声的⽅式得到。</p><p><img src="/images/prml_20191021101806.png" alt="⾼斯过程的数据点t_n的取样"></p><p>$t_1,\dots,t_{N+1}$ 的联合概率分布为</p><script type="math/tex; mode=display">p(\mathbf{t}_{N+1})=\mathcal{N}(\mathbf{t}_{N+1}|\mathbf{0},\boldsymbol{C}_{N+1})\tag{6.43}</script><p>其中 $\boldsymbol{C}_{N+1}$ 是⼀个 $(N+1) \times (N+1)$ 的协⽅差矩阵。将协⽅差矩阵分块如下</p><script type="math/tex; mode=display">\boldsymbol{C}_{N+1}=\begin{pmatrix}\boldsymbol{C}_N & \boldsymbol{k} \\\boldsymbol{k}^{T} & c\end{pmatrix}\tag{6.44}</script><p>其中 $\boldsymbol{C}_{N}$ 是⼀个 $N \times N$ 的协⽅差矩阵，元素由公式(6.41)中的相关表达式给出，其中 $n,m=1,\dots,N$，向量 $\boldsymbol{k}$ 的元素为 $k(\boldsymbol{x}_n,\boldsymbol{k}_{N+1})$ ，其中 $n=1,\dots,N$ ，标量 $c=k(\boldsymbol{x}_{N+1},\boldsymbol{k}_{N+1})+\beta−1$ 。条件概率分布 $p(t_{N+1}|\mathbf{t})$ 是⼀个⾼斯分布，均值和协⽅差为</p><script type="math/tex; mode=display">m(\boldsymbol{x}_{N+1})=\boldsymbol{k}^{T}\boldsymbol{C}_{N}^{-1}\mathbf{t}\\\sigma^{2}(\boldsymbol{x}_{N+1})=c-\boldsymbol{k}^{T}\boldsymbol{C}_{N}^{-1}\boldsymbol{k}</script><p>预测分布的均值可以写成 $\boldsymbol{x}_{N+1}$ 的函数，形式为</p><script type="math/tex; mode=display">m(\boldsymbol{x}_{N+1})=\sum_{n=1}^{N}a_nk(\boldsymbol{x}_{n},\boldsymbol{x}_{N+1})\tag{6.45}</script><p>其中 $a_n$ 是 $\boldsymbol{C}_{N}^{-1}\mathbf{t}$ 的第 $n$ 个元素。</p><p>如图6.17，⾼斯过程回归应⽤于正弦数据集的说明，其中三个最右侧的点被略去。绿⾊曲线给出了正弦函数，其中数据点（蓝⾊点）通过对这个函数取样并且添加⾼斯噪声的⽅式得到。红线表⽰⾼斯过程预测分布的均值，阴影区域对应于两个标准差的位置。</p><p><img src="/images/prml_20191021104350.png" alt="⾼斯过程回归应⽤"></p><h2 id="3，学习超参数"><a href="#3，学习超参数" class="headerlink" title="3，学习超参数"></a>3，学习超参数</h2><p><strong>学习超参数</strong>的⽅法基于计算似然函数 $p(\mathbf{t}|\boldsymbol{\theta})$ ，其中 $\boldsymbol{\theta}$ 表⽰⾼斯过程模型的超参数。最简单的⽅法是通过最⼤化似然函数的⽅法进⾏ $\boldsymbol{\theta}$ 的点估计。由于 $\boldsymbol{\theta}$ 表⽰回归问题的⼀组超参数，因此这可以看成类似于线性回归模型的第⼆类最⼤似然步骤。可以使⽤⾼效的基于梯度的最优化算法 （例如共轭梯度法）来最⼤化对数似然函数（<code>Fletcher</code>, 1987; <code>Nocedal and Wright</code>, 1999; <code>Bishop and Nabney</code>, 2008）。</p><p>对数似然函数的形式为</p><script type="math/tex; mode=display">\ln p(\mathbf{t}|\boldsymbol{\theta})=-\frac{1}{2}\ln|\boldsymbol{C}_{N}|-\frac{1}{2}\mathbf{t}^{T}\boldsymbol{C}_{N}^{-1}\mathbf{t}-\frac{N}{2}\ln(2\pi)\tag{6.46}</script><p>有，</p><script type="math/tex; mode=display">\frac{\partial}{\partial\theta_i}\ln p(\mathbf{t}|\boldsymbol{\theta})=-\frac{1}{2}\text{Tr}\left(\boldsymbol{C}_{N}^{-1}\frac{\partial\boldsymbol{C}_{N}}{\partial\theta_i}\right)+\frac{1}{2}\mathbf{t}^{T}\boldsymbol{C}_{N}^{-1}\frac{\partial\boldsymbol{C}_{N}}{\partial\theta_i}\boldsymbol{C}_{N}^{-1}\mathbf{t}\tag{6.47}</script><p>由于 $\ln p(\mathbf{t}|\boldsymbol{\theta})$ 通常是⼀个⾮凸函数，因此它由多个极⼤值点。</p><h2 id="4，⾃动相关性确定"><a href="#4，⾃动相关性确定" class="headerlink" title="4，⾃动相关性确定"></a>4，⾃动相关性确定</h2><p>通过最⼤似然⽅法进⾏的参数最优化，能够将不同输⼊的相对重要性从数据中推断出来，这是⾼斯过程中的<strong>⾃动相关性确定</strong>（<code>automatic relevance detemination</code>）或者 <strong><code>ARD</code></strong> 的实例。</p><p>考虑⼆维输⼊空间 $\boldsymbol{x}=(x_1,x_2)$ ，有⼀个下⾯形式的核函数</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\theta_0\exp\left\{-\frac{1}{2}\sum_{i=1}^{2}\eta_i(x_i-x_i^{\prime})^{2}\right\}\tag{6.48}</script><p>如图6.18～6.19，来⾃⾼斯过程的<code>ARD</code>先验的样本，其中核函数由公式(6.48)给出。 图6.18对应于 $\eta_1=\eta_2=1$ ，图6.19对应于 $\eta_1=1$ , $\eta_2=0.01$ 。</p><p><img src="/images/prml_20191021104654.png" alt="⾼斯过程的ARD先验的样本1"></p><p><img src="/images/prml_20191021104701.png" alt="⾼斯过程的ARD先验的样本2"></p><p>如图6.20，使⽤⼀个具有三个输⼊ $x_1$ , $x_2$ 和 $x_3$ 的简单⼈造数据集来说明<code>ARD</code>（<code>Nabney</code>, 2002）。⽬标变量 $t$ 的⽣成⽅式为：从⼀个⾼斯分布中采样100个 $x_1$ ，计算函数 $\sin(2\pi x_1)$ ，然后加上添加上⾼斯噪声。 $x_2$ 的值通过复制对应的 $x_1$ 然后添加噪声的⽅式获得，$x_3$ 的值从⼀个独⽴的⾼斯分布中采样。 因此，$x_1$ 很好地预测了 $t$ ，$x_2$ 对 $t$ 的预测的噪声更⼤，$x_3$ 与 $t$ 之间只有偶然的相关性。曲线表⽰对应的超参数的值与最优化边缘似然函数时的迭代次数的关系，红⾊表⽰ $\eta_1$ ，绿⾊表⽰ $\eta_2$ ，蓝⾊表⽰ $\eta_3$ 。</p><p><img src="/images/prml_20191021105507.png" alt="⼈造数据集来说明ARD"></p><p><code>ARD</code>框架很容易整合到指数-⼆次核中，得到下⾯形式的核函数</p><script type="math/tex; mode=display">k(\boldsymbol{x}_n,\boldsymbol{x}_m)=\theta_0\exp\left\{-\frac{1}{2}\sum_{i=1}^{D}\eta_i(x_{ni}-x_{mi})^{2}\right\}+\theta_2+\theta_3\sum_{i=1}^{D}x_{ni}x_{mi}\tag{6.49}</script><p>其中 $D$ 是输⼊空间的维度。</p><h2 id="5，⽤于分类的⾼斯过程"><a href="#5，⽤于分类的⾼斯过程" class="headerlink" title="5，⽤于分类的⾼斯过程"></a>5，⽤于分类的⾼斯过程</h2><p>在分类的概率⽅法中，⽬标是在给定⼀组训练数据的情况下，对于⼀个新的输⼊向量，为⽬标变量的后验概率建模。这些概率⼀定位于区间 $(0, 1)$ 中，⽽⼀个⾼斯过程模型做出的预测位于整个实数轴上。调整⾼斯过程，使其能够处理分类问题，⽅法为：使⽤⼀个恰当的⾮线性激活函数，将⾼斯过程的输出进⾏变换。<br>⾸先考虑⼀个⼆分类问题，它的⽬标变量为 $t\in\{0,1\}$ 。如果定义函数 $a(\boldsymbol{x})$ 上的⼀个⾼斯过程，然后使⽤<code>logistic sigmoid</code>函数 $y=\sigma(a)$ 进⾏变换，那么就得到了函数 $y(\boldsymbol{x})$ 上的⼀个⾮⾼斯随机过程，其中 $y\in(0,1)$ 。<br>如图6.21～6.22，图6.21给出了在函数 $a(\boldsymbol{x})$ 上定义了⼀个⾼斯过程先验的样本，图6.22给出了使⽤<code>logistic sigmoid</code>对这个样本进⾏变换得到的结果。</p><p><img src="/images/prml_20191021105516.png" alt="⾼斯过程先验"></p><p><img src="/images/prml_20191021105525.png" alt="logistic sigmoid变换"></p><p>⼀维输⼊空间的情况，其中⽬标变量 $t$ 上的概率分布是伯努利分布</p><script type="math/tex; mode=display">p(t|a)=\sigma(a)^t(1-\sigma(a))^{1-t}\tag{6.50}</script><p>把训练集的输⼊记作 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ ，对应的观测⽬标变量为 $\mathbf{t}=(t_1,\dots,t_N)^T$ ，考虑⼀个单⼀的测试数据点 $\boldsymbol{x}_{N+1}$ ，⽬标值为 $t_{N+1}$ 。⽬标是确定预测分布 $p(t_{N+1}|\mathbf{t})$ ，其中没有显式地写出它对于输⼊变量的条件依赖。为了完成这个⽬标，引⼊向量 $a_{N+1}$ 上的⾼斯过程先验，它的分量为 $a(\boldsymbol{x}_1),\dots,a(\boldsymbol{x}_{N+1})$ 。通过以训练数据 $\mathbf{t}_N$ 为条件，得到了求解的预测分布。$\boldsymbol{a}_{N+1}$ 上的⾼斯过程先验的形式为</p><script type="math/tex; mode=display">p(\boldsymbol{a}_{N+1})=\mathcal{N}(\boldsymbol{a}_{N+1}|\boldsymbol{0},\boldsymbol{C}_{N+1})\tag{6.51}</script><p>其中，协⽅差矩阵 $\boldsymbol{C}_{N+1}$ 的元素为</p><script type="math/tex; mode=display">C(\boldsymbol{x}_n,\boldsymbol{x}_m)=k(\boldsymbol{x}_n,\boldsymbol{x}_m)+\nu\delta_{nm}</script><p>其中 $k(\boldsymbol{x}_n,\boldsymbol{x}_m)$ 是⼀个任意的半正定核函数，$\nu$ 的值通常事先固定，假定核函数 $k(\boldsymbol{x},\boldsymbol{x}^{\prime})$ 由参数向量 $\boldsymbol{\theta}$ 控制。</p><p>对于⼆分类问题，因为 $p(t_{N+1}=0|\mathbf{t}_N)$ 的值等于 $1−p(t_{N+1}=1|\mathbf{t}_N)$ 故预测分布为</p><script type="math/tex; mode=display">p(t_{N+1}=1|\mathbf{t}_N)=\int p(t_{N+1}=1|a_{N+1})p(a_{N+1}|\mathbf{t}_N)\mathrm{d}a_{N+1}\tag{6.52}</script><p>其中 $p(t_{N+1}=1|a_{N+1})=\sigma(a_{N+1})$ 。</p><h2 id="6，拉普拉斯近似"><a href="#6，拉普拉斯近似" class="headerlink" title="6，拉普拉斯近似"></a>6，拉普拉斯近似</h2><p>考虑三种不同的获得⾼斯近似的⽅法。<br>⽅法⼀：基于<strong>变分推断</strong>（<code>variational inference</code>）（<code>Gibbs and MacKay</code>, 2000），并且使⽤了<code>logistic sigmoid</code>函数的局部变分界。<br>⽅法二：使⽤<strong>期望传播</strong>（<code>expectation propagation</code>）（<code>Opper and Winther</code>, 2000<code>b</code>; <code>Minka</code>, 2001<code>b</code>; <code>Seeger</code>, 2003）。<br>方法三：基于<strong>拉普拉斯近似</strong>。</p><p>为了计算预测分布，寻找 $a_{N+1}$ 的后验概率分布的⾼斯近似，使⽤贝叶斯定理，后验概率分布为</p><script type="math/tex; mode=display">\begin{aligned}p(a_{N+1}|\mathbf{t}_N)&=\int p(a_{N+1},\boldsymbol{a}_N|\mathbf{t}_N)\mathrm{d}\boldsymbol{a}_N\\&=\frac{1}{p(\mathbf{t}_N)}\int p(a_{N+1},\boldsymbol{a}_N)p(\mathbf{t}_N|a_{N+1},\boldsymbol{a}_N)\mathrm{d}\boldsymbol{a}_N\\&=\frac{1}{p(\mathbf{t}_N)}\int p(a_{N+1}|\boldsymbol{a}_N)p(\boldsymbol{a}_N)p(\mathbf{t}_N|\boldsymbol{a}_N)\mathrm{d}\boldsymbol{a}_N\\&=\int p(a_{N+1}|\boldsymbol{a}_N)p(\boldsymbol{a}_N|\mathbf{t}_N)\mathrm{d}\boldsymbol{a}_N\end{aligned}\tag{6.53}</script><p>条件概率分布 $p(a_{N+1} |\boldsymbol{a}_N)$ 为</p><script type="math/tex; mode=display">p(a_{N+1}|\boldsymbol{a}_N)=\mathcal{N}(a_{N+1}|\boldsymbol{k}^{T}\boldsymbol{C}_{N}^{-1}\boldsymbol{a}_N,c-\boldsymbol{k}^{T}\boldsymbol{C}_{N}^{-1}\boldsymbol{k})\tag{6.54}</script><p>先验概率 $p(\boldsymbol{a}_N)$ 由⼀个零均值⾼斯过程给出，协⽅差矩阵为 $\boldsymbol{C}_N$ ，数据项（假设数据点之间具有独⽴性）为</p><script type="math/tex; mode=display">p(\mathbf{t}_N|\boldsymbol{a}_N)=\prod_{n=1}^{N}\sigma(a_n)^{t_n}(1-\sigma(a_n))^{1-t_n}=\prod_{n=1}^{N}e^{a_nt_n}\sigma(-a_n)\tag{6.55}</script><p>通过对 $p(\boldsymbol{a}_N|\mathbf{t}_N)$ 的对数进⾏泰勒展开，就可以得到拉普拉斯近似。 忽略掉⼀些具有可加性的常数，这个概率的对数为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{\Psi}(\boldsymbol{a}_N)&=\ln p(\boldsymbol{a}_N)+\ln p(\mathbf{t}_N|\boldsymbol{a}_N)\\&=-\frac{1}{2}\boldsymbol{a}_N^{T}\boldsymbol{C}_{N}^{-1}\boldsymbol{a}_N-\frac{N}{2}    \ln p(2\pi)-\frac{1}{2}\ln|\boldsymbol{C}_N|+\mathbf{t}_{N}^{T}\boldsymbol{a}_N-\sum_{n=1}^{N}\ln(1+e^{a_n})\end{aligned}\tag{6.56}</script><p>⾸先需要找到后验概率分布的众数，计算 $\boldsymbol{\Psi}(\boldsymbol{a}_N)$ 的梯度为</p><script type="math/tex; mode=display">\nabla\boldsymbol{\Psi}(\boldsymbol{a}_N)=\mathbf{t}_N-\boldsymbol{\sigma}_{N}-\boldsymbol{C}_{N}^{-1}\boldsymbol{a}_N\tag{6.57}</script><p>其中 $\boldsymbol{\sigma}_N$ 是⼀个元素为 $\sigma(a_n)$ 的向量。</p><p>对 $\boldsymbol{\Psi}(\boldsymbol{a}_N)$ 的⼆阶导数进⾏拉普拉斯近似，为</p><script type="math/tex; mode=display">\nabla\nabla\boldsymbol{\Psi}(\boldsymbol{a}_N)=-\boldsymbol{W}_N-\boldsymbol{C}_N^{-1}\tag{6.58}</script><p>其中 $\boldsymbol{W}_N$ 是⼀个对角矩阵，元素为 $\sigma(a_n)(1−\sigma(a_n))$ 。</p><p>使⽤<code>Newton-Raphson</code>公式，$\boldsymbol{a}_N$ 的迭代更新⽅程为</p><script type="math/tex; mode=display">\boldsymbol{a}_N^{新}=\boldsymbol{C}_N(\boldsymbol{I}+\boldsymbol{W}_N\boldsymbol{C}_N)^{-1}\{\mathbf{t}_N-\boldsymbol{\sigma}_N+\boldsymbol{W}_N\boldsymbol{a}_N\}\tag{6.59}</script><p>这个⽅程反复迭代，直到收敛于众数（记作 $\boldsymbol{a}_N^{*}$ ）。在这个众数位置，梯度 $\nabla\boldsymbol{\Psi}(\boldsymbol{a}_N)$ 为零，因此 $\boldsymbol{a}_N^{*}$ 满⾜</p><script type="math/tex; mode=display">\boldsymbol{a}_{N}^{*}=\boldsymbol{C}_N(\mathbf{t}_N-\boldsymbol{\sigma}_N)\tag{6.60}</script><p>计算<code>Hessian</code>矩阵，</p><script type="math/tex; mode=display">\boldsymbol{H}=-\nabla\nabla\boldsymbol{\Psi}(\boldsymbol{a}_N)=\boldsymbol{W}_N+\boldsymbol{C}_N^{-1}\tag{6.61}</script><p>对后验概率分布 $p(\boldsymbol{a}_N|\mathbf{t}_N)$ 的⾼斯近似为</p><script type="math/tex; mode=display">q(\boldsymbol{a}_N)=\mathcal{N}(\boldsymbol{a}_N|\boldsymbol{a}_{N}^{*},\boldsymbol{H}^{-1})\tag{6.62}</script><p>对应于线性⾼斯模型，有</p><script type="math/tex; mode=display">\mathbb{E}[a_{N+1}|\mathbf{t}_N]=\boldsymbol{k}^{T}(\mathbf{t}_N-\boldsymbol{\sigma}_N)\\\text{var}[a_{N+1}|\mathbf{t}_N]=c-\boldsymbol{k}^{T}(\boldsymbol{W}_{N}^{-1}+\boldsymbol{C}_N)^{-1}\boldsymbol{k}</script><p>要确定协⽅差函数的参数 $\boldsymbol{\theta}$ 。⼀种⽅法是最⼤化似然函数 $p(\mathbf{t}_N|\boldsymbol{\theta})$ ，此时需要对数似然函数和它的梯度的表达式。如果必要的话，还可以加上正则化项，产⽣⼀个正则化的 最⼤似然解，最⼤似然函数的定义为</p><script type="math/tex; mode=display">p(\mathbf{t}_N|\boldsymbol{\theta})=\int p(\mathbf{t}_N|\boldsymbol{a}_N)p(\boldsymbol{a}_N|\boldsymbol{\theta})\mathrm{d}\boldsymbol{a}_N\tag{6.63}</script><p>对数似然函数的拉普拉斯近似</p><script type="math/tex; mode=display">\ln p(\mathbf{t}_N|\boldsymbol{\theta})=\boldsymbol{\Psi}(\boldsymbol{a}_{N}^{*})-\frac{1}{2}\ln|\boldsymbol{W}_N+\boldsymbol{C}_{N}^{-1}|+\frac{N}{2}\ln(2\pi)\tag{6.64}</script><p>其中 $\boldsymbol{\Psi}(\boldsymbol{a}_{N}^{*})=\ln p(\boldsymbol{a}_{N}^{*}|\boldsymbol{\theta})+\ln p(\mathbf{t}_N |\boldsymbol{a}_{N}^{*})$ 。当对公式(6.64)关于 $\boldsymbol{\theta}$ 求积分 $N$ 时，我们得到了两个项的集合，第⼀个集合产⽣于协⽅差矩阵 $\boldsymbol{C}_N$ 对 $\boldsymbol{\theta}$ 的依赖关系，第⼆个集合产⽣于 $\boldsymbol{a}_{N}^{*}$ 对 $\boldsymbol{\theta}$ 的依赖关系。 显式地依赖于 $\boldsymbol{\theta}$ 的项，有</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial\ln p(\mathbf{t}_N|\boldsymbol{\theta})}{\partial\theta_j}&=\frac{1}{2}\boldsymbol{a}_{N}^{*T}\boldsymbol{C}_{N}^{-1}\frac{\partial\boldsymbol{C}_N}{\partial\theta_j}\boldsymbol{C}_{N}^{-1}\boldsymbol{a}_{N}^{*}\\&-\frac{1}{2}\text{Tr}\left[(\boldsymbol{I}+\boldsymbol{C}_N\boldsymbol{W}_N)^{-1}\boldsymbol{W}_N\frac{\boldsymbol{C}_N}{\partial\theta_j}\right]\end{aligned}\tag{6.65}</script><p>为了计算由于 $\boldsymbol{a}_{N}^{*}$ 对 $\boldsymbol{\theta}$ 的依赖产⽣的项，注意到已经构造了拉普拉斯近似，从⽽在 $\boldsymbol{a}_{N}=\boldsymbol{a}_{N}^{*}$ 处，$\boldsymbol{\Psi}(\boldsymbol{a}_N)$ 的均值为零，从⽽ $\boldsymbol{\Psi}(\boldsymbol{a}_N^{*})$ 对于梯度没有贡献。剩下的有贡献的项关于 $\boldsymbol{\theta}$ 的的分量 $\theta_j$ 的导数为</p><script type="math/tex; mode=display">\begin{aligned}-\frac{1}{2}\sum_{n=1}^{N}\frac{\partial\ln|\boldsymbol{W}_N+\boldsymbol{C}_{N}^{-1}|}{\partial{a_{n}^{*}}}\frac{\partial{a_{n}^{*}}}{\partial\theta_j}=-\frac{1}{2}\sum_{n=1}^{N}\left[(\boldsymbol{I}+\boldsymbol{C}_N\boldsymbol{W}_N)^{-1}\boldsymbol{C}_N\right]_{nn}\sigma_{n}^{*}(1-\sigma_{n}^{*})(1-2\sigma_{n}^{*})\frac{\partial{a_{n}^{*}}}{\partial\theta_j}\end{aligned}\tag{6.66}</script><p>其中 $\sigma_{n}^{*}=\sigma(a_{n}^{*})$ 。</p><p>$a_{n}^{*}$ 关于 $\theta_j$ 的导数，即</p><script type="math/tex; mode=display">\frac{\partial{a_{n}^{*}}}{\partial\theta_j}=\frac{\partial{\boldsymbol{C}_{N}}}{\partial\theta_j}(\mathbf{t}_N-\boldsymbol{\sigma}_N)-\boldsymbol{C}_N\boldsymbol{W}_N\frac{\partial{a_{n}^{*}}}{\partial\theta_j}\tag{6.67}</script><p>整理，可得</p><script type="math/tex; mode=display">\frac{\partial{a_{n}^{*}}}{\partial\theta_j}=(\boldsymbol{I}+\boldsymbol{C}_N\boldsymbol{W}_N)^{-1}\frac{\partial{\boldsymbol{C}_{N}}}{\partial\theta_j}(\mathbf{t}_N-\boldsymbol{\theta}_N)\tag{6.68}</script><p>如图6.23～6.24，使⽤⾼斯过程进⾏分类的说明。图6.23给出了数据点，以及来⾃真实概率分布的最优决策边界（绿⾊），还有来⾃⾼斯过程分类器的决策边界（⿊⾊）。图6.24给出了蓝⾊类别和红⾊类别的预测后验概率分布，以及⾼斯过程决策边界。</p><p><img src="/images/prml_20191021171241.png" alt="数据点"></p><p><img src="/images/prml_20191021171249.png" alt="预测后验概率分布"></p><h2 id="7，与神经⽹络的联系"><a href="#7，与神经⽹络的联系" class="headerlink" title="7，与神经⽹络的联系"></a>7，与神经⽹络的联系</h2><p>神经⽹络可以表⽰的函数的范围由隐含单元的数量 $M$ 控制， 并且对于⾜够⼤的 $M$ ，⼀个两层神经⽹络可以以任意精度近似任意给定的函数。在最⼤似然的框架中，隐含单元的数量需要有⼀定的限制（根据训练集的规模确定限制的程度），来避免过拟合现象。在贝叶斯神经⽹络中， 参数向量 $\boldsymbol{w}$ 上的先验分布以及⽹络函数 $f(\boldsymbol{x},\boldsymbol{w})$ 产⽣了函数 $y(\boldsymbol{x})$ 上的先验概率分布，其中 $\boldsymbol{y}$ 是⽹络输出向量。<code>Neal</code>（1996）已经证明，在极限 $M \to \infty$ 的情况下，对 于 $\boldsymbol{w}$ 的⼀⼤类先验分布，神经⽹络产⽣的函数的分布将会趋于⾼斯过程。在 这种极限情况下，神经⽹络的输出变量会变为相互独⽴。神经⽹络的优势之⼀是输出之间共享隐含单元，因此它们可以互相“借统计优势”，即与每个隐含结点关联的权值被所有的输出变量影响，⽽不是只被它们中的某⼀个影响。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，对偶表示&quot;&gt;&lt;a href=&quot;#一，对偶表示&quot; class=&quot;headerlink&quot; title=&quot;一，对偶表示&quot;&gt;&lt;/a&gt;一，对偶表示&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】贝叶斯神经网络</title>
    <link href="https://zhangbc.github.io/2019/10/18/prml_05_04/"/>
    <id>https://zhangbc.github.io/2019/10/18/prml_05_04/</id>
    <published>2019-10-18T10:21:17.000Z</published>
    <updated>2019-10-22T09:18:23.784Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，混合密度网络"><a href="#一，混合密度网络" class="headerlink" title="一，混合密度网络"></a>一，混合密度网络</h1><p>作为逆问题，考虑机械臂的运动学问题。<strong>正向问题</strong>（<code>forward problem</code>）是在给定连接角的情况下求解机械臂末端的位置，这个问题有唯⼀解。然⽽，在实际应⽤中，我们想把机械臂末端移动到⼀个具体的位置，为了完成移动，必须设定合适的连接角。正向问题通常对应于物理系统的因果关系，通常有唯⼀解。</p><p>图5.29～5.30，图5.29给展⽰了⼀个具有两个连接的机械臂，其中，末端的笛卡尔坐标 $(x_1, x_2)$ 由两个连接角 $\theta_1$ 和 $\theta_2$ 以及机械臂的（固定）长度 $L_1$ 和 $L_2$ 唯⼀确定。这被称为<strong>机械臂的正向运动学</strong> （<code>forward kinematics</code>）。在实际应⽤中，必须寻找给出所需的末端位置的连接角，如图5.30所⽰，这个<strong>逆向运动学</strong>（<code>inverse kinematics</code>）有两个对应的解，即“<strong>肘部向上</strong>”和“<strong>肘部向下</strong>”。</p><p><img src="/images/prml_20191016115033.png" alt="正向运动学"></p><p><img src="/images/prml_20191016115043.png" alt="逆向运动学"></p><p>考虑⼀个具有多峰性质的问题，数据的⽣成⽅式为：对服从区间 $(0, 1)$ 的均匀分布的变量 $x$ 进⾏取样，得到⼀组值 $\{x_n\}$ ，对应的⽬标值 $t_n$ 通过下⾯的⽅式得到：计算函数 $x_n + 0.3 \sin(2\pi{x_n})$ ，然后添加⼀个服从 $(−0.1, 0.1)$ 上的均匀分布的噪声。这样，逆问题就可以这样得到：使⽤相同的数据点，但是交换 $x$ 和 $t$ 的⾓⾊。</p><p>图5.31～5.32，图5.31是⼀个简单的“正向问题”的数据集，其中红⾊曲线给出了通过最⼩化平⽅和误差函数调节⼀个两层神经⽹络的结果。对应的逆问题，如图5.32所⽰，通过交换 $x$ 和 $t$ 的顺序的⽅式得到。这⾥，通过最⼩化平⽅和误差函数的⽅式训练的神经⽹络给出了对数据的⾮常差的拟合，因为数据集是多峰的。</p><p><img src="/images/prml_20191016115547.png" alt="正向问题数据集"></p><p><img src="/images/prml_20191016115556.png" alt="逆向问题数据集"></p><p>寻找⼀个对条件概率密度建模的⼀般的框架：为 $p(\boldsymbol{t}|\boldsymbol{x})$ 使⽤⼀个混合模型，模型的混合系数和每个分量的概率分布都是输⼊向量 $\boldsymbol{x}$ 的⼀个⽐较灵活的函数，这就构成了<strong>混合密度⽹络</strong>（<code>mixture density network</code>）。对于任意给定的 $\boldsymbol{x}$ 值，混合模型提供了⼀个通⽤的形式，⽤来对任意条件概率密度函数 $p(\boldsymbol{t}|\boldsymbol{x})$ 进⾏建模。</p><p>图5.33，<strong>混合密度⽹络</strong>（<code>mixture density network</code>）可以表⽰⼀般的条件概率密度 $p(\boldsymbol{t}|\boldsymbol{x})$ ， ⽅法为：考虑 $\boldsymbol{t}$ 的⼀个参数化的混合模型，参数由以 $\boldsymbol{x}$ 为输⼊的神经⽹络的输出确定。</p><p><img src="/images/prml_20191016115604.png" alt="混合密度⽹络"></p><p>显式地令模型的分量为⾼斯分布，即</p><script type="math/tex; mode=display">p(\boldsymbol{t}|\boldsymbol{x})=\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})\mathcal{N}(\boldsymbol{t}|\boldsymbol{\mu}(\boldsymbol{x}),\sigma_{k}^{2}(\boldsymbol{x})\boldsymbol{I})\tag{5.99}</script><p>混合系数必须满⾜下⾯的限制</p><script type="math/tex; mode=display">\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})=1,    0 \le \pi_{k}(\boldsymbol{x}) \le 1</script><p>可以通过使⽤⼀组<code>softmax</code>输出来实现</p><script type="math/tex; mode=display">\pi_{k}(\boldsymbol{x})=\frac{\exp(a_{k}^{\pi})}{\sum_{l=1}^{K}\exp(a_{l}^{\pi})}\tag{5.100}</script><p>⽅差必须满⾜ $\sigma_{k}^{2}(\boldsymbol{x})\ge0$ ，因此可以使⽤对应的⽹络激活的指数形式表⽰，即</p><script type="math/tex; mode=display">\sigma_{k}(\boldsymbol{x})=\exp(a_{k}^{\sigma})\tag{5.101}</script><p>由于均值 $\mu_{k}(\boldsymbol{x})$ 有实数分量，因此可以直接⽤⽹络的输出激活表⽰</p><script type="math/tex; mode=display">\mu_{kj}(\boldsymbol{x})=a_{kj}^{\mu}\tag{5.102}</script><p>混合密度⽹络的可调节参数由权向量 $\boldsymbol{w}$ 和偏置组成。这些参数可以通过最⼤似然法确定，或者等价地，使⽤最⼩化误差函数（负对数似然函数）的⽅法确定。对于独⽴的数据，误差函数的形式为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\ln\left\{\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x}_n,\boldsymbol{w})\mathcal{N}(\boldsymbol{t}_n|\boldsymbol{\mu}_k(\boldsymbol{x}_n,\boldsymbol{w}),\sigma_{k}^{2}(\boldsymbol{x}_n,\boldsymbol{w})\boldsymbol{I})\right\}\tag{5.103}</script><p>把混合系数 $\pi_k(\boldsymbol{x})$ 看成与 $\boldsymbol{x}$ 相关的先验概率分布，从⽽就引⼊了对应的后验概率，形式为</p><script type="math/tex; mode=display">\gamma_{nk}=\gamma_{k}(\boldsymbol{t}_n|\boldsymbol{x}_n)=\frac{\pi_{k}\mathcal{N}_{nk}}{\sum_{l=1}^{K}\pi_{l}\mathcal{N}_{nl}}\tag{5.104}</script><p>其中 $\mathcal{N}_{nk}$ 表⽰ $\mathcal{N}(\boldsymbol{t}_n | \boldsymbol{\mu}_k(\boldsymbol{x}_n),\sigma_{k}^{2}(\boldsymbol{x}_n))$ 。 </p><p>关于控制混合系数的⽹络输出激活的导数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{a_{k}^{\pi}}}=\pi_{k}-\gamma_{nk}\tag{5.105}</script><p>关于控制分量均值的⽹络输出激活的导数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{a_{kl}^{\mu}}}=\gamma_{k}\left\{\frac{\mu_{kl}-t_{nl}}{\sigma_{k}^{2}}\right\}\tag{5.106}</script><p>关于控制分量⽅差的⽹络激活函数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{a_{k}^{\sigma}}}=\gamma_{nk}\left\{L-\frac{\|\boldsymbol{t}_n-\boldsymbol{\mu}_{k}\|^{2}}{\sigma_{k}^{2}}\right\}\tag{5.107}</script><p>如图5.34～5.37，(<code>a</code>)对于给出的数据训练的混合密度⽹络的三个核函数，混合系数 $\pi_k(\boldsymbol{x})$ 与 $\boldsymbol{x}$ 的函数关系图像。模型有三个⾼斯分量，使⽤了⼀个多层感知器，在隐含层有五个“ $tanh$ ”单元，同时有9个输出单元 （对应于⾼斯分量的3个均值、3个⽅差以及3个混合系数）。在较⼩的 $x$ 值和较⼤的 $x$ 值处，⽬标数据的条件概率密度是单峰的，对于它的先验概率分布，只有⼀个核具有最⼤的值。⽽在中间的 $x$ 值处，条件概率分布具有3个峰，3个混合系数具有可⽐的值。(<code>b</code>)使⽤与混合系数相同的颜⾊表⽰⽅法来表⽰均值 $\mu_k (x)$ 。 (<code>c</code>)对于同样的混合密度⽹络，⽬标数据的条件概率密度的图像。 (d)条件概率密度的近似条件峰值的图像，⽤红⾊点表⽰。</p><p><img src="/images/prml_20191016151145.png" alt="a"></p><p><img src="/images/prml_20191016151155.png" alt="b"></p><p><img src="/images/prml_20191016151204.png" alt="c"></p><p><img src="/images/prml_20191016151212.png" alt="d"></p><p>⼀旦混合密度⽹络训练结束，可以预测对于任意给定的输⼊向量的⽬标数据的条件密度函数。只要我们关注的是预测输出向量的值的问题，那么这个条件概率密度就能完整地描述⽤于⽣成数据的概率分布。根据这个概率密度函数，可以计算不同应⽤中我们感兴趣的更加具体的量。⼀个最简单的量就是⽬标数据的条件均值，即</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{t}|\boldsymbol{x}]=\int\boldsymbol{t}p(\boldsymbol{t}|\boldsymbol{x})\mathrm{d}\boldsymbol{t}=\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})\boldsymbol{\mu}_{k}(\boldsymbol{x})\tag{5.108}</script><p>密度函数的⽅差，结果为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{s}^{2}(\boldsymbol{x})&=\mathbb{E}[\|\boldsymbol{t}-\mathbb{E}[\boldsymbol{t}|\boldsymbol{x}]\|^{2}|\boldsymbol{x}]\\&=\sum_{k=1}^{K}\pi_{k}(\boldsymbol{x})\left\{\boldsymbol{\sigma}_{k}^{2}+\left\|\boldsymbol{\mu}_k(\boldsymbol{x})-\sum_{l=1}^{K}\pi_{l}(\boldsymbol{x})\boldsymbol{\mu}_{l}(\boldsymbol{x})\right\|^{2}\right\}\end{aligned}\tag{5.109}</script><h1 id="二，贝叶斯神经网络"><a href="#二，贝叶斯神经网络" class="headerlink" title="二，贝叶斯神经网络"></a>二，贝叶斯神经网络</h1><h2 id="1，后验参数分布"><a href="#1，后验参数分布" class="headerlink" title="1，后验参数分布"></a>1，后验参数分布</h2><p>考虑从输⼊向量 $\boldsymbol{x}$ 预测单⼀连续⽬标变量 $t$ 的问题。假设条件概率分布 $p(t|\boldsymbol{x})$ 是⼀个⾼斯分布，均值与 $\boldsymbol{x}$ 有关，由神经⽹络模型的输出 $y(\boldsymbol{x}, \boldsymbol{x})$ 确定，精度（⽅差的倒数）$\beta$ 为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w},\beta)=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1})\tag{5.110}</script><p>将权值 $\boldsymbol{w}$ 的先验概率分布选为⾼斯分布，形式为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\alpha)=\mathcal{N}(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\boldsymbol{I})\tag{5.111}</script><p>对于 $N$ 次独⽴同分布的观测 $\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$ ，对应的⽬标值集合 $\mathcal{D}=\{t_1,\dots, t_N\}$ ，似然函数为</p><script type="math/tex; mode=display">p(\mathcal{D}|\boldsymbol{w},\beta)=\prod_{n=1}^{N}\mathcal{N}(t_n|y(\boldsymbol{x}_n,\boldsymbol{w}),\beta^{-1})\tag{5.112}</script><p>最终的后验概率为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\mathcal{D},\alpha,\beta)\propto p(\boldsymbol{w}|\alpha)p(\mathcal{D}|\boldsymbol{w},\beta)\tag{5.113}</script><p>由于 $y(\boldsymbol{x},\boldsymbol{w})$ 与 $\boldsymbol{w}$ 的关系是⾮线性的，因此后验概率不是⾼斯分布。</p><p>使⽤拉普拉斯近似，可以找到对于后验概率分布的⼀个⾼斯近似。⾸先找到后验概率分布的⼀个（局部）最⼤值，这必须使⽤迭代的数值最优化算法才能找到，⽐较⽅便的做法是最⼤化后验概率分布的对数，可以写成</p><script type="math/tex; mode=display">\ln p(\boldsymbol{w}|\mathcal{D})=-\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}-\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w})-t_n\}^{2}+常数\tag{5.114}</script><p>负对数后验概率的⼆阶导数为</p><script type="math/tex; mode=display">\boldsymbol{A}=-\nabla\nabla\ln p(\boldsymbol{w}|\mathcal{D},\alpha,\beta)=\alpha\boldsymbol{I}+\beta\boldsymbol{H}\tag{5.115}</script><p>其中，$\boldsymbol{H}$ 是⼀个<code>Hessian</code>矩阵，由平⽅和误差函数关于 $\boldsymbol{w}$ 的分量组成，后验概率对应的⾼斯近似形式为</p><script type="math/tex; mode=display">q(\boldsymbol{w}|\mathcal{D})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{w}_{MAP},\boldsymbol{A}^{-1})\tag{5.116}</script><p>预测分布可以通过将后验概率分布求积分的⽅式获得</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D})=\int p(t|\boldsymbol{x},\boldsymbol{w})q(\boldsymbol{w}|\mathcal{D})\mathrm{d}\boldsymbol{w}\tag{5.117}</script><p>现在假设与 $y(\boldsymbol{x}, \boldsymbol{w})$ 发⽣变化造成的 $\boldsymbol{w}$ 的变化幅度相⽐，后验概率分布的⽅差较⼩。这使得可以在 $\boldsymbol{w}_{MAP}$ 附近对⽹络函数进⾏泰勒展开，只保留展开式的现⾏项，可得</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})\simeq y(\boldsymbol{x},\boldsymbol{w}_{WAP})+\boldsymbol{g}^{T}(\boldsymbol{w}-\boldsymbol{w}_{MAP})\tag{5.118}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{g}=\nabla_{\boldsymbol{w}}y(\boldsymbol{x},\boldsymbol{w})|_{\boldsymbol{w}=\boldsymbol{w}_{WAP}}</script><p>使⽤这个近似，现在得到了⼀个线性⾼斯模型，$p(\boldsymbol{w})$ 为⾼斯分布，并且，$p(t|\boldsymbol{w})$ 也是⾼斯分布，均值是 $\boldsymbol{w}$ 的线性函数，分布的形式为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w},\beta)\simeq \mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}_{MAP})+\boldsymbol{g}^{T}(\boldsymbol{w}-\boldsymbol{w}_{MAP}),\beta^{-1})\tag{5.119}</script><p>边缘分布 $p(t)$ 的⼀般结果，得到</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D},\alpha,\beta)=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}_{MAP}),\sigma^{2}(\boldsymbol{x}))\tag{5.120}</script><p>其中，与输⼊相关的⽅差为</p><script type="math/tex; mode=display">\sigma^{2}(\boldsymbol{x})=\beta^{-1}+\boldsymbol{g}^{T}\boldsymbol{A}^{-1}\boldsymbol{g}</script><p>由此可见，预测分布 $p(t|\boldsymbol{x},\mathcal{D})$ 是⼀个⾼斯分布， 它的均值由⽹络函数 $y(\boldsymbol{x},w_{MAP})$ 给出， 参数设置为 $MAP$ 值。⽅差由两项组成，第⼀项来⾃⽬标变量的固有噪声，第⼆项是⼀个与 $\boldsymbol{x}$ 相关的项，表⽰由于模型参数 $\boldsymbol{w}$ 的不确定性造成的内插的不确定性。</p><h2 id="2，超参数最优化"><a href="#2，超参数最优化" class="headerlink" title="2，超参数最优化"></a>2，超参数最优化</h2><p>超参数的边缘似然函数，或者模型证据，可以通过对⽹络权值进⾏积分的⽅法得到，即</p><script type="math/tex; mode=display">p(\mathcal{D}|\alpha,\beta)=\int p(\mathcal{D}|\boldsymbol{w},\beta)p(\boldsymbol{w}|\alpha)\mathrm{d}\boldsymbol{w}\tag{5.121}</script><p>取对数，可得</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\alpha,\beta)\simeq -E(\boldsymbol{w}_{MAP})-\frac{1}{2}\ln|\boldsymbol{A}|+\frac{W}{2}\ln\alpha+\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)\tag{5.122}</script><p>其中 $W$ 是 $\boldsymbol{w}$ 中参数的总数。正则化误差函数的定义为</p><script type="math/tex; mode=display">E(\boldsymbol{w}_{MAP})=\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})-t_n\}^{2}+\frac{\alpha}{2}\boldsymbol{w}_{MAP}^{T}\boldsymbol{w}_{MAP}\tag{5.123}</script><p>在模型证据框架中，我们通过最⼤化 $\ln p(\mathcal{D}|\alpha,\beta)$ 对 $\alpha$ 和 $\beta$ 进⾏点估计。⾸先考虑关于 $\alpha$ 进⾏最⼤化，定义特征值⽅程</p><script type="math/tex; mode=display">\beta\boldsymbol{H}\boldsymbol{\mu}_i=\lambda_{i}\boldsymbol{\mu}_i\tag{5.124}</script><p>其中 $\boldsymbol{H}$ 是在 $\boldsymbol{w}=\boldsymbol{w}_{MAP}$ 处计算的<code>Hessian</code>矩阵，由平⽅和误差函数的⼆阶导数组成。</p><p>有</p><script type="math/tex; mode=display">\alpha=\frac{\gamma}{\boldsymbol{w}_{MAP}^{T}\boldsymbol{w}_{MAP}}\tag{5.125}</script><p>其中 $\gamma$ 表⽰参数的有效数量，定义为</p><script type="math/tex; mode=display">\gamma=\sum_{i=1}^{W}\frac{\lambda_i}{\alpha+\lambda_i}</script><p>关于 $\beta$ 最⼤化模型证据，可以得到下⾯的重估计公式</p><script type="math/tex; mode=display">\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})-t_n\}^{2}\tag{5.126}</script><h2 id="3，⽤于分类的贝叶斯神经⽹络"><a href="#3，⽤于分类的贝叶斯神经⽹络" class="headerlink" title="3，⽤于分类的贝叶斯神经⽹络"></a>3，⽤于分类的贝叶斯神经⽹络</h2><p>考虑的⽹络有⼀个<code>logistic sigmoid</code>输出，对应于⼀个⼆分类问题。</p><p>模型的对数似然函数为</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\boldsymbol{w})=\sum_{n=1}^{N}\{t_n\ln{y_n}+(1-t_n)\ln(1-y_n)\}\tag{5.127}</script><p>其中 $t_n \in\{0,1\}$ 是⽬标值，且 $y_n \equiv y(\boldsymbol{x}_n,\boldsymbol{w})$ 。</p><p>将拉普拉斯框架⽤在这个模型中的第⼀个阶段是初始化超参数 $\alpha$，然后通过最⼤化对数后验概率分布的⽅法确定参数向量 $\boldsymbol{w}$ ，这等价于最⼩化正则化误差函数</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\ln{p(\mathcal{D}|\boldsymbol{w})}+\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}\tag{5.128}</script><p>找到权向量的解 $\boldsymbol{w}_{MAP}$ 之后， 下⼀步是计算由负对数似然函数的⼆阶导数组成的<code>Hessian</code>矩阵 $\boldsymbol{H}$ 。</p><p>为了最优化超参数 $\alpha$ ，再次最⼤化边缘似然函数，边缘似然函数的形式为</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\alpha)\simeq -E(\boldsymbol{w}_{MAP})-\frac{1}{2}\ln|\boldsymbol{A}|+\frac{W}{2}\ln\alpha\tag{5.129}</script><p>其中，正则化的误差函数为</p><script type="math/tex; mode=display">E(\boldsymbol{w}_{MAP})=-\sum_{n=1}^{N}\{t_n\ln{y_n}+(1-t_n)\ln(1-y_n)\}+\frac{\alpha}{2}\boldsymbol{w}_{MAP}^{T}\boldsymbol{w}_{MAP}\tag{5.130}</script><p>其中 $y_n \equiv y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})$ 。</p><p>图5.38，模型证据框架应⽤于⼈⼯⽣成的⼆分类数据集的说明。绿⾊曲线表⽰最优的决策边界，⿊⾊曲线表⽰通过最⼤化似然函数调节⼀个具有8个隐含结点的两层神经⽹络的结果，红⾊曲线表⽰包含⼀个正则化项的结果，其中 $\alpha$ 使⽤模型证据的步骤进⾏了最优化，初始值为 $\alpha = 0$ 。注意，模型证据步骤极⼤地缓 解了模型的过拟合现象。</p><p><img src="/images/prml_20191016161835.png" alt="模型证据框架应⽤于⼈⼯⽣成的⼆分类数据集"></p><p>最后，需要找到的预测分布，由于⽹络函数的⾮线性的性质，积分是⽆法直接计算的。最简单的近似⽅法是假设后验概率⾮常窄，因此可以进⾏下⾯的近似</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D})\simeq p(t|\boldsymbol{x},\boldsymbol{w}_{MAP})\tag{5.131}</script><p>对输出激活函数进⾏线性近似，形式为</p><script type="math/tex; mode=display">a(\boldsymbol{x},\boldsymbol{w})\simeq a_{MAP}(\boldsymbol{x})+\boldsymbol{b}^{T}(\boldsymbol{w}-\boldsymbol{w}_{MAP})\tag{5.132}</script><p>其中，$a_{MAP}(\boldsymbol{x})=a(\boldsymbol{x},\boldsymbol{w}_{MAP})$ 及向量 $\boldsymbol{b} \equiv \nabla a(\boldsymbol{x},\boldsymbol{w}_{MAP})$ 都可以通过反向传播⽅法求出。</p><p>由神经⽹络的权值的分布引出的输出单元激活的值的分布为</p><script type="math/tex; mode=display">p(a|\boldsymbol{x},\mathcal{D})=\int\delta(a-a_{MAP}(\boldsymbol{x})-\boldsymbol{b}^{T}(\boldsymbol{x})(\boldsymbol{w}-\boldsymbol{w}_{MAP}))q(\boldsymbol{w}|\mathcal{D})\mathrm{d}\boldsymbol{w}\tag{5.133}</script><p>其中 $q(\boldsymbol{w}|\mathcal{D})$ 是对后验概率分布的⾼斯近似，这个分布是⼀个⾼斯分布，均值为 $a_{MAP}(\boldsymbol{x})\equiv a(\boldsymbol{x},\boldsymbol{x}_{MAP})$ ，⽅差为</p><script type="math/tex; mode=display">\sigma_{a}^{2}(\boldsymbol{x})=\boldsymbol{b}^{T}(\boldsymbol{x})\boldsymbol{A}^{-1}\boldsymbol{b}(\boldsymbol{x})\tag{5.134}</script><p>为了得到预测分布，必须对 $a$ 进⾏积分</p><script type="math/tex; mode=display">p(t=1|\boldsymbol{x},\mathcal{D})=\int \delta(a)p(a|\boldsymbol{w},\mathcal{D})\mathrm{d}a\tag{5.135}</script><p>⾼斯分布与<code>logistic sigmoid</code>函数的卷积是⽆法计算的。于是</p><script type="math/tex; mode=display">p(t=1|\boldsymbol{x},\mathcal{D})=\sigma\left(\kappa(\sigma_{a}^{2})a_{MAP}\right)\tag{5.136}</script><p>图5.39～5.40，对于⼀个具有8个隐含结点带有 $tanh$ 激活函数和⼀个<code>logistic sigmoid</code>输出结点的贝叶斯⽹络应⽤拉 普拉斯近似的说明。权参数使⽤缩放的共轭梯度⽅法得到，超参数 $\alpha$ 使⽤模型证据框架确定。图5.39是使⽤基于参数的 $\boldsymbol{w}_{MAP}$ 的点估计的简单近似得到的结果，其中绿⾊曲线表⽰ $y = 0.5$ 的决策边界，其他的轮廓线对应于 $y = 0.1, 0.3, 0.7$ 和 $0.9$ 的输出概率。图5.40是使⽤公式(5.136)得到的对应的结果。注意，求边缘概率分布的效果是扩散了轮廓线，使得预测的置信度变低，从⽽在每个输⼊点 $\boldsymbol{x}$ 处，后验概 率分布向着 $0.5$ 的⽅向偏移，⽽ $y = 0.5$ 的边界本⾝不受影响。</p><p><img src="/images/prml_20191016163025.png" alt="点估计"></p><p><img src="/images/prml_20191016163032.png" alt="卷积"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，混合密度网络&quot;&gt;&lt;a href=&quot;#一，混合密度网络&quot; class=&quot;headerlink&quot; title=&quot;一，混合密度网络&quot;&gt;&lt;/a&gt;一，混合密度
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】神经网络正则化</title>
    <link href="https://zhangbc.github.io/2019/10/18/prml_05_03/"/>
    <id>https://zhangbc.github.io/2019/10/18/prml_05_03/</id>
    <published>2019-10-18T10:02:19.000Z</published>
    <updated>2019-10-18T15:46:45.543Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，相容的⾼斯先验"><a href="#一，相容的⾼斯先验" class="headerlink" title="一，相容的⾼斯先验"></a>一，相容的⾼斯先验</h1><p>神经⽹络的输⼊单元和输出单元的数量通常由数据集的维度确定，⽽隐含单元的数量 $M$ 是⼀个⾃由的参数，可以通过调节来给出最好的预测性能。</p><p>控制神经⽹络的模型复杂度来避免过拟合，根据对多项式曲线拟合问题的讨论，⼀种⽅法是选择⼀个相对⼤的 $M$ 值，然后通过给误差函数增加⼀个正则化项，来控制模型的复杂度。最简单的正则化项是⼆次的，给出了正则化的误差函数，形式为</p><script type="math/tex; mode=display">\tilde{E}(\boldsymbol{w})=E(\boldsymbol{w})+\frac{\lambda}{2}\boldsymbol{w}^{T}\boldsymbol{w}\tag{5.73}</script><p>这个正则化项也被称为<strong>权值衰减</strong>（<code>weight decay</code>）。模型复杂度可以通过选择正则化系数 $\lambda$ 来确定，正则化项可以表⽰为权值 $\boldsymbol{w}$ 上的零均值⾼斯先验分布的负对数。</p><p>公式(5.73)给出的简单权值衰减的⼀个<strong>局限性</strong>是：它与⽹络映射的确定缩放性质不相容。考虑⼀个多层感知器⽹络，这个⽹络有两层权值和线性输出单元，给出了从输⼊变量集合 $\{x_i\}$ 到输出变量集合 $\{y_k\}$ 的映射。第⼀个隐含层的隐含单元的激活的形式为</p><script type="math/tex; mode=display">z_j=h\left(\sum_{i}w_{ji}x_{i}+w_{j0}\right)\tag{5.74}</script><p>输出单元的激活为</p><script type="math/tex; mode=display">y_k=\sum_{j}w_{kj}z_j+w_{k0}\tag{5.75}</script><p>假设对输⼊变量进⾏⼀个线性变换，形式为</p><script type="math/tex; mode=display">x_i\to\tilde{x}_{i}=ax_i+b\tag{5.76}</script><p>然后根据这个映射对⽹络进⾏调整，使得⽹络给出的映射不变。调整的⽅法为，对从输⼊单元到隐含层单元的权值和偏置也进⾏⼀个对应的线性变换，形式为</p><script type="math/tex; mode=display">w_{ji}\to\tilde{w}_{ji}=\frac{1}{a}w_{ji}</script><script type="math/tex; mode=display">w_{j0}\to\tilde{w}_{j0}=w_{j0}-\frac{b}{a}\sum_{i}w_{ji}</script><p>⽹络的输出变量的线性变换</p><script type="math/tex; mode=display">y_{k}\to\tilde{y}_{k}=cy_k+d\tag{5.77}</script><p>可以通过对第⼆层的权值和偏置进⾏线性变换的⽅式实现。变换的形式为</p><script type="math/tex; mode=display">w_{kj}\to\tilde{w}_{kj}=cw_{kj}</script><script type="math/tex; mode=display">w_{k0}\to\tilde{w}_{k0}=cw_{k0}+d</script><p>于是要寻找⼀个正则化项，它在上述线性变换和下具有不变性，这需要正则化项应该对于权值的重新缩放不变，对于偏置的平移不变。这样的正则化项为</p><script type="math/tex; mode=display">\frac{\lambda_1}{2}\sum_{w\in\mathcal{W_1}}w^2+\frac{\lambda_2}{2}\sum_{w\in\mathcal{W_2}}w^2</script><p>其中 $\mathcal{W}_1$ 表⽰第⼀层的权值集合，$\mathcal{W}_2$ 表⽰第⼆层的权值集合， 偏置未出现在求和式中。这个正则化项在权值的变换下不会发⽣变化，只要正则化参数进⾏下⾯的重新放缩即可：$\lambda_1 \to a^{\frac{1}{2}}\lambda_1$ 和 $\lambda_2 \to a^{-\frac{1}{2}}\lambda_2$  ， 正则化项对应于下⾯形式的先验概率分布。</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\alpha_1,\alpha_2)\propto\exp\left(-\frac{\alpha_1}{2}\sum_{w\in\mathcal{W_1}}w^2-\frac{\alpha_2}{2}\sum_{w\in\mathcal{W_2}}w^2\right)\tag{5.78}</script><p>注意， 这种形式的先验是反常的（<code>improper</code>）（不能够被归⼀化），因为偏置参数没有限制。</p><p>图5.15～5.18，控制两层神经⽹络的权值和偏置的先验概率分布的超参数的效果说明。其中，神经⽹络有⼀个输⼊，⼀个线性输出，以及12个隐含结点，隐含结点的激活函数为 $tanh$。先验概率分布通过四个超参数 $\alpha_{1}^{b}$， $\alpha_{1}^{w}$， $\alpha_{2}^{b}$ ， $\alpha_{2}^{w}$ 控制，它们分别表⽰第⼀层的偏置、第⼀层的权值、第⼆层的偏置、第⼆层的权值。</p><p><img src="/images/prml_20191016102909.png" alt="a=1,b=1"></p><p><img src="/images/prml_20191016102918.png" alt="a=1,b=10"></p><p><img src="/images/prml_20191016102927.png" alt="a=1000,b=100"></p><p><img src="/images/prml_20191016102936.png" alt="a=1000,b=1000"></p><p>⼀般地，可以考虑权值被分为任意数量的组 $\mathcal{W}_k$ 的情况下的先验，即</p><script type="math/tex; mode=display">p(\boldsymbol{w})\propto\exp\left(-\frac{1}{2}\sum_{k}\alpha_{k}\|\boldsymbol{w}\|_{k}^2\right)\tag{5.79}</script><p>其中，</p><script type="math/tex; mode=display">\|\boldsymbol{w}\|_{k}^2=\sum_{j\in\mathcal{W}_k}w_{j}^{2}</script><h1 id="二，早停止"><a href="#二，早停止" class="headerlink" title="二，早停止"></a>二，早停止</h1><p>另⼀种控制⽹络的复杂度的正则化⽅法是<strong>早停⽌</strong>（<code>early stopping</code>）。⾮线性⽹络模型的训练对应于误差函数的迭代减⼩，其中误差函数是关于训练数据集定义的。对于许多⽤于⽹络训练的最优化算法（例如共轭梯度法），误差函数是⼀个关于迭代次数的不增函数。然⽽，在独⽴数据（通常被称为<strong>验证集</strong>）上测量的误差，通常⾸先减⼩，接下来由于模型开始过拟合⽽逐渐增⼤。于是，训练过程可以在关于验证集误差最⼩的点停⽌，这样可以得到⼀个有着较好泛化性能的⽹络。</p><p>图5.19～5.20，训练集误差和验证集误差在典型的训练阶段的⾏为说明。图像给出了误差与迭代次数的函数，数据集为正弦数据集。得到最好的泛化表现的⽬标表明，训练应该在垂直虚线表⽰的点处停⽌，对应于验证集误差的最⼩值。</p><p><img src="/images/prml_20191016102950.png" alt="训练集误差"></p><p><img src="/images/prml_20191016102959.png" alt="验证集误差"></p><p>图5.21，在⼆次误差函数的情况下，关于早停⽌可以给出与权值衰减类似的结果的原因说明。椭圆给出了常数误差函数的轮廓线，$\boldsymbol{w}_{ML}$ 表⽰误差函数的最⼩值。如果权向量的起始点为原点，按照局部负梯度的⽅向移动，那么它会沿着曲线给出的路径移动。通过对训练过程早停⽌，我们找到了⼀个权值向量 $\tilde{\boldsymbol{w}}$。 定性地说，它类似于使⽤简单的权值衰减正则化项，然后最⼩化正则化误差函数的⽅法得到的权值。</p><p><img src="/images/prml_20191016103345.png" alt="⼆次误差函数的早停⽌"></p><h1 id="三，不变性"><a href="#三，不变性" class="headerlink" title="三，不变性"></a>三，不变性</h1><p>寻找让可调节的模型能够表述所需的不变性，⼤致可以分为四类：</p><blockquote><p>1）通过复制训练模式，同时根据要求的不变性进⾏变换，对训练集进⾏扩展。例如，在⼿写数字识别的例⼦中，我们可以将每个样本复制多次，每个复制后的样本中，图像被平移到 了不同的位置。</p><p>2）为误差函数加上⼀个正则化项，⽤来惩罚当输⼊进⾏变换时，输出发⽣的改变。</p><p>3）通过抽取在要求的变换下不发⽣改变的特征，不变性被整合到预处理过程中。任何后续的使⽤这些特征作为输⼊的回归或者分类系统就会具有这些不变性。</p><p>4）把不变性的性质整合到神经⽹络的构建过程中，或者对于相关向量机的⽅法，整合到核函数中。</p></blockquote><p>图5.22，对⼿写数字进⾏⼈⼯形变的说明。 原始图像见左图。 在右图中， 上⾯⼀⾏给出了三个经过了形变的数字，对应的位移场在下⾯⼀⾏给出。这些位移场按照下⾯的⽅法⽣成：在每个像素处， 对唯 ⼀ $\Delta{x}$ , $\Delta{y}\in(0,1)$ 进⾏随机取样，然后分别与宽度为0.01,30,60的⾼斯分布做卷积，进⾏平滑。</p><p><img src="/images/prml_20191016103406.png" alt="⼿写数字进⾏⼈⼯形变"></p><p>图5.23，⼆维输⼊空间的例⼦，展⽰了在⼀个特定的输⼊向量 $\boldsymbol{x}_n$ 上的连续变换的效果。⼀个参数为连续变量 $\xi$ 的⼀维变换作⽤于 $\boldsymbol{x}_n$ 上会使它扫过⼀个⼀维流形 $\mathcal{M}$ 。局部来看，变换的效果可以⽤切向量 $\boldsymbol{\tau}_n$ 来近似。</p><p><img src="/images/prml_20191016103416.png" alt="⼆维输⼊空间"></p><h1 id="四，切线传播"><a href="#四，切线传播" class="headerlink" title="四，切线传播"></a>四，切线传播</h1><p>通过<strong>切线传播</strong>（<code>tangent propagation</code>）的⽅法，可以使⽤正则化来让模型对于输⼊的变换具有不变性（<code>Simard et al.</code>, 1992）。对于⼀个特定的输⼊向量 $\boldsymbol{x}_n$ ，考虑变换产⽣的效果。 假设变换是连续的（例如平移或者旋转，⽽不是镜像翻转），那么变换的模式会扫过 $D$ 维输⼊空间的⼀个流形 $\mathcal{M}$ 。假设变换由单⼀参数 $\xi$ 控制（例如，$\xi$ 可能是旋转的角度）。那么被 $\boldsymbol{x}_n$ 扫过的⼦空间 $\mathcal{M}$ 是⼀维的，并且以 $\xi$ 为参数。令这个变换作⽤于 $\boldsymbol{x}_n$ 上产⽣的向量为 $\boldsymbol{s}(\boldsymbol{x}_n,\xi)$ ， 且  $\boldsymbol{s}(\boldsymbol{x}_n,0)=\boldsymbol{x}$ 。 这样曲线 $\mathcal{M}$ 的切线就由⽅向导数 $\boldsymbol{\tau} =\frac{\partial\boldsymbol{x}}{\partial\xi}$ 给出，且点 $\boldsymbol{x}_n$ 处的切线向量为</p><script type="math/tex; mode=display">\boldsymbol{\tau}_n=\frac{\partial{\boldsymbol{s}}(\boldsymbol{x}_n,\xi)}{\partial{\xi}}\Bigg{|}_{\xi=0}\tag{5.80}</script><p>对于输⼊向量进⾏变换之后，⽹络的输出通常会发⽣变化。输出 $k$ 关于 $\xi$ 的导数为</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{\xi}}\Bigg{|}_{\xi=0}=\sum_{i=1}^{D}\frac{\partial{y_k}}{\partial{x_i}}\frac{\partial{x_i}}{\partial{\xi}}\Bigg{|}_{\xi=0}=\sum_{i=1}^{D}J_{ki}\tau_{i}</script><p>其中 $J_{ki}$ 为<code>Jacobian</code>矩阵 $\boldsymbol{J}$ 的第 $(k, i)$ 个元素。这个结果可以⽤于修改标准的误差函数，使得在数据点的邻域之内具有不变性。修改的⽅法为：给原始的误差函数 $E$ 增加⼀个正则化函数 $\Omega$ ，得到下⾯形式的误差函数</p><script type="math/tex; mode=display">\tilde{E}=E+\lambda\Omega\tag{5.81}</script><p>其中 $\lambda$ 是正则化系数，且</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\sum_{n}\sum_{k}\left(\frac{\partial{y_{nk}}}{\partial{\xi}}\bigg{|}_{\xi=0}\right)^{2}=\frac{1}{2}\sum_{n}\sum_{k}\left(\sum_{i=1}^{D}J_{nki}\tau_{ni}\right)^{2}</script><p>当⽹络映射函数在每个模式向量的邻域内具有变换不变性时，正则化函数等于零。$\lambda$ 的值确定了训练数据和学习不变性之间的平衡。在实际执⾏过程中，切线向量 $\boldsymbol{\tau}_n$ 可以使⽤有限差近似，即将原始向量 $\boldsymbol{x}$ 从使⽤了⼩的 $\xi$ 进⾏变换后的对应的向量中减去，再除以 $\xi$ 。</p><p>图5.24～5.27，(<code>a</code>)原始的⼿写数字 $\boldsymbol{x}$ ，(<code>b</code>)对应于⽆穷⼩顺时针旋转的切向量 $\boldsymbol{\tau}$ ，其中蓝⾊和黄⾊分别对应于正值和负值，(<code>c</code>)将来⾃这个切向量的微⼩贡献作⽤于原始图像的结果，得到了 $\boldsymbol{x}+\epsilon\boldsymbol{\tau}$  ，其中 $\epsilon=15$ 度。(<code>d</code>)真实的图像旋转，⽤作对⽐。</p><p><img src="/images/prml_20191016103843.png" alt="a"></p><p><img src="/images/prml_20191016103853.png" alt="b"></p><p><img src="/images/prml_20191016103906.png" alt="c"></p><p><img src="/images/prml_20191016103919.png" alt="d"></p><p>如果变换由 $L$ 个参数控制（例如，对于⼆维图像的平移变换与⾯内旋转变换项结合），那么流形 $\mathcal{M}$ 的维度为 $L$ ，对应的正则化项由形如公式 $\Omega$ 的项求和得到，每个变换都对应求和式中的⼀项。如果同时考虑若⼲个变换，并且让⽹络映射对于每个变换分别具有不变性，那么对于变换的组合来说就会具有（局部）不变性（<code>Simard et al.</code>, 1992）。⼀个相关的技术，被称为<strong>切线距离（<code>tangent distance</code>）</strong>，可以⽤来构造基于距离的⽅法（例如最近邻分类器）的不变性（<code>Simard et al.</code>, 1993）。</p><h1 id="五，⽤变换后的数据训练"><a href="#五，⽤变换后的数据训练" class="headerlink" title="五，⽤变换后的数据训练"></a>五，⽤变换后的数据训练</h1><p>考虑由单⼀参数 $\xi$ 控制的变换，且这个变换由函数 $\boldsymbol{s}(\boldsymbol{x},\xi)$ 描述， 其中 $\boldsymbol{s}(\boldsymbol{x}_n,0)=\boldsymbol{x}$ ，也会考虑平⽅和误差函数。对于未经过变换的输⼊，误差函数可以写成 （在⽆限数据集的极限情况下）</p><script type="math/tex; mode=display">E=\frac{1}{2}\int\int\{y(\boldsymbol{x})-t\}^{2}p(t|\boldsymbol{x})p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\mathcal{d}t\tag{5.82}</script><p>为了保持记号的简洁，考虑有⼀个输出单元的⽹络。如果现在考虑每个数据点的⽆穷多个副本，每个副本都由⼀个变换施加了扰动，这个变换的参数为 $\xi$ ，且 $\xi$ 服从概率分布 $p(\xi)$ ，那么在这个扩展的误差函数上定义的误差函数可以写成</p><script type="math/tex; mode=display">\tilde{E}=\frac{1}{2}\int\int\int\{y(\boldsymbol{s}(\boldsymbol{x},\xi))-t\}^{2}p(t|\boldsymbol{x})p(\boldsymbol{x})p(\xi)\mathrm{d}\boldsymbol{x}\mathcal{d}t\mathcal{d}\xi\tag{5.83}</script><p>现在假设分布 $p(\xi)$ 的均值为零，⽅差很⼩，即只考虑对原始输⼊向量的⼩的变换。可以对变换函数进⾏关于 $\xi$ 的展开，可得</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{s}(\boldsymbol{x},\xi)&=\boldsymbol{s}(\boldsymbol{x},0)+\xi\frac{\partial}{\partial{\xi}}\boldsymbol{s}(\boldsymbol{x},\xi)\bigg{|}_{\xi=0}+\frac{\xi^{2}}{2}\frac{\partial^{2}}{\partial{\xi^{2}}}\bigg{|}_{\xi=0}+O(\xi^{3})\\&=\boldsymbol{x}+\xi\boldsymbol{\tau}+\frac{1}{2}\xi^{2}\boldsymbol{\tau}^{\prime}+O(\xi^{3})\end{aligned}\tag{5.84}</script><p>其中 $\boldsymbol{\tau}^{\prime}$ 表⽰ $\boldsymbol{s}(\boldsymbol{x},\xi)$ 关于 $\xi$ 的⼆阶导数在 $\xi = 0$ 处的值。这使得可以展开模型函数，可得</p><script type="math/tex; mode=display">y(\boldsymbol{s}(\boldsymbol{x},\xi))=y(\boldsymbol{x})+\xi\boldsymbol{\tau}^{T}\nabla{y(\boldsymbol{x})}+\frac{\xi^{2}}{2}[(\boldsymbol{\tau}^{\prime})^{T}\nabla{y(\boldsymbol{x})}+\boldsymbol{\tau}^{T}\nabla\nabla{y(\boldsymbol{x})}\boldsymbol{\tau}]+O(\xi^{3})</script><p>代⼊平均误差函数，有</p><script type="math/tex; mode=display">\begin{aligned} \tilde{E} &=\frac{1}{2} \iint\{y(x)-t\}^{2} p(t | \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \mathrm{d} t \\ &+\mathbb{E}[\xi] \iint\{y(\boldsymbol{x})-t\} \boldsymbol{\tau}^{T} \nabla y(\boldsymbol{x}) p(t | \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \mathrm{d} t \\ &+\mathbb{E}\left[\xi^{2}\right] \frac{1}{2} \iint\left[\{y(\boldsymbol{x})-t\}\left\{\left(\boldsymbol{\tau}^{\prime}\right)^{T} \nabla y(\boldsymbol{x})+\boldsymbol{\tau}^{T} \nabla \nabla y(\boldsymbol{x}) \boldsymbol{\tau}\right\}\right.\\ &\left.+\left(\boldsymbol{\tau}^{T} \nabla y(\boldsymbol{x})\right)^{2}\right] p(t | \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \mathrm{d} t+O\left(\xi^{3}\right) \end{aligned}\tag{5.85}</script><p>由于变换的分布的均值为零， 因此有 $\mathbb{E}[\xi] = 0$ ，并且把 $\mathbb{E}[\xi^{2}]$ 记作 $\lambda$ ，省略 $O(\xi^{3})$ 项，这样平均误差函数就变成了</p><script type="math/tex; mode=display">\tilde{E}=E+\lambda\Omega\tag{5.86}</script><p>其中 $E$ 是原始的平⽅和误差，正则化项 $\Omega$ 的形式为</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\int\bigg{[}\{y(\boldsymbol{x})-\mathbb{E}[t|\boldsymbol{x}]\}\left\{(\boldsymbol{\tau}^{\prime})^{T}\nabla{y(\boldsymbol{x})}+\boldsymbol{\tau}^{T}\nabla\nabla{y(\boldsymbol{x})\boldsymbol{\tau}}\right\}+(\boldsymbol{\tau}^{T}\nabla{y(\boldsymbol{x})})^{2}p(\boldsymbol{x})\bigg{]}\mathrm{d}\boldsymbol{x}</script><p>进⼀步简化这个正则化项，发现正则化的误差函数等于⾮正则化的误差函数加上⼀个 $O(\xi^{2})$ 的项，因此最⼩化总误差函数的⽹络函数的形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\mathbb{E}[t|\boldsymbol{x}]+O(\xi^{3})\tag{5.87}</script><p>从⽽，正则化项中的第⼀项消失，剩下的项为</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\int(\boldsymbol{\tau}^{T}\nabla{y(\boldsymbol{x})})^{2}p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}</script><p>这等价于<strong>切线传播</strong>的正则化项。<br>如果考虑⼀个特殊情况，即输⼊变量的变换只是简单地添加随机噪声，从⽽ $\boldsymbol{x}\to\boldsymbol{x}+\boldsymbol{\xi}$ ，那么正则化项的形式为</p><script type="math/tex; mode=display">\Omega=\frac{1}{2}\int\|\nabla{y(\boldsymbol{x})}\|^{2}p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\tag{5.88}</script><p>这被称为 <strong><code>Tikhonov</code>正则化</strong> （<code>Tikohonov and Arsenin</code>, 1977; <code>Bishop</code>, 1995<code>b</code>）。这个正则化项关于⽹络权值的导数可以使⽤扩展的反向传播算法求出（<code>Bishop</code>, 1993）。对于⼩的噪 声，<code>Tikhonov</code>正则化与对输⼊添加随机噪声有关系，在恰当的情况下，这种做法会提升模型的泛化能⼒。</p><h1 id="六，卷积神经⽹络"><a href="#六，卷积神经⽹络" class="headerlink" title="六，卷积神经⽹络"></a>六，卷积神经⽹络</h1><p>另⼀种构造对输⼊变量的变换具有不变性的模型的⽅法是将不变性的性质融⼊到神经⽹络结构的构建中。这是<strong>卷积神经⽹络</strong>（<code>convolutional neural network</code>）（<code>LeCun et al.</code>, 1989; <code>LeCun et al.</code>, 1998）的基础，它被⼴泛地应⽤于图像处理领域。</p><p>考虑⼿写数字识别这个具体的任务。每个输⼊图像由⼀组像素的灰度值组成，输出为10个数字类别的后验概率分布，数字的种类对于平移、缩放以及（微⼩的）旋转具有不变性。⼀种简单的⽅法是把图像作为⼀个完全链接的神经⽹络的输⼊，假设数据集充分⼤，那么这样的⽹络原则上可以产⽣这个问题的⼀个较好的解，从⽽可以从样本中学习到恰当的不变性。然⽽，这种⽅法忽略了图像的⼀个<strong>关键性质</strong>，即距离较近的像素的相关性要远⼤于距离较远的像素的相关性。这些想法被整合到了<strong>卷积神经⽹络</strong>中，通过下⾯三种⽅式：</p><blockquote><p>（1）局部接收场；<br>（2）权值共享；<br>（3）下采样。</p></blockquote><p>在卷积层， 各个单元被组织在⼀系列平⾯中，每个平⾯被称为⼀个<strong>特征地图</strong>（<code>feature map</code>）。⼀个特征地图中的每个单元只从图像的⼀个⼩的⼦区域接收输⼊，且⼀个特征地图中的所有单元被限制为共享相同的权值。</p><p>例如，⼀个 特征地图可能由100个单元组成， 这些单元被放在了10×10的⽹格中， 每个单元从图像的⼀ 个5×5的像素块接收输⼊。于是，整个特征地图就有25个可调节的参数，加上⼀个可调节的偏置参数。 来⾃⼀个像素块的输⼊值被权值和偏置进⾏线性组合， 线性组合的结果通过公式给出的 $S$ 形⾮线性函数进⾏变换。如果我们把每个单元想象成特征检测器，那么特征地图中的所有单元都检测了输⼊图像中的相同的模式，但是位置不同。由于权值共享，这些单元的激活的计算等价于使⽤⼀个由权向量组成和“核”对图像像素的灰度值进⾏卷积。如果输⼊图像发⽣平移，那么特征地图的激活也会发⽣等量的平移，否则就不发⽣改变。这提供了神经⽹络输出对于输⼊图像的平移和变形的（近似）不变性的基础。由于通常需要检测多个特征来构造⼀个有效的模型，因此通常在卷积层会有多个特征地图，每个都有⾃⼰的权值和偏置参数。</p><p>图5.28，卷积神经⽹络的⼀个例⼦，给出了⼀层卷积单元层跟着⼀个下采样单元层，可能连续使⽤这种层对。</p><p><img src="/images/prml_20191016112651.png" alt="卷积神经⽹络"></p><p>卷积单元的输出构成了⽹络的下采样层的输⼊。对于卷积层的每个特征地图，有⼀个下采样层的单元组成的平⾯，并且下采样层的每个单元从对应的卷积层的特征地图中的⼀个⼩的接收场接收输⼊，这些单元完成了下采样。 </p><p>例如，每个下采样单元可能从对应的特征地图中的⼀个2×2单元的区域中接收输⼊，然后计算这些输⼊的平均值，乘以⼀个可调节的权值和可调节的偏置参数，然后使⽤ $S$ 形⾮线性激活函数进⾏变换。选择的接收场是连续的、⾮重叠的，从⽽ 下采样层的⾏数和列数都是卷积层的⼀半。使⽤这种⽅式，下采样层的单元的响应对于对应的输⼊空间区域中的图⽚的微⼩平移相对不敏感。</p><h1 id="七，软权值共享"><a href="#七，软权值共享" class="headerlink" title="七，软权值共享"></a>七，软权值共享</h1><p>降低具有⼤量权值参数的⽹络复杂度的⼀种⽅法是将权值分组，然后令分组内的权值相等。 然⽽，它只适⽤于限制的形式可以事先确定的问题中。</p><p>考虑<strong>软权值共享</strong> （<code>soft weight sharing</code>）（<code>Nowlan and Hinton</code>, 1992）。这种⽅法中，权值相等的硬限制被替换为 ⼀种形式的正则化，其中权值的分组倾向于取近似的值。<br>可以将权值分为若⼲组，⽽不是将所有权值分为⼀个组。分组的⽅法是使⽤⾼斯混合概率分布。混合分布中，每个⾼斯分量的均值、⽅差，以及混合系数，都会作为可调节的参数在学习过程中被确定。于是，有下⾯形式的概率密度</p><script type="math/tex; mode=display">p(\boldsymbol{w})=\prod_{i}p(w_i)\tag{5.89}</script><p>其中，</p><script type="math/tex; mode=display">p(w_i)=\sum_{j=1}^{M}\pi_{j}\mathcal{N}(w_i|\mu_j,\sigma_{j}^{2})</script><p>$\pi_j$ 为混合系数。取负对数，即可得到正则化函数，形式为</p><script type="math/tex; mode=display">\Omega(\boldsymbol{w})=-\sum_{i}\ln\left(\sum_{j=1}^{M}\pi_{j}\mathcal{N}(w_i|\mu_j,\sigma_{j}^{2})\right)\tag{5.90}</script><p>从⽽，总的误差函数为</p><script type="math/tex; mode=display">\tilde{E}(\boldsymbol{w})=E(\boldsymbol{w})+\lambda\Omega(\boldsymbol{w})\tag{5.91}</script><p>其中，$\lambda$ 是正则化系数。这个误差函数同时关于权值 $w_i$ 和混合模型参数 $\{\pi_j,\mu_j,\sigma_j\}$ 进⾏最⼩化。<br>为了最⼩化总的误差函数，把 $\{\pi_j\}$ 当成先验概率，引⼊对应的后验概率，根据相关公式，后验概率由贝叶斯定理给出，形式为</p><script type="math/tex; mode=display">\gamma_{j}(w)=\frac{\pi_{j}\mathcal{N}(w|\mu_j,\sigma_{j}^{2})}{\sum_{k}\pi_{k}\mathcal{N}(w|\mu_k,\sigma_{k}^{2})}\tag{5.92}</script><p>总的误差函数关于权值的导数为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{w_i}}=\frac{\partial{E}}{\partial{w_i}}+\lambda\sum_{j}\gamma_{j}(w_i)\frac{(w_i-\mu_j)}{\sigma_{j}^{2}}\tag{5.93}</script><p>于是，正则化项的效果是把每个权值拉向第 $j$ 个⾼斯分布的中⼼，拉⼒正⽐于对于给定权值的⾼斯分布的后验概率。<br>误差函数关于⾼斯分布的中⼼的导数为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{\mu_j}}=\lambda\sum_{i}\gamma_{j}(w_i)\frac{(\mu_j-w_i)}{\sigma_{j}^{2}}\tag{5.94}</script><p>具有简单的直观含义：把 $\mu_j$ 拉向了权值的平均值，拉⼒为第 $j$ 个⾼斯分量产⽣的权值参数的后验概率。<br>关于⽅差的导数为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{\sigma_j}}=\lambda\sum_{i}\gamma_{j}(w_i)\left(\frac{1}{\sigma_j}-\frac{(w_i-\mu_j)^{2}}{\sigma_{j}^{3}}\right)\tag{5.95}</script><p>将 $\sigma_j$ 拉向权值在对应的中⼼ $\mu_j$ 附近的偏差的平⽅的加权平均，加权平均的权系数等于由第 $j$ 个⾼斯分量产⽣的权值参数的后验概率。注意，在实际执⾏过程中，引⼊⼀个新的变量 $\xi_j$ ，它由下式定义</p><script type="math/tex; mode=display">\sigma_{j}^{2}=\exp(\xi_j)\tag{5.96}</script><p>并且，最⼩化的过程是关于 $\xi_j$ 进⾏的，这确保了参数 $\sigma_j$ 是正数。</p><p>对于关于混合系数 $\pi_j$ 的导数，需要考虑下⾯的限制条件</p><script type="math/tex; mode=display">\sum_{j}\pi_{j}=1,    0 \le \pi_j \le 1</script><p>将混合系数通过⼀组辅助变量 $\{\eta_j\}$ ⽤<code>softmax</code>函数表⽰，即</p><script type="math/tex; mode=display">\pi_j=\frac{\exp(\eta_j)}{\sum_{k=1}^{M}\exp(\eta_k)}\tag{5.97}</script><p>正则化的误差函数关于 $\{\eta_j\}$ 的导数的形式为</p><script type="math/tex; mode=display">\frac{\partial{\tilde{E}}}{\partial{\eta_{j}}}=\sum_{i}\{\pi_j-\gamma_j(w_j)\}\tag{5.98}</script><p>由此可见，$\pi_j$ 被拉向第 $j$ 个⾼斯分量的平均后验概率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，相容的⾼斯先验&quot;&gt;&lt;a href=&quot;#一，相容的⾼斯先验&quot; class=&quot;headerlink&quot; title=&quot;一，相容的⾼斯先验&quot;&gt;&lt;/a&gt;一，相
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】Hessian矩阵</title>
    <link href="https://zhangbc.github.io/2019/10/16/prml_05_02/"/>
    <id>https://zhangbc.github.io/2019/10/16/prml_05_02/</id>
    <published>2019-10-16T01:05:46.000Z</published>
    <updated>2019-10-18T10:04:24.166Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，Hessian-矩阵"><a href="#一，Hessian-矩阵" class="headerlink" title="一，Hessian 矩阵"></a>一，<strong><code>Hessian</code></strong> 矩阵</h1><p>反向传播也可以⽤来计算误差函数的⼆阶导数，形式为</p><script type="math/tex; mode=display">\frac{\partial^{2}{E}}{\partial{w_{ji}}\partial{w_{kl}}}</script><p>注意，有时将所有的权值和偏置参数看成⼀个向量（记作 $\boldsymbol{w}$ ）的元素 $w_i$ 更⽅便，此时⼆阶导数组成了<code>Hessian</code>矩阵 $\boldsymbol{H}$ 的元素 $H_{ij}$ ，其中 $i, j \in \{1,\dots, W\}$ ，且 $W$ 是权值和偏置的总数。<code>Hessian</code>矩阵在神经⽹络计算的重要的作⽤，包括：</p><blockquote><p>1）⼀些⽤来训练神经⽹络的⾮线性最优化算法是基于误差曲⾯的⼆阶性质的， 这些性质由<code>Hessian</code>矩阵控制（<code>Bishop and Nabney</code>, 2008）；<br>2）对于训练数据的微⼩改变，<code>Hessian</code>矩阵构成了快速重新训练前馈⽹络的算法的基础 （<code>Bishop</code>, 1991）；<br>3）<code>Hessian</code>矩阵的逆矩阵⽤来鉴别神经⽹络中最不重要的权值，这是⽹络“剪枝”算法的⼀部分 （<code>LeCun et al.</code>, 1990）；<br>4）<code>Hessian</code>矩阵是贝叶斯神经⽹络的拉普拉斯近似的核⼼。它的逆矩阵⽤来确定训练过的神经⽹络的预测分布，它的特征值确定了超参数的值，它的⾏列式⽤来计算模型证据。</p></blockquote><h1 id="二，对角近似"><a href="#二，对角近似" class="headerlink" title="二，对角近似"></a>二，对角近似</h1><p>对于模式 $n$ ，<code>Hessian</code>矩阵的对角线元素可以写成</p><script type="math/tex; mode=display">\frac{\partial^{2}{E_n}}{\partial{w_{ji}^{2}}}=\frac{\partial^{2}{E_n}}{\partial{a_{j}^{2}}}z_{i}^{2}\tag{5.56}</script><p>从而，反向传播⽅程的形式为</p><script type="math/tex; mode=display">\frac{\partial^{2}{E_n}}{\partial{a_{j}^{2}}}=h^{\prime}(a_j)^2\sum_{k}\sum_{k^{\prime}}w_{kj}w_{k^{\prime}j}\frac{\partial^{2}{E_n}}{\partial{a_k}\partial{a_{k^{\prime}}}}+h^{\prime\prime}(a_j)\sum_{k}w_{kj}\frac{\partial{E_n}}{\partial{a_k}}\tag{5.57}</script><p>如果忽略⼆阶导数中⾮对角线元素， 那么有（<code>Becker and LeCun</code>, 1989; <code>LeCun et al.</code>, 1990）</p><script type="math/tex; mode=display">\frac{\partial^{2}{E_n}}{\partial{a_{j}^{2}}}=h^{\prime}(a_j)^2\sum_{k}w_{kj}^{2}\frac{\partial^{2}{E_n}}{\partial{a_k^{2}}}+h^{\prime\prime}(a_j)\sum_{k}w_{kj}\frac{\partial{E_n}}{\partial{a_k}}\tag{5.58}</script><h1 id="三，外积近似"><a href="#三，外积近似" class="headerlink" title="三，外积近似"></a>三，外积近似</h1><p>当神经⽹络应⽤于回归问题时，通常使⽤下⾯形式的平⽅和误差函数</p><script type="math/tex; mode=display">E=\frac{1}{2}\sum_{n=1}^{N}(y_n-t_n)^2\tag{5.59}</script><p>考虑单⼀输出的情形（推⼴到多个输出是很直接的），可以把<code>Hessian</code>矩阵写成下⾯的形式</p><script type="math/tex; mode=display">\boldsymbol{H}=\nabla\nabla{E}=\sum_{n=1}^{N}\nabla{y_n}(\nabla{y_n})^{T}+\sum_{n=1}^{N}(y_n-t_n)\nabla\nabla{y_n}\tag{5.60}</script><p>通过忽略公式(5.60)的第⼆项，我们就得到了 <strong><code>Levenberg-Marquardt</code>近似</strong>，或者称为<strong>外积近似</strong>（<code>outer product approximation</code>）（因为此时<code>Hessian</code>矩阵由向量外积的求和构造出来），形式为</p><script type="math/tex; mode=display">\boldsymbol{H}\simeq\sum_{n=1}^{N}\boldsymbol{b}_n\boldsymbol{b}_n^{T}\tag{5.61}</script><p>其中 $\boldsymbol{b}_n\equiv\nabla{a_n}=\nabla{y_n}$ ， 因为输出单元的激活函数就是恒等函数。这种近似只在⽹络被恰当地训练时才成⽴， 对于⼀个⼀般的⽹络映 射，公式(5.60)的右侧的⼆阶导数项通常不能忽略。</p><p>在误差函数为<strong>交叉熵误差函数</strong>，输出单元激活函数为<code>logistic sigmoid</code>函数的神经⽹络中，对应的近似为</p><script type="math/tex; mode=display">\boldsymbol{H}\simeq\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{b}_n\boldsymbol{b}_n^{T}\tag{5.62}</script><p>对于输出函数为<code>softmax</code>函数的多类神经⽹络，可以得到类似的结果。</p><h1 id="四，Hessian-矩阵的逆矩阵"><a href="#四，Hessian-矩阵的逆矩阵" class="headerlink" title="四，Hessian 矩阵的逆矩阵"></a>四，<strong><code>Hessian</code></strong> 矩阵的逆矩阵</h1><p>使⽤外积近似，可以提出⼀个计算<code>Hessian</code>矩阵的逆矩阵的⾼效⽅法（<code>Hassibi and Stork</code>, 1993）。⾸先，⽤矩阵的记号写出外积近似，即</p><script type="math/tex; mode=display">\boldsymbol{H}_{N}=\sum_{n=1}^{N}\boldsymbol{b}_n\boldsymbol{b}_n^{T}\tag{5.63}</script><p>其中， $\boldsymbol{b}_n\equiv\nabla_{\boldsymbol{w}}{a_n}$  是数据点 $n$ 产⽣的输出单元激活对梯度的贡献。 </p><p>现在推导⼀个建⽴<code>Hessian</code>矩阵的顺序步骤，每次处理⼀个数据点。假设已经使⽤前 $L$ 个数据点得到了<code>Hessian</code>矩阵的逆矩阵。通过将第 $L+1$ 个数据点的贡献单独写出来，有</p><script type="math/tex; mode=display">\boldsymbol{H}_{L+1}=\boldsymbol{H}_L+\boldsymbol{b}_{L+1}\boldsymbol{b}_{L+1}^{T}\tag{5.64}</script><p>考虑下⾯的矩阵恒等式</p><script type="math/tex; mode=display">(\boldsymbol{M}+\boldsymbol{v}\boldsymbol{v}^{T})^{-1}=\boldsymbol{M}^{-1}-\frac{(\boldsymbol{M}^{-1}\boldsymbol{v})(\boldsymbol{v^{T}\boldsymbol{M}^{-1}})}{1+\boldsymbol{v}^{T}\boldsymbol{M}^{-1}\boldsymbol{v}}\tag{5.65}</script><p>如果令 $\boldsymbol{H}_L=\boldsymbol{M}$ ，且 $\boldsymbol{b}_{L+1}=\boldsymbol{v}$ ，有</p><script type="math/tex; mode=display">\boldsymbol{H}_{L+1}^{-1}=\boldsymbol{H}_{L}^{-1}-\frac{\boldsymbol{H}_{L}^{-1}\boldsymbol{b}_{L+1}\boldsymbol{b}_{L+1}^{T}\boldsymbol{H}_{L}^{-1}}{1+\boldsymbol{b}_{L+1}^{T}\boldsymbol{H}_{L}^{-1}\boldsymbol{b}_{L+1}}\tag{5.66}</script><p>使⽤这种⽅式，数据点可以依次使⽤，直到 $L+1=N$ ，整个数据集被处理完毕。于是，这个结果表⽰⼀个计算<code>Hessian</code>矩阵的逆矩阵的算法， 这个算法只需对数据集扫描⼀次。最开始的矩阵 $\boldsymbol{H}_0$ 被选为 $\alpha\boldsymbol{I}$ ，其中 $\alpha$ 是⼀个较⼩的量，从⽽算法实际找的是 $\boldsymbol{H}+\alpha\boldsymbol{I}$ 的逆矩阵。</p><h1 id="五，有限差"><a href="#五，有限差" class="headerlink" title="五，有限差"></a>五，有限差</h1><p>如果对每对可能的权值施加⼀个扰动，那么有</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial^{2}E}{\partial{w_{ji}}\partial_{w_{lk}}}&=\frac{1}{4\epsilon^{2}}\{E(w_{ji}+\epsilon,w_{lk}+\epsilon)-E(w_{ji}+\epsilon,w_{lk}-\epsilon)\\&-E(w_{ji}-\epsilon,w_{lk}+\epsilon)+-E(w_{ji}-\epsilon,w_{lk}-\epsilon)+O(\epsilon^{2})\}\end{aligned}\tag{5.67}</script><p>通过使⽤对称的中⼼差，确保了残留的误差项是 $O(\epsilon^{2})$ ⽽不是 $O(\epsilon)$ 。 由于在<code>Hessian</code>矩阵中有 $W^2$ 个元素，且每个元素的计算需要四次正向传播过程，每个传播过程需要 $O(W)$ 次操作（每个模式），因此看到这种⽅法计算完整的<code>Hessian</code>矩阵需要 $O(W^3)$ 次操作。所以，这个⽅法的计算性质很差，虽然在实际应⽤中它对于检查反向传播算法的执⾏的正确性很有⽤。 </p><p>⼀个更加⾼效的数值导数的⽅法是将中⼼差应⽤于⼀阶导数，⽽⼀阶导数可以通过反向传播⽅法计算。即</p><script type="math/tex; mode=display">\frac{\partial^{2}E}{\partial{w_{ji}}\partial_{w_{lk}}}=\frac{1}{2\epsilon}\left\{\frac{\partial{E}}{\partial{w_{ji}}}(w_{kl}+\epsilon)-\frac{\partial{E}}{\partial{w_{ji}}}(w_{kl}-\epsilon)\right\}+O(\epsilon^{2})\tag{5.68}</script><p>由于只需要对 $W$ 个权值施加扰动，且梯度可以通过 $O(W)$ 次计算得到，因此看到这种⽅法可以在 $O(W^2)$ 次操作内得到<code>Hessian</code>矩阵。</p><h1 id="六，Hessian-矩阵的精确计算"><a href="#六，Hessian-矩阵的精确计算" class="headerlink" title="六，Hessian 矩阵的精确计算"></a>六，<strong><code>Hessian</code></strong> 矩阵的精确计算</h1><p>考虑⼀个具有两层权值的⽹络，这种⽹络中待求的⽅程很容易推导，将使⽤下标 $i$ 和 $i^{\prime}$ 表⽰输⼊，⽤下标 $j$ 和 $j^{\prime}$ 表⽰隐含单元，⽤下标 $k$ 和 $k^{\prime}$ 表⽰输出。⾸先定义</p><script type="math/tex; mode=display">\delta_k=\frac{\partial{E_n}}{\partial{a_k}},M_{kk^{\prime}}=\frac{\partial^{2}{E_n}}{\partial{a_k}\partial{a_{k^{\prime}}}}</script><p>其中 $E_n$ 是数据点 $n$ 对误差函数的贡献。于是，这个⽹络的<code>Hessian</code>矩阵可以被看成三个独⽴的模块，即</p><p>1）两个权值都在第⼆层</p><script type="math/tex; mode=display">\frac{\partial^{2}E_n}{\partial{w_{kj}^{(2)}}\partial{w_{k^{\prime}j^{\prime}}^{(2)}}}=z_jz_{j^{\prime}}M_{kk^{\prime}}\tag{5.69}</script><p>2）两个权值都在第⼀层</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial^{2}E_n}{\partial{w_{ji}^{(1)}}\partial{w_{j^{\prime}i^{\prime}}^{(1)}}}&=x_ix_{i^{\prime}}h^{\prime\prime}(a_{j^{\prime}})I_{jj^{\prime}}\sum_{k}w_{kj^{\prime}}^{(2)}\delta_{k}\\&+x_ix_{i^{\prime}}h^{\prime}(a_{j^{\prime}})h^{\prime}(a_j)\sum_{k}\sum_{k^{\prime}}w_{k^{\prime}j^{\prime}}^{(2)}w_{kj}^{(2)}M_{kk^{\prime}}\end{aligned}\tag{5.70}</script><p>3）每⼀层有⼀个权值</p><script type="math/tex; mode=display">\frac{\partial^{2}E_n}{\partial{w_{ji}^{(1)}}\partial{w_{kj^{\prime}}^{(2)}}}=x_{i}h^{\prime}(a_{j})\left\{\delta_{k}I_{jj^{\prime}}+z_{j^{\prime}}\sum_{k^{\prime}}w_{k^{\prime}j}^{(2)}M_{kk^{\prime}}\right\}\tag{5.71}</script><p>其中，$I_{jj^{\prime}}$ 是单位矩阵的第 $j$, $j^{\prime}$ 个元素。</p><h1 id="七，Hessian-矩阵的快速乘法"><a href="#七，Hessian-矩阵的快速乘法" class="headerlink" title="七，Hessian 矩阵的快速乘法"></a>七，<strong><code>Hessian</code></strong> 矩阵的快速乘法</h1><p>尝试寻找⼀种只需 $O(W)$ 次操作的直接计算 $\boldsymbol{v}^{T}\boldsymbol{H}$ 的⾼效⽅法。⾸先注意到</p><script type="math/tex; mode=display">\boldsymbol{v}^{T}\boldsymbol{H}=\boldsymbol{v}^{T}\nabla(\nabla{E})\tag{5.72}</script><p>其中 $\nabla$ 表⽰权空间的梯度算符。然后，可以写下计算 $\nabla{E}$ 的标准正向传播和反向传播的⽅程， 继而得到⼀组计算 $\boldsymbol{v}^{T}\boldsymbol{H}$ 的正向传播和反向传播的⽅程（<code>Møller</code>, 1993; <code>Pearlmutter</code>, 1994）。这对应于将微分算符 $\boldsymbol{v}^{T}\nabla$ 作⽤于原始的正向传播和反向传播的⽅程。<code>Pearlmutter</code>（1994）使⽤记号 $\mathcal{R}\{·\}$ 表⽰算符 $\boldsymbol{v}^{T}\nabla$ 。下⾯的分析过程很直接，会使⽤通常的微积分规则，以及下⾯的结果</p><script type="math/tex; mode=display">\mathcal{R}\{\boldsymbol{w}\}=\boldsymbol{v}</script><p>使⽤⼀个简单的例⼦来说明这个⽅法，使⽤两层⽹络，以及线性的输出单元和平⽅和误差函数。考虑数据集⾥的⼀个模式对于误差函数的贡献。这样，所要求解的向量可以通过求出每个模式各⾃的贡献然后求和的⽅式得到。对于两层神经⽹络，正向传播⽅程为</p><script type="math/tex; mode=display">a_j=\sum_{i}w_{ji}x_{i}\\z_j=h(a_j)\\y_k=\sum_{j}w_{kj}z_j</script><p>现在使⽤ $\mathcal{R}\{·\}$ 算符作⽤于这些⽅程上，得到⼀组正向传播⽅程，形式为</p><script type="math/tex; mode=display">\mathcal{R}\{a_j\}=\sum_{i}v_{ji}x_{i}\\\mathcal{R}\{z_j\}=h^{\prime}(a_j)\mathcal{R}\{a_j\}\\\mathcal{R}\{y_k\}=\sum_{j}w_{kj}\mathcal{R}\{z_j\}+\sum_{j}v_{kj}z_j</script><p>其中，$v_{ji}$ 是向量 $\boldsymbol{v}$ 中对应于权值 $w_{ji}$ 的元素。</p><p>由于考虑的是平⽅和误差函数，因此有下⾯的标准的反向传播表达式</p><script type="math/tex; mode=display">\delta_{k}=y_k-t_k\\\delta_j=h^{\prime}(a_j)\sum_{k}w_{kj}\delta_{k}</script><p>将 $\mathcal{R}\{·\}$ 算符作⽤于这些⽅程上，得到⼀组反向传播⽅程，形式为</p><script type="math/tex; mode=display">\mathcal{R}\{\delta_k\}=\mathcal{R}\{y_k\}\\\begin{aligned}\mathcal{R}\{\delta_j\}&=h^{\prime\prime}(a_j)\mathcal{R}\{a_j\}\sum_{k}w_{kj}\delta_{k}\\&+h^{\prime}(a_j)\sum_{k}v_{kj}\delta_{k}+h^{\prime}(a_j)\sum_{k}w_{kj}\mathcal{R}\{\delta_{k}\}\end{aligned}</script><p>最后，有误差函数的⼀阶导数的⽅程</p><script type="math/tex; mode=display">\frac{\partial{E}}{\partial{w_{kj}}}=\delta_{k}z_j\\\frac{\partial{E}}{\partial{w_{ji}}}=\delta_{j}x_i</script><p>使⽤ $\mathcal{R}\{·\}$ 算符作⽤在这些⽅程上，我们得到了下⾯的关于 $\boldsymbol{v}^{T}\boldsymbol{H}$ 的表达式</p><script type="math/tex; mode=display">\mathcal{R}\left\{\frac{\partial{E}}{\partial{w_{kj}}}\right\}=\mathcal{R}\{\delta_{k}\}z_j+\delta_{k}\mathcal{R}\{z_j\}\\\mathcal{R}\left\{\frac{\partial{E}}{\partial{w_{ji}}}\right\}=x_i\mathcal{R}\left\{\delta_j\right\}</script><p>如图5.11～13，使⽤从正弦数据集中抽取的10个数据点训练的两层神经⽹络的例⼦。 各图分别给出了使⽤ $M = 1, 3, 10$ 个隐含单元调节⽹络的结果，调节的⽅法是使⽤放缩的共轭梯度算法来最⼩化平⽅和误差函数。</p><p><img src="/images/prml_20191015235858.png" alt="M=1"></p><p><img src="/images/prml_20191015235909.png" alt="M=3"></p><p><img src="/images/prml_20191015235919.png" alt="M=10"></p><p>如图5.14，对于多项式数据集，测试集的平⽅和误差与⽹络的隐含单元的数量的图像。对于每个⽹络规模，都随机选择了30个初始点，这展⽰了局部最⼩值的效果。对于每个新的初始点，权向量通过从⼀个各向 同性的⾼斯分布中取样，这个⾼斯分布的均值为零，⽅差为10。</p><p><img src="/images/prml_20191015235404.png" alt="多项式数据集"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，Hessian-矩阵&quot;&gt;&lt;a href=&quot;#一，Hessian-矩阵&quot; class=&quot;headerlink&quot; title=&quot;一，Hessian 矩阵
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】前馈神经网络</title>
    <link href="https://zhangbc.github.io/2019/10/16/prml_05_01/"/>
    <id>https://zhangbc.github.io/2019/10/16/prml_05_01/</id>
    <published>2019-10-16T00:29:43.000Z</published>
    <updated>2019-10-16T01:42:01.327Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，前馈神经网络"><a href="#一，前馈神经网络" class="headerlink" title="一，前馈神经网络"></a>一，前馈神经网络</h1><h2 id="1，前馈神经网络"><a href="#1，前馈神经网络" class="headerlink" title="1，前馈神经网络"></a>1，前馈神经网络</h2><p>基于固定⾮线性基函数 $\phi_{j}(\boldsymbol{x})$ 的线性组合，形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})=f\left(\sum_{j=1}^{M}w_{j}\phi_{j}(\boldsymbol{x})\right)\tag{5.1}</script><p>其中 $f(·)$ 在分类问题中是⼀个⾮线性激活函数， 在回归问题中为恒等函数。现在的⽬标是推⼴这个模型，使得基函数 $\phi_{j}(\boldsymbol{x})$ 依赖于参数，从⽽能够让这些参数以及系数 $\{w_j\}$ 能够在训练阶段调节。</p><p><strong>神经⽹络</strong>使⽤与公式(5.1)形式相同的基函数，即每个基函数本⾝是输⼊的线性组合的⾮线性函数，其中线性组合的系数是可调节参数。</p><p>⾸先，构造输⼊变量 $x_1, \dots , x_D$ 的 $M$ 个线性组合，形式为</p><script type="math/tex; mode=display">a_{j}=\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\tag{5.2}</script><p>其中 $j = 1,\dots, M$ ， 且上标<code>(1)</code>表⽰对应的参数是神经⽹络的<strong>第⼀“层”</strong>。参数 $w_{ji}^{(1)}$ 称为 <strong>权</strong>（<code>weight</code>）， 参数 $w_{j0}^{(1)}$ 称为<strong>偏置</strong>（<code>bias</code>），$a_{j}$ 被称为<strong>激活</strong>（<code>activation</code>）。每个激活都使⽤⼀个可微的⾮线性激活函数（<code>activation function</code>）$h(·)$ 进⾏变换，可得</p><script type="math/tex; mode=display">z_{j}=h(a_{j})\tag{5.3}</script><p>这些量对应于公式(5.1)中的基函数的输出， 这些基函数在神经⽹络中被称为<strong>隐含单元</strong> （<code>hidden unit</code>）。⾮线性函数 $h(·)$ 通常被选为 <strong><code>S</code>形的函数</strong>，例如<code>logistic sigmoid</code>函数或者双曲正切函数。根据公式(5.1)，这些值再次线性组合，得到<strong>输出单元激活</strong>（<code>output unit activation</code>）</p><script type="math/tex; mode=display">a_{k}=\sum_{j=1}^{M}w_{kj}^{(2)}z_{j}+w_{k0}^{(2)}\tag{5.4}</script><p>其中 $k = 1, \dots, K$ ， 且 $K$ 是输出的总数量。 这个变换对应于神经⽹络的第⼆层， 并且 $w_{k0}^{(2)}$ 是偏置参数。使⽤⼀个恰当的激活函数对输出单元激活进⾏变换，得到神经⽹络的⼀组输出 $y_k$ ，激活函数的选择由数据本⾝以及⽬标变量的假定的分布确定。因此对于标准的回归问题， 激活函数是恒等函数， 从⽽ $y_k = a_k$ 。 类似地， 对于多个⼆元分类问题， 每个输出单元激活使⽤<code>logistic sigmoid</code>函数进⾏变换，即</p><script type="math/tex; mode=display">y_k=\sigma(a_k)\tag{5.5}</script><p>其中，</p><script type="math/tex; mode=display">\sigma(a)=\frac{1}{1+\exp(-a)}</script><p>综上可知，对于<code>sigmoid</code>输出单元激活函数，整体的⽹络函数为</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})=\sigma\left(\sum_{j=1}^{M}w_{kj}^{(2)}h\left(\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\right)+w_{k0}^{(2)}\right)\tag{5.6}</script><p>如图5.1，对应于公式(5.6)的两层神经⽹络的⽹络图。输⼊变量、隐含变量、输出变量都表⽰为结点，权参数被表⽰为结点之间的链接，其中偏置参数被表⽰为来⾃额外的输⼊变量 $x_0$ 和隐含变量 $z_0$ 的链接。箭头表⽰信息流在⽹络中进⾏前向传播的⽅向。</p><p><img src="/images/prml_20191008084859.png" alt="两层神经⽹络"></p><p>可以通过定义额外的输⼊变量 $x_0$ 的⽅式将公式(5.2)中的偏置参数整合到权参数集合中，其中额外的输⼊变量 $x_0$ 的值被限制为 $x_0 = 1$，因此公式(5.2)的形式为</p><script type="math/tex; mode=display">a_{j}=\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}\tag{5.7}</script><p>类似地，把第⼆层的偏置整合到第⼆层的权参数中，从⽽整体的⽹络函数为</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{w})=\sigma\left(\sum_{j=1}^{M}w_{kj}^{(2)}h\left(\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}\right)\right)\tag{5.8}</script><p>神经⽹络模型由两个处理阶段组成，每个阶段都类似于感知器模型，因此神经⽹络也被称为<strong>多层感知器</strong>（<code>multilayer perceptron</code>），或者 <strong><code>MLP</code></strong>。与感知器模型相⽐，⼀个重要的<strong>区别</strong>是神经⽹络在隐含单元中使⽤连续的<code>sigmoid</code>⾮线性函数，⽽感知器使⽤阶梯函数⾮线性函数。这意味着神经⽹络函数关于神经⽹络参数是可微的，这个性质在神经⽹络的训练过程中起着重要的作⽤。</p><p>神经⽹络结构的⼀个<strong>扩展</strong>是引⼊<strong>跨层</strong>（<code>skip-layer</code>）<strong>链接</strong>，每个跨层链接都关联着⼀个对应的可调节参数。</p><p><strong>前馈</strong>（<code>feed-forward</code>）<strong>结构</strong>：⽹络中不能存在有向圈，从⽽确保了输出是输⼊的确定函数。</p><p>举例：⽹络中每个（隐含或者输出）单元都计算了⼀个下⾯的函数</p><script type="math/tex; mode=display">z_k=h\left(\sum_{j}w_{kj}z_{i}\right)</script><p>其中，求和的对象是所有向单元 $k$ 发送链接的单元（偏置参数也包含在了求和式当中）。</p><p>如图5.2，具有⼀般的前馈拓扑结构的神经⽹络，注意，每个隐含电源和输出单元都与⼀个偏置参数关联。</p><p><img src="/images/prml_20191008091241.png" alt="前馈拓扑结构"></p><p>如图5.3～5.6，多层感知器的能⼒说明，它⽤来近似四个不同的函数。 (<code>a</code>) $f(x) = x^2$ ，(<code>b</code>) $f(x) = \sin(x)$， (<code>c</code>) $f(x) = |x|$，(<code>d</code>) $f(x) = H(x)$，其中 $H(x)$ 是⼀个硬阶梯函数。在每种情况下，$N = 50$ 个数据点（⽤蓝点 表⽰）从区间 $(−1, 1)$ 中均匀分布的 $x$ 中进⾏取样，然后计算出对应的 $f(x)$ 值。这些数据点之后⽤来训练⼀个具有3个隐含单元的两层神经⽹络，隐含单元的激活函数为<code>tanh</code>函数，输出为线性输出单元。⽣成的⽹络函数使⽤红⾊曲线表⽰，三个隐含单元的输出⽤三条虚线表⽰。</p><p><img src="/images/prml_20191008092026.png" alt="a"></p><p><img src="/images/prml_ 20191008092041.png" alt="b"></p><p><img src="/images/prml_20191008092055.png" alt="c"></p><p><img src="/images/prml_20191008092105.png" alt="d"></p><h2 id="2，权空间对称性"><a href="#2，权空间对称性" class="headerlink" title="2，权空间对称性"></a>2，权空间对称性</h2><p>前馈神经⽹络的⼀个<strong>性质</strong>是，对于多个不同的权向量 $\boldsymbol{w}$ 的选择，⽹络可能产⽣同样的从输⼊到输出的映射函数（<code>Chen et al.</code>, 1993）。</p><p>考虑两层⽹络，⽹络有 $M$ 个隐含结点，激活函数是双曲正切函数，且两层之间完全链接。如果把作⽤于某个特定的隐含单元的所有的权值以及偏置全部变号，那么对于给定的输⼊模式， 隐含单元的激活的符号也会改变。 这是因为双曲正切函数是⼀个奇函数， 即 $\tan h(−a) = −\tan h(a)$。这种变换可以通过改变所有从这个隐含单元到输出单元的权值的符号的⽅式进⾏精确补偿。因此，通过改变特定⼀组权值（以及偏置）的符号，⽹络表⽰的输⼊-输出映射函数不会改变，因此我们已经找到了两个不同的权向量产⽣同样的映射函数。对于 $M$ 个 隐含单元，会有 $M$ 个这样的“符号改变”对称性，因此任何给定的权向量都是 $2^M$ 个等价的权向量中的⼀个。</p><p>类似地，假设将与某个特定的隐含结点相关联的所有输⼊和输出的权值（和偏置）都变为与不同的隐含结点相关联的对应的权值（和偏置）。与之前⼀样，这显然使得⽹络的输⼊-输出映射不变，但是对应了⼀个不同的权向量。对于 $M$ 个隐含结点，任何给定的权向量都属于这种交换对称性产⽣的 $M!$ 个等价的权向量中的⼀个，它对应于 $M!$ 个不同的隐含单元的顺序。于是，⽹络有⼀个整体的权空间对称性因⼦ $M!2^M$ 。</p><h1 id="二，网络训练"><a href="#二，网络训练" class="headerlink" title="二，网络训练"></a>二，网络训练</h1><h2 id="1，回归问题"><a href="#1，回归问题" class="headerlink" title="1，回归问题"></a>1，回归问题</h2><p>给定⼀个由输⼊向量 $\{\boldsymbol{x_n}\}(n = 1, \dots , N)$ 组成的训练集，以及⼀ 个对应的⽬标向量 $\boldsymbol{t}_n$ 组成的集合，要最⼩化误差函数</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}||\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w})-\boldsymbol{t}_n||^{2}\tag{5.9}</script><p>⾸先， 讨论回归问题。考虑⼀元⽬标变量 $t$ 的情形，假定 $t$ 服从⾼斯分布，均值与 $\boldsymbol{x}$ 相关，由神经⽹络的输出确定，即</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w})=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1})\tag{5.10}</script><p>其中 $\beta$ 是⾼斯噪声的精度（⽅差的倒数）。</p><p>给定⼀个由 $N$ 个独⽴同分布的观测组成的数据集 $\mathbf{X} = \{\boldsymbol{x}_1, \dots, \boldsymbol{x}_N\}$， 以及对应的⽬标值 $\mathbf{t} = \{t_1, \dots, t_N\}$</p><p>，构造对应的似然函数</p><script type="math/tex; mode=display">p(\mathbf{t}|\boldsymbol{X},\boldsymbol{w},\beta)=\prod_{n=1}^{N}p(t_n|\boldsymbol{x}_n,\boldsymbol{w},\beta)\tag{5.11}</script><p>取负对数，可得到误差函数</p><script type="math/tex; mode=display">\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x},\boldsymbol{w})-t_n\}^{2}-\frac{N}{2}\ln\beta+\frac{N}{2}\ln(2\pi)</script><p>这可以⽤来学习参数 $\boldsymbol{w}$ 和 $\beta$ 。</p><p>⾸先考虑 $\boldsymbol{w}$ 的确定。最⼤化似然函数等价于最⼩化平⽅和误差函数：</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^{N}\{\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w})-\boldsymbol{t}_n\}^{2}\tag{5.12}</script><p>其中去掉了相加的和相乘的常数。通过最⼩化 $E(\boldsymbol{w})$ 的⽅式得到 $\boldsymbol{w}$ 值被记作  $\boldsymbol{w}_{ML}$ ， 因为它对应于最⼤化似然函数。</p><p>$\beta$ 的值可以通过最⼩化似然函数的负对数的⽅式求得，为</p><script type="math/tex; mode=display">\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}\{\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w}_{ML})-\boldsymbol{t}_n\}^{2}\tag{5.13}</script><p>如果有多个⽬标变量，并且假设给定 $\boldsymbol{x}$ 和 $\boldsymbol{w}$ 的条件下，⽬标变量之间相互独⽴，且噪声精度均为 $\beta$ ，那么⽬标变量的条件分布为</p><script type="math/tex; mode=display">p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w})=\mathcal{N}(\boldsymbol{t}|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1}\boldsymbol{I})\tag{5.14}</script><p>噪声的精度为</p><script type="math/tex; mode=display">\frac{1}{\beta_{ML}}=\frac{1}{NK}\sum_{n=1}^{N}\|\boldsymbol{y}(\boldsymbol{x_n},\boldsymbol{w}_{ML})-\boldsymbol{t}_n\|^{2}\tag{5.15}</script><p>其中 $K$ 是⽬标变量的数量。</p><p>在回归问题中，我们可以把神经⽹络看成具有⼀个恒等输出激活函数的模型，即 $y_k = a_k$ 。对应的平⽅和误差函数有下⾯的性质：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial a_k}=y_k-t_k\tag{5.16}</script><p>现在考虑⼆分类的情形。⼆分类问题中，有⼀个单⼀⽬标变量 $t$，且 $t = 1$ 表⽰类别 $\mathcal{C}_1$ ，$t = 0$ 表⽰类别 $\mathcal{C}_2$ 。遵循对于标准链接函数的讨论，考虑⼀个具有单⼀输出的⽹络，它的激活函数是<code>logistic sigmoid</code>函数</p><script type="math/tex; mode=display">y=\sigma(a)\equiv\frac{1}{1+\exp(-a)}\tag{5.17}</script><p>从⽽ $0\le y(\boldsymbol{x}, \boldsymbol{w})\le1$ 。可以把 $y(\boldsymbol{x},\boldsymbol{w})$ 表⽰为条件概率 $p(\mathcal{C}_1|\boldsymbol{x})$ ， 此时 $p(\mathcal{C}_2|\boldsymbol{x})$ 为 $1 − y(\boldsymbol{x},\boldsymbol{w})$。如果给定了输⼊，那么⽬标变量的条件概率分布是⼀个伯努利分布，形式为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\boldsymbol{w})=y(\boldsymbol{x},\boldsymbol{w})^{t}\{1-y(\boldsymbol{x},\boldsymbol{w})\}^{1-t}\tag{5.18}</script><p>如果考虑⼀个由独⽴的观测组成的训练集，那么由负对数似然函数给出的误差函数就是⼀个交叉熵（<code>cross-entropy</code>）误差函数，形式为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\{t_n\ln y_n+(1-t_n)\ln(1-y_n)\}\tag{5.19}</script><p>其中 $y_n$ 表⽰ $y(\boldsymbol{x}_n,\boldsymbol{w})$ 。</p><p>如果有 $K$ 个相互独⽴的⼆元分类问题， 那么可以使⽤具有 $K$ 个输出的神经⽹络， 每个输出都有⼀个<code>logistic sigmoid</code>激活函数，与每个输出相关联的是⼀个⼆元类别标签 $t_k\in\{0,1\}$ ，其中 $k=1, \dots, K$ 。如果假定类别标签是独⽴的，那么给定输⼊向量，⽬标向量的条件概率分布为</p><script type="math/tex; mode=display">p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w})=\prod_{k=1}^{K}y_k(\boldsymbol{x},\boldsymbol{w})^{t_k}[1-y_k(\boldsymbol{x},\boldsymbol{w})]^{1-t_k}\tag{5.20}</script><p>取似然函数的负对数，可以得误差函数</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\sum_{k=1}^{K}\{t_{nk}\ln y_{nk}+(1-t_{nk})\ln(1-y_{nk})\}\tag{5.21}</script><p>其中 $y_{nk}$ 表⽰ $y_k(\boldsymbol{x}_n,\boldsymbol{w})$ 。</p><p>如图5.7，误差函数 $E(\boldsymbol{w})$ 的⼏何表⽰，其中，误差函数被表⽰为权空间上的⼀个曲⾯。点 $\boldsymbol{w}_A$ 是⼀个局部最⼩值，点 $\boldsymbol{w}_B$ 是全局最⼩值。在任意点 $\boldsymbol{w}_C$ 处，误差函数的局部梯度由向量 $\nabla{E}$ 给出。</p><p><img src="/images/prml_20191012111823.png" alt="误差函数E的⼏何表⽰"></p><p>最后，我们考虑标准的多分类问题，其中每个输⼊被分到 $K$ 个互斥的类别中。⼆元⽬标变量 $t_k\in\ {0,1}$ 使 ⽤“<code>1-of-K</code>”表达⽅式来表⽰类别，从⽽⽹络的输出可以表⽰为 $y_k(\boldsymbol{x},\boldsymbol{w}) = p(t_k=1|\boldsymbol{x})$ ，因此误差函数为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln y_{k}(\boldsymbol{x}_n,\boldsymbol{w})\tag{5.22}</script><p>输出单元激活函数（对应于标准链接函数）是<code>softmax</code>函数</p><script type="math/tex; mode=display">y_k(\boldsymbol{x},\boldsymbol{w})=\frac{\exp(a_k(\boldsymbol{x},\boldsymbol{w}))}{\sum_{j}\exp(a_j(\boldsymbol{x},\boldsymbol{w}))}\tag{5.23}</script><p>其中，$0 \le y_k \le 1$ ，且 $\sum_{k} y_k=1$ 。</p><p>总之，根据解决的问题类型，关于输出单元激活函数和对应的误差函数，都存在⼀个⾃然的选择。对于<strong>回归问题</strong>，使⽤线性输出和平⽅和误差函数，对于（多类独⽴的）<strong>⼆元分类问题</strong>， 使⽤<code>logistic sigmoid</code>输出以及交叉熵误差函数，对于<strong>多类分类问题</strong>， 使⽤<code>softmax</code>输出以及对应的多分类交叉熵错误函数。对于涉及到<strong>两类分类问题</strong>，可以使⽤单⼀的<code>logistic sigmoid</code>输出，也可以使⽤神经⽹络，这个神经⽹络有两个输出，且输出激活函数为<code>softmax</code>函数。</p><h2 id="2，参数最优化"><a href="#2，参数最优化" class="headerlink" title="2，参数最优化"></a>2，参数最优化</h2><p>考虑寻找能够使得选定的误差函数 $E(\boldsymbol{w})$ 达到最⼩值的权向量 $\boldsymbol{w}$ 。现在，考虑误差函数的⼏何表⽰是很有⽤的，可以把误差函数看成位于权空间的⼀个曲⾯。⾸先注意到，如果在权空间中⾛⼀⼩步， 从 $\boldsymbol{w}$ ⾛到 $\boldsymbol{w}+\delta\boldsymbol{w}$ ， 那么误差函数的改变为 $\delta E \simeq \delta \boldsymbol{w}^{T}\nabla E(\boldsymbol{w})$ ，其中向量 $\delta E(\boldsymbol{w})$ 在误差函数增加速度最⼤的⽅向上。 由于误差 $E(\boldsymbol{w})$ 是 $\boldsymbol{w}$ 的光滑连续函数，因此它的最⼩值出现在权空间中误差函数梯度等于零的位置上， 即</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=0</script><p>如果最⼩值不在这个位置上，我们就可以沿着⽅向 $−\nabla E(\boldsymbol{w})$ ⾛⼀⼩步，进⼀步减⼩误差。梯度为零的点被称为驻点，它可以进⼀步地被分为极⼩值点、极⼤值点和鞍点。<br>对于所有的权向量，误差函数的最⼩值被称为<strong>全局最⼩值</strong>（<code>golobal minimum</code>）。任何其他的使误差函数的值较⼤的极⼩值被称为<strong>局部极⼩值</strong>（<code>local minima</code>）。</p><p>由于显然⽆法找到⽅程 $\nabla E(\boldsymbol{w})=0$ 的解析解，因此使⽤迭代的数值⽅法。⼤多数⽅法涉及到为权向量选择某个初始值 $\boldsymbol{w}_0$ ，然后在权空间中进⾏⼀系列移动，形式为</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{\tau}+\nabla w^{(\tau)}\tag{5.24}</script><p>其中 $\tau$ 表⽰迭代次数。</p><h2 id="3，局部⼆次近似"><a href="#3，局部⼆次近似" class="headerlink" title="3，局部⼆次近似"></a>3，局部⼆次近似</h2><p>考虑 $E(\boldsymbol{w})$ 在权空间某点 $\hat{\boldsymbol{w}}$ 处的泰勒展开</p><script type="math/tex; mode=display">E(\boldsymbol{w})\simeq E(\hat{\boldsymbol{w}})+(\boldsymbol{w}-\hat{\boldsymbol{w}})^{T}\boldsymbol{b}+\frac{1}{2}(\boldsymbol{w}-\hat{\boldsymbol{w}})^{T}\boldsymbol{H}(\boldsymbol{w}-\hat{\boldsymbol{w}})\tag{5.25}</script><p>其中，已省略⽴⽅项和更⾼阶的项， $\boldsymbol{b}$ 定义为 $E$ 的梯度在 $\hat{\boldsymbol{w}}$ 处的值。 </p><script type="math/tex; mode=display">\boldsymbol{b}\equiv\nabla E|_{\boldsymbol{w}=\hat{\boldsymbol{w}}}</script><p><code>Hessian</code>矩阵 $H=\nabla\nabla E$ 的元素为</p><script type="math/tex; mode=display">(\boldsymbol{H})_{ij}\equiv \frac{\partial E}{\partial w_i \partial w_j}\bigg{|}_{\boldsymbol{w}=\hat{\boldsymbol{w}}}</script><p>根据公式，梯度的局部近似为</p><script type="math/tex; mode=display">\nabla E\simeq \boldsymbol{b}+\boldsymbol{H}(\boldsymbol{w}-\hat{\boldsymbol{w}})\tag{5.26}</script><p>考虑⼀个特殊情况：在误差函数最⼩值点 $\boldsymbol{w}^{*}$ 附近的局部⼆次近似。在这种情况下，没有线性项，因为在 $\boldsymbol{w}^{*}$ 处 $\nabla E=0$ ，公式(5.25)变成了</p><script type="math/tex; mode=display">E(\boldsymbol{w})\simeq E(\boldsymbol{w}^{*})+\frac{1}{2}(\boldsymbol{w}-\boldsymbol{w}^{*})^{T}\boldsymbol{H}(\boldsymbol{w}-\boldsymbol{w}^{*})\tag{5.27}</script><p>这⾥<code>Hessian</code>矩阵在点 $\boldsymbol{w}^{*}$ 处计算。 为了⽤⼏何的形式表⽰这个结果， 考虑<code>Hessian</code>矩阵的特征值⽅程</p><script type="math/tex; mode=display">\boldsymbol{H}\boldsymbol{\mu}_i=\lambda_i\boldsymbol{\mu}_i\tag{5.28}</script><p>其中特征向量 $\boldsymbol{\mu}_i$ 构成了完备的单位正交集合，即</p><script type="math/tex; mode=display">\boldsymbol{\mu}_{i}^{T}\boldsymbol{\mu}_{j}=\delta_{ij}</script><p>现在把 $(\boldsymbol{w}-\boldsymbol{w}^{*})$ 展开成特征值的线性组合的形式</p><script type="math/tex; mode=display">\boldsymbol{w}-\boldsymbol{w}^{*}=\sum_{i}\alpha_{i}\boldsymbol{\mu}_i\tag{5.29}</script><p>这可以被看成坐标系的变换，坐标系的原点变为了 $\boldsymbol{w}^{*}$ ，坐标轴旋转，与特征向量对齐（通过列为 $\boldsymbol{\mu}_i$ 的正交矩阵），误差函数可以写成下⾯的形式</p><script type="math/tex; mode=display">E(\boldsymbol{w})=E(\boldsymbol{w}^{*})+\frac{1}{2}\sum_{i}\alpha_{i}\boldsymbol{\mu}_{i}\tag{5.30}</script><p>矩阵 $\boldsymbol{H}$ 是正定的（<code>positive definite</code>）当且仅当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}&gt;0$ 对所有的 $\boldsymbol{v}\ne 0$ 都成立。</p><p>如图5.8，在最⼩值 $\boldsymbol{w}^{*}$ 的邻域中，误差函数可以⽤⼆次函数近似。这样，常数误差函数的轮廓线为椭圆，它的轴与<code>Hessian</code>矩阵的特征向量 $\boldsymbol{\mu}_i$ 给出，长度与对应的特征值 $\lambda_i$ 的平⽅根成反⽐。</p><p><img src="/images/prml_20191013230336.png" alt="⼆次函数近似"></p><p>由于特征向量 $\{\boldsymbol{\mu}_i\}$ 组成了⼀个完备集，因此任意的向量 $\boldsymbol{v}$ 都可以写成下⾯的形式</p><script type="math/tex; mode=display">\boldsymbol{v}=\sum_{i}c_i\boldsymbol{\mu}_i</script><p>有，</p><script type="math/tex; mode=display">\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}=\sum_{i}c_{i}^{2}\lambda_{i}\tag{5.31}</script><p>因此 $\boldsymbol{H}$ 是正定的，当且仅当它的所有的特征值均严格为正。在新的坐标系中，基向量是特征向量 $\{\boldsymbol{\mu}_i\}$ ，$E$ 为常数的轮廓线是以原点为中⼼的椭圆。对于⼀维权空间，驻点 $\boldsymbol{w}^{∗}$ 满⾜下⾯条件时取得最⼩值</p><script type="math/tex; mode=display">\frac{\partial^{2}E}{\partial w^{2}}\Bigg{|}_{\boldsymbol{w}^{*}}>0</script><p>对应的 $\boldsymbol{D}$ 维的结论是，在 $\boldsymbol{w}^{*}$ 处的<code>Hessian</code>矩阵是正定矩阵。</p><h2 id="4，使⽤梯度信息"><a href="#4，使⽤梯度信息" class="headerlink" title="4，使⽤梯度信息"></a>4，使⽤梯度信息</h2><p>可以使⽤误差反向传播的⽅法⾼效地计算误差函数的梯度，这个梯度信息的使⽤可以⼤幅度加快找到极⼩值点的速度。<br>在公式(5.25)给出的误差函数的⼆次近似中，误差曲⾯由 $\boldsymbol{b}$ 和 $\boldsymbol{H}$ 确定， 它包含了总共 $\frac{W(W+3)}{2}$ 个独⽴的元素（因为矩阵 $\boldsymbol{H}$ 是对称的），其中 $W$ 是 $\boldsymbol{w}$ 的维度（即⽹络中可调节参数的总数）。这个⼆次近似的极⼩值点的位置因此依赖于 $O(W^2)$ 个参数，并且不应该奢求能够在收集到 $O(W^2)$ 条独⽴的信息之前就能够找到最⼩值。如果不使⽤梯度信息，不得不进⾏ $O(W^2)$ 次函数求值，每次求值都需要 $O(W)$ 个步骤。 因此，使⽤这种⽅法求最⼩值需要的计算复杂度为 $O(W^3)$ 。现在将这种⽅法与使⽤梯度信息的⽅法进⾏对⽐，由于每次计算 $\nabla{E}$ 都会带来 $W$ 条信息，因此可能预计找到函数的极⼩值需要计算 $O(W)$ 次梯度。通过使⽤误差反向传播算法，每个这样的计算只需要 $O(W)$ 步， 因此使⽤这种⽅法可以在 $O(W^2)$ 个步骤内找到极⼩值。</p><h2 id="5，梯度下降最优化"><a href="#5，梯度下降最优化" class="headerlink" title="5，梯度下降最优化"></a>5，梯度下降最优化</h2><p>最简单的使⽤梯度信息的⽅法是：每次权值更新都是在负梯度⽅向上的⼀次⼩的移动，即</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{(\tau)}-\eta\nabla E\left(\boldsymbol{w}^{(\tau)}\right)\tag{5.32}</script><p>其中参数 $\eta&gt;0$ 被称为<strong>学习率</strong>（<code>learning rate</code>）。</p><p>误差函数是关于训练集定义的，因此为了计算 $\nabla{E}$ ，每⼀步都需要处理整个数据集。在每⼀步，权值向量都会沿着误差函数下降速度最快的⽅向移动， 因此这种⽅法被称为<strong>梯度下降法</strong>（<code>gradient descent</code>）或者<strong>最陡峭下降法</strong>（<code>steepest descent</code>）。</p><p>对于批量最优化⽅法，存在更⾼效的⽅法，例如<strong>共轭梯度法</strong>（<code>conjugate gradient</code>）或者<strong>拟⽜顿法</strong>（<code>quasi-Newton</code>）。与简单的梯度下降⽅法相⽐，这些⽅法更鲁棒，更快（<code>Gill et al.</code>, 1981; <code>Fletcher</code>, 1987; <code>Nocedal and Wright</code>, 1999）。 与梯度下降⽅法不同， 这些算法具有这样的<strong>性质</strong>： 误差函数在每次迭代时总是减⼩的，除⾮权向量到达了局部的或者全局的最⼩值。</p><p>基于⼀组独⽴观测的最⼤似然函数的误差函数由⼀个求和式构成，求和式的每⼀项都对应着⼀个数据点</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\sum_{n=1}^{N}E_n(\boldsymbol{w})\tag{5.33}</script><p>在线梯度下降，也被称为<strong>顺序梯度下降</strong>（<code>sequential gradient descent</code>） 或者<strong>随机梯度下降</strong>（<code>stochastic gradient descent</code>），使得权向量的更新每次只依赖于⼀个数据点，即</p><script type="math/tex; mode=display">\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{(\tau)}-\eta\nabla E_n\left(\boldsymbol{w}^{(\tau)}\right)\tag{5.34}</script><p>这个更新在数据集上循环重复进⾏，并且既可以顺序地处理数据，也可以随机地有重复地选择数据点。</p><h1 id="三，误差反向传播"><a href="#三，误差反向传播" class="headerlink" title="三，误差反向传播"></a>三，误差反向传播</h1><p>在局部信息传递的思想中，信息在神经⽹络中交替地向前、向后传播， 这种⽅法被称为<strong>误差反向传播</strong>（<code>error backpropagation</code>），有时简称“<strong>反传</strong>”（<code>backprop</code>）。</p><h2 id="1，误差函数导数的计算"><a href="#1，误差函数导数的计算" class="headerlink" title="1，误差函数导数的计算"></a>1，误差函数导数的计算</h2><p>针对⼀组独⽴同分布的数据的最⼤似然⽅法定义的误差函数，由若⼲项的求和式组成，每⼀项对应于训练集的⼀个数据点，即</p><script type="math/tex; mode=display">E(\boldsymbol{w})=\sum_{n=1}^{N}E_n(\boldsymbol{w})</script><p>考虑⼀个简单的线性模型，其中输出 $y_k$ 是输⼊变量 $x_i$ 的线性组合，即</p><script type="math/tex; mode=display">y_k=\sum_{i}w_{ki}x_{i}\tag{5.35}</script><p>对于⼀个特定的输⼊模式 $n$，误差函数的形式为</p><script type="math/tex; mode=display">E_n=\frac{1}{2}\sum_{k}(y_{nk}-t_{nk})^2\tag{5.36}</script><p>其中 $y_{nk} = y_k(\boldsymbol{x}_n,\boldsymbol{w})$ 。这个误差函数关于⼀个权值 $w_{ji}$ 的梯度为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=(y_{nj}-t_{nj})x_{ni}\tag{5.37}</script><p>它可以表⽰为与链接 $w_{ji}$ 的输出端相关联的“误差信号” $y_{nj}−t_{nj}$ 和与链接的输⼊端相关联的变量 $x_{ni}$ 的乘积。</p><p>在⼀个⼀般的前馈⽹络中，每个单元都会计算输⼊的⼀个加权和，形式为</p><script type="math/tex; mode=display">a_j=\sum_{i}w_{ji}z_{i}\tag{5.38}</script><p>其中 $z_i$ 是⼀个单元的激活，或者是输⼊，它向单元 $j$ 发送⼀个链接，$w_{ji}$ 是与这个链接关联的权值。偏置可以被整合到这个求和式中，整合的⽅法是引⼊⼀个额外的单元或输⼊，然后令激活恒为 $+1$。求和式通过⼀个⾮线性激活函数 $h(·)$ 进⾏变换，得到单元 $j$ 的激活 $z_j$ ，形式为</p><script type="math/tex; mode=display">z_{i}=h(a_j)\tag{5.39}</script><p>对于训练集⾥的每个模式，假定给神经⽹络提供了对应的输⼊向量，然后通过反复应⽤公式(5.38)和公式(5.39)，计算神经⽹络中所有隐含单元和输出单元的激活。这个过程通常被称为<strong>正向传播</strong>（<code>forward propagation</code>），因为它可以被看做⽹络中的⼀个向前流动的信息流。</p><p>现在考虑计算 $E_n$ 关于权值 $w_{ji}$ 的导数，各个单元的输出会依赖于某个特定的输⼊模式 $n$ 。⾸先，注意到 $E_n$ 只通过 单元 $j$ 的经过求和之后的输⼊ $a_j$ 对权值 $w_{ji}$ 产⽣依赖。因此，可以应⽤偏导数的<strong>链式法则</strong>， 得到</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\frac{\partial{E_n}}{\partial{a_{j}}}\frac{\partial{a_j}}{\partial{w_{ji}}}</script><p>如图5.9，对于隐含单元 $j$ ，计算 $\delta_{j}$ 的说明。计算时使⽤了向单元 $j$ 发送信息的那些单元 $k$ 的 $\delta$ ，使⽤反向误差传播⽅法进⾏计算。蓝⾊箭头表⽰在正向传播阶段信息流的⽅向，红⾊箭头表⽰误差信息的反向传播。</p><p><img src="/images/prml_20191014091853.png" alt="正反向传播"></p><p>现在引⼊⼀个有⽤的记号</p><script type="math/tex; mode=display">\delta_{j}\equiv\frac{\partial{E_n}}{\partial{a_{j}}}\tag{5.40}</script><p>其中 $\delta$ 通常被称为<strong>误差</strong>（<code>error</code>），使⽤公式(5.38)，有</p><script type="math/tex; mode=display">\frac{\partial{a_j}}{\partial{w_{ji}}}=z_i</script><p>继而有</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\delta_{j}z_{i}\tag{5.41}</script><p>从而可知，要找的导数可以通过简单地将权值输出单元的 $\delta$ 值与权值输⼊端的 $z$ 值相乘的⽅式得到（对于偏置的情形，$z = 1$ ）。</p><script type="math/tex; mode=display">\delta_{k}=y_k-t_k\tag{5.42}</script><p>为了计算隐含单元的 $\delta$ 值，使⽤偏导数的链式法则</p><script type="math/tex; mode=display">\delta_j\equiv\frac{\partial{E_n}}{\partial{a_{j}}}=\sum_{k}\frac{\partial{E_n}}{\partial{a_{k}}}\frac{\partial{a_k}}{\partial{a_{j}}}</script><p>其中求和式的作⽤对象是所有向单元 $j$ 发送链接的单元 $k$ 。注 意，单元 $k$ 可以包含其他的隐含单元和（或）输出单元。$a_j$ 的改变所造成的误差函数的改变的唯⼀来源是变量 $a_k$ 的改变。经计算可得，<strong>反向传播</strong>（<code>backpropagation</code>）公式</p><script type="math/tex; mode=display">\delta_{j}=h^{\prime}(a_j)\sum_{k}w_{kj}\delta_{k}\tag{5.43}</script><p>这表明，⼀个特定的隐含单元的 $\delta$ 值可以通过将⽹络中更⾼层单元的 $\delta$ 进⾏反向传播来实现。</p><p>反向传播算法可以总结如下：</p><blockquote><p>1）对于⽹络的⼀个输⼊向量 $\boldsymbol{x}_n$ ，使⽤公式(5.38)和公式(5.39)进⾏正向传播，找到所有隐含单元和输出单元的激活；<br>2） 使⽤公式(5.42)计算所有输出单元的 $\delta_k$ ；<br>3）使⽤公式(5.43)反向传播 $\delta$ ，获得⽹络中所有隐含单元的 $\delta_j$ ；<br>4）使⽤公式(5.41)计算导数。</p></blockquote><p>对于批处理⽅法， 总误差函数 $E$ 的导数可以通过下⾯的⽅式得到：对于训练集⾥的每个模式，重复上⾯的步骤，然后对所有的模式求和，即</p><script type="math/tex; mode=display">\frac{\partial{E}}{\partial{w_{ji}}}=\sum_{n}\frac{\partial{E_n}}{\partial{w_{ji}}}\tag{5.44}</script><h2 id="2，⼀个简单的例⼦"><a href="#2，⼀个简单的例⼦" class="headerlink" title="2，⼀个简单的例⼦"></a>2，⼀个简单的例⼦</h2><p>考虑简单的两层神经⽹络，误差函数为平⽅和误差函数，输出单元的激活函数为线性激活函数，即 $y_k=a_k$ ，⽽隐含单元的激活函数为 $S$ 形函数，形式为</p><script type="math/tex; mode=display">h(a)\equiv\tanh(a)\tag{5.45}</script><p>其中，</p><script type="math/tex; mode=display">\tanh(a)=\frac{e^a-e^{-a}}{e^a+e^{-a}}</script><p>此函数的⼀个有⽤的<strong>特征</strong>是：其导数可以表⽰成⼀个相当简单形式</p><script type="math/tex; mode=display">h^{\prime}(a)=1-h(a)^{2}</script><p>考虑⼀个标准的平⽅和误差函数，即对于模式 $n$ ，误差为</p><script type="math/tex; mode=display">E_n=\frac{1}{2}\sum_{k=1}^{K}(y_{k}-t_{k})^{2}\tag{5.46}</script><p>其中，对于⼀个特定的输⼊模式 $\boldsymbol{x}_n$ ，$y_k$ 是输出单元 $k$ 的激活，$t_k$ 是对应的⽬标值。<br>对于训练集⾥的每个模式，⾸先使⽤下⾯的公式组进⾏前向传播。</p><script type="math/tex; mode=display">a_j=\sum_{i=0}^{D}w_{ji}^{(1)}x_{i}\\z_j=\tanh(a_j)\\y_k=\sum_{j=0}^{M}w_{kj}^{(2)}z_j</script><p>再使⽤下⾯的公式计算每个输出单元的 $\delta$ 值。</p><script type="math/tex; mode=display">\delta_k=y_k-t_k\tag{5.47}</script><p>然后，使⽤下⾯的公式将这些值反向传播，得到隐含单元的 $\delta$ 值。 </p><script type="math/tex; mode=display">\delta_j=(1-z_{j}^{2})\sum_{k=1}^{K}w_{kj}\delta_{k}</script><p>最后，关于第⼀层权值和第⼆层权值的导数为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}^{(1)}}}=\delta_j x_i\\\frac{\partial{E_n}}{\partial{w_{kj}^{(2)}}}=\delta_k z_j</script><h2 id="3，反向传播的效率"><a href="#3，反向传播的效率" class="headerlink" title="3，反向传播的效率"></a>3，反向传播的效率</h2><p>计算误差函数导数的反向传播⽅法是使⽤有限差。⾸先让每个权值有⼀个扰动，然后使⽤下⾯的表达式来近似导数</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\frac{E_n(w_{ji}+\epsilon)-E_n(w_{ji})}{\epsilon}+O(\epsilon)\tag{5.48}</script><p>其中 $\epsilon\ll1$ 。在软件仿真中，通过让 $\epsilon$ 变⼩，对于导数的近似的精度可以提升，直到 $\epsilon$ 过⼩，造成下溢问题。通过使⽤对称的<strong>中⼼差</strong>（<code>central difference</code>），有限差⽅法的精度可以极⼤地提⾼。 <strong>中⼼差</strong>的形式为</p><script type="math/tex; mode=display">\frac{\partial{E_n}}{\partial{w_{ji}}}=\frac{E_n(w_{ji}+\epsilon)-E_n(w_{ji}-\epsilon)}{2\epsilon}+O(\epsilon^{2})\tag{5.49}</script><p>计算数值导数的⽅法的主要问题是，计算复杂度为 $O(W)$ 这⼀性质不再成⽴。每次正向传播需要 $O(W)$ 步，⽽⽹络中有 $W$ 个权值，每个权值必须被单独地施加扰动， 因此整体的时间复杂度为 $O(W^2)$ 。</p><h2 id="4，Jacobian-矩阵"><a href="#4，Jacobian-矩阵" class="headerlink" title="4，Jacobian 矩阵"></a>4，<strong><code>Jacobian</code></strong> 矩阵</h2><p>考虑<code>Jacobian</code>矩阵的计算，它的元素的值是⽹络的输出关于输⼊的导数</p><script type="math/tex; mode=display">J_{ki}\equiv\frac{\partial{y_{k}}}{\partial{x_{i}}}\tag{5.50}</script><p>其中，计算每个这样的导数时，其他的输⼊都固定。</p><p>如图5.10，模块化模式识别系统，其中<code>Jacobian</code>矩阵可以⽤来将误差信号从输出模块在系统中反向传播 到更早的模块。</p><p><img src="/images/prml_20191014230852.png" alt="模块化模式识别系统"></p><p>假设我们想关于图5.10中的参数 $w$ ，最⼩化误差函数 $E$ 。误差函数的导数为</p><script type="math/tex; mode=display">\frac{\partial{E}}{\partial{w}}=\sum_{k,j}\frac{\partial{E}}{\partial{y_k}}\frac{\partial{y_k}}{\partial{z_{j}}}\frac{\partial{z_j}}{\partial{w}}</script><p>其中，图5.10中的红⾊模块的<code>Jacobian</code>矩阵出现在中间项。</p><p>由于<code>Jacobian</code>矩阵度量了输出对于每个输⼊变量的改变的敏感性，因此它也允许与输⼊关联的任意已知的误差 $\Delta{x_i}$ 在训练过的⽹络中传播，从⽽估计他们对于输出误差 $\Delta{y_k}$ 的贡献。⼆者的关系为</p><script type="math/tex; mode=display">\Delta{y_k}\simeq\sum_{i}\frac{\partial{y_k}}{\partial{x_i}}\Delta{x_i}\tag{5.51}</script><p><code>Jacobian</code>矩阵可以使⽤反向传播的⽅法计算，计算⽅法类似于之前推导误差函数关于权值的导数的⽅法。⾸先，把元素 $J_{ki}$ 写成下⾯的形式</p><script type="math/tex; mode=display">\begin{aligned}J_{ki}=\frac{\partial{y_k}}{\partial{x_i}}&=\sum_{j}\frac{\partial{y_k}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{x_i}}\\&=\sum_{j}w_{ji}\frac{\partial{y_k}}{\partial{a_j}}\end{aligned}\tag{5.52}</script><p>其中求和式作⽤于所有单元 $i$ 发送链接的单元 $j$ 上 。现在⼀个递归的反向传播公式来确定导数 $\frac{\partial{y_k}}{\partial{a_j}}$ 。</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial{y_k}}{\partial{a_j}}&=\sum_{l}\frac{\partial{y_k}}{\partial{a_l}}\frac{\partial{a_l}}{\partial{a_j}}\\&=h^{\prime}(a_j)\sum_{j}w_{lj}\frac{\partial{y_k}}{\partial{a_l}}\end{aligned}</script><p>其中求和的对象为所有单元 $j$ 发送链接的单元 $l$（对应于 $w_{lj}$ 的第⼀个下标）。如果对于每个输出单元，都有各⾃的<code>sigmoid</code>函数，那么</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{a_l}}=\delta_{kl}\sigma^{\prime}(a_l)\tag{5.53}</script><p>对于<code>softmax</code>输出，有</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{a_l}}=\delta_{kl}y_{k}-y_{k}y_{l}\tag{5.54}</script><p>计算<code>Jacobian</code>矩阵的⽅法总结：将输⼊空间中要寻找<code>Jacobian</code>矩阵的点映射成⼀个输⼊向量，将这个输⼊向量作为⽹络的输⼊，使⽤通常的正向传播⽅法，得到⽹络的所有隐含单元和输出单元的激活。然后，对于<code>Jacobian</code>矩阵的每⼀⾏ $k$ （对应于输出单元 $k$ ），使⽤递归关系进⾏反向传播。对于⽹络中所有的隐含结点，反向传播开始于公式(5.53)和公式(5.54)。 最后， 使⽤公式(5.52)进⾏对输⼊单元的反向传播。</p><p><code>Jacobian</code>矩阵的另⼀种计算⽅法是正向传播算法，它可以使⽤与这⾥给出的反向传播算法相类似的⽅式推导出来。这个算法的执⾏可以通过下⾯的数值导数的⽅法检验正确性。</p><script type="math/tex; mode=display">\frac{\partial{y_k}}{\partial{x_i}}=\frac{y_{k}(x_i+\epsilon)-y_k(x_i-\epsilon)}{2\epsilon}+O(\epsilon^{2})\tag{5.55}</script><p>对于⼀个有着 $D$ 个输⼊的⽹络来说，这种⽅法需要 $2D$ 次正向传播。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，前馈神经网络&quot;&gt;&lt;a href=&quot;#一，前馈神经网络&quot; class=&quot;headerlink&quot; title=&quot;一，前馈神经网络&quot;&gt;&lt;/a&gt;一，前馈神经
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】概率判别式模型</title>
    <link href="https://zhangbc.github.io/2019/10/10/prml_04_03/"/>
    <id>https://zhangbc.github.io/2019/10/10/prml_04_03/</id>
    <published>2019-10-10T15:29:58.000Z</published>
    <updated>2019-10-11T00:57:50.071Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，概率判别式模型"><a href="#一，概率判别式模型" class="headerlink" title="一，概率判别式模型"></a>一，概率判别式模型</h1><p>考察⼆分类问题，对于⼀⼤类的类条件概率密度 $p(\boldsymbol{x}|\mathcal{C}_k)$ 的选择， 类别 $\mathcal{C}_1$ 后验概率分布可以写成作⽤于 $\boldsymbol{x}$ 的线性函数上的<code>logistic sigmoid</code>函数的形式。类似地，对于多分类的情形，类别 $\mathcal{C}_k$ 的后验概率由 $\boldsymbol{x}$ 的线性函数的<code>softmax</code>变换给出。对于类条件概率密度 $p(\boldsymbol{x}|\mathcal{C}_k)$ 的具体的选择， 我们已经使⽤了最⼤似然⽅法估计了概率密度的参数以及类别先验 $p(\mathcal{C}_k)$ ，然后使⽤贝叶斯定理就可以求出后验类概率。<br>寻找⼀般的线性模型参数的间接⽅法是，分别寻找类条件概率密度和类别先验，然后使⽤贝叶斯定理。</p><h2 id="1，固定基函数"><a href="#1，固定基函数" class="headerlink" title="1，固定基函数"></a>1，固定基函数</h2><p>考虑直接对输⼊向量 $(x)$ 进⾏分类的分类模型，然⽽，如果⾸先使⽤⼀个基函数向量 $\boldsymbol{\phi}(\boldsymbol{x})$ 对输⼊变量进⾏⼀个固定的⾮线性变换，所有的这些算法仍然同样适⽤，最终的决策边界在特征空间 $\boldsymbol{\phi}$ 中是线性的，因此对应于原始 $\boldsymbol{x}$ 空间中的⾮线性决策边界。在特征空间 $\boldsymbol{\phi}(\boldsymbol{x})$ 线性可分的类别未必在原始的观测空间 $\boldsymbol{x}$ 中线性可分，基函数中的某⼀个通常设置为常数，例如 $\phi_{0}(\boldsymbol{x})=1$ ，使得对应的参数 $w_0$ 扮演偏置的作⽤。</p><h2 id="2，logistic回归"><a href="#2，logistic回归" class="headerlink" title="2，logistic回归"></a>2，<code>logistic</code>回归</h2><p>考虑⼆分类问题在⼀般的假设条件下，类别 $\mathcal{C}_1$ 的后验概率可以写成作⽤在特征向量 $\boldsymbol{\phi}$ 的线性函数上的<code>logistic sigmoid</code>函数的形式，即</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{\phi})=y(\boldsymbol{\phi})=\sigma(\boldsymbol{w}^T\boldsymbol{\phi})\tag{4.55}</script><p>且 $p(\mathcal{C}_2|\boldsymbol{\phi})=1-p(\mathcal{C}_1|\boldsymbol{\phi})$ ， $\sigma(·)$ 是<code>logistic sigmoid</code>函数。使⽤统计学的术语，这个模型被称为 <strong><code>logistic</code>回归</strong> ，特别注意，这是⼀个分类模型⽽不是回归模型。对于⼀个 $M$ 维特征空间 $\boldsymbol{\phi}$ ，这个模型有 $M$ 个可调节参数。</p><p>现在使⽤最⼤似然⽅法来确定<code>logistic</code>回归模型的参数。使⽤<code>logistic sigmoid</code>函数的导数</p><script type="math/tex; mode=display">\frac{\mathrm{d}\sigma}{\mathrm{d}a}=\sigma(1-\sigma)\tag{4.56}</script><p>对于⼀个数据集 $\boldsymbol{\phi}_n$ ,  $t_n$ ，其中 $t_n\in\{0,1\}$ 且  $\boldsymbol{\phi}_n=\boldsymbol{\phi}(\boldsymbol{x}_n)$ ，并且 $n=1,\dots,N$，似然函数可以写成</p><script type="math/tex; mode=display">p(\mathbf{t}|\boldsymbol{w})=\prod_{n=1}^{N}y_{n}^{t_n}\{1-y_n\}^{1-t_n}\tag{4.57}</script><p>其中 $\mathbf{t} = (t_1,\dots,t_N)^T$ 且 $y_n=p(\mathcal{C}_1|\boldsymbol{\phi}_n)$ 。通过取似然函数的负对数的⽅式，定义⼀个误差函数，这种⽅式产⽣了<strong>交叉熵</strong>（<code>cross-entropy</code>）误差函数，形式为</p><script type="math/tex; mode=display">E(\boldsymbol{w})=-\ln p(\mathbf{t}|\boldsymbol{w}) = -\sum_{n=1}^{N}\{t_n\ln y_{n}+(1-t_n)\ln(1-y_n)\}\tag{4.58}</script><p>其中 $y_n=\sigma(a_n)$ 且 $a_n=\boldsymbol{w}^{T}\boldsymbol{\phi}_n$ 。两侧关于 $\boldsymbol{w}$ 取误差函数的梯度，有</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})= -\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n\tag{4.59}</script><h2 id="3，迭代重加权最⼩平⽅"><a href="#3，迭代重加权最⼩平⽅" class="headerlink" title="3，迭代重加权最⼩平⽅"></a>3，迭代重加权最⼩平⽅</h2><p>误差函数可以通过⼀种⾼效的迭代⽅法求出最⼩值，这种迭代⽅法基于<code>Newton-Raphson</code>迭代最优化框架， 使⽤了对数似然函数的局部⼆次近似。为了最⼩化函数 $E(\boldsymbol{w})$ ，<code>Newton-Raphson</code>对权值的更新形式为（<code>Fletcher</code>, 1987; <code>Bishop and Nabney</code>, 2008）</p><script type="math/tex; mode=display">\boldsymbol{w}^{新}=\boldsymbol{w}^{旧}-\boldsymbol{H}^{-1}\nabla E(\boldsymbol{w})\tag{4.60}</script><p>其中 $\boldsymbol{H}$ 是⼀个 <strong><code>Hessian</code>矩阵</strong>，它的元素由 $E(\boldsymbol{w})$ 关于 $\boldsymbol{w}$ 的⼆阶导数组成。</p><p>⾸先，把<code>Newton-Raphson</code>⽅法应⽤到线性回归模型上，误差函数为平⽅和误差函数。这个误差函数的梯度和<code>Hessian</code>矩阵为</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}(\boldsymbol{w}^{T}\boldsymbol{\phi}_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\boldsymbol{w}-\boldsymbol{\Phi}^{T}\mathbf{t}\tag{4.61}</script><script type="math/tex; mode=display">\boldsymbol{H}=\nabla\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}=\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\tag{4.62}</script><p>其中 $\boldsymbol{\Phi}$ 是 $N \times M$ 矩阵，第 $n$ ⾏为 $\boldsymbol{\phi}_{n}^{T}$ 。于是，<code>Newton-Raphson</code>更新形式为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w}^{新}&=\boldsymbol{w}^{旧}-(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\{\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\boldsymbol{w}^{旧}-\boldsymbol{\Phi}^{T}\mathbf{t}\}\\&=(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\mathbf{t}\end{aligned}\tag{4.63}</script><p>这是标准的最⼩平⽅解。</p><p>现在，把<code>Newton-Raphson</code>更新应⽤到<code>logistic</code>回归模型的交叉熵误差函数上。这个误差函数的梯度和<code>Hessian</code>矩阵为</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}^{T}(\mathbf{y}-\mathbf{t})\tag{4.64}</script><script type="math/tex; mode=display">\boldsymbol{H}=\nabla\nabla E(\boldsymbol{w})=\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}=\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi}\tag{4.65}</script><p>其中，引⼊了⼀个 $N \times N$ 的对⾓矩阵 $\boldsymbol{R}$ ，元素为</p><script type="math/tex; mode=display">R_{nn}=y_n(1-y_n)\tag{4.66}</script><p>由此可见，<code>Hessian</code>矩阵不再是常量，⽽是通过权矩阵 $\boldsymbol{R}$ 依赖于 $\boldsymbol{w}$ 。对于任意向量 $\boldsymbol{\mu}$ 都有 $\boldsymbol{\mu}^{T}\boldsymbol{H}\boldsymbol{\mu}&gt;0$ ， 因此<code>Hessian</code>矩阵 $\boldsymbol{H}$ 是正定的，误差函数是 $\boldsymbol{w}$ 的⼀个凸函数， 从 ⽽有唯⼀的最⼩值。</p><p><code>logistic</code>回归模型的<code>Newton-Raphson</code>更新公式就变成了</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w}^{新}&=\boldsymbol{w}^{旧}-(\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}(\mathbf{y}-\mathbf{t})\\&=(\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi})^{-1}\{\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi}\boldsymbol{w}^{旧}-\boldsymbol{\Phi}^{T}(\mathbf{y}-\mathbf{t})\}\\&=(\boldsymbol{\Phi}^{T}\boldsymbol{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^{T}\boldsymbol{R}\mathbf{z}\end{aligned}\tag{4.67}</script><p>其中 $\mathbf{z}$ 是⼀个 $N$ 维向量，元素为</p><script type="math/tex; mode=display">\mathbf{z}=\boldsymbol{\Phi}\boldsymbol{w}^{旧}-\boldsymbol{R}^{-1}(\mathbf{y}-\mathbf{t})</script><p>更新公式(4.67)的形式为⼀组加权最⼩平⽅问题的规范⽅程，由于权矩阵 $\boldsymbol{R}$ 不是常量，⽽是依赖于参数向量 $\boldsymbol{w}$ ， 因此必须迭代地应⽤规范⽅程， 每次使⽤新的权向量 $\boldsymbol{w}$ 计算⼀个修正的权矩阵 $\boldsymbol{R}$ ，这个算法被称为<strong>迭代重加权最⼩平⽅</strong>（<code>iterative reweighted least squares</code>）， 或者简称为 <strong><code>IRLS</code></strong>（<code>Rubin</code>, 1983）。</p><p>对角矩阵 $\boldsymbol{R}$ 可以看成⽅差，因为<code>logistic</code>回归模型的 $t$ 的均值和⽅差为</p><script type="math/tex; mode=display">\mathbb{E}[t]=\sigma(\boldsymbol{x})=y\tag{4.68}</script><script type="math/tex; mode=display">\text{var}[t]=\mathbb{E}[t^2]-\mathbb{E}[t]^2=\sigma(\boldsymbol{x})-\sigma(\boldsymbol{x})^2=y(1-y)\tag{4.69}</script><p>事实上， 可以把 <strong><code>IRLS</code></strong> 看成变量空间 $a=\boldsymbol{w}^{T}\boldsymbol{\phi}$ 的线性问题的解。这样，$\mathbf{z}$ 的第 $n$ 个元素 $z_n$ 就可以简单地看成这个空间中的有效的⽬标值。$z_n$ 可以通过对当前操作点 $\boldsymbol{w}^{旧}$ 附近的<code>logistic sigmoid</code>函数的局部线性近似的⽅式得到。</p><script type="math/tex; mode=display">\begin{aligned}a_n(\boldsymbol{w}) &\simeq a_n(\boldsymbol{w}^{旧})+\frac{\mathrm{d}a_n}{\mathrm{d}y_n}\Bigg{|}_{\boldsymbol{w}^{旧}}(t_n-y_n)\\&=\boldsymbol{\phi}_{n}^{T}\boldsymbol{w}^{旧}-\frac{y_n-t_n}{y_n(1-y_n)}=z_n\end{aligned}\tag{4.70}</script><h2 id="4，多类logistic回归"><a href="#4，多类logistic回归" class="headerlink" title="4，多类logistic回归"></a>4，多类<code>logistic</code>回归</h2><p>对于⼀⼤类概率分布来说，后验概率由特征变量的线性函数的<code>softmax</code>变换给出，即</p><script type="math/tex; mode=display">p(\mathcal{C}_k|\boldsymbol{\phi})=y_k(\boldsymbol{\phi})=\frac{\exp(a_k)}{\sum_{j}\exp(a_j)}\tag{4.71}</script><p>其中，“激活” $a_k$ 为</p><script type="math/tex; mode=display">a_k=\boldsymbol{w}_{k}^{T}\boldsymbol{\phi}</script><p>$y_k$ 关于所有激活 $a_j$ 的导数为</p><script type="math/tex; mode=display">\frac{\partial y_k}{\partial a_j}=y_k(\boldsymbol{I}_{kj}-y_j)\tag{4.72}</script><p>其中 $\boldsymbol{I}_{kj}$ 为单位矩阵的元素。</p><p>使⽤“<code>1-of-K</code>”表达⽅式计算似然函数。这种表达⽅式中，属于类别 $\mathcal{C}_k$ 的特征向量 $\boldsymbol{\phi}_k$ 的⽬标向量 $\mathbf{t}_n$ 是⼀个⼆元向量，这个向量的第 $k$ 个元素等于1，其余元素都等于0。从⽽，似然函数为</p><script type="math/tex; mode=display">p(\boldsymbol{T}|\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=\prod_{n=1}^{N}\prod_{k=1}^{K}p(\mathcal{C}_k|\boldsymbol{\phi}_n)^{t_{nk}}=\prod_{n=1}^{N}\prod_{k=1}^{K}y_{nk}^{t_{nk}}\tag{4.73}</script><p>其中 $y_{nk}=y_k(\boldsymbol{\phi}_n)$ ，$\boldsymbol{T}$ 是⽬标变量的⼀个 $N \times K$ 的矩阵，元素为 $t_nk$ 。取负对数，可得</p><script type="math/tex; mode=display">E(\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=-\ln p(\boldsymbol{T}|\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln y_{nk}\tag{4.74}</script><p>被称为<strong>多分类问题的交叉熵</strong>（<code>cross-entropy</code>）<strong>误差函数</strong>。</p><p>现在取误差函数关于参数向量 $\boldsymbol{w}_j$ 的梯度。利⽤<code>softmax</code>函数的导数，有</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}_j} E(\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=\sum_{n=1}^{N}(y_{nj}-t_{nj})\boldsymbol{\phi}_n\tag{4.75}</script><p>其中, $\sum_{k}t_{nk}=1$ 。</p><p>为了找到⼀个批处理算法，再次使⽤<code>Newton-Raphson</code>更新来获得多类问题的对应的<code>IRLS</code>算法。这需要求出由⼤⼩为 $M \times M$ 的块组成的<code>Hessian</code>矩阵，其中块 $i, j$ 为</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}_k}\nabla_{\boldsymbol{w}_j} E(\boldsymbol{w}_1,\dots,\boldsymbol{w}_K)=\sum_{n=1}^{N}y_{nk}(\boldsymbol{I}_{kj}-y_{nj})\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}\tag{4.76}</script><p>多类<code>logistic</code>回归模型的<code>Hessian</code>矩阵是正定的，因此误差函数有唯⼀的最⼩值。</p><h2 id="5，probit回归"><a href="#5，probit回归" class="headerlink" title="5，probit回归"></a>5，<code>probit</code>回归</h2><p>考察⼆分类的情形，使⽤⼀般的线性模型的框架，即</p><script type="math/tex; mode=display">p(t=1|a)=f(a)\tag{4.77}</script><p>其中 $a=\boldsymbol{w}^{T}\boldsymbol{\phi}$ ，且 $f(·)$ 为激活函数。<br>对于每个输 ⼊ $\boldsymbol{\phi}_n$ ，我们计算 $a_n=\boldsymbol{w}^{T}\boldsymbol{\phi}_n$ ，然后按照下⾯的⽅式设置⽬标值</p><script type="math/tex; mode=display">\begin{cases}t_n=1, & 如果 a_n\ge \theta \\ t_n=0, & 其他情况\end{cases}</script><p>如果 $\theta$ 的值从概率密度 $p(\theta)$ 中抽取，那么对应的激活函数由累积分布函数给出</p><script type="math/tex; mode=display">f(a)=\int_{-\infty}^{a}p(\theta)\mathrm{d}\theta\tag{4.78}</script><p>假设概率密度 $p(\theta)$ 是零均值、单位⽅差的⾼斯概率密度，对应的累积分布函数为</p><script type="math/tex; mode=display">\boldsymbol{\Phi}(a)=\int_{-\infty}^{a}\mathcal{N}(\theta|0,1)\mathrm{d}\theta\tag{4.79}</script><p>这被称为 <strong>逆<code>probit</code>（<code>inverse probit</code>）函数</strong>。</p><p>如图4.17，概率分布 $p(\theta)$ 的图形表⽰，这个概率分布⽤蓝⾊曲线标记出。这个分布由两个⾼斯分布混合⽽成，同时给出的还有它的累积密度函数 $f(a)$ ，⽤红⾊曲线表⽰。注意，蓝⾊曲线上的任意⼀点，例如垂直绿⾊直线标记出的点，对应于红⾊曲线在相同⼀点处的斜率。相反，红⾊曲线在这点上的值对应于蓝⾊曲线下⽅的绿⾊阴影的⾯积。</p><p><img src="/images/prml_20191010130423.png" alt="概率分布"></p><p><strong><code>erf</code>函数</strong>，或者被称为 <strong><code>error</code>函数</strong></p><script type="math/tex; mode=display">\text{erf}(a)=\frac{2}{\sqrt{\pi}}\int_{0}^{a}\exp(-\theta^{2})\mathrm{d}\theta\tag{4.80}</script><p>与逆<code>probit</code>函数的关系为</p><script type="math/tex; mode=display">\boldsymbol{\Phi}(a)=\frac{1}{2}\left\{1+\text{erf}\left(\frac{a}{\sqrt{2}}\right)\right\}\tag{4.81}</script><p>基于<code>probit</code>激活函数的⼀般的线性模型被称为 <strong><code>probit</code>回归</strong> 。<br>在实际应⽤中经常出现的⼀个问题是<strong>离群点</strong>，它可能由输⼊向量 $\boldsymbol{x}$ 的测量误差产⽣，或者由⽬标值 $t$ 的错误标记产⽣。 由于这些点可以位于错误的⼀侧中距离理想决策边界相当远的位置上，因此他们会严重地⼲扰分类器。注意，在这⼀点上，<code>logistic</code>回归模型与<code>probit</code>回归模型的表现不同， 因为对于 $x \to \infty$ ，<code>logistic sigmoid</code>函数像 $\exp(−x)$ 那样渐进地衰减， ⽽<code>probit</code>激活函数像 $\exp(−x^2)$ 那样衰减，因此<code>probit</code>模型对于离群点会更加敏感。</p><p>引⼊⼀个概率 $\epsilon$ ，它是⽬标值 $t$ 被翻转到错误值的概率（<code>Opper and Winther</code>, 2000a）。这时，数据点 $\boldsymbol{x}$ 的⽬标值的分布为</p><script type="math/tex; mode=display">\begin{aligned}p(t|\boldsymbol{x})&=(1-\epsilon)\sigma(\boldsymbol{x})+\epsilon(1-\sigma(\boldsymbol{x}))\\&=\epsilon+(1-2\epsilon)\sigma(\boldsymbol{x})\end{aligned}\tag{4.82}</script><p>其中 $\sigma(\boldsymbol{x})$ 是输⼊向量 $\boldsymbol{x}$ 的激活函数。这⾥， $\epsilon$ 可以事先设定，也可以被当成超参数，然后从数据中推断它的值。</p><h2 id="6，标准链接函数"><a href="#6，标准链接函数" class="headerlink" title="6，标准链接函数"></a>6，标准链接函数</h2><p>把指数族分布的假设应⽤于⽬标变量 $t$ ，⽽不是应⽤于输⼊向量 $\boldsymbol{x}$ 。考虑⽬标变量的条件分布</p><script type="math/tex; mode=display">p(t|\eta,s)=\frac{1}{s}h(\frac{t}{s})g(\eta)\exp\left\{\frac{\eta t}{s}\right\}\tag{4.83}</script><p>$t$ 的条件均值（记作 $y$ ）为</p><script type="math/tex; mode=display">y\equiv \mathbb{E}[t|\eta]=-s\frac{\mathrm{d}}{\mathrm{d}\eta}\ln g(\eta)\tag{4.84}</script><p>因此 $y$ 和 $\eta$ ⼀定相关，记作 $\eta=\psi(y)$ 。</p><p>按照<code>Nelder and Wedderburn</code>（1972）的⽅法，我们将⼀般线性模型（<code>generalised linear model</code>）定义为这样的模型：$y$ 是输⼊变量（或者特征变量）的线性组合的⾮线性函数，即</p><script type="math/tex; mode=display">y=f(\boldsymbol{w}^{T}\boldsymbol{\phi})\tag{4.85}</script><p>其中 $f(·)$ 在机器学习的⽂献中被称为<strong>激活函数</strong>（<code>activation function</code>），$f^{-1}(·)$ 在统计学中被称为<strong>链接函数</strong>（<code>link function</code>）。</p><p>现在考虑这个模型的对数似然函数。它是 $\eta$ 的⼀个函数，形式为</p><script type="math/tex; mode=display">\ln p(\mathbf{t}|\eta,s)=\sum_{n=1}^{N}\ln p(t_n|\eta,s)=\sum_{n=1}^{N}\left\{\ln g(\eta_n)+\frac{\eta_n t_n}{s}\right\}+常数\tag{4.86}</script><p>其中假定所有的观测有⼀个相同的缩放参数（它对应着例如服从⾼斯分布的噪声的⽅差），因此 $s$ 与 $n$ ⽆关。对数似然函数关于模型参数 $\boldsymbol{w}$ 的导数为</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{w}}&=\sum_{n=1}^{N}\left\{\frac{\mathrm{d}}{\mathrm{d}\eta}\ln g(\eta_n)+\frac{t_n}{s}\right\}\frac{\mathrm{d}\eta_n}{\mathrm{d}y_n}\frac{\mathrm{d}y_n}{\mathrm{d}a_n}\nabla a_n\\&=\sum_{n=1}^{N}\frac{1}{s}\left\{t_n-y_n\right\}\psi^{\prime}(y_n)f^{\prime}(a_n)\boldsymbol{\phi}_n\end{aligned}\tag{4.87}</script><p>其中 $a_n=\boldsymbol{w}^{T}\boldsymbol{\phi}_n$ 。</p><p>令</p><script type="math/tex; mode=display">f^{-1}(y)=\psi(y)</script><p>则误差函数的梯度可以化简为</p><script type="math/tex; mode=display">\nabla E(\boldsymbol{w})=\frac{1}{s}\sum_{n=1}^{N}\left\{t_n-y_n\right\}\boldsymbol{\phi}_n\tag{4.88}</script><p>对于⾼斯分布，$s = \beta^ {-1}$ ，⽽对于<code>logistic</code>模型，$s=1$ 。</p><h1 id="二，拉普拉斯近似"><a href="#二，拉普拉斯近似" class="headerlink" title="二，拉普拉斯近似"></a>二，拉普拉斯近似</h1><h2 id="1，拉普拉斯近似"><a href="#1，拉普拉斯近似" class="headerlink" title="1，拉普拉斯近似"></a>1，拉普拉斯近似</h2><p><strong>拉普拉斯近似</strong>的<strong>⽬标</strong>是找到定义在⼀组连续变量上的概率密度的⾼斯近似。⾸先考虑单⼀连续变量 $z$ 的情形，假设分布 $p(z)$ 的定义为</p><script type="math/tex; mode=display">p(z)=\frac{1}{Z}f(z)\tag{4.89}</script><p>其中 $Z =\int f(z)\mathrm{d}z$ 是归⼀化系数。假定 $Z$ 的值是未知的，在拉普拉斯⽅法中，⽬标是寻找⼀个⾼斯近似 $q(z)$ ， 它的中⼼位于 $p(z)$ 的众数的位置。<br>第⼀步是寻找 $p(z)$ 的众数， 即寻找⼀个点 $z_0$ 使得 $p^{\prime}(z_0)=0$ ，或者等价地</p><script type="math/tex; mode=display">\frac{\mathrm{d}f(z)}{\mathrm{d}z}\bigg{|}_{z=z_0}=0</script><p>⾼斯分布有⼀个<strong>性质</strong>：它的对数是变量的⼆次函数。于是考虑 $\ln f(z)$ 以众数 $z_0$ 为中⼼的泰勒展开，即</p><script type="math/tex; mode=display">\ln f(z)\simeq \ln f(z_0)-\frac{1}{2}A(z-z_0)^2\tag{4.90}</script><p>其中，</p><script type="math/tex; mode=display">A=-\frac{\mathrm{d}^2}{\mathrm{d}z^2}\ln f(z)\bigg{|}_{z=z_0}</script><p>注意，泰勒展开式中的⼀阶项没有出现，因为 $z_0$ 是概率分布的局部最⼤值。两侧同时取指数， 有</p><script type="math/tex; mode=display">f(z)\simeq f(z_0)\exp\left\{-\frac{A}{2}(z-z_0)^2\right\}</script><p>这样，使⽤归⼀化的⾼斯分布的标准形式，就可以得到归⼀化的概率分布 $q(z)$ ，即</p><script type="math/tex; mode=display">q(z)=\left(\frac{A}{2\pi}\right)^{\frac{1}{2}}\exp\left\{-\frac{A}{2}(z-z_0)^2\right\}\tag{4.91}</script><p><strong>举例</strong>：应⽤于概率分布 $p(z) \propto \exp(−\frac{z^2}{2})\sigma(20z+4)$ 的拉普拉斯近似，其中 $\sigma(z)$ 是<code>logistic sigmoid</code>函数，定义为 $\sigma(z) = (1 + \exp^{−z})^{-1}$ 。<br>如图4.18，归⼀化的概率分布 $p(z)$，⽤黄⾊表⽰。同时给出了以 $p(z)$ 的众数 $z_0$ 为中⼼的拉普拉斯近似，⽤红⾊表⽰。</p><p><img src="/images/prml_20191010135922.png" alt="拉普拉斯近似"></p><p>如图4.19，图4.18中对应的曲线的负对数。</p><p><img src="/images/prml_20191010135947.png" alt="曲线的负对数"></p><p>将拉普拉斯⽅法推⼴，去近似定义在 $M$ 维空间 $\boldsymbol{z}$ 上的概率分布 $p(\boldsymbol{z}) =\frac{f(\boldsymbol{z})}{Z}$ 。在驻点 $\boldsymbol{z}_0$ 处，梯度 $\nabla f(\boldsymbol{z})$ 将会消失。在驻点处展开，有</p><script type="math/tex; mode=display">\ln f(\boldsymbol{z})\simeq \ln f(\boldsymbol{z}_0)-\frac{1}{2}(\boldsymbol{z}-\boldsymbol{z}_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\tag{4.92}</script><p>其中 $M \times M$ 的<code>Hessian</code>矩阵 $\boldsymbol{A}$ 的定义为</p><script type="math/tex; mode=display">\boldsymbol{A}=-\nabla\nabla\ln f(\boldsymbol{z})|_{\boldsymbol{z}=\boldsymbol{z}_0}</script><p>其中 $\nabla$ 为梯度算⼦。两边同时取指数，有</p><script type="math/tex; mode=display">f(\boldsymbol{z})\simeq f(\boldsymbol{z}_0)\exp\left\{-\frac{1}{2}(z-z_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\right\}</script><p>分布 $q(\boldsymbol{z})$ 正⽐于 $f(\boldsymbol{z})$ ，归⼀化系数可以通过观察归⼀化的多元⾼斯分布的标准形式得到。因此</p><script type="math/tex; mode=display">q(\boldsymbol{z})=\frac{|\boldsymbol{A}|^{\frac{1}{2}}}{(2\pi)^{\frac{M}{2}}}\exp\left\{-\frac{1}{2}(z-z_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\right\}=\mathcal{N(\boldsymbol{z}|\boldsymbol{z}_0,\boldsymbol{A}^{-1})}\tag{4.93}</script><p>其中 $|\boldsymbol{A}|$ 是 $\boldsymbol{A}$ 的⾏列式。这个⾼斯分布有良好定义的前提是，精度矩阵 $\boldsymbol{A}$ 是正定的， 表明驻点 $\boldsymbol{z}_0$ ⼀定是⼀个局部最⼤值，⽽不是⼀个最⼩值或者鞍点。</p><p>拉普拉斯近似的⼀个<strong>主要缺点</strong>是，由于它是以⾼斯分布为基础的，因此它只能直接应⽤于实值变量。<br>拉普拉斯框架的最严重的<strong>局限性</strong>是，它完全依赖于真实概率分布在变量的某个具体值位置上的性质，因此会⽆法描述⼀些重要的全局属性。</p><h2 id="2，模型⽐较和-BIC"><a href="#2，模型⽐较和-BIC" class="headerlink" title="2，模型⽐较和 BIC"></a>2，模型⽐较和 <strong><code>BIC</code></strong></h2><p>除了近似概率分布 $p(\boldsymbol{z})$ ，也可以获得对归⼀化常数 $Z$ 的⼀个近似，有</p><script type="math/tex; mode=display">\begin{aligned}Z&=\int f(\boldsymbol{z})\mathrm{d}\boldsymbol{z}\\&\simeq f(\boldsymbol{z}_0)\int\exp\left\{-\frac{1}{2}(z-z_0)^{T}\boldsymbol{A}(\boldsymbol{z}-\boldsymbol{z}_0)\right\}\mathrm{d}\boldsymbol{z}\\&=f(\boldsymbol{z}_0)\frac{(2\pi)^{\frac{M}{2}}}{|\boldsymbol{A}|^{\frac{1}{2}}}\end{aligned}\tag{4.94}</script><p>考虑⼀个数据集 $\mathcal{D}$ 以及⼀组模型 $\{\mathcal{M}_i\}$ ， 模型参数为 $\{\boldsymbol{\theta}_i\}$ 。 对于每个模型， 定义⼀个似然函数 $p(\mathcal{D}|\boldsymbol{\theta}_i,\mathcal{M}_i)$ 。如果引⼊⼀个参数的先验概率 $p(\boldsymbol{\theta}_i|\mathcal{M}_i)$ ，那么感兴趣的是计算不同模型的模型证据 $p(\mathcal{D}|\mathcal{M}_i)$ 。为了简化记号，省略对于 $\{\mathcal{M}_i\}$ 的条件依赖。 根据贝叶斯定理，模型证据为</p><script type="math/tex; mode=display">p(\mathcal{D})=\int p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})\mathrm{d}\boldsymbol{\theta}\tag{4.95}</script><p>令 $f(\boldsymbol{\theta})=p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})$ 以及 $Z=p(\mathcal{D})$，然后使⽤公式(4.94)，有</p><script type="math/tex; mode=display">\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} | \boldsymbol{\theta}_{MAP}\right)+\underbrace{\ln p\left(\boldsymbol{\theta}_{MAP}\right)+\frac{M}{2} \ln (2 \pi)-\frac{1}{2} \ln |\boldsymbol{A}|}_{\text {Occam因子}}\tag{4.96}</script><p>其中 $\boldsymbol{\theta}_{MAP}$ 是在后验概率分布众数位置的 $\boldsymbol{\theta}$ 值，$\boldsymbol{A}$ 是负对数后验概率的⼆阶导数组成的<code>Hessian</code>矩阵。</p><script type="math/tex; mode=display"> \boldsymbol{A}=-\nabla\nabla\ln p(\mathcal{D}|\boldsymbol{\theta}_{MAP})p(\boldsymbol{\theta}_{MAP})=-\nabla\nabla\ln p(\boldsymbol{\theta}_{MAP}|\mathcal{D})</script><p>公式(4.96)表⽰使⽤最优参数计算的对数似然值，⽽余下的三项由“ <strong><code>Occam</code>因⼦</strong> ”组成， 它对模型的复杂度进⾏惩罚。</p><p>如果假设参数的⾼斯先验分布⽐较宽，且<code>Hessian</code>矩阵是<strong>满秩</strong>的， 那么可以使⽤下式来⾮常粗略地近似公式(4.96)</p><script type="math/tex; mode=display">\ln p(\mathcal{D}) \simeq \ln p(\mathcal{D} | \boldsymbol{\theta}_{MAP})-\frac{1}{2}M\ln N\tag{4.97}</script><p>其中 $N$ 是数据点的总数，$M$ 是 $\boldsymbol{\theta}$ 中参数的数量， 并且省略了⼀些额外的常数。 这被称为<strong>贝叶斯信息准则</strong>（<code>Bayesian Information Criterion</code>）（<strong><code>BIC</code></strong>）， 或者称为 <strong><code>Schwarz</code>准则</strong>（<code>Schwarz</code>, 1978）。</p><p>历史上各种各样的“信息准则”被提出来，这些“信息准则”尝试修正最⼤似然的偏差。修正的⽅法是增加⼀个惩罚项来补偿过于复杂的模型造成的过拟合。 例如，<strong>⾚池信息准则</strong>（<code>Akaike information criterion</code>），或者简称为 <strong><code>AIC</code></strong>（<code>Akaike</code>, 1974），选择下⾯使这个量最⼤的模型：</p><script type="math/tex; mode=display">\ln p(\mathcal{D}|\boldsymbol{w}_{ML})-M</script><p>其中，$p(\mathcal{D}|\boldsymbol{w}_{ML})$ 是最合适的对数似然函数，$M$ 是模型中可调节参数的数量。</p><p>与 <strong><code>AIC</code></strong> 相比，<strong><code>BIC</code></strong> 对模型复杂度的惩罚更严重。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，概率判别式模型&quot;&gt;&lt;a href=&quot;#一，概率判别式模型&quot; class=&quot;headerlink&quot; title=&quot;一，概率判别式模型&quot;&gt;&lt;/a&gt;一，概
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】概率生成式模型</title>
    <link href="https://zhangbc.github.io/2019/10/10/prml_04_02/"/>
    <id>https://zhangbc.github.io/2019/10/10/prml_04_02/</id>
    <published>2019-10-10T15:03:30.000Z</published>
    <updated>2019-10-11T01:08:23.340Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，概率生成式模型"><a href="#一，概率生成式模型" class="headerlink" title="一，概率生成式模型"></a>一，概率生成式模型</h1><p>⾸先考虑⼆分类的情形。类别 $\mathcal{C}_1$ 的后验概率可以写成</p><script type="math/tex; mode=display">\begin{aligned}p(\mathcal{C}_1|\boldsymbol{x})&=\frac{p(\boldsymbol{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\boldsymbol{x}|\mathcal{C}_1)p(\mathcal{C}_1)+p(\boldsymbol{x}|\mathcal{C}_2)p(\mathcal{C}_2)}\\&=\frac{1}{1+\exp(-a)}=\sigma(a)\end{aligned}\tag{4.36}</script><p>其中，</p><script type="math/tex; mode=display">a=\ln\frac{p(\boldsymbol{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\boldsymbol{x}|\mathcal{C}_2)p(\mathcal{C}_2)}</script><p>$\sigma(a)$ 称之为 <strong><code>logistic sigmoid</code>函数</strong> 。</p><p>如图4.12，<code>logistic sigmoid</code>函数 $\sigma(a)$ 的图像， ⽤红⾊表⽰，同时给出的是放缩后的逆<code>probit</code>函数 $\Phi(\lambda a)$ 的图像， 其中 $\lambda^2=\frac{\pi}{8}$ ， ⽤蓝⾊曲线表⽰。</p><p><img src="/images/prml_20191010091804.png" alt="logistic sigmoid函数"></p><p><strong><code>logistic sigmoid</code>函数</strong> 在许多分类算法中都有着重要的作⽤，满⾜下⾯的对称性</p><script type="math/tex; mode=display">\sigma(-a)=1-\sigma(a)\tag{4.37}</script><p><code>logistic sigmoid</code>的反函数为</p><script type="math/tex; mode=display">a=\ln\left(\frac{\sigma}{1-\sigma}\right)\tag{4.38}</script><p>被称为 <strong><code>logit</code>函数</strong>。它表⽰两类的概率⽐值的对数 $\ln[\frac{p(\mathcal{C}_1|\boldsymbol{x})}{p(\mathcal{C}_2|\boldsymbol{x})}]$ ，也被称为 <strong><code>log odds</code>函数</strong> 。</p><p>对于 $K &gt; 2$ 个类别的情形，有</p><script type="math/tex; mode=display">\begin{aligned}p(\mathcal{C}_k|\boldsymbol{x})&=\frac{p(\boldsymbol{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{\sum_{j}p(\boldsymbol{x}|\mathcal{C}_j)p(\mathcal{C}_j)}\\&=\frac{\exp(a_k)}{\sum_{j}\exp(a_j)}\end{aligned}\tag{4.39}</script><p>被称为<strong>归⼀化指数</strong>（<code>normalized exponential</code>），也叫 <strong><code>softmax</code>函数</strong> ，可以被当做<code>logistic sigmoid</code>函数对于多类情况的推⼴。其中， $a_k$ 被定义为</p><script type="math/tex; mode=display">a_k=\ln p(\boldsymbol{x}|\mathcal{C}_k)p(\mathcal{C}_k)</script><p>如果对于所有的 $j\ne k$ 都有 $a_k \gg a_j$ ，那么 $p(\mathcal{C}_k|\boldsymbol{x})\simeq 1$ 且 $p(\mathcal{C}_j|\boldsymbol{x}) \simeq 0$。</p><h2 id="1，连续输⼊"><a href="#1，连续输⼊" class="headerlink" title="1，连续输⼊"></a>1，连续输⼊</h2><p>假设类条件概率密度是⾼斯分布，然后求解后验概率的形式。假定所有的类别的协⽅差矩阵相同，这样类别 $\mathcal{C}_k$ 的类条件概率为</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\mathcal{C}_k)=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}}\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_k)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_k)\right\}\tag{4.40}</script><p>⾸先考虑两类的情形。根据公式(4.36)，有</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{x})=\sigma(\boldsymbol{w}^{T}\boldsymbol{x}+w_0)\tag{4.41}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{w}=\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)\\w_0=-\frac{1}{2}\boldsymbol{\mu}_{1}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{1}+\frac{1}{2}\boldsymbol{\mu}_{2}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{2}+\ln\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}</script><p>如图4.13，左图给出了两个类别的类条件概率密度，分别⽤红⾊和蓝⾊表⽰。 右图给出了对应的后验概率分布 $p(\mathcal{C}_1|\boldsymbol{x})$ ， 它由 $\boldsymbol{x}$ 的线性函数的 <code>logistic sigmoid</code> 函数给出。 右图的曲⾯的颜⾊中， 红⾊所占的⽐例由 $p(\mathcal{C}_1|\boldsymbol{x})$ 给出，蓝⾊所占的⽐例由 $p(\mathcal{C}_2|\boldsymbol{x})=1-p(\mathcal{C}_1|\boldsymbol{x})$ 给出。</p><p><img src="/images/prml_20191010092734.png" alt="⼆维输⼊空间x的情况"></p><p>对于 $K$ 个类别的⼀般情形，根据公式(4.39)，有</p><script type="math/tex; mode=display">a_k(\boldsymbol{x})=\boldsymbol{w}_{k}^{T}\boldsymbol{x}+w_{k0}\tag{4.42}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{w}_k=\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{k}\\w_{k0}=-\frac{1}{2}\boldsymbol{\mu}_{k}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_{k}+\ln p(\mathcal{C}_k)</script><p>如图4.14，左图给出了三个类别的类条件概率密度，每个都是⾼斯分布，分别⽤红⾊、绿⾊、蓝⾊表⽰，其中红⾊和绿⾊的类别有相同的协⽅差矩阵。右图给出了对应的后验概率分布，其中 $RGB$ 的颜⾊向量表⽰三个类别各⾃的后验概率。决策边界也被画出。注意，具有相同协⽅差矩阵的红⾊类别和绿⾊类别的决策边界是线性的，⽽其他类别之间的类别的决策边界是⼆次的。</p><p><img src="/images/prml_20191010100117.png" alt="线性决策边界和⼆次决策边界"></p><h2 id="2，最⼤似然解"><a href="#2，最⼤似然解" class="headerlink" title="2，最⼤似然解"></a>2，最⼤似然解</h2><p>⾸先考虑两类的情形，每个类别都有⼀个⾼斯类条件概率密度，且协⽅差矩阵相同。假设有⼀个数据集 $\{\boldsymbol{x}_n, t_n\}$ ，其中 $n = 1,\dots,N$ 。这⾥ $t_n = 1$ 表⽰类别 $\mathcal{C}_1$ ，$t_n=0$ 表⽰类别 $\mathcal{C}_2$ 。 把先验概率记作 $p(\mathcal{C}_1)=\pi$ ， 从⽽ $p(\mathcal{C}_2)=1-\pi$ 。 对于⼀个来⾃类别 $\mathcal{C}_1$ 的数据点 $\boldsymbol{x}_n$ ， 有 $t_n = 1$ ，因此</p><script type="math/tex; mode=display">p(\boldsymbol{x}_n,\mathcal{C}_1)=p(\mathcal{C}_1)p(\boldsymbol{x}_n|\mathcal{C}_1)=\pi\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})</script><p>类似地，对于类别 $\mathcal{C}_2$ ，有 $t_n = 0$ ，因此</p><script type="math/tex; mode=display">p(\boldsymbol{x}_n,\mathcal{C}_2)=p(\mathcal{C}_2)p(\boldsymbol{x}_n|\mathcal{C}_2)=(1-\pi)\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_2,\boldsymbol{\Sigma})</script><p>于是似然函数为</p><script type="math/tex; mode=display">p(\mathbf{t},\mathbf{X}|\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=\prod_{n=1}^{N}[\pi\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})]^{t_n}[(1-\pi)\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_2,\boldsymbol{\Sigma})]^{1-t_n}\tag{4.43}</script><p>其中，$\mathbf{t}=(t_1,\dots,t_n)^{T}$ 。</p><p>⾸先考虑关于 $\pi$ 的最⼤化，对数似然函数中与 $\pi$ 相关的项为</p><script type="math/tex; mode=display">\sum_{n=1}^{N}\{t_n\ln\pi+(1-\pi)\ln(1-\pi)\}</script><p>令其关于 $\pi$ 的导数等于零，整理，可得</p><script type="math/tex; mode=display">\pi=\frac{1}{N}\sum_{n=1}^{N}t_n=\frac{N_1}{N}=\frac{N_1}{N_1+N_2}\tag{4.44}</script><p>其中 $N_1$ 表⽰类别 $\mathcal{C}_1$  的数据点的总数，⽽ $N_2$ 表⽰类别 $\mathcal{C}_2$ 的数据点总数。</p><p>现在考虑关于 $\mu_1$ 的最⼤化，把对数似然函数中与 $\mu_1$ 相关的量挑出来，即</p><script type="math/tex; mode=display">\sum_{n=1}^{N}t_n\ln\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_1,\boldsymbol{\Sigma})=-\frac{1}{2}\sum_{n=1}^{N}t_n(\boldsymbol{x}_n-\boldsymbol{\mu}_1)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_n-\boldsymbol{\mu}_1)+常数\tag{4.45}</script><p>令它关于 $\mu_1$ 的导数等于零，整理可得</p><script type="math/tex; mode=display">\boldsymbol{\mu}_1=\frac{1}{N_1}\sum_{n=1}^{N}t_n\boldsymbol{x}_n\tag{4.46}</script><p>通过类似的推导，对应的 $\mu_2$ 的结果为</p><script type="math/tex; mode=display">\boldsymbol{\mu}_2=\frac{1}{N_2}\sum_{n=1}^{N}(1-t_n)\boldsymbol{x}_n\tag{4.47}</script><p>最后，考虑协⽅差矩阵 $\boldsymbol{\Sigma}$ 的最⼤似然解。选出与 $\boldsymbol{\Sigma}$ 相关的项，有</p><script type="math/tex; mode=display">\begin{array}{lcl}-\frac{1}{2}\sum_{n=1}^{N}t_n\ln|\boldsymbol{\Sigma}|-\frac{1}{2}\sum_{n=1}^{N}t_n(\boldsymbol{x}_n-\boldsymbol{\mu}_1)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_n-\boldsymbol{\mu}_1) \\ -\frac{1}{2}\sum_{n=1}^{N}(1-t_n)\ln|\boldsymbol{\Sigma}|-\frac{1}{2}\sum_{n=1}^{N}(1-t_n)(\boldsymbol{x}_n-\boldsymbol{\mu}_2)^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_n-\boldsymbol{\mu}_2) \\ =-\frac{1}{2}\ln|\boldsymbol{\Sigma}|-\frac{N}{2}\text{Tr}\{\boldsymbol{\Sigma}^{-1}\boldsymbol{S}\}\end{array}\tag{4.48}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}=\frac{N_1}{N}\boldsymbol{S}_1+\frac{N_2}{N}\boldsymbol{S}_2 \\ \boldsymbol{S}_1=\frac{1}{N_1}\sum_{n\in\mathcal{C}_1}(\boldsymbol{x}_n-\boldsymbol{\mu}_1)(\boldsymbol{x}_n-\boldsymbol{\mu}_1)^{T}\\\boldsymbol{S}_2=\frac{1}{N_2}\sum_{n\in\mathcal{C}_2}(\boldsymbol{x}_n-\boldsymbol{\mu}_2)(\boldsymbol{x}_n-\boldsymbol{\mu}_2)^{T}</script><p>使⽤⾼斯分布的最⼤似然解的标准结果，我们看到 $\boldsymbol{\Sigma} = \boldsymbol{S}$ ，它表⽰对⼀个与两类都有关系的协⽅差矩阵求加权平均。</p><h2 id="3，离散特征"><a href="#3，离散特征" class="headerlink" title="3，离散特征"></a>3，离散特征</h2><p>⾸先考察⼆元特征值 $x_i\in\{0, 1\}$ ，如果有 $D$ 个输⼊，那么⼀般的概率分布会对应于⼀个⼤⼩为 $2^D$ 的表格，包含 $2^D−1$ 个独⽴变量（由于要满⾜加和限制）。由于这会 随着特征的数量指数增长，因此我们想寻找⼀个更加严格的表⽰⽅法。这⾥，我们做出<strong>朴素贝叶斯</strong>（<code>naive Bayes</code>）的假设，这个假设中，特征值被看成相互独⽴的，以类别 $\mathcal{C}_k$ 为条件。因此得到类条件分布，形式为</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\mathcal{C}_k)=\prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i}\tag{4.49}</script><p>其中对于每个类别，都有 $D$ 个独⽴的参数，有</p><script type="math/tex; mode=display">a_k(\boldsymbol{x})=\sum_{i=1}^{D}\{x_i\ln\mu_{ki}+(1-x_i)\ln(1-\mu_{ki})\}+\ln p(\mathcal{C}_k)\tag{4.50}</script><p>这是输⼊变量 $x_i$ 的线性函数。</p><h2 id="4，指数族分布"><a href="#4，指数族分布" class="headerlink" title="4，指数族分布"></a>4，指数族分布</h2><p>使⽤指数族分布的形式，可以看到 $\boldsymbol{x}$ 的分布可以写成</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\boldsymbol{\lambda}_k)=h(\boldsymbol{x})g(\boldsymbol{\lambda}_k)\exp\{\boldsymbol{\lambda}_{k}^{T}\boldsymbol{\mu}(\boldsymbol{x})\}\tag{4.51}</script><p>现在把注意⼒集中在 $\boldsymbol{\mu}(\boldsymbol{x})=\boldsymbol{x}$ 这种分布上，引⼊⼀个缩放参数 $s$，这样就得到了指数族类条件概率分布的⼀个⼦集</p><script type="math/tex; mode=display">p(\boldsymbol{x}|\boldsymbol{\lambda}_k,s)=\frac{1}{s}h(\frac{1}{s}\boldsymbol{x})g(\boldsymbol{\lambda}_k)\exp\left\{\frac{1}{s}\boldsymbol{\lambda}_{k}^{T}\boldsymbol{x}\right\}\tag{4.52}</script><p>注意让每个类别有⾃⼰的参数向量 $\boldsymbol{\lambda}_k$ ，并且假定各个类别有同样的缩放参数 $s$ 。</p><p>对于⼆分类问题，把这个类条件概率密度的表达式代⼊相关公式，计算后可得后验概率是⼀个作⽤在线性函数 $a(\boldsymbol{x})$ 上的<code>logistic sigmoid</code>函数。$a(\boldsymbol{x})$ 的形式为</p><script type="math/tex; mode=display">a(\boldsymbol{x})=\frac{1}{s}(\boldsymbol{\lambda}_1-\boldsymbol{\lambda}_2)^{T}\boldsymbol{x}+\ln g(\boldsymbol{\lambda}_1)-\ln g(\boldsymbol{\lambda}_2)+\ln p(\mathcal{C}_1)-\ln p(\mathcal{C}_2)\tag{4.53}</script><p>类似地，对于 $K$ 类问题，有</p><script type="math/tex; mode=display">a_k(\boldsymbol{x})=\frac{1}{s}(\boldsymbol{\lambda}_k)^{T}\boldsymbol{x}+\ln g(\boldsymbol{\lambda}_k)+\ln p(\mathcal{C}_k)\tag{4.54}</script><p>这又是⼀个 $\boldsymbol{x}$ 的线性函数。</p><p>如图4.15，线性分类模型的⾮线性基函数的作⽤的说明。下图给出了原始的输⼊空间 $(x_1,x_2)$ 以及标记为红⾊和蓝⾊的数据点。这个空间中定义了两个“⾼斯”基函数 $\phi_1(\boldsymbol{x})$ 和 $\phi_2(\boldsymbol{x})$ ，中⼼⽤绿⾊⼗字表⽰，轮廓线⽤绿⾊圆形表⽰。</p><p><img src="/images/prml_20191010105434.png" alt="特征空间"></p><p>如图4.16，对应图4.15中的特征空间 $(\phi_1,\phi_2)$ 以及线性决策边界。</p><p><img src="/images/prml_20191010105443.png" alt="线性决策边界"></p><h1 id="二，贝叶斯logistics回归"><a href="#二，贝叶斯logistics回归" class="headerlink" title="二，贝叶斯logistics回归"></a>二，贝叶斯<code>logistics</code>回归</h1><p>对于<code>logistic</code>回归，精确的贝叶斯推断是⽆法处理的。特别地，计算后验概率分布需要对先验概率分布于似然函数的乘积进⾏归⼀化，⽽似然函数本⾝由⼀系列<code>logistic sigmoid</code>函数的乘积组成，每个数据点都有⼀个<code>logistic sigmoid</code>函数，对于预测分布的计算类似地也是⽆法处理的。</p><h2 id="1，-拉普拉斯近似"><a href="#1，-拉普拉斯近似" class="headerlink" title="1， 拉普拉斯近似"></a>1， 拉普拉斯近似</h2><p>由于寻找后验概率分布的⼀个⾼斯表⽰，因此在开始的时候选择⾼斯先验是很⾃然的。故把⾼斯先验写成⼀般的形式</p><script type="math/tex; mode=display">p(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_0,\boldsymbol{S}_0)\tag{4.98}</script><p>其中 $\boldsymbol{m}_0$ 和  $\boldsymbol{S}_0$ 是固定的超参数。 $\boldsymbol{w}$  的后验概率分布为</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\mathbf{t})\propto p(\boldsymbol{w})p(\mathbf{t}|\boldsymbol{w})</script><p>其中 $\mathbf{t} = (t_1,\dots, t_N)^{T}$ 。两侧取对数，然后代⼊先验分布，对于似然函数，有</p><script type="math/tex; mode=display">\begin{aligned}\ln p(\boldsymbol{w}|\mathbf{t}) &= -\frac{1}{2}(\boldsymbol{w}-\boldsymbol{m}_0)^{T}\boldsymbol{S}_{0}^{-1}(\boldsymbol{w}-\boldsymbol{m}_0)\\&+\sum_{n=1}^{N}\{t_n\ln y_{n}+(1-t_n)\ln(1-y_n)\}+常数\end{aligned}\tag{4.99}</script><p>其中 $y_n=\sigma(\boldsymbol{w}^{T}\boldsymbol{\phi}_n)$ 。为了获得后验概率的⾼斯近似，⾸先最⼤化后验概率分布，得到<code>MAP</code>（最⼤后验）解 $\boldsymbol{w}_{WAP}$ ，定义了⾼斯分布的均值，这样协⽅差就是负对数似然函数的⼆阶导数矩阵的逆矩阵，形式为</p><script type="math/tex; mode=display">\boldsymbol{S}_{N}^{-1}=-\nabla\nabla \ln p(\boldsymbol{w}|\mathbf{t})=\boldsymbol{S}_{0}^{-1}+\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_{n}^{T}\tag{4.100}</script><p>后验概率分布的⾼斯近似的形式为</p><script type="math/tex; mode=display">q(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{w}_{MAP},\boldsymbol{S}_{N})\tag{4.101}</script><h2 id="2，-预测分布"><a href="#2，-预测分布" class="headerlink" title="2， 预测分布"></a>2， 预测分布</h2><p>给定⼀个新的特征向量 $\boldsymbol{\phi}(\boldsymbol{x})$ ，类别 $\mathcal{C}_1$ 的预测分布可以通过对后验概率 $p(\boldsymbol{w} | \mathbf{t})$ 积分，后验概率本⾝由⾼斯分布 $q(\boldsymbol{w})$ 近似，即</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{\phi},\mathbf{t})=\int p(\mathcal{C}_1|\boldsymbol{\phi},\boldsymbol{w})p(\boldsymbol{w}|\mathbf{t})\mathrm{d}\boldsymbol{w}\simeq \int \sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})q(\boldsymbol{w})\mathrm{d}\boldsymbol{w}\tag{4.102}</script><p>且类别 $\mathcal{C}_2$ 的对应的概率为 $p(\mathcal{C}_2|\boldsymbol{\phi},\mathbf{t})=1-p(\mathcal{C}_1|\boldsymbol{\phi},\mathbf{t})$ 。为了计算预测分布，⾸先注意到函数 $\sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})$ 对于 $\boldsymbol{w}$ 的依赖只通过它在 $\boldsymbol{\phi}$ 上的投影⽽实现。记 $a=\boldsymbol{w}^{T}\boldsymbol{\phi}$，有</p><script type="math/tex; mode=display">\sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})=\int \delta(a-\boldsymbol{w}^{T}\boldsymbol{\phi})\sigma(a)\mathrm{d}a</script><p>其中 $\delta(·)$ 是 <strong>狄拉克<code>Delta</code>函数</strong> 。由此有</p><script type="math/tex; mode=display">\int \sigma(\boldsymbol{w}^{T}\boldsymbol{\phi})q(\boldsymbol{w})\mathrm{d}\boldsymbol{w}=\int\sigma(a)p(a)\mathrm{d}a\tag{4.103}</script><p>其中，</p><script type="math/tex; mode=display">p(a)=\int \delta(a-\boldsymbol{w}^{T}\boldsymbol{\phi})q(\boldsymbol{w})\mathrm{d}\boldsymbol{w}</script><p>计算 $p(a)$ ：注意到<code>Delta</code>函数给 $\boldsymbol{w}$ 施加了⼀个线性限制，因此在所有与 $\boldsymbol{\phi}$ 正交的⽅向上积分，就得到了联合概率分布 $q(\boldsymbol{w})$ 的边缘分布。<br>通过计算各阶矩然后交换 $a$ 和 $\boldsymbol{w}$ 的积分顺序的⽅式计算均值和协⽅差，即</p><script type="math/tex; mode=display">\mu_a=\mathbb{E}[a]=\int p(a)a\mathrm{d}a=\int q(\boldsymbol{w})\boldsymbol{w}^{T}\boldsymbol{\phi}\mathrm{d}\boldsymbol{w}=\boldsymbol{w}_{WAP}^{T}\boldsymbol{\phi}\tag{4.104}</script><script type="math/tex; mode=display">\begin{aligned}\sigma_{a}^{2}&=\text{var}[a]=\int p(a)\{a^2-\mathbb{E}[a]^{2}\}\mathrm{d}a\\&=\int q(\boldsymbol{w})\{(\boldsymbol{w}^{T}\boldsymbol{\phi})^{2}-(\boldsymbol{m}_{N}^{T}\boldsymbol{\phi})^{2}\}\mathrm{d}\boldsymbol{w}=\boldsymbol{\phi}^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}\end{aligned}\tag{4.105}</script><p>注意，$a$ 的分布的函数形式与线性回归模型的预测分布相同， 其中噪声⽅差被设置为零。因此对于预测分布的近似变成了</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\mathbf{t})=\int \sigma(a)p(a)\mathrm{d}a=\int \sigma(a)\mathcal{N}(a|\mu_{a},\sigma_{a}^{2})\mathrm{d}a\tag{4.106}</script><p>使⽤ <strong>逆<code>probit</code>函数</strong> 的⼀个<strong>优势</strong>是：它与⾼斯的卷积可以⽤另⼀个逆<code>probit</code>函数解析地表⽰出来。 特别地，可以证明</p><script type="math/tex; mode=display">\int \Phi(\lambda a)\mathcal{N}(a|\mu,\sigma^{2})\mathrm{d}a=\Phi\left(\frac{\mu}{(\lambda^{-2}+\sigma^{2})^{\frac{1}{2}}}\right)\tag{4.107}</script><p>现在将逆<code>probit</code>函数的近似 $\sigma(a)\simeq\Phi(\lambda a)$ 应⽤于这个⽅程的两侧， 得到下⾯的对于<code>logistic sigmoid</code>函数与⾼斯的卷积近似</p><script type="math/tex; mode=display">\int \sigma(a)\mathcal{N}(a|\mu,\sigma^{2})\mathrm{d}a\simeq\sigma\left(\kappa(\sigma^{2})\mu\right)\tag{4.108}</script><p>其中，</p><script type="math/tex; mode=display">\kappa(\sigma^{2})=\left(1+\frac{\pi\sigma^{2}}{8}\right)^{-\frac{1}{2}}</script><p>得到了近似的预测分布，形式为</p><script type="math/tex; mode=display">p(\mathcal{C}_1|\boldsymbol{\phi},\mathbf{t})=\sigma\left(\kappa(\sigma_{a}^{2})\mu_{a}\right)\tag{4.109}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，概率生成式模型&quot;&gt;&lt;a href=&quot;#一，概率生成式模型&quot; class=&quot;headerlink&quot; title=&quot;一，概率生成式模型&quot;&gt;&lt;/a&gt;一，概
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】判别函数</title>
    <link href="https://zhangbc.github.io/2019/10/09/prml_04_01/"/>
    <id>https://zhangbc.github.io/2019/10/09/prml_04_01/</id>
    <published>2019-10-09T14:36:47.000Z</published>
    <updated>2019-10-09T15:08:23.208Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，分类线性模型概述"><a href="#一，分类线性模型概述" class="headerlink" title="一，分类线性模型概述"></a>一，分类线性模型概述</h1><p>分类的⽬标是将输⼊变量 $\boldsymbol{x}$ 分到 $K$ 个离散的类别 $\mathcal{C}_k$ 中的某⼀类。 最常见的情况是， 类别互相不相交， 因此每个输⼊被分到唯⼀的⼀个类别中。因此输⼊空间被划分为不同的<strong>决策区域</strong>（<code>decision region</code>），它的边界被称为<strong>决策边界</strong>（<code>decision boundary</code>）或者<strong>决策⾯</strong>（<code>decision surface</code>）。</p><p><strong>分类线性模型</strong>是指决策⾯是输⼊向量 $\boldsymbol{x}$ 的线性函数，因此被定义为 $D$ 维输⼊空间中的 $(D − 1)$ 维超平⾯。如果数据集可以被线性决策⾯精确地分类，那么我们说这个数据集是<strong>线性可分</strong>的（<code>linearly separable</code>）。</p><p>在线性回归模型中，使⽤⾮线性函数 $f(·)$ 对 $\boldsymbol{w}$ 的线性函数进⾏变换，即</p><script type="math/tex; mode=display">y(\boldsymbol{x})=f(\boldsymbol{w}^{T}\boldsymbol{x}+w_0)\tag{4.1}</script><p>在机器学习的⽂献中，$f(·)$ 被称为<strong>激活函数</strong>（<code>activation function</code>），⽽它的反函数在统计学的⽂献中被称为<strong>链接函数</strong>（<code>link function</code>）。决策⾯对应于 $y(\boldsymbol{x}) = 常数$，即 $\boldsymbol{w}^{T}\boldsymbol{x} + w_0 = 常数$，因此决策⾯是 $\boldsymbol{x}$ 的线性函数，即使函数 $f(·)$ 是⾮线性函数也是如此。因此，由公式(4.1)描述的⼀类模型被称为<strong>推⼴的线性模型</strong>（<code>generalized linear model</code>）（<code>McCullagh and Nelder</code>, 1989）。</p><p>如图4.1，⼆维线性判别函数的⼏何表⽰。决策⾯（红⾊）垂直于 $\boldsymbol{w}$ ，它距离原点的偏移量由偏置参数 $w_0$ 控制。</p><p><img src="/images/prml_20191008233456.png" alt="⼆维线性判别函数的⼏何表⽰"></p><h1 id="二，判别函数"><a href="#二，判别函数" class="headerlink" title="二，判别函数"></a>二，判别函数</h1><p>判别函数是⼀个以向量 $\boldsymbol{x}$ 为输⼊，把它分配到 $K$ 个类别中的某⼀个类别（记作 $\mathcal{C}_k$ ）的函数。</p><h2 id="1，⼆分类"><a href="#1，⼆分类" class="headerlink" title="1，⼆分类"></a>1，⼆分类</h2><p>线性判别函数的最简单的形式是输⼊向量的线性函数，即</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+w_0\tag{4.2}</script><p>其中 $\boldsymbol{w}$ 被称为<strong>权向量</strong>（<code>weight vector</code>），$w_0$ 被称为<strong>偏置</strong>（<code>bias</code>）。偏置的相反数有时被称为<strong>阈值</strong>（<code>threshold</code>）。</p><p>考虑两个点 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ ，两个点都位于决策⾯上。 由于 $y(\boldsymbol{x}_A)=y(\boldsymbol{x}_B)=0$，我们有 $\boldsymbol{w}^{T}(\boldsymbol{x}_A-\boldsymbol{x}_B) = 0$，因此向量 $\boldsymbol{w}$ 与决策⾯内的任何向量都正交，从⽽ $\boldsymbol{w}$ 确定了决策⾯的⽅向。类似地，如果 $\boldsymbol{x}$ 是决策⾯内的⼀个点，那么 $y(\boldsymbol{x}) = 0$ ，因此从原点到决策⾯的垂直距离为</p><script type="math/tex; mode=display">\frac{\boldsymbol{w}^{T}\boldsymbol{x}}{\|\boldsymbol{w}\|}=-\frac{w_0}{\|\boldsymbol{x}\|}\tag{4.3}</script><p>其中，偏置参数 $\boldsymbol{w}_0$ 确定了决策⾯的位置。</p><p>记任意⼀点 $\boldsymbol{x}$ 到决策⾯的垂直距离 $r$ ，在决策⾯上的投影 $\boldsymbol{x}_{\perp}$ ，则有</p><script type="math/tex; mode=display">\boldsymbol{x}=\boldsymbol{x}_{\perp}+r \frac{\boldsymbol{w}}{\|\boldsymbol{w}\|}\tag{4.4}</script><p>利用已知公式和 $y(\boldsymbol{x}_{\perp})=0$ 可得</p><script type="math/tex; mode=display">r=\frac{y(\boldsymbol{x})}{\|w\|}\tag{4.5}</script><p>为方便简洁，引⼊“虚”输⼊ $x_0=1$ ，并且定义 $\tilde{\boldsymbol{w}} = (w_0,\boldsymbol{w})$ 以及 $\tilde{\boldsymbol{x}} = (x_0,\boldsymbol{x})$ ，从⽽</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\tilde{\boldsymbol{w}}^{T}\tilde{\boldsymbol{x}}\tag{4.6}</script><p>在这种情况下， 决策⾯是⼀个 $D$ 维超平⾯， 并且这个超平⾯会穿过 $D+1$ 维扩展输⼊空间的原点。</p><h2 id="2，多分类"><a href="#2，多分类" class="headerlink" title="2，多分类"></a>2，多分类</h2><p>考虑把线性判别函数推⼴到 $K&gt;2$ 个类别。</p><p>方法一，使⽤ $K − 1$ 个分类器，每个分类器⽤来解决⼀个⼆分类问题，把属于类别 $\mathcal{C}_k$ 和不属于那个类别的点分开。这被称为“<strong>1对其他</strong>”（<code>one-versus-the-rest</code>）<strong>分类器</strong>。此方法的缺点在于产⽣了输⼊空间中⽆法分类的区域。</p><p>方法二，引⼊ $\frac{K(K−1)}{2}$ 个⼆元判别函数， 对每⼀对类别都设置⼀个判别函数。 这被称为“<strong>1对1</strong>”（<code>one-versus-one</code>）<strong>分类器</strong>。每个点的类别根据这些判别函数中的⼤多数输出类别确定，但是，这也会造成输⼊空间中的⽆法分类的区域。</p><p>如图4.2，尝试从⼀组两类的判别准则中构建出⼀个 $K$ 类的判别准则会导致具有奇异性的区域， ⽤绿⾊表⽰。</p><p><img src="/images/prml_20191009093808.png" alt="判别准则"></p><p>方法三，通过引⼊⼀个 $K$ 类判别函数，可以避免上述问题。这个 $K$ 类判别函数由 $K$ 个线性函数组成，形式为</p><script type="math/tex; mode=display">y_{k}(\boldsymbol{x})=\boldsymbol{w}_{k}^{T}\boldsymbol{x}+w_{k0}\tag{4.7}</script><p>对于点 $\boldsymbol{x}$ ，如果对于所有的 $j \ne k$ 都 有 $y_{k}(\boldsymbol{x})\gt y_{j}(\boldsymbol{x})$ ，那么就把它分到 $\mathcal{C}_k$ 。 于是类别 $\mathcal{C}_k$ 和 $\mathcal{C}_j$ 之间的决策⾯为 $y_{k}(\boldsymbol{x})=y_{j}(\boldsymbol{x})$，并且对应于⼀个 $(D − 1)$ 维超平⾯，形式为</p><script type="math/tex; mode=display">(\boldsymbol{w}_{k}-\boldsymbol{w}_{j})^{T}\boldsymbol{x}+(w_{k0}-w_{j0})=0\tag{4.8}</script><p>考虑两个点 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ ，两个点都位于决策区域 $\mathcal{R}_k$ 中， 任何位于连接 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ 的线段上的点都可以表⽰成下⾯的形式</p><script type="math/tex; mode=display">\hat{\boldsymbol{x}}=\lambda \boldsymbol{x}_{A}+(1-\lambda)\boldsymbol{x}_{B}\tag{4.9}</script><p>其中，$0\le\lambda\le1$ 。根据判别函数的线性性质，有</p><script type="math/tex; mode=display">y_{k}(\hat{\boldsymbol{x}})=\lambda y_{k}(\boldsymbol{x}_{A})+(1-\lambda)y_{k}(\boldsymbol{x}_{B})\tag{4.10}</script><p>由于 $\boldsymbol{x}_A$ 和 $\boldsymbol{x}_B$ 位于 $\mathcal{R}_k$ 内部，因此对于所有 $j \ne k$ ， 都有 $y_{k}(\boldsymbol{x}_{A})\gt y_{j}(\boldsymbol{x}_{A})$ 以及 $y_{k}(\boldsymbol{x}_{B})\gt y_{j}(\boldsymbol{x}_{B})$ ，因此 $y_{k}(\hat{\boldsymbol{x}})\gt y_{j}(\hat{\boldsymbol{x}})$ ，从⽽ $\hat{\boldsymbol{x}}$ 也位于 $\mathcal{R}_k$ 内部，即 $\mathcal{R}_k$ 是单连通的并且是凸的。</p><p>如图4.3，多类判别函数的决策区域的说明， 决策边界⽤红⾊表⽰。</p><p><img src="/images/prml_20191009101103.png" alt="多类判别函数的决策区域"></p><h2 id="3，⽤于分类的最⼩平⽅⽅法"><a href="#3，⽤于分类的最⼩平⽅⽅法" class="headerlink" title="3，⽤于分类的最⼩平⽅⽅法"></a>3，⽤于分类的最⼩平⽅⽅法</h2><p>每个类别 $\mathcal{C}_k$ 由⾃⼰的线性模型描述，即公式(4.7)，其中 $k = 1, \dots , K$ 。使⽤向量记号表⽰，即</p><script type="math/tex; mode=display">\boldsymbol{y}(\boldsymbol{x})=\tilde{\boldsymbol{W}}^{T}\tilde{\boldsymbol{x}}\tag{4.11}</script><p>其中 $\tilde{\boldsymbol{W}}$ 是⼀个矩阵，第 $k$ 列由 $D + 1$ 维向量 $\tilde{\boldsymbol{w}}_k=(w_{k0},w_{k}^{T})^{T}$ 组成，$\tilde{\boldsymbol{x}}$ 是对应的增⼴输⼊向量 $(1, \boldsymbol{x}^{T})^{T}$， 它带有⼀个虚输⼊ $x_0 = 1$ 。</p><p>现在通过最⼩化平⽅和误差函数来确定参数矩阵 $\tilde{\boldsymbol{W}}$  ，考虑⼀个训练数据集 $\{\boldsymbol{x}_n, \boldsymbol{t}_n\}$，其中 $n = 1,\dots , N $，然后定义⼀个矩阵 $\boldsymbol{T}$ ，它的第 $n$ ⾏是向量 $\boldsymbol{t}_{n}^{T}$ ，定义⼀个矩阵 $\tilde{\boldsymbol{X}}$ ，它的第 $n$ ⾏是 $\tilde{\boldsymbol{x}}_{n}^{T}$ 。这样，平⽅和误差函数可以写成</p><script type="math/tex; mode=display">E_{D}(\tilde{\boldsymbol{W}})=\frac{1}{2}\text{Tr}\{(\tilde{\boldsymbol{X}}\tilde{\boldsymbol{W}}-\boldsymbol{T})^{T}(\tilde{\boldsymbol{X}}\tilde{\boldsymbol{W}}-\boldsymbol{T})\}\tag{4.12}</script><p>令关于 $\tilde{\boldsymbol{W}}$ 的导数等于零，整理，可以得到 $\tilde{\boldsymbol{W}}$ 的解，形式为</p><script type="math/tex; mode=display">\tilde{\boldsymbol{W}}=(\tilde{\boldsymbol{X}}^{T}\tilde{\boldsymbol{W}})^{-1}\tilde{\boldsymbol{X}}^{T}\boldsymbol{T}=\tilde{\boldsymbol{X}}^{\dagger}\boldsymbol{T}\tag{4.13}</script><p>其中 $\tilde{\boldsymbol{X}}^{\dagger}$ 是矩阵 $\tilde{\boldsymbol{X}}$ 的伪逆矩阵。即得判别函数，形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=\tilde{\boldsymbol{W}}^{T}\tilde{\boldsymbol{x}}=\boldsymbol{T}^{T}(\tilde{\boldsymbol{X}}^{\dagger})^{T}\tilde{\boldsymbol{x}}\tag{4.14}</script><p>如图4.4，左图给出了来⾃两个类别的数据，⽤红⾊叉形和蓝⾊圆圈表⽰。同时给出的还有通过最⼩平⽅⽅法找到的决策边界（洋红⾊曲线）以及<code>logistic</code>回归模型给出的决策边界（绿⾊曲线）；右图给出了当额外的数据点被添加到左图的底部之后得到的结果，这表明最⼩平⽅⽅法对于异常点很敏感，这与<code>logistic</code>回归不同。</p><p><img src="/images/prml_20191009104504.png" alt="⽤于分类的最⼩平⽅⽅法"></p><p>多⽬标变量的最⼩平⽅解的⼀个重要的<strong>性质</strong>是：如果训练集⾥的每个⽬标向量都满⾜某个线性限制</p><script type="math/tex; mode=display">\boldsymbol{a}^{T}\boldsymbol{t}_{n}+b=0\tag{4.15}</script><p>其中 $\boldsymbol{a}$ 和 $b$ 为常量，那么对于任何 $\boldsymbol{x}$ 值，模型的预测也满⾜同样的限制，即</p><script type="math/tex; mode=display">\boldsymbol{a}^{T}\boldsymbol{y}(\boldsymbol{x})+b=0\tag{4.16}</script><p>因此如果使⽤ $K$ 分类的“<strong>1-of-K</strong> ”表达⽅式，那么这个模型做出的预测会具有下⾯的<strong>性质</strong>：对于任意的 $\boldsymbol{x}$ 的值， $\boldsymbol{y}(\boldsymbol{x})$ 的元素的和等于1。</p><p>举例，由三个类别组成的⼈⼯数据集，训练数据点分别⽤红⾊（×）、绿⾊（+）、蓝⾊（◦）标出。 直线表⽰决策边界， 背景颜⾊表⽰决策区域代表的类别。<br>如图4.5，使⽤最⼩平⽅判别函数，分配到绿⾊类别的输⼊空间的区域过⼩，⼤部分来⾃这个类别的点都被错误分类。</p><p><img src="/images/prml_20191009110353.png" alt="最⼩平⽅判别函数训练"></p><p>如图4.6，使⽤<code>logistic</code>回归的结果，给出了训练数据的正确分类情况。</p><p><img src="/images/prml_20191009110405.png" alt="logistic回归训练"></p><h2 id="4，Fisher线性判别函数"><a href="#4，Fisher线性判别函数" class="headerlink" title="4，Fisher线性判别函数"></a>4，<code>Fisher</code>线性判别函数</h2><p>假设有⼀个 $D$ 维输⼊向量 $\boldsymbol{x}$ ，然后使⽤下式投影到⼀维</p><script type="math/tex; mode=display">y=\boldsymbol{w}^{T}\boldsymbol{x}\tag{4.17}</script><p>如果在 $y$ 上设置⼀个阈值，然后把 $y\ge -w_0$ 的样本分为 $\mathcal{C}_1$ 类，把其余的样本分为 $\mathcal{C}_2$ 类，那么就得到了一个标准的线性分类器。</p><p>考虑⼀个⼆分类问题，这个问题中有 $\mathcal{C}_1$ 类的 $N_1$ 个点以及 $\mathcal{C}_2$ 类的 $N_2$ 个点。因此两类的均值向量为</p><script type="math/tex; mode=display">\boldsymbol{m}_{1}=\frac{1}{N_1}\sum_{n\in\mathcal{C_1}}\boldsymbol{x}_{n}\\\boldsymbol{m}_{2}=\frac{1}{N_2}\sum_{n\in\mathcal{C_2}}\boldsymbol{x}_{n}</script><p>如果投影到 $\boldsymbol{w}$ 上，那么最简单的度量类别之间分开程度的⽅式就是类别均值投影之后的距离。这说明可以选择 $\boldsymbol{w}$ 使得下式取得最⼤值</p><script type="math/tex; mode=display">m_2-m_1=\boldsymbol{w}^{T}(\boldsymbol{m}_2-\boldsymbol{m}_1)\tag{4.18}</script><p>其中，</p><script type="math/tex; mode=display">m_k=\boldsymbol{w}^{T}\boldsymbol{m}_{k}</script><p>是来⾃类别 $\mathcal{C}_k$ 的投影数据的均值。</p><p>如图4.7，左图给出了来⾃两个类别（表⽰为红⾊和蓝⾊）的样本，以及在连接两个类别的均值的直线上的投影的直⽅图。注意，在投影空间中，存在⼀个⽐较严重的类别重叠。右图给出的基于<code>Fisher</code>线性判别准则的对应投影，表明了类别切分的效果得到了极⼤的提升。</p><p><img src="/images/prml_20191009110828.png" alt="Fisher线性判别准则"></p><p><code>Fisher</code>提出的<strong>思想</strong>是最⼤化⼀个函数，这个函数能够让类均值的投影分开得较⼤，同时让每个类别内部的⽅差较⼩，从⽽最⼩化了类别的重叠。</p><p>投影公式(4.17)将 $\boldsymbol{x}$ 的⼀组有标记的数据点变换为⼀位空间 $y$ 的⼀组有标记数据点。来⾃类别 $\mathcal{C}_k$ 的数据经过变换后的类内⽅差为</p><script type="math/tex; mode=display">s_{k}^{2}=\sum_{n\in \mathcal{C}_k}(y_n-m_k)^{2}\tag{4.19}</script><p>其中，$y_n=\boldsymbol{w}^{T}\boldsymbol{x}_{n}$ 。把整个数据集的总的类内⽅差定义为 $s_1^2+s_2^2$ ，<strong><code>Fisher</code>准则</strong> 根据类间⽅差和类内⽅差的⽐值定义，即</p><script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{(m_2-m_1)^{2}}{s_1^2+s_2^2}\tag{4.20}</script><p>不难推导， $J(\boldsymbol{w})$ 对 $\boldsymbol{w}$ 的依赖</p><script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{\boldsymbol{w}^{T}\boldsymbol{S}_B\boldsymbol{w}}{\boldsymbol{w}^{T}\boldsymbol{S}_W\boldsymbol{w}}\tag{4.21}</script><p>其中 $\boldsymbol{S}_B$ 是<strong>类间（<code>between-class</code>）协⽅差矩阵</strong>，形式为</p><script type="math/tex; mode=display">\boldsymbol{S}_B=(\boldsymbol{m}_2-\boldsymbol{m}_1)(\boldsymbol{m}_2-\boldsymbol{m}_1)^{T}</script><p>$\boldsymbol{S}_W$ 被称为<strong>类内（<code>within-class</code>）协⽅差矩阵</strong>，形式为</p><script type="math/tex; mode=display">\boldsymbol{S}_W=\sum_{n\in \mathcal{C}_1}(\boldsymbol{x}_n-\boldsymbol{m}_1)(\boldsymbol{x}_n-\boldsymbol{m}_1)^{T}+\sum_{n\in \mathcal{C}_2}(\boldsymbol{x}_n-\boldsymbol{m}_2)(\boldsymbol{x}_n-\boldsymbol{m}_2)^{T}</script><p>对公式(4.21)关于 $\boldsymbol{w}$ 求导，发现 $J(\boldsymbol{w})$ 取得最⼤值的条件为</p><script type="math/tex; mode=display">(\boldsymbol{w}^{T}\boldsymbol{S}_B\boldsymbol{w})\boldsymbol{S}_W\boldsymbol{w}=(\boldsymbol{w}^{T}\boldsymbol{S}_W\boldsymbol{w})\boldsymbol{S}_B\boldsymbol{w}\tag{4.22}</script><p>可以发现， $\boldsymbol{S}_B\boldsymbol{w}$ 总是在 $(\boldsymbol{m}_2−\boldsymbol{m}_1)$ 的⽅向上。 更重要的是， 若不关⼼ $\boldsymbol{w}$ 的⼤⼩， 只关⼼它的⽅向， 因此可以忽略标量因⼦ $(\boldsymbol{w}^{T}\boldsymbol{S}_B\boldsymbol{w})$ 和 $(\boldsymbol{w}^{T}\boldsymbol{S}_W\boldsymbol{w})$ 。 将公式(4.22)的两侧乘以 $\boldsymbol{S}_{W}^{-1}$ ，即得 <strong><code>Fisher</code>线性判别函数</strong>（<code>Fisher linear discriminant</code>）</p><script type="math/tex; mode=display">\boldsymbol{w}\propto \boldsymbol{S}_{W}^{-1}(\boldsymbol{m}_2-\boldsymbol{m}_1)\tag{4.23}</script><p>如果类内协⽅差矩阵是各向同性的，从⽽ $\boldsymbol{S}_W$ 正⽐于单位矩阵，那么我们看到 $\boldsymbol{w}$ 正⽐于类均值的差。</p><p>构建 <strong><code>Fisher</code>线性判别函数</strong> ，其⽅法为：选择⼀个阈值 $y_0$ ，使得当 $y(\boldsymbol{x})\ge y_0$ 时，把数据点分到 $\mathcal{C}_1$ ，否则把数据点分到 $\mathcal{C}_2$ 。</p><h2 id="5，与最⼩平⽅的关系"><a href="#5，与最⼩平⽅的关系" class="headerlink" title="5，与最⼩平⽅的关系"></a>5，与最⼩平⽅的关系</h2><p><strong>最⼩平⽅⽅法</strong>确定线性判别函数的⽬标是使模型的预测尽可能地与⽬标值接近。相反， <strong><code>Fisher</code>判别准则</strong> 的⽬标是使输出空间的类别有最⼤的区分度。</p><p>对于⼆分类问题，<code>Fisher</code>准则可以看成最⼩平⽅的⼀个特例。作如下假设：让属于 $\mathcal{C}_1$ 的⽬标值等于 $\frac{N}{N_1}$ ，其中 $N_1$ 是类别 $\mathcal{C}_1$ 的模式的数量，$N$ 是总的模式数量。这个⽬标值近似于类别 $\mathcal{C}_1$ 的先验概率的导数。 对于类别 $\mathcal{C}_2$ ， 令⽬标值等于 $−\frac{N}{N_2}$ ， 其中 $N_2$ 是类别 $\mathcal{C}_2$ 的模式的数量。平⽅和误差函数可以写成</p><script type="math/tex; mode=display">E=\frac{1}{2}\sum_{n=1}^{N}(\boldsymbol{w}^{T}\boldsymbol{x}_{n}+w_0-t_n)^{2}\tag{4.24}</script><p>令 $E$ 关于 $w_0$ 和 $\boldsymbol{w}$ 的导数等于零，使⽤对于⽬标值 $t_n$ 的表⽰⽅法，可以得到偏置的表达式</p><script type="math/tex; mode=display">w_0=-\boldsymbol{w}^{T}\boldsymbol{m}\tag{4.25}</script><p>其中，</p><script type="math/tex; mode=display">\sum_{n=1}^{N}t_n=N_1\frac{N}{N_1}-N_2\frac{N}{N_2}=0\\\boldsymbol{m}=\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{x}_n=\frac{1}{N}(N_1\boldsymbol{m}_1+N_2\boldsymbol{m}_2)</script><p>使⽤对于 $t_n$ 的新的表⽰⽅法可得</p><script type="math/tex; mode=display">\left(\boldsymbol{S}_W+\frac{N_1N_2}{N}\boldsymbol{S}_B\right)\boldsymbol{w}=N(\boldsymbol{m_1}-\boldsymbol{m}_2)\tag{4.26}</script><p>由此可见，可以推导出公式(4.23)，即权向量恰好与根据<code>Fisher</code>判别准则得到的结果相同。</p><h2 id="6，多分类的Fisher判别函数"><a href="#6，多分类的Fisher判别函数" class="headerlink" title="6，多分类的Fisher判别函数"></a>6，多分类的<code>Fisher</code>判别函数</h2><p>现在考虑<code>Fisher</code>判别函数对于 $K&gt;2$ 个类别的推⼴。 假设输⼊空间的维度 $D$ ⼤于类别数量 $K$ ，引⼊ $D^{\prime} &gt; 1$ 个线性“特征” $y_k = \boldsymbol{w}_k^{T}\boldsymbol{x}$ ，其中 $k=1,\dots,D^{\prime}$ 。 为了⽅便， 这些特征值可以聚集起来组成向量 $\boldsymbol{y}$ ，类似地，权向量 $\{\boldsymbol{w}_k\}$ 可以被看成矩阵 $\boldsymbol{W}$ 的列。因此</p><script type="math/tex; mode=display">\boldsymbol{y}=\boldsymbol{W}^{T}\boldsymbol{x}\tag{4.27}</script><p>类内协⽅差矩阵推⼴到 $K$ 类，有</p><script type="math/tex; mode=display">\boldsymbol{S}_{W}=\sum_{k=1}^{K}\boldsymbol{S}_{k}\tag{4.28}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}_{k}=\sum_{n\in \mathcal{C}_k}(\boldsymbol{x}_n-\boldsymbol{m}_k)(\boldsymbol{x}_n-\boldsymbol{m}_k)^{T}\\\boldsymbol{m}_{k}=\frac{1}{N_k}\sum_{n\in\mathcal{C_k}}\boldsymbol{x}_{n}</script><p>其中 $N_k$ 是类别 $\mathcal{C}_k$ 中模式的数量。</p><p>为了找到类间协⽅差矩阵的推⼴，使⽤<code>Duda and Hart</code>（1973）的⽅法，⾸先考虑整体的协⽅差矩阵</p><script type="math/tex; mode=display">\boldsymbol{S}_{T}=\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{m})(\boldsymbol{x}_n-\boldsymbol{m})^{T}\tag{4.29}</script><p>其中 $\boldsymbol{m}$ 是全体数据的均值</p><script type="math/tex; mode=display">\boldsymbol{m}=\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{x}_{n}=\frac{1}{N}\sum_{k=1}^{K}N_k\boldsymbol{m}_{k}</script><p>其中 $N = \sum_{k} N_k$ 是数据点的总数。</p><p>整体的协⽅差矩阵可以分解为公式(4.28)给出的类内协⽅差矩阵，加上另⼀个矩阵 $\boldsymbol{S}_B$ ，它可以看做类间协⽅差矩阵。</p><script type="math/tex; mode=display">\boldsymbol{S}_{T}=\boldsymbol{S}_{W}+\boldsymbol{S}_{B}\tag{4.30}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}_B=\sum_{k=1}^{K}N_k(\boldsymbol{m}_k-\boldsymbol{m})(\boldsymbol{m}_k-\boldsymbol{m})^{T}</script><p>协⽅差矩阵被定义在原始的 $\boldsymbol{x}$ 空间中。现在在投影的 $D^{\prime}$ 维 $\boldsymbol{y}$ 空间中定义类似的矩阵</p><script type="math/tex; mode=display">\boldsymbol{S}_{W}=\sum_{k=1}^{K}\sum_{n\in \mathcal{C}_k}(\boldsymbol{y}_n-\boldsymbol{\mu}_k)(\boldsymbol{y}_n-\boldsymbol{\mu}_k)^{T}\\\boldsymbol{S}_B=\sum_{k=1}^{K}N_k(\boldsymbol{\mu}_k-\boldsymbol{\mu})(\boldsymbol{\mu}_k-\boldsymbol{\mu})^{T}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{\mu}_k=\frac{1}{N_k}\sum_{n\in \mathcal{C}_k}\boldsymbol{y}_n\\\boldsymbol{\mu}=\frac{1}{N}\sum_{k=1}^{K}N_k\boldsymbol{\mu}_k</script><p>我们想构造⼀个标量，当类间协⽅差较⼤且类内协⽅差较⼩时，这个标量会较⼤。有许多可能的准则选择⽅式（<code>Fukunaga</code>, 1990）。其中⼀种选择是</p><script type="math/tex; mode=display">J(\boldsymbol{W})=\text{Tr}\{\boldsymbol{s}_{W}^{-1}\boldsymbol{s}_{B}\}\tag{4.31}</script><p>这个判别准则可以显式地写成投影矩阵 $\boldsymbol{W}$ 的函数，形式为</p><script type="math/tex; mode=display">J(\boldsymbol{W})=\text{Tr}\{(\boldsymbol{W}^{T}\boldsymbol{S}_{W}\boldsymbol{W})^{-1}(\boldsymbol{W}^{T}\boldsymbol{S}_{B}\boldsymbol{W})\}\tag{4.32}</script><h2 id="7，感知器算法"><a href="#7，感知器算法" class="headerlink" title="7，感知器算法"></a>7，感知器算法</h2><p>线性判别模型的另⼀个例⼦是<code>Rosenblatt</code>（1962）提出的<strong>感知器算法</strong>。对应于⼀个⼆分类的模型，输⼊向量 $\boldsymbol{x}$ ⾸先使⽤⼀个固定的⾮线性变换得到⼀个特征向量 $\boldsymbol{\phi}(\boldsymbol{x})$ ， 这个特征向量然后被⽤于构造⼀个⼀般的线性模型，形式为</p><script type="math/tex; mode=display">y(\boldsymbol{x})=f(\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}))\tag{4.33}</script><p>其中⾮线性激活函数 $f(·)$ 是⼀个阶梯函数，形式为</p><script type="math/tex; mode=display">f(a)=\begin{cases}+1,&a\ge 0\\ -1,&a<0\end{cases}</script><p>向量 $\boldsymbol{\phi}(\boldsymbol{x})$ 通常包含⼀个偏置分量 $\phi_{0}(\boldsymbol{x})=0$ 。对于感知器，使⽤ $t=+1$ 表⽰ $\mathcal{C}_1$ ，使⽤ $t=−1$ 表⽰ $\mathcal{C}_2$ ，这与激活函数的选择相匹配。<br>为了推导误差函数，即<strong>感知器准则</strong>（<code>perceptron criterion</code>）， 注意到我们正在做的是寻找⼀个权向量 $\boldsymbol{w}$ 使得对于类别 $\mathcal{C}_1$ 中的模式 $\boldsymbol{x}_n$ 都有 $\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)&gt;0$ ， ⽽对于类别 $\mathcal{C}_2$ 中的模式 $\boldsymbol{x}_n$ 都有 $\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)<0$ 。 使⽤ $t \in\{−1, +1\}$ 这种⽬标变量的表⽰⽅法，要做的就是使得所有的模式都满⾜ $\boldsymbol{w}^{t}\boldsymbol{\phi}(\boldsymbol{x}_n)t_{n}>0$ 。 对于正确分类的模式，感知器准则赋予零误差，⽽对于误分类的模式 $\boldsymbol{x}_n$ ，它试着最⼩化 $-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_n)t_{n}$ 。因此，<strong>感知器准则</strong>为</0$></p><script type="math/tex; mode=display">E_{P}(\boldsymbol{w})=-\sum_{n\in\mathcal{M}}\boldsymbol{w}^{T}\boldsymbol{\phi}_nt_{n}\tag{4.34}</script><p>其中 $\boldsymbol{\phi}_n=\boldsymbol{\phi}(\boldsymbol{x}_n)$ 和 $\mathcal{M}$ 表⽰所有误分类模式的集合。某个特定的误分类模式对于误差函数的贡献是 $\boldsymbol{w}$ 空间中模式被误分类的区域中 $\boldsymbol{w}$ 的线性函数， ⽽在正确分类的区域，误差函数等于零。 总的误差函数因此是分段线性的。</p><p>现在对这个误差函数使⽤随机梯度下降算法。这样，权向量 $\boldsymbol{w}$ 的变化为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w}^{(\tau+1)}&=\boldsymbol{w}^{(\tau)}-\eta\nabla E_{P}(\boldsymbol{w})\\&=\boldsymbol{w}^{(\tau)}+\eta\boldsymbol{\phi}_{n}t_n\end{aligned}\tag{4.35}</script><p>其中 $\eta$ 是学习率参数，$\tau$ 是⼀个整数，是算法运⾏次数的索引。</p><p>感知器学习算法可以简单地表⽰如下：我们反复对于训练模式进⾏循环处理，对于每个模式 $\boldsymbol{x}_n$ 计算感知器函数(4.33)。如果模式正确分类，那么权向量保持不变，⽽如果模式被错误分类，那么对于类别 $\mathcal{C}_1$ ， 我们把向量 $\boldsymbol{\phi}(\boldsymbol{x}_n)$ 加到当前对于权向量 $\boldsymbol{w}$ 的估计值上，⽽对于类别 $\mathcal{C}_2$ ，我们从 $\boldsymbol{w}$ 中减掉向量 $\boldsymbol{\phi}(\boldsymbol{x}_n)$。</p><p>如图4.8～4.11，感知器算法收敛性的说明， 给出了⼆维特征空间 $(\phi_1,\phi_2)$ 中的来⾃两个类别的数据点（红⾊和蓝 ⾊）。图4.8给出了初始参数向量 $\boldsymbol{w}$ ，表⽰为⿊⾊箭头，以及对应的决策边界（⿊⾊直线），其中箭头指向被分类为红⾊类别的决策区域。⽤绿⾊圆圈标出的数据点被误分类，因此它的特征向量被加到当前的权向量中，给出了新的决策边界，如图4.9所⽰。 图4.10给出了下⼀个误分类的点，⽤绿⾊圆圈标出，它的特征向量再次被加到权向量上，给出了图4.11的决策边界。这个边界中所有的数据点都被正确分类。</p><p><img src="/images/prml_20191009110918.png" alt="初始参数向量"></p><p><img src="/images/prml_20191009110936.png" alt="新的决策边界"></p><p><img src="/images/prml_20191009110946.png" alt="下⼀个误分类的点"></p><p><img src="/images/prml_20191009111003.png" alt="正确分类"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，分类线性模型概述&quot;&gt;&lt;a href=&quot;#一，分类线性模型概述&quot; class=&quot;headerlink&quot; title=&quot;一，分类线性模型概述&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
  <entry>
    <title>【机器学习基础】贝叶斯线性模型</title>
    <link href="https://zhangbc.github.io/2019/10/07/prml_03_02/"/>
    <id>https://zhangbc.github.io/2019/10/07/prml_03_02/</id>
    <published>2019-10-07T08:42:29.000Z</published>
    <updated>2019-10-10T01:12:49.406Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列为《模式识别与机器学习》的读书笔记。</p></blockquote><h1 id="一，贝叶斯线性回归"><a href="#一，贝叶斯线性回归" class="headerlink" title="一，贝叶斯线性回归"></a>一，贝叶斯线性回归</h1><h2 id="1，参数分布"><a href="#1，参数分布" class="headerlink" title="1，参数分布"></a>1，参数分布</h2><p>关于线性拟合的贝叶斯⽅法的讨论，⾸先引⼊模型参数 $\boldsymbol{w}$ 的先验概率分布。现在这个阶段，把噪声精度参数 $\beta$ 当做已知常数。⾸先，由公式(3.8)定义的似然函数 $p(t|\boldsymbol{w})$ 是 $\boldsymbol{w}$ 的⼆次函数的指数形式，于是对应的共轭先验是⾼斯分布，形式为：</p><script type="math/tex; mode=display">p(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_{0},\boldsymbol{S}_{0})\tag{3.30}</script><p>均值为 $\boldsymbol{m}_{0}$ ，协⽅差为 $\boldsymbol{S}_{0}$ 。</p><p>由于共轭⾼斯先验分布的选择，后验分布也将是⾼斯分布。 我们可以对指数项进⾏配平⽅， 然后使⽤归⼀化的⾼斯分布的标准结果找到归⼀化系数，这样就计算出了后验分布的形式：</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\boldsymbol{t})=\mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_{N},\boldsymbol{S}_{N})\tag{3.31}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{m}_{N}=\boldsymbol{S}_{N}(\boldsymbol{S}_{0}^{-1}\boldsymbol{m}_{0}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{t}) \\\boldsymbol{S}_{N}^{-1}=\boldsymbol{S}_{0}^{-1}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{\Phi}</script><p>为了简化起见，考虑⾼斯先验的⼀个特定的形式，即考虑零均值各向同性⾼斯分布，这个分布由⼀个精度参数 $\alpha$ 控制，即：</p><script type="math/tex; mode=display">p(\boldsymbol{w}|\alpha)=\mathcal{N}(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\boldsymbol{I})\tag{3.32}</script><p>对应的 $\boldsymbol{w}$ 后验概率分布由公式(3.31)给出，其中，</p><script type="math/tex; mode=display">\boldsymbol{m}_{N}=\beta \boldsymbol{S}_{N}\boldsymbol{\Phi}^{T}\boldsymbol{t}\\\boldsymbol{S}_{N}^{-1}=\alpha \boldsymbol{I}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{\Phi}</script><p>后验概率分布的对数由对数似然函数与先验的对数求和的⽅式得到。它是 $\boldsymbol{w}$ 的函数，形式为：</p><script type="math/tex; mode=display">\ln p(\boldsymbol{w}|\boldsymbol{t})=-\frac{\beta}{2}\sum_{n=1}^{N}\{t_n-\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}-\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}+常数\tag{3.33}</script><h2 id="2，预测分布"><a href="#2，预测分布" class="headerlink" title="2，预测分布"></a>2，预测分布</h2><p>在实际应⽤中，我们通常感兴趣的不是 $\boldsymbol{w}$ 本⾝的值，⽽是对于新的 $\boldsymbol{x}$ 值预测出 $t$ 的值。这需要我们计算出<strong>预测分布</strong>（<code>predictive distribution</code>），定义为：</p><script type="math/tex; mode=display">p(t|\mathbf{t},\alpha,\beta)=\int p(t|\boldsymbol{w},\beta)p(\boldsymbol{w}|\mathbf{t},\alpha,\beta)\mathrm{d}\boldsymbol{w}\tag{3.34}</script><p>其中 $\mathbf{t}$ 是训练数据⽬标变量的值组成的向量。经综合分析，预测分布的形式可以进一步具体化为：</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathbf{t},\alpha,\beta)=\mathcal{N}(t|\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x}),\sigma_{N}^{2}(\boldsymbol{x}))\tag{3.35}</script><p>其中，</p><script type="math/tex; mode=display">\sigma_{N}^{2}(\boldsymbol{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x})</script><p>其中，式中第⼀项表⽰数据中的噪声，第⼆项反映了与参数 $\boldsymbol{w}$ 关联的不确定性。当额外的数据点被观测到的时候，后验概率分布会变窄。从⽽可以证明出 $\sigma_{N+1}^{2}(\boldsymbol{x})\le \sigma_{N}^{2}(\boldsymbol{x})$（<code>Qazaz et al.</code>, 1997）。 在极限 $N \to \infty$ 的情况下， 式中第⼆项趋于零， 从⽽预测分布的⽅差只与参数 $\beta$ 控制的具有可加性的噪声有关。</p><p>在下图3.15～3.18中，我们调整⼀个由⾼斯基函数线性组合的模型，使其适应于不同规模的数据集，然后观察对应的后验概率分布。其中，绿⾊曲线对应着产⽣数据点的函数 $\sin(2\pi x)$（带有附加的⾼斯噪声），⼤⼩为 $N = 1, N = 2, N = 4$ 和 $N = 25$ 的数据集在四幅图中⽤蓝⾊圆圈表⽰。对于每幅图，红⾊曲线是对应的⾼斯预测分布的均值，红⾊阴影区域是均值两侧的⼀个标准差范围的区域。注意，预测的不确定性依赖于 $x$，并且在数据点的邻域内最⼩。</p><p><img src="/images/prml_20191005233709.png" alt="N=1"></p><p><img src="/images/prml_20191005233718.png" alt="N=2"></p><p><img src="/images/prml_20191005233730.png" alt="N=4"></p><p><img src="/images/prml_20191005233748.png" alt="N=25"></p><p>为了更加深刻地认识对于不同的 $x$ 值的预测之间的协⽅差，我们可以从 $\boldsymbol{w}$ 的后验概率分布中抽取样本，然后画出对应的函数 $y(x, \boldsymbol{w})$ ，如图3.19～3.22所⽰。</p><p><img src="/images/prml_20191005233947.png" alt="N=1"></p><p><img src="/images/prml_20191005233957.png" alt="N=2"></p><p><img src="/images/prml_20191005234011.png" alt="N=4"></p><p><img src="/images/prml_20191005234022.png" alt="N=25"></p><h2 id="3，等价核"><a href="#3，等价核" class="headerlink" title="3，等价核"></a>3，等价核</h2><p>考虑以下<strong>预测均值</strong>形式：</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{m}_{N})=\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x})=\beta \boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\Phi}^{T}\mathbf{t}=\sum_{n=1}^{N}\beta \boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x}_{n})t_{n}\tag{3.36}</script><p>其中，</p><script type="math/tex; mode=display">\boldsymbol{S}_{N}^{-1}=\boldsymbol{S}_{0}^{-1}+\beta \boldsymbol{\Phi}^{T}\boldsymbol{\Phi}</script><p>因此在点 $\boldsymbol{x}$ 处的预测均值由训练集⽬标变量 $t_n$ 的线性组合给出，即：</p><script type="math/tex; mode=display">y(\boldsymbol{x},\boldsymbol{m}_{N})=\sum_{n=1}^{N}k(\boldsymbol{x},\boldsymbol{x}_{n})t_{n}\tag{3.37}</script><p>其中，    </p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{x}^{\prime})=\beta \boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})\tag{3.38}</script><p>被称为<strong>平滑矩阵</strong>（<code>smoother matrix</code>）或者<strong>等价核</strong>（<code>equivalent kernel</code>）。像这样的回归函数，通过对训练集⾥⽬标值进⾏线性组合做预测，被称为<strong>线性平滑</strong>（<code>linear smoother</code>）。</p><p>如图3.23，⾼斯基函数的等价核 $k(x, x^{\prime})$ ， 图中给出了 $x$ 关于 $x^{\prime}$ 的图像， 以及通过这个矩阵的三个切⽚， 对应于三个不同的 $x$ 值，⽤来⽣成这个核的数据集由 $x$ 的200个值组成，$x$ 均匀地分布在区 间 $(−1, 1)$ 中。</p><p><img src="/images/prml_20191006132738.png" alt="⾼斯基函数的等价核"></p><p>如图3.24，多项式基函数在 $x=0$ 的等价核 $k(x, x^{\prime})$ 。 </p><p><img src="/images/prml_20191006132829.png" alt="多项式基函数的等价核"></p><p>如图3.25，<code>Sigmoid</code>基函数在 $x=0$ 的等价核 $k(x, x^{\prime})$ 。 </p><p><img src="/images/prml_20191006132838.png" alt="Sigmoid基函数的等价核"></p><p>考虑 $y(\boldsymbol{x})$ 和 $y(\boldsymbol{x}^{\prime})$ 的协⽅差</p><script type="math/tex; mode=display">\begin{aligned}\text{cov}[y(\boldsymbol{x}),\boldsymbol{x}^{\prime}]&=\text{cov}[\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{w},\boldsymbol{w}^{T}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})]\\&=\boldsymbol{\phi}(\boldsymbol{x})^{T}\boldsymbol{S}_{N}\boldsymbol{\phi}(\boldsymbol{x}^{\prime})\\&=\beta^{-1}k(\boldsymbol{x},\boldsymbol{x}^{\prime})\end{aligned}\tag{3.39}</script><p>⽤核函数表⽰线性回归给出了解决回归问题的另⼀种⽅法。通过不引⼊⼀组基函数（它隐式地定义了⼀个等价的核），⽽是直接定义⼀个局部的核函数，然后在给定观测数据集的条件下， 使⽤这个核函数对新的输⼊变量  $\boldsymbol{x}$ 做预测。 这就引出了⽤于回归问题（以及分类问题）的⼀个很实⽤的框架，被称为<strong>⾼斯过程</strong>（<code>Gaussian process</code>）。</p><p>⼀个等价核定义了模型的权值，通过这个权值，训练数据集⾥的⽬标值被组合，然后对新的 $\boldsymbol{x}$ 值做预测，可以证明这些权值的和等于1，即</p><script type="math/tex; mode=display">\sum_{n=1}^{N}k(\boldsymbol{x},\boldsymbol{x}_{n})=1\tag{3.40}</script><p>对于所有的 $\boldsymbol{x}$ 值都成⽴。</p><p>公式(3.38)给出的等价核满⾜⼀般的核函数共有的⼀个<strong>重要性质</strong>：可以表⽰为⾮线性函数的向量 $\boldsymbol{\psi}(\boldsymbol{x})$ 的内积的形式，即</p><script type="math/tex; mode=display">k(\boldsymbol{x},\boldsymbol{z})=\boldsymbol{\psi}(\boldsymbol{x})^{T}\boldsymbol{\psi}(\boldsymbol{z})\tag{3.41}</script><p>其中，$\boldsymbol{\psi}(\boldsymbol{x})=\beta^{\frac{1}{2}}\boldsymbol{S}_{N}^{\frac{1}{2}}\boldsymbol{\phi}(\boldsymbol{x})$ 。</p><h1 id="二，贝叶斯模型比较"><a href="#二，贝叶斯模型比较" class="headerlink" title="二，贝叶斯模型比较"></a>二，贝叶斯模型比较</h1><p>假设我们想⽐较 $L$ 个模型 ${\mathcal{M}_i}$ ，其中 $i=1, \dots,L$ ，这⾥，⼀个模型指的是观测数据 $\mathcal{D}$ 上的概率分布。在多项式曲线拟合的问题中，概率分布被定义在⽬标值 $\mathbf{t}$ 上， ⽽输⼊值 $\mathbf{X}$ 被假定为已知的。其他类型的模型定义了 $\mathbf{X}$ 和 $\mathbf{t}$ 上的联合分布。我们会假设数据是由这些模型中的⼀个⽣成的， 但是我们不知道究竟是哪⼀个，其不确定性通过先验概率分布 $p(\mathcal{M}_i)$ 表⽰。给定⼀个训练数据集 $\mathcal{D}$ ，估计后验分布</p><script type="math/tex; mode=display">p(\mathcal{M}_{i}|\mathcal{D})\propto p(\mathcal{M}_{i})p(\mathcal{D}|\mathcal{M}_{i})</script><p>其中，<strong>模型证据</strong>（<code>model evidence</code>） $p(\mathcal{D}|\mathcal{M}_{i})$ ，也叫<strong>边缘似然</strong>（<code>marginal likelihood</code>），它表达了数据展现出的不同模型的优先级，也可以被看做在模型空间中的似然函数，在这个空间中参数已经被求和或者积分。两个模型的模型证据的⽐值 $\frac{p(\mathcal{D}|\mathcal{M}_{i})}{p(\mathcal{D}|\mathcal{M}_{j})}$ 被称为<strong>贝叶斯因⼦</strong>（<code>Bayes factor</code>）（<code>Kass and Raftery</code>, 1995）。</p><p>⼀旦知道了模型上的后验概率分布，那么根据概率的加和规则与乘积规则，预测分布为</p><script type="math/tex; mode=display">p(t|\boldsymbol{x},\mathcal{D})=\sum_{i=1}^{L}p(t|\boldsymbol{x},\mathcal{M}_{i},\mathcal{D})p(\mathcal{M}_{i}|\mathcal{D})\tag{3.42}</script><p>对于模型求平均的⼀个简单的近似是使⽤最可能的⼀个模型⾃⼰做预测，这被称为<strong>模型选择</strong> （<code>model selection</code>）。</p><p>对于⼀个由参数 $\boldsymbol{w}$ 控制的模型，根据概率的加和规则和乘积规则，模型证据为</p><script type="math/tex; mode=display">p(\mathcal{D}|\mathcal{M}_{i})=\int p(\mathcal{D}|\boldsymbol{w},\mathcal{M}_{i})p(\boldsymbol{w}|\mathcal{M}_{i})\mathrm{d}\boldsymbol {w}\tag{3.43}</script><p>⾸先考虑模型有⼀个参数 $w$ 的情形。这个参数的后验概率正⽐于 $p(\mathcal{D}|w)p(w)$ ，其中为了简化记号，我们省略 了它对于模型  ${\mathcal{M}_i}$ 的依赖。 假设后验分布在最⼤似然值 $w_{MAP}$ 附近是⼀个尖峰，宽度为 $\Delta w_{后验}$ ，那么可以⽤被积函数的值乘以尖峰的宽度来近似这个积分。进⼀步假设先验分布是平的，宽度为 $\Delta w_{先验}$ ，即 $p(w)=\frac{1}{\Delta w_{先验}}$ ，那么有</p><script type="math/tex; mode=display">\begin{aligned}p(\mathcal{D})&=\int p(\mathcal{D}|w)p(w)\mathrm{d}w \\ &\simeq p(\mathcal{D}|w_{MAP})\frac{\Delta w_{后验}}{\Delta w_{先验}}\end{aligned}\tag{3.44}</script><p>取对数，可得</p><script type="math/tex; mode=display">\ln p(\mathcal{D})\simeq \ln p(\mathcal{D}|w_{MAP}) + \ln\left(\frac{\Delta w_{后验}}{\Delta w_{先验}}\right)\tag{3.45}</script><p>其中，式中第⼀项表⽰拟合由最可能参数给出的数据，对于平的先验分布来说，这对应于对数似然；第⼆项⽤于根据模型的复杂度来惩罚模型。</p><p>如图3.26，近似模型证据，如果我们假设参数上的后验概率分布在众数 $w_{MAP}$ 附近有⼀个尖峰。</p><p><img src="/images/prml_20191006201043.png" alt="近似模型证据"></p><p>对于⼀个有 $M$ 个参数的模型，可以对每个参数进⾏类似的近似。假设所有的参数 $\frac{\Delta w_{后验}}{\Delta w_{先验}}$ 都相同，则有</p><script type="math/tex; mode=display">\ln p(\mathcal{D})\simeq \ln p(\mathcal{D}|w_{MAP}) + M\ln\left(\frac{\Delta w_{后验}}{\Delta w_{先验}}\right)\tag{3.46}</script><p>如图3.27，对于三个具有不同复杂度的模型，数据集的概率分布的图形表⽰，其中 $\mathcal{M}_{1}$ 是最简单的，$\mathcal{M}_{3}$ 是 最复杂的。</p><p><img src="/images/prml_20191006201647.png" alt="M个参数的模型"></p><p><strong>贝叶斯模型⽐较框架</strong>中隐含的⼀个假设是，⽣成数据的真实的概率分布包含在考虑的模型集合当中。如果这个假设确实成⽴，那么可以证明，平均来看，贝叶斯模型⽐较会倾向于选择出正确的模型。</p><p>考虑两个模型 $\mathcal{M}_{1}$ 和 $\mathcal{M}_{2}$ ，其中真实的概率分布对应于模型 $\mathcal{M}_{1}$ 。对于给定的有限数据集，确实有可能出现错误的模型反⽽使贝叶斯因⼦较⼤的事情。 但是，如果把贝叶斯因⼦在数据集分布上进⾏平均，那么可以得到期望贝叶斯因⼦，即关于数据的真实分布求的平均值：</p><script type="math/tex; mode=display">\int p(\mathcal{D}|\mathcal{M}_{1})\ln \frac{p(\mathcal{D}|\mathcal{M}_{1})}{p(\mathcal{D}|\mathcal{M}_{2})}\mathrm{d}\mathcal{D}</script><h1 id="三，证据近似"><a href="#三，证据近似" class="headerlink" title="三，证据近似"></a>三，证据近似</h1><p>⾸先对参数 $\boldsymbol{w}$ 求积分， 得到边缘似然函数（<code>marginal likelihood function</code>），然后通过最⼤化边缘似然函数，确定超参数的值。 这个框架在统计学的⽂献中被称为<strong>经验贝叶斯</strong>（<code>empirical Bayes</code>）（<code>Bernardo and Smith</code>, 1994; <code>Gelman et al.</code>, 2004），或者被称为<strong>第⼆类最⼤似然</strong>（<code>type 2 maximum likelihood</code>）（<code>Berger</code>, 1985），或者被称为<strong>推⼴的最⼤似然</strong>（<code>generalized maximum likelihood</code>）。在机器学习的⽂献中， 这种⽅法也被称为<strong>证据近似</strong>（<code>evidence approximation</code>）（<code>Gull</code>, 1989; <code>MacKay</code>, 1992<code>a</code>）。</p><p>引⼊ $\alpha$ 和 $\beta$ 上的超先验分布，那么预测分布可以通过对 $\boldsymbol{w}$ ,  $\alpha$ 和 $\beta$ 求积分的⽅法得到， 即</p><script type="math/tex; mode=display">p(t|\mathbf{t})=\iiint p(t|\boldsymbol{w},\beta)p(\boldsymbol{w}|\mathbf{t},\alpha,\beta)p(\alpha,\beta|\mathbf{t})\mathrm{d}\boldsymbol{w} \mathrm{d}\alpha \mathrm{d}\beta\tag{3.47}</script><p>如果后验分布 $p(\alpha,\beta|\mathbf{t})$ 在 $\hat{\alpha}$ 和 $\hat{\beta}$ 附近有尖峰，那么预测分布可以通过对 $\boldsymbol{w}$ 积分的⽅式简单地得到，其中 $\alpha$ 和 $\beta$ 被固定为 $\hat{\alpha}$ 和 $\hat{\beta}$  ，即</p><script type="math/tex; mode=display">\begin{aligned}p(t|\mathbf{t}) &\simeq p(t|\mathbf{t},\hat{\alpha},\hat{\beta})\\&=\int p(t|\boldsymbol{w},\hat{\beta})p(\boldsymbol{w}|\mathbf{t},\hat{\alpha},\hat{\beta})\mathrm{d}\boldsymbol{w}\end{aligned}\tag{3.48}</script><h2 id="1，计算证据函数"><a href="#1，计算证据函数" class="headerlink" title="1，计算证据函数"></a>1，计算证据函数</h2><p>边缘似然函数 $p(\mathbf{t}|\alpha,\beta)$ 是通过对权值参数 $\boldsymbol{w}$ 进⾏积分得到的，即</p><script type="math/tex; mode=display">p(\mathbf{t}|\alpha,\beta)=\int p(\mathbf{t}|\boldsymbol{w},\beta)p(\boldsymbol{w}|\alpha)\mathrm{d}\boldsymbol{w}\tag{3.49}</script><p>根据以前的相关公式，也可以写成</p><script type="math/tex; mode=display">p(\mathbf{t}|\alpha,\beta)=\left(\frac{\beta}{2\pi}\right)^{\frac{N}{2}}\left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2}}\int \exp\{-E(\boldsymbol{w})\}\mathrm{d}\boldsymbol{w}\tag{3.50}</script><p>其中 $M$ 是 $\boldsymbol{w}$ 的维数，并且，</p><script type="math/tex; mode=display">\begin{aligned}E(\boldsymbol{w})&=\beta E_{D}(\boldsymbol{w})+\alpha E_{W}(\boldsymbol{w})\\&=\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\boldsymbol{w}||^{2}+\frac{\alpha}{2}\boldsymbol{w}^{T}\boldsymbol{w}\end{aligned}\tag{3.51}</script><p>现在对 $\boldsymbol{w}$ 配平⽅，可得</p><script type="math/tex; mode=display">E(\boldsymbol{w})=E(\boldsymbol{m}_{N})+\frac{1}{2}(\boldsymbol{w}-\boldsymbol{m}_{N})^{T}\boldsymbol{A}(\boldsymbol{w}-\boldsymbol{m}_{N})\tag{3.52}</script><p>令</p><script type="math/tex; mode=display">\boldsymbol{A}=\alpha\boldsymbol{I}+\beta\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\\\boldsymbol{m}_{N}=\beta \boldsymbol{A}^{-1}\boldsymbol{\Phi}\mathbf{t}\\E(\boldsymbol{m}_{N})=\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\boldsymbol{m}_{N}||^{2}+\frac{\alpha}{2}\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}</script><p><strong>注意</strong> $\boldsymbol{A}$ 对应于误差函数的⼆阶导数 $\boldsymbol{A} = \nabla\nabla E(\boldsymbol{w})$ 被称为 <strong><code>Hessian</code>矩阵</strong>。</p><p>经计算，可得边缘似然函数的对数，即证据函数的表达式：</p><script type="math/tex; mode=display">\ln p(\mathbf{t}|\alpha,\beta)=\frac{M}{2}\ln\alpha+\frac{N}{2}\ln\beta-E(\boldsymbol{m}_{N})-\frac{1}{2}\ln |\boldsymbol{A}|-\frac{N}{2}\ln(2\pi)\tag{3.53}</script><p>如图3.28，模型证据与多项式阶数之间的关系。</p><p><img src="/images/prml_20191007134944.png" alt="模型证据与多项式阶数之间的关系"></p><h2 id="2，最⼤化证据函数"><a href="#2，最⼤化证据函数" class="headerlink" title="2，最⼤化证据函数"></a>2，最⼤化证据函数</h2><p>⾸先考虑 $p(\mathbf{t}|\alpha,\beta)$ 关于 $\alpha$ 的最⼤化，定义下⾯的特征向量⽅程</p><script type="math/tex; mode=display">(\beta\boldsymbol{\Phi}^{T}\boldsymbol{\Phi})\boldsymbol{\mu}_{i}=\lambda_{i}\boldsymbol{\mu}_{i}\tag{3.54}</script><p>根据公式，可知 $\boldsymbol{A}$ 的特征值为 $\alpha + \lambda_{i}$ 。 现在考虑公式(3.53)中涉及到 $\ln|\boldsymbol{A}|$ 的项关于 $\alpha$ 的导数</p><script type="math/tex; mode=display">\frac{\mathrm{d}}{\mathrm{d} \alpha} \ln |\boldsymbol{A}|=\frac{\mathrm{d}}{\mathrm{d} \alpha} \ln \prod_{i}\left(\lambda_{i}+\alpha\right)=\frac{\mathrm{d}}{\mathrm{d} \alpha} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)=\sum_{i} \frac{1}{\lambda_{i}+\alpha}\tag{3.55}</script><p>因此函数公式(3.53)关于 $\alpha$ 的驻点满⾜</p><script type="math/tex; mode=display">0=\frac{M}{2\alpha}-\frac{1}{2}\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}-\frac{1}{2}\sum_{i}\frac{1}{\lambda_{i}+\alpha}</script><p>整理，有</p><script type="math/tex; mode=display">\alpha\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}=M-\alpha\sum_{i}\frac{1}{\lambda_{i}+\alpha}=\gamma</script><p>由于 $i$ 的求和式中⼀共有 $M$ 项，因此 $\gamma$ 可以写成</p><script type="math/tex; mode=display">\gamma=\sum_{i}\frac{\lambda_{i}}{\alpha+\lambda_{i}}\tag{3.56}</script><p>因而，最⼤化边缘似然函数的 $\alpha$ 满⾜</p><script type="math/tex; mode=display">\alpha=\frac{\gamma}{\boldsymbol{m}_{N}^{T}\boldsymbol{m}_{N}}\tag{3.57}</script><p><strong>注意</strong>： $\alpha$ 的值是纯粹通过观察训练集确定的。</p><p>类似地，关于 $\beta$ 最⼤化对数边缘似然函数，注意到公式(3.54)定义的特征值 $\lambda_{i}$ 正⽐于 $\beta$ ，因此  $\frac{\mathrm{d}}{\mathrm{d}\beta}=\frac{\lambda}{\beta_{i}}$ 。于是</p><script type="math/tex; mode=display">\frac{\mathrm{d}}{\mathrm{d} \beta} \ln |\boldsymbol{A}|=\frac{\mathrm{d}}{\mathrm{d} \beta} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)=\frac{1}{\beta}\sum_{i} \frac{\lambda_{i}}{\lambda_{i}+\alpha}=\frac{\gamma}{\beta}\tag{3.58}</script><p>边缘似然函数的驻点因此满⾜</p><script type="math/tex; mode=display">0=\frac{N}{2\beta}-\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}-\frac{\gamma}{2\beta}</script><p>整理，可以得到最⼤化边缘似然函数的 $\beta$ 满⾜</p><script type="math/tex; mode=display">\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^{N}\{t_{n}-\boldsymbol{m}_{N}^{T}\boldsymbol{\phi}(\boldsymbol{x}_{n})\}^{2}\tag{3.59}</script><h2 id="3，参数的有效数量"><a href="#3，参数的有效数量" class="headerlink" title="3，参数的有效数量"></a>3，参数的有效数量</h2><p>如图3.29，似然函数的轮廓线（红⾊）和先验概率分布（绿⾊），其中参数空间中的坐标轴被旋转，与<code>Hessian</code>矩阵的特征向量 $\boldsymbol{\mu}_i$ 对齐。</p><p><img src="/images/prml_20191007143628.png" alt="似然函数的轮廓线"></p><p>考察单⼀变量 $x$ 的⾼斯分布的⽅差的最⼤似然估计为</p><script type="math/tex; mode=display">\sigma_{ML}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}\tag{3.60}</script><p>这个估计是有偏的，因为均值的最⼤似然解 $\mu_{ML}$ 拟合了数据中的⼀些噪声。从效果上来看，这占⽤了模型的⼀个⾃由度。对应的⽆偏的估计形式为</p><script type="math/tex; mode=display">\sigma_{MAP}^{2}=\frac{1}{N-1}\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}\tag{3.61}</script><p>分母中的因⼦ $N−1$ 反映了模型中的⼀个⾃由度被⽤于拟合均值的事实，它抵消了最⼤似然解的偏差。</p><p>如图3.30，$\gamma$ 与 $\ln\alpha$ 的关系（红⾊曲线）以及 $2\alpha E_{W}(\boldsymbol{m}_{N})$ 与 $\ln\alpha$ 的关系（蓝⾊曲线）， 数据集为正弦数据集。这两条曲线的交点定义了 $\alpha$ 的最优解，由模型证据的步骤给出。</p><p><img src="/images/prml_20191007145425.png" alt="证据框架来确定α"></p><p>如图3.31，对应的对数证据 $\ln p(\mathbf{t}|\alpha,\beta)$ 关于 $\ln\alpha$ 的图像（红⾊曲线），说明了峰值与图3.30中曲线的交点恰好重合。同样给出的时测试集误差（蓝⾊曲线），说明模型证据最⼤值的位置接近于具有最好泛化能⼒的点。</p><p><img src="/images/prml_20191007145440.png" alt="对数证据"></p><p>如图3.32，独⽴的参数关于有效参数数量 $\gamma$ 的函数图像。⾼斯基函数模型中的10个参数 $w_i$ 与参数有效数量 $\gamma$ 的关系，其中超参数的变化范围为 $0\le\alpha\le\infty$，使得 $\gamma$ 的变化范围为 $0\le\gamma\le M$ 。</p><p><img src="/images/prml_20191007155836.png" alt="独⽴的参数关于有效参数数量"></p><p>如果考虑极限情况 $N\gg M$ ， 数据点的数量⼤于参数的数量，那么根据公式，所有的参数都可以根据数据良好确定。因为 $\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}$ 涉及到数据点的隐式求和，因此特征值 $\lambda_{i}$ 随着数据集规模的增加⽽增⼤。在这种情况下，$\gamma = M$ ，并且 $\alpha$ 和 $\beta$ 的重新估计⽅程变为</p><script type="math/tex; mode=display">\alpha=\frac{M}{2E_{W}(\boldsymbol{m}_{N})}\\\beta=\frac{N}{2E_{D}(\boldsymbol{m}_{N})}</script><h1 id="四，固定基函数的局限性"><a href="#四，固定基函数的局限性" class="headerlink" title="四，固定基函数的局限性"></a>四，固定基函数的局限性</h1><p>基函数的数量随着输⼊空间的维度 $D$ 迅速增长，通常是指数⽅式的增长。</p><p>真实数据集有两个<strong>性质</strong>：第⼀， 数据向量 $\{\boldsymbol{x}_n\}$ 通常位于⼀个⾮线性流形内部。由于输⼊变量之间的相关性，这个流形本⾝的维度⼩于输⼊空间的维度。如果我们使⽤局部基函数，那么可以让基函数只分布在输⼊空间中包含数据的区域。这种⽅法被⽤在径向基函数⽹络中，也被⽤在⽀持向量机和相关向量机当中。神经⽹络模型使⽤可调节的基函数，这些基函数有着<code>sigmoid</code>⾮线性的性质。神经⽹络可以通过调节参数，使得在输⼊空间的区域中基函数会按照数据流形发⽣变化。第⼆，⽬标变量可能只依赖于数据流形中的少量可能的⽅向。利⽤这个性质，神经⽹络可以通过选择输⼊空间中基函数产⽣响应的⽅向。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本系列为《模式识别与机器学习》的读书笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一，贝叶斯线性回归&quot;&gt;&lt;a href=&quot;#一，贝叶斯线性回归&quot; class=&quot;headerlink&quot; title=&quot;一，贝叶斯线性回归&quot;&gt;&lt;/a&gt;一，贝
      
    
    </summary>
    
      <category term="机器学习" scheme="https://zhangbc.github.io/categories/machine-learning/"/>
    
    
      <category term="机器学习基础" scheme="https://zhangbc.github.io/tags/machine-learning-foundation/"/>
    
  </entry>
  
</feed>
